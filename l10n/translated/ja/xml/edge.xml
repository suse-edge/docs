<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?><?asciidoc-numbered?><book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="ja">
<info>
<title>SUSE Edgeドキュメント</title>
<!-- https://tdg.docbook.org/tdg/5.2/info -->
<date>2024年7月10日</date>


<dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
    <dm:bugtracker>
        <dm:url>https://github.com/suse-edge/suse-edge.github.io/issues/new</dm:url>
    </dm:bugtracker>
</dm:docmanager>
</info>
<preface xml:id="id-suse-edge-documentation">
<title>SUSE Edgeドキュメント</title>
<para>『SUSE
Edgeドキュメント』をお読みいただきありがとうございます。このドキュメントには、クイックスタートガイド、検証済みの設計、コンポーネントの使用に関するガイダンス、サードパーティ統合、エッジコンピューティングインフラストラクチャとワークロードを管理するためのベストプラクティスが記載されています。</para>
<section xml:id="id-what-is-suse-edge">
<title>SUSE Edgeとは</title>
<para>SUSE
Edgeは、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという独自の課題に対処することに特化した、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。SUSE
Edgeが重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル全体の運用にまで対応することです。このプラットフォームは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
</section>
<section xml:id="id-design-philosophy">
<title>設計理念</title>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。こうした課題の多くは、従来の考え方(たとえば、データセンター内やパブリッククラウドへのインフラストラクチャやアプリケーションのデプロイメント)とは異なるため、はるかに細かく設計を検討し、一般的な前提の多くを再検討する必要があります。</para>
<para>たとえば、SUSEはミニマリズム、モジュール性、操作のしやすさに価値を見出しています。システムは複雑化するほど故障しやすくなるため、エッジ環境ではミニマリズムが重要です。数百、数十万カ所に及ぶとなると、複雑なシステムは複雑な故障が発生します。また、SUSEのソリューションはモジュール性を備えているため、ユーザの選択肢を増やしながら、デプロイしたプラットフォームが不必要に複雑になることを解消できます。さらに、ミニマリズムおよびモジュール性と、操作のしやすさとのバランスを取ることも必要です。人間はプロセスを何千回も繰り返すとミスを犯す可能性があるため、プラットフォーム側で潜在的なミスを確実に回復し、技術者が現場に出向かなくても済むようにすると同時に、一貫性と標準化を実現するよう努める必要もあります。</para>
</section>
<section xml:id="id-which-quick-start-should-you-use">
<title>どのクイックスタートを使用すべきか</title>
<para>動作環境とライフサイクル要件はさまざまであるため、SUSEでは、SUSE
Edgeを運用する市場セグメントやユースケースに大まかに一致する別個のデプロイメントパターンを多数サポートしています。また、これらの各デプロイメントパターンに対応するクイックスタートガイドを作成し、ユーザのニーズに基づいてSUSE
Edgeプラットフォームに習熟できるようにしています。以下に、現在サポートされている3つのデプロイメントパターンを、各クイックスタートのページへのリンクと併せて説明します。</para>
<section xml:id="id-direct-network-provisioning">
<title>ダイレクトネットワークプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングでは、デプロイ先のハードウェアの詳細がわかっている場合に、アウトオブバンド管理インタフェースに直接アクセスして、プロビジョニングプロセス全体をオーケストレーションして自動化します。このシナリオで顧客が期待するソリューションとは、エッジサイトを一元的な場所から完全に自動化してプロビジョニングすることができ、ブートイメージの作成をはるかに上回る機能を備えていて、エッジロケーションでの手動操作を最小限に抑えられるソリューションです。つまり、ラックに搭載して電源をオンにし、必要なネットワークを物理ハードウェアに接続するだけで、自動化プロセスによってアウトオブバンド管理(Redfish
APIなど)を介してマシンの電源が投入され、ユーザの介入なしにインフラストラクチャのプロビジョニング、オンボーディング、デプロイメントが処理されるソリューションです。これが機能するための鍵は、管理者がシステムを把握している、つまりどのハードウェアがどこにあるかを管理者が把握していることと、デプロイメントが中央で処理されることが想定されていることです。</para>
<para>このソリューションは最も堅牢です。管理者がハードウェアの管理インタフェースを直接操作して既知のハードウェアを扱うことに加え、ネットワークの利用可否に対する制約が少ないためです。機能面では、このソリューションは、Cluster
APIとMetal<superscript>3</superscript>を広範に使用して、ベアメタルからオペレーティングシステム、Kubernetes、階層化アプリケーションまでを自動プロビジョニングし、デプロイメント後にSUSE
Edgeの他の一般的なライフサイクル管理機能にリンクする機能を提供します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-phone-home-network-provisioning">
<title>「Phone Home」ネットワークプロビジョニング</title>
<para>場合によっては、中央管理クラスタでハードウェアを直接管理できない環境で運用することがあります(たとえば、リモートネットワークがファイアウォールの背後にある場合や、アウトオブバンド管理インタフェースがない場合などがあり、エッジでよく見られる「PC」タイプのハードウェアで一般的です)。このシナリオの場合のために、SUSEでは、ハードウェアのブートストラップ時にその配置先がわかっていなくても、クラスタとそのワークロードをリモートでプロビジョニングできるツールを提供しています。エッジコンピューティングについて考える場合、ほとんどの人はこう考えます。エッジコンピューティングとは、不明な部分がある数千あるいは数万台のシステムがエッジロケーションで起動し、安全にPhone
Home通信を行い、そのシステムの身元を検証し、実行すべき処理についての指示を受信することです。ここで要件として期待されるのは、工場でマシンを事前イメージングしたり、USBなどでブートイメージをアタッチしたりする以外には、ユーザがほとんど介入しなくてもプロビジョニングとライフサイクル管理ができることです。この領域での主な課題は、こうしたデバイスの規模、一貫性、セキュリティ、ライフサイクルに対処することです。</para>
<para>このソリューションでは、非常に柔軟で一貫性のある方法でシステムをプロビジョニングおよびオンボーディングできます。システムの場所、タイプや仕様、初回電源投入日時などは問いません。SUSE
Edgeでは、Edge Image
Builderを使用してシステムを非常に柔軟にカスタマイズできます。また、ノードのオンボーディングとKubernetesのプロビジョニングにはRancherのElementalが提供する登録機能を活用するとともに、オペレーティングシステムへのパッチの適用にはSUSE
Managerを活用します。このソリューションのクイックスタートについては、<xref
linkend="quickstart-elemental"/>を参照してください。</para>
</section>
<section xml:id="id-image-based-provisioning">
<title>イメージベースのプロビジョニング</title>
<para>スタンドアロン環境、エアギャップ環境、またはネットワークが制限された環境で運用する必要があるお客様向けに、SUSE
Edgeでは、必要なデプロイメントアーティファクトがすべて含まれる、完全にカスタマイズされたインストールメディアを生成できるソリューションを提供しています。これにより、シングルノードとマルチノード両方の高可用性Kubernetesクラスタを、必要なワークロードと追加の階層化コンポーネントを含めてエッジに設定できます。これはすべて、外部とのネットワーク接続や集中管理プラットフォームの介入なしに行うことができます。ユーザエクスペリエンスは、インストールメディアをターゲットシステムに提供するという点では「Phone
Home」ソリューションによく似ていますが、このソリューションは「インプレースでブートストラップ」する点が異なります。このシナリオでは、生成されたクラスタをRancherに接続して継続的に管理する(つまり、大幅な再設定や再デプロイメントなしに、「非接続」動作モードから「接続」動作モードに移行する)ことも、分離した状態のまま動作を続行することもできます。どちらの場合も、一貫した同じメカニズムを適用してライフサイクル操作を自動化できることに注意してください。</para>
<para>さらに、このソリューションを使用すると、「ダイレクトネットワークプロビジョニング」モデルと「Phone
Homeネットワークプロビジョニング」モデルの両方をサポートする集中型インフラストラクチャをホストできる管理クラスタを迅速に作成することもできます。この方法では、あらゆるタイプのエッジインフラストラクチャを最も迅速・簡単にプロビジョニングできます。このソリューションでは、SUSE
Edge Image
Builderの機能を多用して、完全にカスタマイズされた無人インストールメディアを作成します。クイックスタートについては、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</section>
</section>
<section xml:id="id-components-used-in-suse-edge">
<title>SUSE Edgeで使用されるコンポーネント</title>
<para>SUSE
Edgeは、SUSEの既存コンポーネント(LinuxチームとRancherチームが構築したコンポーネントを含む)と、SUSEがインフラストラクチャの要件と複雑さの両方に対処できるようにするためにEdgeチームが構築した追加機能とコンポーネントの両方で構成されています。コンポーネントのリスト、各コンポーネントの概要説明へのリンク、およびSUSE
Edgeでの用途については、以下を参照してください。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref linkend="components-rancher"/>)</para>
</listitem>
<listitem>
<para>Rancher Dashboard拡張機能(<xref
linkend="components-rancher-dashboard-extensions"/>)</para>
</listitem>
<listitem>
<para>Fleet (<xref linkend="components-fleet"/>)</para>
</listitem>
<listitem>
<para>SLE Micro (<xref linkend="components-slmicro"/>)</para>
</listitem>
<listitem>
<para>Metal³ (<xref linkend="components-metal3"/>)</para>
</listitem>
<listitem>
<para>Edge Image Builder (<xref linkend="components-eib"/>)</para>
</listitem>
<listitem>
<para>NetworkManager Configurator (<xref linkend="components-nmc"/>)</para>
</listitem>
<listitem>
<para>Elemental (<xref linkend="components-elemental"/>)</para>
</listitem>
<listitem>
<para>Akri (<xref linkend="components-akri"/>)</para>
</listitem>
<listitem>
<para>K3s (<xref linkend="components-k3s"/>)</para>
</listitem>
<listitem>
<para>RKE2 (<xref linkend="components-rke2"/>)</para>
</listitem>
<listitem>
<para>Longhorn (<xref linkend="components-longhorn"/>)</para>
</listitem>
<listitem>
<para>NeuVector (<xref linkend="components-neuvector"/>)</para>
</listitem>
<listitem>
<para>MetalLB (<xref linkend="components-metallb"/>)</para>
</listitem>
<listitem>
<para>KubeVirt (<xref linkend="components-kubevirt"/>)</para>
</listitem>
</itemizedlist>
</section>
</preface>
<part xml:id="id-quick-starts">
<title>クイックスタート</title>
<partintro>
<para>クイックスタートはこちら</para>
</partintro>
<chapter xml:id="quickstart-metal3">
<title>Metal<superscript>3</superscript>を使用したBMCの自動デプロイメント</title>
<para>Metal<superscript>3</superscript>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供する<link
xl:href="https://metal3.io/">CNCFプロジェクト</link>です。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-why-use-this-method">
<title>この方法を使用する理由</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>管理クラスタは宣言型APIを提供するように設定されており、このAPIによってダウンストリームクラスタのベアメタルサーバのインベントリと状態を管理できます。これには、自動検査、クリーニング、プロビジョニング/プロビジョニング解除も含まれます。</para>
</section>
<section xml:id="id-high-level-architecture">
<title>アーキテクチャの概要</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-metal3-architecture.png"
width=""/> </imageobject>
<textobject><phrase>クイックスタートmetal3アーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-prerequisites">
<title>前提条件</title>
<para>ダウンストリームクラスタのサーバハードウェアとネットワーキングに関連する固有の制約がいくつかあります。</para>
<itemizedlist>
<listitem>
<para>管理クラスタ</para>
<itemizedlist>
<listitem>
<para>ターゲットサーバ管理/BMC APIへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>ターゲットサーバのコントロールプレーンネットワークへのネットワーク接続が必要</para>
</listitem>
<listitem>
<para>マルチノード管理クラスタの場合、追加の予約済みIPアドレスが必要</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>制御対象ホスト</para>
<itemizedlist>
<listitem>
<para>Redfish、iDRAC、またはiLOのインタフェースを介したアウトオブバンド管理のサポートが必要</para>
</listitem>
<listitem>
<para>仮想メディアを使用したデプロイメントのサポートが必要(PXEは現在未サポート)</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>プロビジョニングAPIにアクセスするために管理クラスタへのネットワーク接続が必要</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para>ツールがいくつか必要です。ツールは管理クラスタにインストールするか、管理クラスタにアクセス可能なホストにインストールできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://helm.sh">Helm</link>、および<link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">Clusterctl</link></para>
</listitem>
<listitem>
<para><link xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナ ランタイム</para>
</listitem>
</itemizedlist>
<para><literal>SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz</literal>
OSイメージファイルは、<link xl:href="https://scc.suse.com/">SUSE Customer
Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
<section xml:id="id-setup-management-cluster">
<title>管理クラスタのセットアップ</title>
<para>管理クラスタをインストールし、Metal<superscript>3</superscript>を使用する基本的な手順は次のとおりです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>RKE2管理クラスタをインストールします。</para>
</listitem>
<listitem>
<para>Rancherのインストール</para>
</listitem>
<listitem>
<para>ストレージプロバイダをインストールします。</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>の依存関係をインストールします。</para>
</listitem>
<listitem>
<para>CAPIの依存関係をインストールします。</para>
</listitem>
<listitem>
<para>ダウンストリームクラスタホスト用のSLEMicro OSイメージを構築します。</para>
</listitem>
<listitem>
<para>BareMetalHost CRを登録し、ベアメタルのインベントリを定義します。</para>
</listitem>
<listitem>
<para>CAPIリソースを定義して、ダウンストリームクラスタを作成します。</para>
</listitem>
</orderedlist>
<para>このガイドでは、既存のRKE2クラスタとRancher (cert-managerを含む)が、たとえばEdge Image Builder (<xref
linkend="components-eib"/>)を使用してインストールされていることを前提としています。</para>
<tip>
<para>ここでの手順は、ATIP管理クラスタのドキュメント(<xref
linkend="atip-management-cluster"/>)で説明されているように、完全に自動化することもできます。</para>
</tip>
</section>
<section xml:id="id-installing-metal3-dependencies">
<title>Metal<superscript>3</superscript>の依存関係のインストール</title>
<para>cert-managerがまだRancherのインストールの一部としてインストールされていない場合は、cert-managerをインストールして実行する必要があります。</para>
<para>永続ストレージプロバイダをインストールする必要があります。Longhornを推奨しますが、開発/PoC環境ではローカルパスを使用することもできます。以下の手順は、StorageClassが<link
xl:href="https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">デフォルトとしてマーク</link>されていることを前提としています。マークされていない場合は、Metal<superscript>3</superscript>チャートに追加の設定が必要です。</para>
<para>追加のIPが必要です。このIPは<link
xl:href="https://metallb.universe.tf/">MetalLB</link>によって管理され、Metal<superscript>3</superscript>管理サービスに一貫したエンドポイントを提供します。このIPは、コントロールプレーンサブネットに属していて、静的設定用に予約されている必要があります(どのDHCPプールにも属していてはなりません)。</para>
<tip>
<para>管理クラスタがシングルノードである場合、MetalLBを介して管理されるフローティングIPを追加する必要はありません。「シングルノード設定」(<xref
linkend="id-single-node-configuration"/>)を参照してください。</para>
</tip>
<orderedlist numeration="arabic">
<listitem>
<para>まず、MetalLBをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metallb oci://registry.suse.com/edge/metallb-chart \
  --namespace metallb-system \
  --create-namespace</screen>
</listitem>
<listitem>
<para>続いて、次のように、予約済みIPを使用して<literal>IPAddressPool</literal>と<literal>L2Advertisment</literal>を定義し、<literal>STATIC_IRONIC_IP</literal>として定義します。</para>
<screen language="yaml" linenumbering="unnumbered">export STATIC_IRONIC_IP=&lt;STATIC_IRONIC_IP&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ironic-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${STATIC_IRONIC_IP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ironic-ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ironic-ip-pool
EOF</screen>
</listitem>
<listitem>
<para>これでMetal<superscript>3</superscript>をインストールできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install \
  metal3 oci://registry.suse.com/edge/metal3-chart \
  --namespace metal3-system \
  --create-namespace \
  --set global.ironicIP="${STATIC_IRONIC_IP}"</screen>
</listitem>
<listitem>
<para>initContainerがこのデプロイメントで実行されるまでに2分ほどかかる場合があります。そのため、Podがすべて実行されていることを確認してから次に進んでください。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n metal3-system
NAME                                                    READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          15m
metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          15m
metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          15m</screen>
</listitem>
</orderedlist>
<warning>
<para><literal>metal3-system</literal>ネームスペースのすべてのPodが実行されるまで、次の手順に進まないでください。</para>
</warning>
</section>
<section xml:id="id-installing-cluster-api-dependencies">
<title>Cluster APIの依存関係のインストール</title>
<para>まず、Rancherに組み込みのCAPIコントローラを無効にする必要があります。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: management.cattle.io/v3
kind: Feature
metadata:
  name: embedded-cluster-api
spec:
  value: false
EOF

kubectl delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
kubectl wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s</screen>
<para>次に、SUSEイメージを使用するために、設定ファイルが必要です。</para>
<screen language="bash" linenumbering="unnumbered">mkdir ~/.cluster-api
cat &gt;  ~/.cluster-api/clusterctl.yaml &lt;&lt;EOF
images:
  all:
    repository: registry.suse.com/edge
EOF</screen>
<para><link
xl:href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl">clusterctl</link>
1.6.xをインストールし、その後、次のようにコア、インフラストラクチャ、ブートストラップ、およびコントロールプレーンのプロバイダをインストールします。</para>
<screen language="bash" linenumbering="unnumbered">clusterctl init --core "cluster-api:v1.6.2" --infrastructure "metal3:v1.6.0" --bootstrap "rke2:v0.2.6" --control-plane "rke2:v0.2.6"</screen>
<para>しばらくすると、コントローラPodが<literal>capi-system</literal>、<literal>capm3-system</literal>、<literal>rke2-bootstrap-system</literal>、および<literal>rke2-control-plane-system</literal>の各ネームスペースで実行されているはずです。</para>
</section>
<section xml:id="id-prepare-downstream-cluster-image">
<title>ダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>ほとんどの設定はEdge Image
Builderを使用して行うことができますが、このガイドではダウンストリームクラスタをセットアップするために必要な最小限の設定について説明します。</para>
<section xml:id="id-image-configuration">
<title>イメージの設定</title>
<para>Edge Image
Builderを実行する場合、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="metal3-add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>custom/scriptsディレクトリには、初回ブート時に実行されるスクリプトが含まれています。現在は、デプロイメント時にOSルートパーティションのサイズを変更するために、<literal>growfs.sh</literal>スクリプトが必要です。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw
  outputImageName: SLE-Micro-eib-output.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>${USERKEY1}</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはmetal3チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSLEMicroの設定が失敗することにも注意してください。</para>
</note>
</section>
<section xml:id="id-growfs-script">
<title>Growfsスクリプト</title>
<para>現在は、カスタムスクリプト(<literal>custom/scripts/growfs.sh</literal>)があります。これは、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するために必要です。<literal>growfs.sh</literal>スクリプトには次の情報が含まれています。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
<para>この回避策に関連するバグは、<link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1217430">https://bugzilla.suse.com/show_bug.cgi?id=1217430</link>です。</para>
</note>
</section>
</section>
<section xml:id="id-image-creation">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいて、<literal>SLE-Micro-eib-output.raw</literal>という名前の出力イメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、Metal3チャートを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="id-adding-baremetalhost-inventory">
<title>BareMetalHostインベントリの追加</title>
<para>自動デプロイメント用にベアメタルサーバを登録するには、リソースを2つ作成する必要があります。BMCアクセス資格情報を保存するシークレットと、BMC接続とその他の詳細を定義するMetal<superscript>3</superscript>
BareMetalHostリソースです。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: "00:f3:65:8a:a3:b0"
  bmc:
    address: redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab
    disableCertificateVerification: true
    credentialsName: controlplane-0-credentials</screen>
<para>次の点に注意してください。</para>
<itemizedlist>
<listitem>
<para>シークレットのユーザ名/パスワードはbase64でエンコードされている必要があります。また、末尾に改行を含めないでください(たとえば、単なる<literal>echo</literal>ではなく、<literal>echo
‑n</literal>を使用してください)。</para>
</listitem>
<listitem>
<para><literal>cluster-role</literal>ラベルは、この時点で設定することも、後でクラスタの作成時に設定することもできます。以下の例では、<literal>control-plane</literal>または<literal>worker</literal>を想定しています。</para>
</listitem>
<listitem>
<para><literal>bootMACAddress</literal>は、ホストのコントロールプレーンNICに一致する有効なMACである必要があります。</para>
</listitem>
<listitem>
<para><literal>bmc</literal>のアドレスはBMC管理APIへの接続です。次のアドレスがサポートされています。</para>
<itemizedlist>
<listitem>
<para><literal>redfish-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/&lt;SYSTEM ID&gt;</literal>:
Redfish仮想メディア(たとえば、SuperMicro)</para>
</listitem>
<listitem>
<para><literal>idrac-virtualmedia://&lt;IP
ADDRESS&gt;/redfish/v1/Systems/System.Embedded.1</literal>: Dell iDRAC</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>BareMetalHost APIの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">アップストリームのAPIドキュメント</link>を参照してください。</para>
</listitem>
</itemizedlist>
<section xml:id="id-configuring-static-ips">
<title>静的IPの設定</title>
<para>上記のBareMetalHostの例では、DHCPでコントロールプレーンネットワークの設定を提供することを想定していますが、静的IPなどの手動設定が必要なシナリオでは、以下に説明するように追加の設定を指定できます。</para>
<section xml:id="metal3-add-network-eib">
<title>静的ネットワーク設定用の追加スクリプト</title>
<para>Edge Image
Builderでゴールデンイメージを作成する際には、<literal>network</literal>フォルダ内に次の<literal>configure-network.sh</literal>ファイルを作成します。</para>
<para>このファイルにより、初回ブート時に設定ドライブのデータを使用して、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使ってホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
<section xml:id="id-additional-secret-with-host-network-configuration">
<title>ホストネットワーク設定の追加シークレット</title>
<para>NM Configurator (<xref linkend="components-nmc"/>)でサポートされている<link
xl:href="https://nmstate.io/">nmstate</link>形式のデータを含む追加シークレットをホストごとに定義できます。</para>
<para>その後、このシークレットは、<literal>BareMetalHost</literal>リソースで<literal>preprovisioningNetworkDataName</literal>の指定フィールドを使用して参照されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: enp1s0
      type: ethernet
      state: up
      mac-address: "00:f3:65:8a:a3:b0"
      ipv4:
        address:
        - ip:  192.168.125.200
          prefix-length: 24
        enabled: true
        dhcp: false
    dns-resolver:
      config:
        server:
        - 192.168.125.1
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: 192.168.125.1
        next-hop-interface: enp1s0
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: controlplane-0
  labels:
    cluster-role: control-plane
spec:
  preprovisioningNetworkDataName: controlplane-0-networkdata
# Remaining content as in previous example</screen>
<note>
<para>mac-addressは、nmstate APIではオプションですが、NM
Configuratorを介した設定では必須であり、必ず指定する必要があります。</para>
</note>
</section>
</section>
<section xml:id="id-baremetalhost-preparation">
<title>BareMetalHostの準備</title>
<para>上記の説明に従ってBareMetalHostリソースと関連するシークレットを作成すると、次のようにホスト準備ワークフローがトリガされます。</para>
<itemizedlist>
<listitem>
<para>ターゲットホストのBMCに接続された仮想メディアによってramdiskイメージがブートされる</para>
</listitem>
<listitem>
<para>ramdiskがハードウェア詳細を検査し、ホストをプロビジョニング用に準備する(たとえば、ディスクから以前のデータを消去する)</para>
</listitem>
<listitem>
<para>このプロセスが完了すると、BareMetalHostの<literal>status.hardware</literal>フィールドのハードウェア詳細が更新され、検証可能になる</para>
</listitem>
</itemizedlist>
<para>このプロセスには数分かかる場合がありますが、完了すると、BareMetalHostの状態が<literal>available</literal>になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get baremetalhost
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              true             9m44s
worker-0         available              true             9m44s</screen>
</section>
</section>
<section xml:id="id-creating-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>続いて、ダウンストリームクラスタを定義するCluster
APIリソースと、BareMetalHostリソースをプロビジョニングしてからブートストラップを実行してRKE2クラスタを形成するマシンリソースを作成します。</para>
</section>
<section xml:id="id-control-plane-deployment">
<title>コントロールプレーンのデプロイメント</title>
<para>コントロールプレーンをデプロイするために、以下のリソースを含む次のようなyamlマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>クラスタリソースでは、クラスタ名、ネットワーク、およびコントロールプレーン/インフラストラクチャプロバイダのタイプ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>Metal3Clusterでは、コントロールプレーンのエンドポイント(シングルノードの場合はホストIP、マルチノードの場合はLoadBalancerエンドポイント。この例ではシングルノードを想定)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ControlPlaneでは、RKE2のバージョンと、クラスタのブートストラップ時に必要な追加設定を定義します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(networkDataは現在のところEdgeソリューションではサポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<para>シンプルにするために、この例では、BareMetalHostにIP
<literal>192.168.125.200</literal>が設定されたシングルノードのコントロールプレーンを想定しています。より高度なマルチノードの例については、ATIPのドキュメント(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: sample-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: sample-cluster
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: sample-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: 192.168.125.200
    port: 6443
  noCloudProvider: true
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: sample-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: sample-cluster-controlplane
  replicas: 1
  agentConfig:
    format: ignition
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    version: v1.28.9+rke2r1
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-controlplane-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>上記の例をコピーし、自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して適用し、<literal>clusterctl</literal>でクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-control-plane.yaml

# Wait for the cluster to be provisioned - status can be checked via clusterctl
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     22m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     27m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     22m
│ └─Machine/sample-cluster-chflc                        True                     23m</screen>
</section>
<section xml:id="id-workercompute-deployment">
<title>ワーカー/コンピュートのデプロイメント</title>
<para>コントロールプレーンと同様に、次のリソースを含むyamlマニフェストを定義します。</para>
<itemizedlist>
<listitem>
<para>MachineDeploymentでは、レプリカ(ホスト)の数とブートストラップ/インフラストラクチャプロバイダ(この場合はRKE2/Metal3)を定義します。</para>
</listitem>
<listitem>
<para>RKE2ConfigTemplateでは、エージェントホストのブートストラップ用のRKE2のバージョンと初回ブート設定を記述します。</para>
</listitem>
<listitem>
<para>Metal3MachineTemplateではBareMetalHostリソースに適用するOSイメージを定義し、hostSelectorでは使用するBareMetalHostを定義します。</para>
</listitem>
<listitem>
<para>Metal3DataTemplateでは、BareMetalHostに渡す追加のメタデータを定義します(networkDataは現在のところEdgeソリューションではサポートされていないことに注意してください)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: sample-cluster
  name: sample-cluster
  namespace: default
spec:
  clusterName: sample-cluster
  replicas: 1
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: sample-cluster
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: sample-cluster
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
          kind: RKE2ConfigTemplate
          name: sample-cluster-workers
      clusterName: sample-cluster
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: Metal3MachineTemplate
        name: sample-cluster-workers
      nodeDrainTimeout: 0s
      version: v1.28.9+rke2r1
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1
kind: RKE2ConfigTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      agentConfig:
        format: ignition
        version: v1.28.9+rke2r1
        kubelet:
          extraArgs:
            - provider-id=metal3://BAREMETALHOST_UUID
        additionalUserData:
          config: |
            variant: fcos
            version: 1.4.0
            systemd:
              units:
                - name: rke2-preinstall.service
                  enabled: true
                  contents: |
                    [Unit]
                    Description=rke2-preinstall
                    Wants=network-online.target
                    Before=rke2-install.service
                    ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                    [Service]
                    Type=oneshot
                    User=root
                    ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                    ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                    ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                    ExecStartPost=/bin/sh -c "umount /mnt"
                    [Install]
                    WantedBy=multi-user.target
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: sample-cluster-workers
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: sample-cluster-workers-template
      hostSelector:
        matchLabels:
          cluster-role: worker
      image:
        checksum: http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: sample-cluster-workers-template
  namespace: default
spec:
  clusterName: sample-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>上記の例をコピーし、自身の環境に合わせて調整したら、<literal>kubectl</literal>を使用して適用し、<literal>clusterctl</literal>でクラスタのステータスを監視できます。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl apply -f rke2-agent.yaml

# Wait some time for the compute/agent hosts to be provisioned
% clusterctl describe cluster sample-cluster
NAME                                                    READY  SEVERITY  REASON  SINCE  MESSAGE
Cluster/sample-cluster                                  True                     25m
├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     30m
├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     25m
│ └─Machine/sample-cluster-chflc                        True                     27m
└─Workers
  └─MachineDeployment/sample-cluster                    True                     22m
    └─Machine/sample-cluster-56df5b4499-zfljj           True                     23m</screen>
</section>
<section xml:id="id-cluster-deprovisioning">
<title>クラスタのプロビジョニング解除</title>
<para>ダウンストリームクラスタをプロビジョニング解除するには、上記の作成手順で適用したリソースを削除します。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl delete -f rke2-agent.yaml
% kubectl delete -f rke2-control-plane.yaml</screen>
<para>これにより、BareMetalHostリソースのプロビジョニング解除がトリガされます。これには数分かかることがあり、その後リソースは再び利用可能な状態になります。</para>
<screen language="bash" linenumbering="unnumbered">% kubectl get bmh
NAME             STATE            CONSUMER                            ONLINE   ERROR   AGE
controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            10m
worker-0         deprovisioning   sample-cluster-workers-785x5        false            10m

...

% kubectl get bmh
NAME             STATE       CONSUMER   ONLINE   ERROR   AGE
controlplane-0   available              false            15m
worker-0         available              false            15m</screen>
</section>
</section>
<section xml:id="id-known-issues">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在、アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>はサポートされていません。このコントローラには、SLEMicroで選択されているネットワーク設定ツールと初回ブートツールチェーンとの互換性がまだないためです。</para>
</listitem>
<listitem>
<para>関連して、 IPAMリソースと、Metal3DataTemplateのnetworkDataフィールドは現在のところサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
<listitem>
<para>デプロイされたクラスタは現在、Rancherにインポートされません。</para>
</listitem>
<listitem>
<para>Rancherの組み込みCAPIコントローラが無効化されるため、上記の方法でMetal<superscript>3</superscript>用に設定した管理クラスタを、Elemental
(<xref linkend="components-elemental"/>)などの他のクラスタプロビジョニング方法でも使用することはできません。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-planned-changes">
<title>予定されている変更</title>
<itemizedlist>
<listitem>
<para>Rancherへのデプロイされたクラスタのインポート。これは今後、<link
xl:href="https://turtles.docs.rancher.com/">Rancher
Turtles</link>を介してインポートできるようになる予定です。</para>
</listitem>
<listitem>
<para>Rancher
Turtlesとの連携。これにより、Rancherの組み込みCAPIを無効にする必要がなくなるため、管理クラスタを介して他のクラスタ方法も使用できるようになります。</para>
</listitem>
<listitem>
<para>networkDataフィールドを使用した、IPAMリソースと設定のサポートの有効化。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-additional-resources">
<title>追加のリソース</title>
<para>ATIPのドキュメント(<xref
linkend="atip"/>)に、通信事業者のユースケースにおけるMetal<superscript>3</superscript>のより高度な使用例が記載されています。</para>
<section xml:id="id-single-node-configuration">
<title>シングルノード設定</title>
<para>管理クラスタがシングルノードであるテスト/PoC環境では、MetalLBを介して管理されるフローティングIPを追加する必要はありません。</para>
<para>このモードでは、管理クラスタAPIのエンドポイントが管理クラスタのIPになるため、DHCPを使用している場合はそのIPを予約するか、管理クラスタのIPが変更されないように静的に設定する必要があります(以下では<literal>&lt;MANAGEMENT_CLUSTER_IP&gt;</literal>と表記しています)。</para>
<para>このシナリオを有効にするために必要なmetal3チャートの値は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: &lt;MANAGEMENT_CLUSTER_IP&gt;
metal3-ironic:
  service:
    type: NodePort</screen>
</section>
<section xml:id="id-disabling-tls-for-virtualmedia-iso-attachment">
<title>仮想メディアISOをアタッチするためのTLSの無効化</title>
<para>一部のサーバベンダは、仮想メディアISOイメージをBMCにアタッチする際にSSL接続を検証しますが、Metal3のデプロイメント用に生成された証明書は自己署名されているため、問題が発生する可能性があります。この問題を回避するには、次のようなmetal3チャートの値を使用して、仮想メディアディスクをアタッチする場合にのみTLSを無効にすることができます。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  enable_vmedia_tls: false</screen>
<para>別の解決策は、CA証明書を使用してBMCを設定することです。この場合、<literal>kubectl</literal>を使用してクラスタから証明書を読み込むことができます。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml</screen>
<para>これにより、証明書をサーバのBMCコンソールで設定できますが、そのプロセスはベンダ固有です(すべてのベンダで可能というわけではなく、可能でない場合は<literal>enable_vmedia_tls</literal>フラグが必要なことがあります)。</para>
</section>
</section>
</chapter>
<chapter xml:id="quickstart-elemental">
<title>Elementalを使用したリモートホストのオンボーディング</title>
<para>このセクションでは、SUSE Edgeの一部としての「Phone
Homeネットワークプロビジョニング」ソリューションについて説明します。このソリューションは、Elementalを使用してノードのオンボーディングを支援します。Elementalは、Kubernetesを使用してリモートホスト登録と一元化された完全なクラウドネイティブOS管理を可能にするソフトウェアスタックです。SUSE
Edgeスタックでは、Elementalの登録機能を使用して、リモートホストをRancherにオンボーディングできます。これにより、ホストを集中管理プラットフォームに統合し、そこからKubernetesクラスタに加えて、階層化コンポーネント、アプリケーション、およびそのライフサイクルをすべて共通の場所からデプロイおよび管理できるようになります。</para>
<para>このアプローチが役立つシナリオとしては、制御するデバイスがアップストリームクラスタと同じネットワーク上にないか、アウトオブバンド管理コントローラが搭載されておらず、より直接的に制御できない場合や、さまざまな「不明」なシステムをエッジで多数ブートしており、それらを安全にオンボーディングして大規模に管理する必要がある場合が考えられます。これは、小売や産業用IoTなど、デバイスが設置されるネットワークをほとんど制御できない分野のユースケースによく見られるシナリオです。</para>
<section xml:id="id-high-level-architecture-2">
<title>アーキテクチャの概要</title>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="quickstart-elemental-architecture.png"
width=""/> </imageobject>
<textobject><phrase>クイックスタートElementalアーキテクチャ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-resources-needed">
<title>必要なリソース</title>
<para>このクイックスタートを実行するためのシステムと環境の最小要件を次に示します。</para>
<itemizedlist>
<listitem>
<para>集中管理クラスタ(RancherとElementalをホストするクラスタ)用のホスト:</para>
<itemizedlist>
<listitem>
<para>開発またはテスト用の場合、最小8GBのRAMと20GBのディスク容量 (運用環境での使用については<link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/installation-requirements#hardware-requirements">こちら</link>
を参照)</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>プロビジョニングするターゲットノード、すなわちエッジデバイス(デモまたはテストの場合は仮想マシンを使用可能)</para>
<itemizedlist>
<listitem>
<para>最小4GBのRAM、2 CPUコア、20GBのディスク</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>管理クラスタの解決可能なホスト名、またはsslip.ioなどのサービスで使用する静的IPアドレス</para>
</listitem>
<listitem>
<para>Edge Image Builderでインストールメディアを構築するためのホスト</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP5、openSUSE Leap 15.5、またはPodmanをサポートする他の互換オペレーティングシステムを実行していること</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/kubectl/">Kubectl</link>、<link
xl:href="https://podman.io">Podman</link>、および<link
xl:href="https://helm.sh">Helm</link>がインストールされていること</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ブート用のUSBフラッシュ ドライブ(物理ハードウェアを使用する場合)</para>
</listitem>
</itemizedlist>
<note>
<para>ターゲットマシンにある既存のデータはこのプロセスの一環として上書きされます。ターゲットデプロイメントノードに接続されているUSBストレージデバイスやディスク上のデータは、必ずバックアップしてください。</para>
</note>
<para>このガイドは、アップストリームクラスタをホストするためにDigital Oceanドロップレットを使用し、ダウンストリームデバイスとしてIntel
NUCを使用して作成されています。インストールメディアの構築には、SUSE Linux Enterprise Serverを使用しています。</para>
</section>
<section xml:id="id-how-to-use-elemental">
<title>Elementalの使用方法</title>
<para>Elementalをインストールして使用するための基本的な手順は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><xref linkend="build-bootstrap-cluster"/></para>
</listitem>
<listitem>
<para><xref linkend="install-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="install-elemental"/></para>
</listitem>
<listitem>
<para><xref linkend="build-installation-media"/></para>
</listitem>
<listitem>
<para><xref linkend="boot-downstream-nodes"/></para>
</listitem>
<listitem>
<para><xref linkend="create-downstream-clusters"/></para>
</listitem>
</itemizedlist>
<section xml:id="build-bootstrap-cluster">
<title>ブートストラップクラスタの構築</title>
<para>まず、RancherとElementalをホストできるクラスタを作成します。このクラスタは、ダウンストリームノードが接続されているネットワークからルーティングできる必要があります。</para>
<section xml:id="id-create-kubernetes-cluster">
<title>Kubernetesクラスタの作成</title>
<para>ハイパースケーラ(Azure、AWS、Google
Cloudなど)を使用している場合、クラスタを設定する最も簡単な方法は、ハイパースケーラのビルトインツールを使用することです。このガイドでは、簡潔にするために、これらの各オプションのプロセスについては詳述しません。</para>
<para>ベアメタルや別のホスティングサービスにインストールしようとしていて、Kubernetesディストリビューションそのものも用意する必要がある場合は、<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>を使用することをお勧めします。</para>
</section>
<section xml:id="id-set-up-dns">
<title>DNSの設定</title>
<para>続行する前に、クラスタへのアクセスを設定する必要があります。クラスタ自体のセットアップと同様に、DNSの設定方法は、クラスタがホストされている場所によって異なります。</para>
<tip>
<para>DNSレコードの設定を扱わない場合(たとえば、これが一時的なテストサーバである場合)、代わりに<link
xl:href="https://sslip.io">sslip.io</link>などのサービスを使用できます。このサービスを使用すると、<literal>&lt;address&gt;.sslip.io</literal>を使用して任意のIPアドレスを解決できます。</para>
</tip>
</section>
</section>
<section xml:id="install-rancher">
<title>Rancherのインストール</title>
<para>Rancherをインストールするには、作成したクラスタのKubernetes
APIにアクセスする必要があります。これは、使用しているKubernetesのディストリビューションによって異なります。</para>
<para>RKE2の場合、kubeconfigファイルは<literal>/etc/rancher/rke2/rke2.yaml</literal>に書き込まれます。このファイルをローカルシステムに<literal>~/.kube/config</literal>として保存します。このファイルを編集して、外部にルーティング可能な正しいIPアドレスまたはホスト名を含めなければならない場合があります。</para>
<para><link
xl:href="https://ranchermanager.docs.rancher.com/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster">Rancherのドキュメント</link>に記載されているコマンドを使用して、Rancherを簡単にインストールできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><link xl:href="https://cert-manager.io">cert-manager</link>をインストールします。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime

kubectl create namespace cattle-system

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml
helm repo add jetstack https://charts.jetstack.io

helm repo update

helm install cert-manager jetstack/cert-manager \
 --namespace cert-manager \
 --create-namespace</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime

kubectl create namespace cattle-system

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml

helm repo add jetstack https://charts.jetstack.io

helm repo update
helm install cert-manager jetstack/cert-manager `
  --namespace cert-manager `
  --create-namespace</screen>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<para>次に、Rancher自体をインストールします。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm install rancher rancher-prime/rancher \
  --namespace cattle-system \
  --set hostname=&lt;DNS or sslip from above&gt; \
  --set replicas=1 \
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt;</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows</term>
<listitem>
<screen language="bash" linenumbering="unnumbered">helm install rancher rancher-prime/rancher `
  --namespace cattle-system `
  --set hostname=&lt;DNS or sslip from above&gt; `
  --set replicas=1 `
  --set bootstrapPassword=&lt;PASSWORD_FOR_RANCHER_ADMIN&gt;</screen>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
<note>
<para>これを運用システムにする予定の場合は、cert-managerを使用して、実際の証明書(Let's Encryptの証明書など)を設定してください。</para>
</note>
<para>設定したホスト名をブラウズし、使用した<literal>bootstrapPassword</literal>でRancherにログインします。ガイドに従って簡単なセットアッププロセスを完了します。</para>
</section>
<section xml:id="install-elemental">
<title>Elementalのインストール</title>
<para>Rancherをインストールしたら、続いてElementalのオペレータと必要なCRDをインストールできます。Elemental用のHelmチャートはOCIアーティファクトとして公開されているため、インストールは他のチャートよりも若干シンプルです。Rancherのインストールに使用したものと同じシェルからインストールすることも、ブラウザでRancherのシェル内からインストールすることもできます。</para>
<screen language="bash" linenumbering="unnumbered">helm install --create-namespace -n cattle-elemental-system \
 elemental-operator-crds \
 oci://registry.suse.com/rancher/elemental-operator-crds-chart \
 --version 1.4.4

helm install --create-namespace -n cattle-elemental-system \
 elemental-operator \
 oci://registry.suse.com/rancher/elemental-operator-chart \
 --version 1.4.4</screen>
<section xml:id="id-optionally-install-the-elemental-ui-extension">
<title>(オプション) Elemental UI拡張機能のインストール</title>
<orderedlist numeration="arabic">
<listitem>
<para>Elemental UIを使用するには、Rancherインスタンスにログインし、左上の3点リーダーメニューをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-1.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>このページの［Available (使用可能)］タブから、Elementalカードの［Install (インストール)］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-2.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>拡張機能をインストールすることを確認します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-3.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>インストール後、ページを再ロードするよう求められます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installing-elemental-extension-4.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能のインストール4</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>再ロードすると、［OS Management (OS管理)］グローバルアプリからElemental拡張機能にアクセスできるようになります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="accessing-elemental-extension.png"
width=""/> </imageobject>
<textobject><phrase>Elemental拡張機能へのアクセス</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
<section xml:id="id-configure-elemental">
<title>Elementalの設定</title>
<para>シンプルにするために、変数<literal>$ELEM</literal>を、設定ディレクトリを配置する場所のフルパスに設定することをお勧めします。</para>
<screen language="shell" linenumbering="unnumbered">export ELEM=$HOME/elemental
mkdir -p $ELEM</screen>
<para>マシンがElementalに登録できるようにするために、<literal>fleet-default</literal>ネームスペースに<literal>MachineRegistration</literal>オブジェクトを作成する必要があります。</para>
<para>このオブジェクトの基本的なバージョンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/registration.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineRegistration
metadata:
  name: ele-quickstart-nodes
  namespace: fleet-default
spec:
  machineName: "\${System Information/Manufacturer}-\${System Information/UUID}"
  machineInventoryLabels:
    manufacturer: "\${System Information/Manufacturer}"
    productName: "\${System Information/Product Name}"
EOF

kubectl apply -f $ELEM/registration.yaml</screen>
<note>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</note>
<para>オブジェクトが作成されたら、割り当てられるエンドポイントを見つけてメモを取ります。</para>
<screen language="bash" linenumbering="unnumbered">REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n fleet-default -o jsonpath='{.status.registrationURL}')</screen>
<para>または、UIからこの操作を実行することもできます。</para>
<variablelist>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<orderedlist numeration="arabic">
<listitem>
<para>［OS Management extension (OS管理拡張機能)］から［Create Registration Endpoint
(登録エンドポイントの作成) ］をクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="click-create-registration.png" width=""/>
</imageobject>
<textobject><phrase>［Create Registration (登録の作成)］をクリックします。</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>この設定に名前を付けます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-name.png" width=""/>
</imageobject>
<textobject><phrase>名前の追加</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>［Cloud Configuration (クラウドの設定)］フィールドは無視して構いません。ここのデータは、Edge Image
Builderを使用した次の手順で上書きされるためです。</para>
</note>
</listitem>
<listitem>
<para>次に、下にスクロールして、マシンの登録時に作成されるリソースに付ける各ラベルに対して［Add Label
(ラベルの追加)］をクリックします。これはマシンを区別するのに役立ちます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-labels.png" width=""/>
</imageobject>
<textobject><phrase>ラベルの追加</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>最後に、［Create (作成)］をクリックして、設定を保存します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="create-registration-create.png" width=""/>
</imageobject>
<textobject><phrase>［Create (作成)］をクリック</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<para>設定を作成した直後の場合は、［Registration URL (登録URL)］が一覧にされます。［Copy
(コピー)］をクリックしてアドレスをコピーできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="get-registration-url.png" width=""/>
</imageobject>
<textobject><phrase>URLのコピー</phrase></textobject>
</mediaobject>
</informalfigure>
<tip>
<para>クリックしてその画面から移動してしまった場合は、左側のメニューの［Registration Endpoints
(登録エンドポイント)］をクリックし、先ほど作成したエンドポイント名をクリックできます。</para>
</tip>
</listitem>
</varlistentry>
</variablelist>
<para>このURLは次の手順で使用します。</para>
</section>
</section>
<section xml:id="build-installation-media">
<title>インストールメディアの構築</title>
<para>Elementalの現在のバージョンには独自のインストールメディアを構築する方法が用意されていますが、SUSE Edge 3.0では代わりにEdge
Image Builderでインストールメディアを構築します。したがって、生成されるシステムは、<link
xl:href="https://www.suse.com/products/micro/">SLE
Micro</link>をベースオペレーティングシステムとして構築されます。</para>
<tip>
<para>Edge Image Builderの詳細については、導入ガイド(<xref
linkend="quickstart-eib"/>)のほかに、コンポーネントのドキュメント(<xref
linkend="components-eib"/>)も参照してください。</para>
</tip>
<para>PodmanがインストールされたLinuxシステムから、次のコマンドを実行します。</para>
<screen language="bash" linenumbering="unnumbered">mkdir -p $ELEM/eib_quickstart/base-images
mkdir -p $ELEM/eib_quickstart/elemental</screen>
<screen language="bash" linenumbering="unnumbered">curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml</screen>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/eib_quickstart/eib-config.yaml
apiVersion: 1.0
image:
    imageType: iso
    arch: x86_64
    baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso
    outputImageName: elemental-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<itemizedlist>
<listitem>
<para>エンコードされていないパスワードは<literal>eib</literal>です。</para>
</listitem>
<listitem>
<para>この<literal>cat</literal>コマンドでは、各<literal>$</literal>をバックスラッシュ(<literal>\</literal>)でエスケープしています。このため、バッシュではテンプレート化されていません。手動でコピーする場合は、バックスラッシュを削除してください。</para>
</listitem>
</itemizedlist>
</note>
<screen language="bash" linenumbering="unnumbered">podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file eib-config.yaml</screen>
<para>物理デバイスをブートする場合は、イメージをUSBフラッシュ ドライブに書き込む必要があります。これは、次のコマンドで実行できます。</para>
<screen language="bash" linenumbering="unnumbered">sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/&lt;PATH_TO_DISK_DEVICE&gt; status=progress</screen>
</section>
<section xml:id="boot-downstream-nodes">
<title>ダウンストリームノードのブート</title>
<para>インストールメディアを作成したので、それを使用してダウンストリームノードをブートできます。</para>
<para>Elementalで制御するシステムごとに、インストールメディアを追加してデバイスをブートします。インストールが完了すると、デバイスは再起動して自身を登録します。</para>
<para>UI拡張機能を使用している場合は、［Inventory of Machines (マシンのインベントリ)］にノードが表示されます。</para>
<note>
<para>ログインプロンプトが表示されるまでインストールメディアを取り外さないでください。初回ブート時には、USBスティック上のファイルにアクセスしたままになります。</para>
</note>
</section>
<section xml:id="create-downstream-clusters">
<title>ダウンストリームクラスタの作成</title>
<para>Elementalを使用して新しいクラスタをプロビジョニングする際に作成する必要があるオブジェクトが2つあります。</para>
<variablelist role="tabs">
<varlistentry>
<term>Linux</term>
<listitem>
<para>最初のオブジェクトは<literal>MachineInventorySelectorTemplate</literal>です。このオブジェクトにより、クラスタとインベントリ内のマシン間のマッピングを指定できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>インベントリ内のマシンをラベルに一致させるセレクタを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/selector.yaml
apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventorySelectorTemplate
metadata:
  name: location-123-selector
  namespace: fleet-default
spec:
  template:
    spec:
      selector:
        matchLabels:
          locationID: '123'
EOF</screen>
</listitem>
<listitem>
<para>リソースをクラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f $ELEM/selector.yaml</screen>
</listitem>
<listitem>
<para>マシンの名前を取得し、一致するラベルを追加します。</para>
<screen language="bash" linenumbering="unnumbered">MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR&gt;1 {print $1}')

kubectl label MachineInventory -n fleet-default \
 $MACHINENAME locationID=123</screen>
</listitem>
<listitem>
<para>シンプルなシングルノードK3sクラスタリソースを作成し、クラスタに適用します。</para>
<screen language="bash" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $ELEM/cluster.yaml
apiVersion: provisioning.cattle.io/v1
kind: Cluster
metadata:
  name: location-123
  namespace: fleet-default
spec:
  kubernetesVersion: v1.28.9+k3s1
  rkeConfig:
    machinePools:
      - name: pool1
        quantity: 1
        etcdRole: true
        controlPlaneRole: true
        workerRole: true
        machineConfigRef:
          kind: MachineInventorySelectorTemplate
          name: location-123-selector
          apiVersion: elemental.cattle.io/v1beta1
EOF

kubectl apply -f $ELEM/cluster.yaml</screen>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>UI拡張機能</term>
<listitem>
<para>UI拡張機能では、ショートカットをいくつか使用できます。複数の場所を管理する場合は、手動による作業が多くなりすぎる可能性があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>先ほどと同様に、左側の3点リーダーメニューを開き、［OS Management
(OS管理)］を選択します。これにより、Elementalシステムを管理するためのメイン画面に戻ります。</para>
</listitem>
<listitem>
<para>左側のサイドバーで、［Inventory of Machines (マシンのインベントリ)］をクリックします。登録済みのマシンのインベントリが開きます。</para>
</listitem>
<listitem>
<para>これらのマシンからクラスタを作成するには、必要なシステムを選択し、［Actions (アクション)］ドロップダウンリストから［Create
Elemental Cluster (Elementalクラスタの作成)］をクリックします。［Cluster Creation
(クラスタの作成)］ダイアログが開き、それと同時に、使用するMachineSelectorTemplateがバックグラウンドで作成されます。</para>
</listitem>
<listitem>
<para>この画面で、構築するクラスタを設定します。このクイックスタートでは、［K3s v1.28.9+k3s1］を選択し、残りのオプションはそのままにします。</para>
<tip>
<para>他のオプションを表示するには、下にスクロールする必要がある場合があります。</para>
</tip>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
<para>これらのオブジェクトを作成したら、先ほどインストールした新しいノードを使用して新しいKubernetesクラスタがスピンアップするはずです。</para>
<tip>
<para>システムを簡単にグループ化できるようにするには、環境内でその場所に固有であることがわかっている項目を検索する起動スクリプトを追加できます。</para>
<para>たとえば、各場所に固有のサブネットがあることがわかっている場合は、そのネットワークプレフィックスを検索して、対応するMachineInventoryにラベルを追加するスクリプトを記述できます。</para>
<para>これは通常、ご使用のシステムの設計に合わせたカスタムスクリプトになりますが、次のようになります。</para>
<screen language="bash" linenumbering="unnumbered">INET=`ip addr show dev eth0 | grep "inet\ "`
elemental-register --label "network=$INET" \
 --label "network=$INET" /oem/registration</screen>
</tip>
</section>
</section>
<section xml:id="id-node-reset">
<title>ノードのリセット</title>
<para>SUSE Rancher
Elementalは、「ノードリセット」を実行する機能をサポートしています。ノードリセットは、Rancherからクラスタ全体が削除されたとき、クラスタからシングルノードが削除されたとき、またはマシンインベントリからノードが手動で削除されたときに任意でトリガできます。これは、孤立したリソースをリセットしてクリーンアップし、クリーンアップされたノードを自動的にマシンインベントリに戻して再利用可能にする場合に役立ちます。ノードリセットはデフォルトでは有効になっていないため、削除されたシステムはクリーンアップされず(つまり、データは削除されず、Kubernetesクラスタリソースはダウンストリームクラスタで動作し続けます)、データを消去してマシンをElemental経由でRancherに再登録するには手動操作が必要となります。</para>
<para>この機能をデフォルトで有効にするには、<literal>MachineRegistration</literal>に<literal>config.elemental.reset.enabled:
true</literal>を追加して明示的に有効にする必要があります。例:</para>
<screen language="yaml" linenumbering="unnumbered">config:
  elemental:
    registration:
      auth: tpm
    reset:
      enabled: true</screen>
<para>その後、この<literal>MachineRegistration</literal>に登録されているすべてのシステムが自動的に<literal>elemental.cattle.io/resettable:'true'</literal>のアノテーションを受け取って設定に反映します。既存の<literal>MachineInventory</literal>にこのアノテーションがない場合や、すでにノードをデプロイ済みである場合などに、個々のノードで手動でこの操作を実行する場合は、<literal>MachineInventory</literal>を変更し、<literal>resettable</literal>設定を追加します。例:</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: elemental.cattle.io/v1beta1
kind: MachineInventory
metadata:
  annotations:
    elemental.cattle.io/os.unmanaged: 'true'
    elemental.cattle.io/resettable: 'true'</screen>
<para>SUSE Edge 3.0では、Elemental
Operatorによってオペレーティングシステム上にマーカが配置され、これによってクリーンアッププロセスが自動的にトリガされます。クリーンアッププロセスは、すべてのKubernetesサービスを停止して永続データをすべて削除し、すべてのKubernetesサービスをアンインストールして、残っているKubernetes/Rancherディレクトリをクリーンアップし、元のElemental
<literal>MachineRegistration</literal>設定を使用して強制的にRancherに再登録します。これは自動的に行われるため、手動での操作は必要ありません。呼び出されるスクリプトは<literal>/opt/edge/elemental_node_cleanup.sh</literal>にあり、マーカが配置されるとすぐに<literal>systemd.path</literal>を介してトリガされるため、直ちに実行されます。</para>
<warning>
<para><literal>resettable</literal>機能を使用する場合、Rancherからノード/クラスタを削除する際の望ましい動作は、データを消去して再登録を強制することであると想定されています。この状況ではデータが確実に失われるため、この機能は、自動リセットを実行することがわかっている場合にのみ使用してください。</para>
</warning>
</section>
<section xml:id="id-next-steps">
<title>次の手順</title>
<para>このガイドの使用後に調べるべき推奨リソースを次に示します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-fleet"/>のエンドツーエンドの自動化</para>
</listitem>
<listitem>
<para><xref linkend="components-nmc"/>の追加のネットワーク設定オプション</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="quickstart-eib">
<title>Edge Image Builderを使用したスタンドアロンクラスタ</title>
<para>Edge Image Builder (EIB)は、完全なエアギャップシナリオでもマシンをブートストラップできるCustomized,
Ready-to-Boot (CRB)ディスクイメージの生成プロセスを効率化するツールです。EIBを使用すると、SUSE
Edgeの3つのデプロイメントフットプリントすべてで使用するデプロイメントイメージを作成できます。これは、EIBが十分に柔軟であり、最小限のカスタマイズ(例:
ユーザの追加やタイムゾーンの設定)から、あらゆる設定を網羅したイメージ(例:
複雑なネットワーク設定を行い、マルチノードKubernetesクラスタをデプロイして、顧客ワークロードをデプロイし、Rancher/ElementalとSUSE
Managerを介して集中管理プラットフォームに登録するイメージ)までを提供できるためです。EIBはコンテナイメージ内で動作するため、プラットフォーム間できわめて容易に移植可能です、さらに、必要な依存関係をすべて備えた自己完結型であるため、EIBツールの操作に使用するシステムにインストール済みのパッケージに及ぼす影響が最小限に抑えられます。</para>
<para>詳細については、Edge Image Builderの紹介(<xref linkend="components-eib"/>)を参照してください。</para>
<section xml:id="id-prerequisites-2">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>SLES 15 SP5、openSUSE Leap 15.5、またはopenSUSE
Tumbleweedを実行しているx86_64物理ホスト(または仮想マシン)</para>
</listitem>
<listitem>
<para>利用可能なコンテナランタイム(Podmanなど)</para>
</listitem>
<listitem>
<para>最新のSLE Micro 5.5 SelfInstall「GM2」ISOイメージのダウンロードコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあります)</para>
</listitem>
</itemizedlist>
<note>
<para>互換性のあるコンテナランタイムが利用可能であれば、他のオペレーティングシステムでも機能する可能性はありますが、他のプラットフォームでは広範なテストは行われていません。このドキュメントではPodmanに焦点を当てていますが、Dockerでも同じ機能を実現できるはずです。</para>
</note>
<section xml:id="id-getting-the-eib-image">
<title>EIBイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、イメージ構築ホストで次のコマンドを実行することでSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/edge-image-builder:1.0.2</screen>
</section>
</section>
<section xml:id="id-creating-the-image-configuration-directory">
<title>イメージ設定ディレクトリの作成</title>
<para>EIBはコンテナ内で動作するため、ホストから設定ディレク
トリをマウントして、必要な設定を指定できるようにする必要があります。ビルドプロセス中に、EIBは必要な入力ファイルやサポートアーティファクトすべてにアクセスできます。このディレクトリは、特定の構造に従う必要があります。このディレクトリがホームディレクトリに存在し、「eib」という名前であると仮定して、ディレクトリを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>前の手順では、SLE Micro
5.5の入力イメージをホストする「base-images」ディレクトリを作成しました。ダウンロードしたイメージをこの設定ディレクトリに確実にコピーしましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp /path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBの実行中に元のゴールデンイメージは変更<emphasis
role="strong">「されません」</emphasis>。EIBの設定ディレクトリのルートに、目的の設定でカスタマイズされた新しいバージョンが作成されます。</para>
</note>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── slemicro.iso</screen>
</section>
<section xml:id="quickstart-eib-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image
Builderがサポートする設定可能なオプションの大部分が記述されています。オプションの完全な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/pkg/image/testdata/full-valid-example.yaml">こちら</link>を参照してください。以下で説明する例よりも広範な例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照することをお勧めします。まずは、OSイメージの非常に基本的な定義ファイルから始めましょう。</para>
<screen language="console" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
EOF</screen>
<para>この定義では、<literal>x86_64</literal>ベースのシステム用の出力イメージを生成するように指定しています。さらに変更を加えるためのベースとして使用するイメージは、<literal>slemicro.iso</literal>という名前の<literal>iso</literal>イメージであり、<literal>$CONFIG_DIR/base-images/slemicro.iso</literal>にあることが想定されています。また、EIBがイメージの変更を完了すると、出力イメージは<literal>eib-image.iso</literal>という名前になり、デフォルトでは<literal>$CONFIG_DIR</literal>に存在することも記述されています。</para>
<para>これで、ディレクトリ構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
└── base-images/
    └── slemicro.iso</screen>
<para>以降のセクションでは、一般的な操作の例をいくつか紹介していきます。</para>
<section xml:id="id-configuring-os-users">
<title>OSユーザの設定</title>
<para>EIBを使用すると、パスワードやSSHキーなどのログイン情報を事前にユーザに設定できます(固定されたルートパスワードの設定も含む)。この例の一部として、ルートパスワードを修正します。最初の手順は、<literal>OpenSSL</literal>を使用して一方向暗号化パスワードを作成することです。</para>
<screen language="console" linenumbering="unnumbered">openssl passwd -6 SecurePassword</screen>
<para>これは次のような出力になります。</para>
<screen language="console" linenumbering="unnumbered">$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<para>次に、定義ファイルに<literal>operatingSystem</literal>というセクションを追加し、その中に<literal>users</literal>配列を含めます。作成したファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1</screen>
<note>
<para>ユーザの追加、ホームディレクトリの作成、ユーザIDの設定、ssh-key認証の追加、グループ情報の変更も行うことができます。他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">アップストリームのイメージ構築ガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="id-configuring-rpm-packages">
<title>RPMパッケージの設定</title>
<para>EIBの主な特徴の1つは、イメージにソフトウェアパッケージを追加するメカニズムを備えていることです。このため、インストールが完了した時点で、システムはインストールされたパッケージをすぐに利用できます。EIBでは、ユーザは以下を行うことができます。</para>
<itemizedlist>
<listitem>
<para>イメージ定義のリスト内の名前でパッケージを指定する</para>
</listitem>
<listitem>
<para>これらのパッケージを検索するネットワークリポジトリを指定する</para>
</listitem>
<listitem>
<para>一覧にされたパッケージをSUSEの公式リポジトリで検索するためのSUSE Customer Center (SCC)資格情報を指定する</para>
</listitem>
<listitem>
<para><literal>$CONFIG_DIR/rpms</literal>ディレクトリ経由で、ネットワークリポジトリに存在しないカスタムRPMをサイドロードする</para>
</listitem>
<listitem>
<para>同じディレクトリ(<literal>$CONFIG_DIR/rpms/gpg-keys</literal>)経由で、サードパーティ製パッケージの検証を有効にするためにGPGキーを指定する</para>
</listitem>
</itemizedlist>
<para>これにより、EIBは、イメージの構築時にパッケージ解決プロセスを実行し、ゴールデンイメージを入力として受け取り、提供されているパッケージ(リストで指定されているか、ローカルで提供されているパッケージ)をすべてプルしてインストールしようと試みます。EIBは、依存関係を含むすべてのパッケージを出力イメージ内に存在するリポジトリにダウンロードし、そのパッケージを初回ブートプロセス中にインストールするようにシステムに指示します。イメージの構築中にこのプロセスを実行することで、初回ブート時にパッケージが目的のプラットフォーム(エッジのノードなど)が正常にインストールされることが保証されます。これは、操作時に追加パッケージをネットワーク経由でプルするのではなく、イメージに事前に書き込んでおきたい環境でも便利です。たとえば、エアギャップ環境や、ネットワークが制限された環境です。</para>
<para>これを示すシンプルな例として、サードパーティベンダがサポートするNVIDIAリポジトリにある<literal>nvidia-container-toolkit</literal>
RPMパッケージをインストールします。</para>
<screen language="yaml" linenumbering="unnumbered">  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>生成される定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64</screen>
<para>上記はシンプルな例ですが、完全を期するために、イメージ生成を実行する前にNVIDIAパッケージ署名キーをダウンロードしてください。</para>
<screen language="bash" linenumbering="unnumbered">$ mkdir -p $CONFIG_DIR/rpms/gpg-keys
$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey &gt; rpms/gpg-keys/nvidia.gpg</screen>
<warning>
<para>この方法でRPMを追加することは、サポートされているサードパーティコンポーネント、またはユーザが提供(および保守)するパッケージを追加することを目的としています。このメカニズムは、通常ではSLE
Microでサポートされないパッケージを追加する目的では使用しないでください。このメカニズムを使用して、openSUSEのリポジトリから新しいリリースやサービスパックなどのコンポーネントを追加した場合(この操作はサポートされません)、最終的にサポート対象外の設定になるおそれがあります。特に、依存関係の解決によってオペレーティングシステムのコア部分が置き換えられる場合は、作成されたシステムが期待どおりに機能しているように見えても注意が必要です。確信が持てない場合は、SUSEの担当者に連絡してサポートを依頼し、目的の設定がサポート可能かどうかを判断してください。</para>
</warning>
<note>
<para>追加の例を含む総合的なガイドについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/installing-packages.md">アップストリームのパッケージインストールガイド</link>を参照してください。</para>
</note>
</section>
<section xml:id="id-configuring-kubernetes-cluster-and-user-workloads">
<title>Kubernetesクラスタとユーザワークロードの設定</title>
<para>EIBのもう1つの特徴は、EIBを使用すると、「インプレースでブートストラップ」する、つまり調整のためにどのような形態の集中管理インフラストラクチャも必要としない、シングルノードとマルチノード両方の高可用性Kubernetesクラスタのデプロイメントを自動化できることです。このアプローチは主にエアギャップデプロイメント、すなわちネットワークが制限された環境のためのものですが、ネットワークに制限なく完全にアクセスできる場合であっても、スタンドアロンクラスタを迅速にブートストラップする方法として役立ちます。</para>
<para>この方法を使用すると、カスタマイズされたオペレーティングシステムをデプロイできるだけでなく、Kubernetesの設定を指定したり、Helmチャートを介して追加の階層化コンポーネントを指定したり、指定したKubernetesマニフェストを介してユーザワークロードを指定したりすることもできます。ただしこの方法を使用する場合、その背景にある設計理念として、ユーザがエアギャップ化を望んでいるとデフォルトで想定します。したがって、イメージ定義で指定されているすべての項目を、ユーザが指定したワークロードを含めてイメージにプルします。その際に、EIBは、提供された定義で要求されている検出済みイメージをすべてローカルにコピーし、作成されたデプロイ済みシステムの組み込みのイメージレジストリで提供します。</para>
<para>次の例では、既存のイメージ定義を使用してKubernetesの設定を指定します(この例では、複数のシステムとその役割が一覧にされていないため、デフォルトでシングルノードを想定します)。この設定により、シングルノードのRKE2
KubernetesクラスタをプロビジョニングするようにEIBに指示します。ユーザが指定したワークロード(マニフェストを使用)と階層化コンポーネント(Helmを使用)の両方のデプロイメントの自動化を説明するために、SUSE
EdgeのHelmチャートを使用してKubeVirtをインストールし、Kubernetesマニフェストを使用してNGINXをインストールします。既存のイメージ定義に追加する必要がある設定は次のとおりです。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: v1.28.9+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.2.4
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge</screen>
<para>作成された完全な定義ファイルは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1
  packages:
    packageList:
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
kubernetes:
  version: v1.28.9+rke2r1
  manifests:
    urls:
      - https://k8s.io/examples/application/nginx-app.yaml
  helm:
    charts:
      - name: kubevirt-chart
        version: 0.2.4
        repositoryName: suse-edge
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge</screen>
<note>
<para>マルチノードデプロイメント、カスタムネットワーキング、Helmチャートのオプション/値など、オプションの他の例については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes">アップストリームドキュメント</link>を参照してください。</para>
</note>
</section>
<section xml:id="quickstart-eib-network">
<title>ネットワークの設定</title>
<para>このクイックスタートの最後の例では、EIBで生成したイメージを使ってシステムをプロビジョニングした場合に作成されるネットワークを設定しましょう。ネットワーク設定を指定しない限り、ブート時に検出されたすべてのインタフェースでDHCPが使用されるのがデフォルトのモデルであることを理解することが重要です。ただし、これが常に望ましい設定であるとは限りません。これは特に、DHCPが利用できず静的な設定を指定する必要がある場合や、より複雑なネットワーキング構造(ボンド、LACP、VLANなど)を設定する必要がある場合、特定のパラメータ(ホスト名、DNSサーバ、ルートなど)を上書きする必要がある場合に該当します。</para>
<para>EIBでは、ノードごとの設定を指定することも(対象のシステムをMACアドレスで一意に識別します)、上書きによって各マシンに同一の設定を指定することもできます(システムのMACアドレスが不明な場合に便利です)。またEIDでは、Network
Manager Configurator (<literal>nmc</literal>)というツールも追加で使用されます。Network
Manager ConfiguratorはSUSE Edgeチームによって構築されたツールであり、<link
xl:href="https://nmstate.io/">nmstate.io</link>の宣言型ネットワークスキーマに基づいてカスタムネットワーキング設定を適用できるようにします。また、ブートしているノードをブート時に識別し、必要なネットワーク設定をサービス開始前に適用します。</para>
<para>ここでは、1つのインタフェースを持つシステムに静的ネットワーク設定を適用します。そのために、ネットワークの望ましい状態を、必要な<literal>network</literal>ディレクトリ内にある(目的のホスト名に基づく)ノード固有のファイルに記述します。</para>
<screen language="console" linenumbering="unnumbered">mkdir $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.122.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.122.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.122.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.122.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<warning>
<para>上記の例は、テストを仮想マシン上で実行すると仮定して、デフォルトの<literal>192.168.122.0/242</literal>サブネット用に設定されています。ご使用の環境に合わせて、MACアドレスも忘れずに変更してください。同じイメージを使用して複数のノードをプロビジョニングできるため、
EIBで(<literal>nmc</literal>を介して)設定されたネットワーキングは、各ノードをMACアドレスで一意に識別できるかどうかに依存しており、その後、ブート中に<literal>nmc</literal>は正しいネットワーキング設定を各マシンに適用します。つまり、インストール先のシステムのMACアドレスを把握する必要があります。また、デフォルトの動作ではDHCPに依存しますが、<literal>configure-network.sh</literal>フックを利用して、すべてのノードに共通の設定を適用することもできます。詳細については、ネットワーキングのガイド
(<xref linkend="components-nmc"/>)を参照してください。</para>
</warning>
<para>作成されるファイル構造は、次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>作成したネットワーク設定が解析され、必要なNetworkManager接続ファイルが自動的に生成されて、EIBが作成する新しいインストールイメージに挿入されます。これらのファイルはホストのプロビジョニング中に適用され、完全なネットワーク設定が生成されます。</para>
<note>
<para>上記の設定のより包括的な説明と、この機能の例については、エッジネットワーキングのコンポーネント(<xref
linkend="components-nmc"/>)を参照してください。</para>
</note>
</section>
</section>
<section xml:id="id-building-the-image">
<title>イメージの構築</title>
<para>EIBで使用するゴールデンイメージとイメージ定義ができたので、イメージを構築してみましょう。これには、<literal>podman</literal>を使用し、定義ファイルを指定して「build」コマンドでEIBコンテナを呼び出すだけです。</para>
<screen language="bash" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file iso-definition.yaml</screen>
<para>コマンドの出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Setting up Podman API listener...
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 5.9 MB/s)
Populating Embedded Artifact Registry... 100%  (3/3, 11 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 98 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 100 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100%  (34/34 MB, 101 MB/s)
Downloading file: sha256sum-amd64.txt 100%  (3.9/3.9 kB, 1.5 MB/s)
Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 7.2 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築されたISOイメージは<literal>$CONFIG_DIR/eib-image.iso</literal>に保存されます。</para>
<screen language="console" linenumbering="unnumbered">├── iso-definition.yaml
├── eib-image.iso
├── _build
│   └── cache/
│       └── ...
│   └── build-&lt;timestamp&gt;/
│       └── ...
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
<para>ビルドごとに、<literal>$CONFIG_DIR/_build/</literal>内にタイムスタンプ付きのフォルダが作成されます。このフォルダには、ビルドのログ、ビルド中に使用されたアーティファクト、およびCRBイメージに追加されたすべてのスクリプトとアーティファクトを含む<literal>combustion</literal>ディレクトリと<literal>artefacts</literal>ディレクトリが含まれます。</para>
<para>このディレクトリの内容は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── build-&lt;timestamp&gt;/
│   │── combustion/
│   │   ├── 05-configure-network.sh
│   │   ├── 10-rpm-install.sh
│   │   ├── 12-keymap-setup.sh
│   │   ├── 13b-add-users.sh
│   │   ├── 20-k8s-install.sh
│   │   ├── 26-embedded-registry.sh
│   │   ├── 48-message.sh
│   │   ├── network/
│   │   │   ├── host1.local/
│   │   │   │   └── eth0.nmconnection
│   │   │   └── host_config.yaml
│   │   ├── nmc
│   │   └── script
│   │── artefacts/
│   │   │── registry/
│   │   │   ├── hauler
│   │   │   ├── nginx:1.14.2-registry.tar.zst
│   │   │   ├── rancher_kubectl:v1.28.7-registry.tar.zst
│   │   │   └── registry.suse.com_suse_sles_15.5_virt-operator:1.1.1-150500.8.12.1-registry.tar.zst
│   │   │── rpms/
│   │   │   └── rpm-repo
│   │   │       ├── addrepo0
│   │   │       │   └── x86_64
│   │   │       │       ├── nvidia-container-toolkit-1.15.0-1.x86_64.rpm
│   │   │       │       ├── ...
│   │   │       ├── repodata
│   │   │       │   ├── ...
│   │   │       └── zypper-success
│   │   └── kubernetes/
│   │       ├── rke2_installer.sh
│   │       ├── registries.yaml
│   │       ├── server.yaml
│   │       ├── images/
│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst
│   │       │   └── rke2-images-core.linux-amd64.tar.zst
│   │       ├── install/
│   │       │   ├── rke2.linux-amd64.tar.gz
│   │       │   └── sha256sum-amd64.txt
│   │       └── manifests/
│   │           ├── dl-manifest-1.yaml
│   │           └── kubevirt-chart.yaml
│   ├── createrepo.log
│   ├── eib-build.log
│   ├── embedded-registry.log
│   ├── helm
│   │   └── kubevirt-chart
│   │       └── kubevirt-0.2.4.tgz
│   ├── helm-pull.log
│   ├── helm-template.log
│   ├── iso-build.log
│   ├── iso-build.sh
│   ├── iso-extract
│   │   └── ...
│   ├── iso-extract.log
│   ├── iso-extract.sh
│   ├── modify-raw-image.sh
│   ├── network-config.log
│   ├── podman-image-build.log
│   ├── podman-system-service.log
│   ├── prepare-resolver-base-tarball-image.log
│   ├── prepare-resolver-base-tarball-image.sh
│   ├── raw-build.log
│   ├── raw-extract
│   │   └── ...
│   └── resolver-image-build
│       └──...
└── cache
    └── ...</screen>
<para>ビルドが失敗した場合、情報が含まれる最初のログは<literal>eib-build.log</literal>です。そこから、失敗したコンポーネントに移動してデバッグを行います。</para>
<para>この時点で、以下を行う、すぐに使用できるイメージができているはずです。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SLE Micro 5.5をデプロイする</para>
</listitem>
<listitem>
<para>ルートパスワードを設定する</para>
</listitem>
<listitem>
<para><literal>nvidia-container-toolkit</literal>パッケージをインストールする</para>
</listitem>
<listitem>
<para>コンテンツをローカルに提供する組み込みのコンテナレジストリを設定する</para>
</listitem>
<listitem>
<para>シングルノードRKE2をインストールする</para>
</listitem>
<listitem>
<para>静的ネットワーキングを設定する</para>
</listitem>
<listitem>
<para>KubeVirtをインストールする</para>
</listitem>
<listitem>
<para>ユーザが指定したマニフェストをデプロイする</para>
</listitem>
</orderedlist>
</section>
<section xml:id="quickstart-eib-image-debug">
<title>イメージ構築プロセスのデバッグ</title>
<para>イメージ構築プロセスが失敗する場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/debugging.md">アップストリームのデバッグガイド</link>を参照してください。</para>
</section>
<section xml:id="quickstart-eib-image-test">
<title>新しく構築されたイメージのテスト</title>
<para>新しく構築されたCRBイメージをテストする方法については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/testing-guide.md">アップストリームのイメージテストガイド</link>を参照してください。</para>
</section>
</chapter>
</part>
<part xml:id="id-components-used">
<title>使用するコンポーネント</title>
<partintro>
<para>Edgeのコンポーネントのリスト</para>
</partintro>
<chapter xml:id="components-rancher">
<title>Rancher</title>
<para><link
xl:href="https://ranchermanager.docs.rancher.com">https://ranchermanager.docs.rancher.com</link>にあるRancherのアップストリームドキュメントを参照してください。</para>
<para>Rancherは、オープンソースの強力なKubernetes管理プラットフォームであり、複数の環境にまたがるKubernetesクラスタのデプロイメント、運用、および監視を効率化します。オンプレミス、クラウド、エッジのいずれのクラスタを管理する場合でも、Rancherは、Kubernetesに関するあらゆるニーズに対応する、統合された中央プラットフォームを提供します。</para>
<section xml:id="id-key-features-of-rancher">
<title>Rancherの主な機能</title>
<itemizedlist>
<listitem>
<para><emphasis role="strong">マルチクラスタ管理:</emphasis>
Rancherの直感的なインタフェースを使用して、パブリッククラウド、プライベートデータセンター、エッジロケーションのどこからでもKubernetesクラスタを管理できます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">セキュリティとコンプライアンス:</emphasis>
Rancherでは、Kubernetes環境全体にセキュリティポリシー、ロールベースのアクセス制御(RBAC)、およびコンプライアンス標準が適用されます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">クラスタ操作のシンプル化:</emphasis>
Rancherはクラスタのプロビジョニング、アップグレード、トラブルシューティングを自動化し、あらゆる規模のチームでKubernetesの操作をシンプル化します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">中央型のアプリケーションカタログ:</emphasis>
Rancherアプリケーションカタログは、多種多様なHelmチャートとKubernetes
Operatorを提供し、コンテナ化アプリケーションのデプロイと管理を容易にします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">継続的デリバリ:</emphasis>
RancherはGitOpsと継続的インテグレーション/継続的デリバリパイプラインをサポートしており、自動化および効率化されたアプリケーションデリバリプロセスを実現します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-ranchers-use-in-suse-edge">
<title>SUSE EdgeでのRancherの使用</title>
<para>Rancherは、SUSE Edgeスタックに複数のコア機能を提供します。</para>
<section xml:id="id-centralized-kubernetes-management">
<title>Kubernetesの集中管理</title>
<para>大量の分散クラスタが存在する一般的なエッジデプロイメントでは、Rancherは、これらのKubernetesクラスタを管理するための中央コントロールプレーンとして機能します。プロビジョニング、アップグレード、監視、およびトラブルシューティングのための統合インタフェースを提供し、操作をシンプル化し、一貫性を確保します。</para>
</section>
<section xml:id="id-simplified-cluster-deployment">
<title>クラスタデプロイメントのシンプル化</title>
<para>Rancherは、軽量なSLE Micro (SUSE Linux Enterprise
Micro)オペレーティングシステム上でのKubernetesクラスタの作成を効率化し、Kubernetesの堅牢な機能を備えたエッジインフラストラクチャの展開を容易にします。</para>
</section>
<section xml:id="id-application-deployment-and-management">
<title>アプリケーションのデプロイメントと管理</title>
<para>統合されたRancherアプリケーションカタログは、SUSE
Edgeクラスタ全体でのコンテナ化アプリケーションのデプロイと管理をシンプル化し、エッジワークロードのシームレスなデプロイメントを可能にします。</para>
</section>
<section xml:id="id-security-and-policy-enforcement">
<title>セキュリティとポリシーの適用</title>
<para>Rancherは、ポリシーベースのガバナンスツール、ロールベースのアクセス制御(RBAC)を備えているほか、外部の認証プロバイダと統合できます。これにより、SUSE
Edgeのデプロイメントは、分散環境において重要なセキュリティとコンプライアンスを維持できます。</para>
</section>
</section>
<section xml:id="id-best-practices">
<title>ベストプラクティス</title>
<section xml:id="id-gitops">
<title>GitOps</title>
<para>RancherにはビルトインコンポーネントとしてFleetが含まれており、Gitに保存されたコードでクラスタ設定やアプリケーションのデプロイメントを管理できます。</para>
</section>
<section xml:id="id-observability">
<title>可観測性</title>
<para>Rancherには、PrometheusやGrafanaなどのビルトインのモニタリングおよびログツールが含まれており、クラスタのヘルスとパフォーマンスについて包括的な洞察を得ることができます。</para>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、SLE Micro OSのベースイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBでプロビジョニングしたKubernetesクラスタ上にRancherをエアギャップインストールするには、<xref
linkend="rancher-install"/>に従ってください。</para>
</section>
<section xml:id="id-additional-resources-2">
<title>追加リソース</title>
<itemizedlist>
<listitem>
<para><link xl:href="https://rancher.com/docs/">Rancherのドキュメント</link></para>
</listitem>
<listitem>
<para><link xl:href="https://www.rancher.academy/">Rancher Academy</link></para>
</listitem>
<listitem>
<para><link xl:href="https://rancher.com/community/">Rancher Community</link></para>
</listitem>
<listitem>
<para><link xl:href="https://helm.sh/">Helmチャート</link></para>
</listitem>
<listitem>
<para><link xl:href="https://operatorhub.io/">Kubernetes Operator</link></para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-rancher-dashboard-extensions">
<title>Rancher Dashboard拡張機能</title>
<para>拡張機能により、ユーザ、開発者、パートナー、および顧客はRancher UIを拡張および強化できます。SUSE Edge
3.0では、KubeVirtとAkriのダッシュボード拡張機能を提供しています。</para>
<para>Rancher Dashboard拡張機能の一般情報については、<literal><link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions">Rancherのドキュメント</link></literal>を参照してください。</para>
<section xml:id="id-prerequisites-3">
<title>前提条件</title>
<para>拡張機能を有効にするには、Rancherにui-plugin operatorがインストールされている必要があります。Rancher Dashboard
UIを使用している場合、左側のナビゲーションの<emphasis role="strong">［Configuration
(設定)］</emphasis>セクションの<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>に移動します。ui-plugin operatorがインストールされていない場合、<literal><link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions">こちら</link></literal>で説明されているように、拡張機能のサポートを有効にするように求めるプロンプトが表示されます。</para>
<para>operatorは、次のようにHelmを使用してインストールすることもできます。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-charts https://charts.rancher.io/
helm upgrade --create-namespace -n cattle-ui-plugin-system \
  --install ui-plugin-operator rancher-charts/ui-plugin-operator
helm upgrade --create-namespace -n cattle-ui-plugin-system \
  --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd</screen>
<para>または、専用のGitRepoリソースを作成してFleetを使用してインストールすることもできます。詳細については、<link
xl:href="fleet.xml">Fleet</link>のセクションと<literal><link
xl:href="https://github.com/suse-edge/fleet-examples/blob/main/gitrepos/rancher-ui-plugin-operator-gitrepo.yaml">fleet-examples</link></literal>リポジトリを参照してください。</para>
</section>
<section xml:id="id-installation">
<title>インストール</title>
<para>ダッシュボード拡張機能を含むすべてのSUSE Edge 3.0コンポーネントは、OCIアーティファクトとして配布されます。Rancher
Dashboard Apps/Marketplaceは、OCIベースのHelmリポジトリを<literal><link
xl:href="https://github.com/rancher/dashboard/issues/9815">まだ</link></literal>サポートしていません。したがって、SUSE
Edge拡張機能をインストールするには、HelmまたはFleetを使用できます。</para>
<section xml:id="id-installing-with-helm">
<title>Helmを使用したインストール</title>
<screen language="bash" linenumbering="unnumbered"># KubeVirt extension
helm install kubevirt-dashboard-extension oci://registry.suse.com/edge/kubevirt-dashboard-extension-chart --version 1.0.0 --namespace cattle-ui-plugin-system

# Akri extension
helm install akri-dashboard-extension oci://registry.suse.com/edge/akri-dashboard-extension-chart --version 1.0.0 --namespace cattle-ui-plugin-system</screen>
<note>
<para>拡張機能は<literal>cattle-ui-plugin-system</literal>ネームスペースにインストールする必要があります。</para>
</note>
<note>
<para>拡張機能がインストールされたら、Rancher Dashboard UIを再ロードする必要があります。</para>
</note>
</section>
<section xml:id="id-installing-with-fleet">
<title>Fleetを使用したインストール</title>
<para>Fleetを使用してダッシュボード拡張機能をインストールするには、カスタムの<literal>fleet.yaml</literal>バンドル設定ファイルを使用して、Gitリポジトリを指す<literal>gitRepo</literal>リソースを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered"># KubeVirt extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: kubevirt-dashboard-extension
  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart
  version: "1.0.0"</screen>
<screen language="yaml" linenumbering="unnumbered"># Akri extension fleet.yaml
defaultNamespace: cattle-ui-plugin-system
helm:
  releaseName: akri-dashboard-extension
  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart
  version: "1.0.0"</screen>
<note>
<para><literal>releaseName</literal>プロパティは必須です。また、拡張機能をui-plugin-operatorで正しくインストールするには、拡張機能の名前に一致する必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: fleet.cattle.io/v1alpha1
metadata:
  name: edge-dashboard-extensions
  namespace: fleet-local
spec:
  repo: https://github.com/suse-edge/fleet-examples.git
  branch: main
  paths:
  - fleets/kubevirt-dashboard-extension/
  - fleets/akri-dashboard-extension/
EOF</screen>
<para>詳細については、<link xl:href="fleet.xml">Fleet</link>のセクションおよび<literal><link
xl:href="https://github.com/suse-edge/fleet-examples">fleet-examples</link></literal>のリポジトリを参照してください。</para>
<para>拡張機能がインストールされると、その拡張機能が<emphasis role="strong">［Extensions
(拡張機能)］</emphasis>セクションの<emphasis role="strong">［Installed
(インストール済み)］</emphasis>タブに表示されます。拡張機能はApps/Marketplace経由でインストールされたものではないため、「<literal>Third-Party
(サードパーティ)</literal>」というラベルが付きます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="installed-dashboard-extensions.png"
width=""/> </imageobject>
<textobject><phrase>インストール済みダッシュボード拡張機能</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
<section xml:id="id-kubevirt-dashboard-extension">
<title>KubeVirtダッシュボード拡張機能</title>
<para>KubeVirt拡張機能は、Rancher Dashboard UIに基本的な仮想マシン管理機能を提供します。その機能については、<link
xl:href="virtualization.xml#kubevirt-dashboard-extension">エッジの仮想化</link>を参照してください。</para>
</section>
<section xml:id="id-akri-dashboard-extension">
<title>Akriダッシュボード拡張機能</title>
<para>Akriは、異種リーフデバイス(IPカメラやUSBデバイスなど)をKubernetesクラスタのリソースとして簡単に公開できると同時に、GPUやFPGAなどの組み込みハードウェアリソースの公開もサポートするKubernetesリソースインタフェースです。Akriは、このようなデバイスにアクセスできるノードを継続的に検出し、それらに基づいてワークロードをスケジュールします。</para>
<para>Akriダッシュボード拡張機能を使用すると、Rancher
Dashboardユーザインタフェースを使用して、リーフデバイスを管理および監視し、デバイスが検出されたらワークロードを実行できます。</para>
<para>拡張機能については、<link
xl:href="akri.xml#akri-dashboard-extension">Akriに関するセクション</link>で詳しく説明されています。</para>
</section>
</chapter>
<chapter xml:id="components-fleet">
<title>Fleet</title>
<para><link
xl:href="https://fleet.rancher.io">Fleet</link>は、ユーザがローカルクラスタをより細かく制御できるようにするとともに、GitOpsを通じて常時監視を行えるようにすることを目的に設計されたコンテナ管理およびデプロイメントエンジンです。Fleetはスケール能力に重点を置いているだけでなく、クラスタに何がインストールされているかを正確に監視するための高度な制御と可視性もユーザに提供します。</para>
<para>Fleetは、生のKubernetes
YAML、Helmチャート、Kustomize、またはこれら3つの組み合わせのGitからデプロイメントを管理できます。ソースにかかわらず、すべてのリソースは動的にHelmチャートに変換され、すべてのリソースをクラスタにデプロイするエンジンとしてHelmが使用されます。その結果、ユーザはクラスタの高度な制御、一貫性、監査能力を実現できます。</para>
<para>Fleetの仕組みについては、<link
xl:href="https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/architecture">こちらのページ</link>を参照してください。</para>
<section xml:id="id-installing-fleet-with-helm">
<title>Helmを使用したFleetのインストール</title>
<para>FleetはRancherにビルトインされていますが、Helmを使用して、スタンドアロンアプリケーションとして任意のKubernetesクラスタに<link
xl:href="https://fleet.rancher.io/installation">インストール</link>することもできます。</para>
</section>
<section xml:id="id-using-fleet-with-rancher">
<title>RancherでのFleetの使用</title>
<para>Rancherは、Fleetを使用してアプリケーションを管理対象クラスタ全体にデプロイします。Fleetを使用した継続的デリバリにより、大量のクラスタで実行されるアプリケーションを管理するために設計された、大規模なGitOpsが導入されます。</para>
<para>FleetはRancherと統合してその一部として機能します。Rancherで管理されるクラスタには、インストール/インポートプロセスの一部としてFleetエージェントが自動的にデプロイされるため、クラスタはすぐにFleetで管理できるようになります。</para>
</section>
<section xml:id="id-accessing-fleet-in-the-rancher-ui">
<title>Rancher UIでのFleetへのアクセス</title>
<para>FleetはRancherにプリインストールされており、Rancher UIの<emphasis role="strong">［Continuous
Delivery
(継続的デリバリ)］</emphasis>オプションで管理されます。継続的デリバリに関する追加情報、およびFleetのトラブルシューティングに関する他のヒントについては、<link
xl:href="https://fleet.rancher.io/troubleshooting">こちら</link>を参照してください。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-dashboard.png" width=""/>
</imageobject>
<textobject><phrase>Fleetダッシュボード</phrase></textobject>
</mediaobject>
</informalfigure>
<para>［Continuous Delivery (継続的デリバリ)］セクションは次の項目で構成されます。</para>
<section xml:id="id-dashboard">
<title>Dashboard (ダッシュボード)</title>
<para>すべてのワークスペースにわたるすべてのGitOpsリポジトリの概要ページ。リポジトリのあるワークスペースのみが表示されます。</para>
</section>
<section xml:id="id-git-repos">
<title>Git repos (Gitリポジトリ)</title>
<para>選択したワークスペース内のGitOpsリポジトリのリスト。ページ上部のドロップダウンリストを使用してアクティブなワークスペースを選択します。</para>
</section>
<section xml:id="id-clusters">
<title>Clusters (クラスタ)</title>
<para>管理対象クラスタのリスト。デフォルトでは、Rancherで管理されているすべてのクラスタが<literal>fleet-default</literal>ワークスペースに追加されます。<literal>fleet-local</literal>ワークスペースにはローカル(管理)クラスタが含まれます。ここから、クラスタを<literal>Pause
(一時停止)</literal>または<literal>Force Update
(強制的に更新)</literal>したり、クラスタを別のワークスペースに移動したりすることができます。クラスタを編集すると、クラスタのグループ化に使用するラベルや注釈を更新できます。</para>
</section>
<section xml:id="id-cluster-groups">
<title>Cluster groups (クラスタグループ)</title>
<para>このセクションでは、セレクタを使用してワークスペース内のクラスタのカスタムグループを作成できます。</para>
</section>
<section xml:id="id-advanced">
<title>Advanced (詳細)</title>
<para>［Advanced (詳細)］セクションでは、ワークスペースやその他の関連するFleetリソースを管理できます。</para>
</section>
</section>
<section xml:id="id-example-of-installing-kubevirt-with-rancher-and-fleet-using-rancher-dashboard">
<title>Rancher Dashboardを使用してRancherおよびFleetとともにKubeVirtをインストールする例</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>fleet.yaml</literal>ファイルを含むGitリポジトリを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: kubevirt
helm:
  chart: "oci://registry.suse.com/edge/kubevirt-chart"
  version: "0.2.4"
  # kubevirt namespace is created by kubevirt as well, we need to take ownership of it
  takeOwnership: true</screen>
</listitem>
<listitem>
<para>Rancher Dashboardで、<emphasis role="strong">☰ &gt; ［Continuous Delivery
(継続的デリバリ)］ &gt; ［Git Repos (Gitリポジトリ)］</emphasis>に移動して、［<literal>Add
Repository (リポジトリの追加)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>リポジトリの作成ウィザードの指示に従ってGitリポジトリを作成します。<emphasis role="strong">［Name
(名前)］</emphasis>、<emphasis role="strong">［Repository URL
(リポジトリのURL)］</emphasis>(前の手順で作成したGitリポジトリを参照)を指定し、適切なブランチまたはリビジョンを選択します。より複雑なリポジトリの場合は、<emphasis
role="strong">［Paths (パス)］</emphasis>を指定して、1つのリポジトリで複数のディレクトリを使用します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo1.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Next (次へ)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>次の手順では、ワークロードをデプロイする場所を定義できます。クラスタの選択では複数の基本オプションがあります。クラスタをまったく選択しないことも、すべてのクラスタを選択することも、特定の管理対象クラスタやクラスタグループ(定義されている場合)を直接選択することもできます。［Advanced
(詳細)］オプションを使用すると、YAML経由でセレクタを直接編集できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-create-repo2.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリの作成2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［<literal>Create
(作成)</literal>］をクリックします。リポジトリが作成されます。今後、ワークロードはリポジトリ定義に一致するクラスタにインストールされ、同期が維持されます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-debugging-and-troubleshooting">
<title>デバッグとトラブルシューティング</title>
<para>ナビゲーションの［Advanced (詳細)］セクションでは、下位レベルのFleetリソースの概要が表示されます。<link
xl:href="https://fleet.rancher.io/ref-bundle-stages">バンドル</link>は、Gitからのリソースのオーケストレーションに使用される内部リソースです。Gitリポジトリがスキャンされると、バンドルが1つ以上生成されます。</para>
<para>特定のリポジトリに関連するバンドルを見つけるには、［Git Repos (Gitリポジトリ)］の［Detail
(詳細)］ページに移動し、［<literal>Bundles (バンドル)</literal>］タブをクリックします。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-bundles.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリバンドル</phrase></textobject>
</mediaobject>
</informalfigure>
<para>クラスタごとに、作成されたBundleDeploymentリソースにバンドルが適用されます。BundleDeploymentの詳細を表示するには、［Git
Repos (Gitリポジトリ)］の［Detail (詳細)］ページの右上にある ［<literal>Graph
(グラフ)</literal>］ボタンをクリックします。<emphasis role="strong">［Repo (リポジトリ)
］&gt;［Bundles
(バンドル)］&gt;［BundleDeployments］</emphasis>のグラフがロードされます。グラフ内のBundleDeploymentをクリックすると詳細が表示され、［<literal>Id
(ID)</literal>］をクリックするとBundleDeployment YAMLが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="fleet-repo-graph.png" width=""/>
</imageobject>
<textobject><phrase>Fleetリポジトリのグラフ</phrase></textobject>
</mediaobject>
</informalfigure>
<para>Fleetのトラブルシューティングのヒントに関する追加情報については、<link
xl:href="https://fleet.rancher.io/troubleshooting">こちら</link>を参照してください。</para>
</section>
<section xml:id="id-fleet-examples">
<title>Fleetの例</title>
<para>Edgeチームは、Fleetを使用してEdgeプロジェクトをインストールする例を含む<link
xl:href="https://github.com/suse-edge/fleet-examples">リポジトリ</link>を維持しています。</para>
<para>Fleetプロジェクトには、<link
xl:href="https://fleet.rancher.io/gitrepo-content">Gitリポジトリ構造</link>のすべてのユースケースをカバーする<link
xl:href="https://github.com/rancher/fleet-examples">fleet-examples</link>リポジトリが含まれています。</para>
</section>
</chapter>
<chapter xml:id="components-slmicro">
<title>SLE Micro</title>
<para><link xl:href="https://documentation.suse.com/sle-micro/5.5/">SLE
Microの公式ドキュメント</link>を参照してください</para>
<blockquote>
<para>SUSE Linux Enterprise Microは、エッジ向けの軽量でセキュアなオペレーティングシステムです。SUSE Linux
Enterprise Microには、SUSE Linux
Enterpriseのエンタープライズ向けに強化されたコンポーネントと、開発者が最新のイミュータブルオペレーティングシステムに求める機能が統合されています。その結果、クラス最高のコンプライアンスを備えた信頼性の高いインフラストラクチャプラットフォームが実現し、使いやすさも向上しています。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-sle-micro">
<title>SUSE EdgeでのSLE Microの用途</title>
<para>SUSEでは、SLE
Microをプラットフォームスタックのベースオペレーティングシステムとして使用します。これにより、構築基盤となる、安全で安定した最小限のベースが提供されます。</para>
<para>SLE
Microでは、独自の方法でファイルシステム(Btrfs)スナップショットを使用しており、アップグレードで問題が発生した場合に簡単にロールバックできます。これにより、問題が発生した場合、物理的にアクセスしなくてもプラットフォーム全体をリモートで安全にアップグレードできます。</para>
</section>
<section xml:id="id-best-practices-2">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media">
<title>インストールメディア</title>
<para>SUSE Edgeは、Edge Image Builder (<xref linkend="components-eib"/>)を使用して、SLE
Microのセルフインストールのインストールイメージを事前設定します。</para>
</section>
<section xml:id="id-local-administration">
<title>ローカル管理</title>
<para>SLE Microには、Webアプリケーションでホストをローカルに管理できるCockpitが付属しています。</para>
<para>このサービスはデフォルトでは無効になっていますが、systemdサービス<literal>cockpit.socket</literal>を有効にすることで開始できます。</para>
</section>
</section>
<section xml:id="id-known-issues-2">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現時点では、SLE Microで利用可能なデスクトップ環境はありませんが、コンテナ化されたソリューションが開発中です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-metal3">
<title>Metal<superscript>3</superscript></title>
<para><link
xl:href="https://metal3.io/">Metal<superscript>3</superscript></link>は、Kubernetesにベアメタルインフラストラクチャ管理機能を提供するCNCFプロジェクトです。</para>
<para>Metal<superscript>3</superscript>は、<link
xl:href="https://www.dmtf.org/standards/redfish">Redfish</link>などのアウトオブバンドプロトコルを介した管理をサポートするベアメタルサーバのライフサイクルを管理するためのKubernetesネイティブリソースを提供します。</para>
<para>また、<link xl:href="https://cluster-api.sigs.k8s.io/">Cluster API
(CAPI)</link>も十分にサポートされており、広く採用されているベンダニュートラルなAPIを使用して、複数のインフラストラクチャプロバイダにわたってインフラストラクチャリソースを管理できます。</para>
<section xml:id="id-how-does-suse-edge-use-metal3">
<title>SUSE EdgeでのMetal3の用途</title>
<para>この方法は、ターゲットハードウェアがアウトオブバンド管理をサポートしていて、完全に自動化されたインフラストラクチャ管理フローが望まれるシナリオで役立ちます。</para>
<para>この方法では宣言型APIが提供されており、このAPIを使用することで、検査、クリーニング、プロビジョニング/プロビジョニング解除の自動化を含む、ベアメタルサーバのインベントリと状態の管理が可能になります。</para>
</section>
<section xml:id="id-known-issues-3">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>アップストリームの<link
xl:href="https://github.com/metal3-io/ip-address-manager">IPアドレス管理コントローラ</link>は、SUSEが選択したネットワーク設定ツールとの互換性がまだないため、現在はサポートされていません。</para>
</listitem>
<listitem>
<para>関連して、IPAMリソースとMetal3DataTemplateのnetworkDataフィールドはサポートされていません。</para>
</listitem>
<listitem>
<para>redfish-virtualmediaを介したデプロイメントのみが現在サポートされています。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-eib">
<title>Edge Image Builder</title>
<para><link
xl:href="https://github.com/suse-edge/edge-image-builder">公式リポジトリ</link>を参照してください。</para>
<para>Edge Image Builder (EIB)は、マシンをブートストラップするためのCustomized, Ready-to-Boot
(CRB)ディスクイメージの生成を効率化するツールです。これらのイメージにより、SUSEソフトウェアスタック全体を単一のイメージでエンドツーエンドにデプロイできます。</para>
<para>EIBはあらゆるプロビジョニングシナリオ向けのCRBイメージを作成できますが、EIBが非常に大きな価値を発揮するのは、ネットワークが制限されているか、完全に分離されているエアギャップデプロイメントにおいてです。</para>
<section xml:id="id-how-does-suse-edge-use-edge-image-builder">
<title>SUSE EdgeでのEdge Image Builderでの用途</title>
<para>SUSE Edgeでは、さまざまなシナリオ用にカスタマイズされたSLE
Microイメージをシンプルかつ迅速に設定するためにEIBを使用します。これらのシナリオには、以下を使用する仮想マシンとベアメタル
マシンのブートストラップが含まれます。</para>
<itemizedlist>
<listitem>
<para>K3s/RKE2 Kubernetesの完全なエアギャップデプロイメント(シングルノードとマルチノード)</para>
</listitem>
<listitem>
<para>HelmチャートとKubernetesマニフェストの完全なエアギャップデプロイメント</para>
</listitem>
<listitem>
<para>Elemental APIを介したRancherへの登録</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript></para>
</listitem>
<listitem>
<para>カスタマイズされたネットワーキング(静的 IP、ホスト名、VLAN、ボンディングなど)</para>
</listitem>
<listitem>
<para>カスタマイズされたオペレーティングシステム設定(ユーザ、グループ、パスワード、SSHキー、プロキシ、NTP、カスタムSSL証明書など)</para>
</listitem>
<listitem>
<para>ホストレベルおよびサイドロードRPMパッケージのエアギャップインストール(依存関係の解決を含む)</para>
</listitem>
<listitem>
<para>OS管理のためのSUSE Managerへの登録</para>
</listitem>
<listitem>
<para>組み込みコンテナイメージ</para>
</listitem>
<listitem>
<para>カーネルコマンドライン引数</para>
</listitem>
<listitem>
<para>ブート時に有効化/無効化されるsystemdユニット</para>
</listitem>
<listitem>
<para>手動タスク用のカスタムスクリプトとファイル</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-started">
<title>はじめに</title>
<para>Edge Image Builderの使用とテストに関する包括的なドキュメントについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs">こちら</link>を参照してください。</para>
<para>また、Edge Image Builderのクイックスタートガイド(<xref
linkend="quickstart-eib"/>)では、基本的なデプロイメントシナリオを説明しています。</para>
</section>
<section xml:id="id-known-issues-4">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>EIBは、Helmチャートをテンプレート化してテンプレート内のすべてのイメージを解析することで、Helmチャートをエアギャップ化します。Helmチャートですべてのイメージをテンプレート内に保持せず、代わりにイメージをサイドロードする場合、EIBではそれらのイメージを自動的にエアギャップ化できません。これを解決するには、検出されないイメージを定義ファイルの<literal>embeddedArtifactRegistry</literal>セクションに手動で追加します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-nmc">
<title>Edgeネットワーキング</title>
<para>このセクションでは、SUSE Edgeソリューションにおけるネットワーク設定へのアプローチについて説明します。宣言的な方法でSLE
Micro上でNetworkManagerを設定する方法を示し、関連ツールの統合方法について説明します。</para>
<section xml:id="id-overview-of-networkmanager">
<title>NetworkManagerの概要</title>
<para>NetworkManagerは、プライマリネットワーク接続と他の接続インタフェースを管理するツールです。</para>
<para>NetworkManagerは、ネットワーク設定を、望ましい状態が含まれる接続ファイルとして保存します。これらの接続は、<literal>/etc/NetworkManager/system-connections/</literal>ディレクトリにファイルとして保存されます。</para>
<para>NetworkManagerの詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-nm-configuration.html">アップストリームのSLE
Microのドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-overview-of-nmstate">
<title>nmstateの概要</title>
<para>nmstateは広く採用されているライブラリ(CLIツールが付属)であり、定義済みスキーマを使用したネットワーク設定用の宣言型APIを提供します。</para>
<para>nmstateの詳細については、<link
xl:href="https://nmstate.io/">アップストリームドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-enter-networkmanager-configurator-nmc">
<title>NetworkManager Configurator (nmc)の概要</title>
<para>SUSE Edgeで利用可能なネットワークのカスタマイズオプションは、NetworkManager Configurator
(短縮名は<emphasis>nmc</emphasis>)と呼ばれるCLIツールを使用して実行します。このツールはnmstateライブラリによって提供される機能を利用しているため、静的IPアドレス、DNSサーバ、VLAN、ボンディング、ブリッジなどを完全に設定できます。このツールを使用して、事前定義された望ましい状態からネットワーク設定を生成し、その設定を多数のノードに自動的に適用できます。</para>
<para>NetworkManager Configurator (nmc)の詳細については、<link
xl:href="https://github.com/suse-edge/nm-configurator">アップストリームリポジトリ</link>を参照してください。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-networkmanager-configurator">
<title>SUSE EdgeでのNetworkManager Configuratorの用途</title>
<para>SUSE
Edgeは、<emphasis>nmc</emphasis>を利用して次のようなさまざまなプロビジョニングモデルでネットワークをカスタマイズします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングシナリオにおけるカスタムネットワーク設定(<xref linkend="quickstart-metal3"/>)</para>
</listitem>
<listitem>
<para>イメージベースのプロビジョニング シナリオにおける宣言的な静的設定(<xref linkend="quickstart-eib"/>)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-configuring-with-edge-image-builder">
<title>Edge Image Builderを使用した設定</title>
<para>Edge Image Builder
(EIB)は、1つのOSイメージで複数のホストを設定できるツールです。このセクションでは、宣言型アプローチを使用して、どのように目的のネットワーク状態を記述するかと、それらがどのように各NetworkManager接続に変換され、プロビジョニングプロセス中に適用されるかを示します。</para>
<section xml:id="id-prerequisites-4">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLES 15 SP5またはopenSUSE Leap 15.5を実行しているx86_64物理ホスト(または仮想マシン)</para>
</listitem>
<listitem>
<para>利用可能なコンテナランタイム(Podmanなど)</para>
</listitem>
<listitem>
<para>SLE Micro 5.5のRAWイメージのコピー(<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあります)</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-getting-the-edge-image-builder-container-image">
<title>Edge Image Builderのコンテナイメージの取得</title>
<para>EIBのコンテナイメージは一般に公開されており、次のコマンドを実行してSUSE Edgeレジストリからダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered">podman pull registry.suse.com/edge/edge-image-builder:1.0.2</screen>
</section>
<section xml:id="image-config-dir-creation">
<title>イメージ設定ディレクトリの作成</title>
<para>まず設定ディレクトリを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR/base-images</screen>
<para>ダウンロードしたゴールデンイメージのコピーを確実に設定ディレクトリに移動します。</para>
<screen language="shell" linenumbering="unnumbered">mv /path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-GM.raw $CONFIG_DIR/base-images/</screen>
<blockquote>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
</section>
<section xml:id="id-creating-the-image-definition-file">
<title>イメージ定義ファイルの作成</title>
<para>定義ファイルには、Edge Image Builderがサポートする設定オプションの大部分を記述します。</para>
<para>OSイメージの非常に基本的な定義ファイルから開始しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/definition.yaml
apiVersion: 1.0
image:
  arch: x86_64
  imageType: raw
  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw
  outputImageName: modified-image.raw
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<para><literal>image</literal>セクションは必須で、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。<literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定を含めます。</para>
<blockquote>
<note>
<para><literal>openssl passwd -6
&lt;password&gt;</literal>を実行して、独自の暗号化パスワードを自由に使用してください。</para>
</note>
</blockquote>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
</section>
<section xml:id="default-network-definition">
<title>ネットワーク設定の定義</title>
<para>先ほど作成したイメージ定義ファイルには、望ましいネットワーク設定が含まれていません。そこで、<literal>network/</literal>という特別なディレクトリの下にその設定を入力します。では、作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network</screen>
<para>前述のように、NetworkManager Configurator
(<emphasis>nmc</emphasis>)ツールでは、事前定義されたスキーマの形式での入力が必要です。さまざまなネットワーキングオプションの設定方法については、<link
xl:href="https://nmstate.io/examples.html">アップストリームのNMStateの例のドキュメント</link>を参照してください。</para>
<para>このガイドでは、次の3つの異なるノードでネットワーキングを設定する方法について説明します。</para>
<itemizedlist>
<listitem>
<para>2つのEthernetインタフェースを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークボンディングを使用するノード</para>
</listitem>
<listitem>
<para>ネットワークブリッジを使用するノード</para>
</listitem>
</itemizedlist>
<warning>
<para>特にKubernetesクラスタを設定する場合、まったく異なるネットワークセットアップを運用ビルドで使用することは推奨されません。ネットワーキング設定は通常、特定のクラスタ内のノード間、または少なくともロール間で同種にすることをお勧めします。このガイドにはさまざまな異なるオプションが含まれていますが、これは参考例として提供することのみを目的としています。</para>
</warning>
<blockquote>
<note>
<para>以下では、IPアドレス範囲<literal>192.168.122.1/24</literal>を使用するデフォルトの<literal>libvirt</literal>ネットワークを想定しています。ご自身の環境でこの範囲が異なる場合は、適宜調整してください。</para>
</note>
</blockquote>
<para><literal>node1.suse.com</literal>という名前の最初のノードに対して、望ましい状態を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node1.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: eth0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: eth0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E1
    ipv4:
      address:
        - ip: 192.168.122.50
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
  - name: eth3
    type: ethernet
    state: down
    mac-address: 34:8A:B1:4B:16:E2
    ipv4:
      address:
        - ip: 192.168.122.55
          prefix-length: 24
      dhcp: false
      enabled: true
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、2つのEthernetインタフェース(eth0とeth3)、要求されたIPアドレス、ルーティング、およびDNS解決の望ましい状態を定義しています。</para>
<warning>
<para>必ず、すべてのEthernetインタフェースのMACアドレスを記述してください。これらのMACアドレスは、プロビジョニングプロセス中にノードの識別子として使用され、どの設定を適用すべきかを判断するのに役立ちます。このようにして、1つのISOまたはRAWイメージを使用して複数のノードを設定できます。</para>
</warning>
<para>次は、<literal>node2.suse.com</literal>という名前の2つ目のノードです。このノードではネットワークボンディングを使用します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node2.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: bond99
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: bond99
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: bond99
    type: bond
    state: up
    ipv4:
      address:
        - ip: 192.168.122.60
          prefix-length: 24
      enabled: true
    link-aggregation:
      mode: balance-rr
      options:
        miimon: '140'
      port:
        - eth0
        - eth1
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E3
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: eth1
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E4
    ipv4:
      enabled: false
    ipv6:
      enabled: false
EOF</screen>
<para>この例では、IPアドレス指定を有効にしていない2つのEthernetインタフェース(eth0とeth1)の望ましい状態と、ラウンドロビンポリシーによるボンディング、およびネットワークトラフィックを転送するために使用する各アドレスを定義します。</para>
<para>最後に、3つ目となる、望ましい状態の最後のファイルを作成します。これはネットワークブリッジを利用し、<literal>node3.suse.com</literal>という名前です。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/node3.suse.com.yaml
routes:
  config:
    - destination: 0.0.0.0/0
      metric: 100
      next-hop-address: 192.168.122.1
      next-hop-interface: linux-br0
      table-id: 254
    - destination: 192.168.122.0/24
      metric: 100
      next-hop-address:
      next-hop-interface: linux-br0
      table-id: 254
dns-resolver:
  config:
    server:
      - 192.168.122.1
      - 8.8.8.8
interfaces:
  - name: eth0
    type: ethernet
    state: up
    mac-address: 34:8A:B1:4B:16:E5
    ipv4:
      enabled: false
    ipv6:
      enabled: false
  - name: linux-br0
    type: linux-bridge
    state: up
    ipv4:
      address:
        - ip: 192.168.122.70
          prefix-length: 24
      dhcp: false
      enabled: true
    bridge:
      options:
        group-forward-mask: 0
        mac-ageing-time: 300
        multicast-snooping: true
        stp:
          enabled: true
          forward-delay: 15
          hello-time: 2
          max-age: 20
          priority: 32768
      port:
        - name: eth0
          stp-hairpin-mode: false
          stp-path-cost: 100
          stp-priority: 32
EOF</screen>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── network/
│   │── node1.suse.com.yaml
│   │── node2.suse.com.yaml
│   └── node3.suse.com.yaml
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
<blockquote>
<note>
<para><literal>network/</literal>ディレクトリにあるファイル名は意図的なものです。これらの名前は、プロビジョニングプロセス中に設定されるホスト名に対応しています。</para>
</note>
</blockquote>
</section>
<section xml:id="id-building-the-os-image">
<title>OSイメージの構築</title>
<para>これで必要な設定はすべて完了したので、次のコマンドを実行するだけでイメージを構築できます。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="shell" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Embedded Artifact Registry ... [SKIPPED]
Keymap ....................... [SUCCESS]
Kubernetes ................... [SKIPPED]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>上のスニペットから<literal>Network</literal>コンポーネントが正常に設定されていることがわかるので、エッジノードのプロビジョニングに進むことができます。</para>
<blockquote>
<note>
<para>ログファイル(<literal>network-config.log</literal>)とそれぞれのNetworkManager接続ファイルは、イメージ実行のタイムスタンプ付きディレクトリの下にある、結果の<literal>_build</literal>ディレクトリで検査できます。</para>
</note>
</blockquote>
</section>
<section xml:id="id-provisioning-the-edge-nodes">
<title>エッジノードのプロビジョニング</title>
<para>作成されたRAWイメージをコピーしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir edge-nodes &amp;&amp; cd edge-nodes
for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done</screen>
<para>構築されたイメージを4回コピーしましたが、3つのノードのネットワーク設定しか指定していません。これは、どの目的の設定にも一致しないノードをプロビジョニングするとどうなるかも紹介したいためです。</para>
<blockquote>
<note>
<para>このガイドでは、ノードのプロビジョニングの例に仮想化を使用します。必要な拡張機能がBIOSで有効になっていることを確認してください(詳細については、<link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照してください)。</para>
</note>
</blockquote>
<para><literal>virt-install</literal>を使用し、コピーしたRAWディスクを使用して仮想マシンを作成します。各仮想マシンは10GBのRAMと6個のvCPUを使用します。</para>
<section xml:id="id-provisioning-the-first-node">
<title>1つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 --virt-type kvm --import</screen>
<blockquote>
<note>
<para>上記で説明した望ましい状態のMACアドレスと同じMACアドレスを持つネットワークインタフェースを作成することが重要です。</para>
</note>
</blockquote>
<para>操作が完了すると、次のような内容が表示されます。</para>
<screen language="console" linenumbering="unnumbered">Starting install...
Creating domain...

Running text console command: virsh --connect qemu:///system console node1
Connected to domain 'node1'
Escape character is ^] (Ctrl + ])


Welcome to SUSE Linux Enterprise Micro 5.5  (x86_64) - Kernel 5.14.21-150500.55.19-default (ttyS0).

SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)
SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)
SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)
SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)
eth0: 192.168.122.50
eth1:


Configured with the Edge Image Builder
Activate the web console with: systemctl enable --now cockpit.socket

node1 login:</screen>
<para>これで、<literal>root:eib</literal>の資格情報ペアを使用してログインできます。ここで提示されている<literal>virsh
console</literal>よりもSSHでホストに接続したい場合は、SSHで接続することもできます。</para>
<para>ログインしたら、すべての設定が完了していることを確認しましょう。</para>
<para>ホスト名が適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # hostnamectl
 Static hostname: node1.suse.com
 ...</screen>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip r
default via 192.168.122.1 dev eth0 proto static metric 100
192.168.122.0/24 dev eth0 proto static scope link metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.2 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.4 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1002ms
rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms</screen>
<para>2つのEthernetインタフェースが設定されていて、そのうちの1つだけがアクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute eth0
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    altname ens3

node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
<para>2つ目のインタフェースが、目的のネットワーキング状態で指定されている定義済みの<literal>eth3</literal>ではなく、<literal>eth1</literal>になっていることがわかります。これは、NetworkManager
Configurator
(<emphasis>nmc</emphasis>)が、MACアドレス<literal>34:8a:b1:4b:16:e2</literal>を持つNICにOSによって別の名前が付けられていることを検出することができ、それに応じて設定を調整するためです。</para>
<para>プロビジョニングのCombustionのフェーズを検査して、この調整が実際に行われたことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node1:~ # journalctl -u combustion | grep nmc
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth0'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface 'eth3'...
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' instead of the preconfigured 'eth3'
Apr 23 09:20:19 localhost.localdomain combustion[1360]: [2024-04-23T09:20:19Z INFO  nmc] Successfully applied config</screen>
<para>続いて残りのノードをプロビジョニングしますが、ここでは最終的な設定の違いのみを示します。これからプロビジョニングするすべてのノードに対して、上記のチェックのいずれか、またはすべてを自由に適用してください。</para>
</section>
<section xml:id="id-provisioning-the-second-node">
<title>2つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node2 --ram 10000 --vcpus 6 --disk path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがボンディングされたインタフェースを使用しているかどうかを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: eth1: &lt;BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master bond99 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr 34:8a:b1:4b:16:e4
    altname enp0s3
    altname ens3
4: bond99: &lt;BROADCAST,MULTICAST,MASTER,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute bond99
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでボンディングが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # ip r
default via 192.168.122.1 dev bond99 proto static metric 100
192.168.122.0/24 dev bond99 proto static scope link metric 100
192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 metric 300</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME    UUID                                  TYPE      DEVICE  FILENAME
bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  /etc/NetworkManager/system-connections/bond99.nmconnection
eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-third-node">
<title>3つ目のノードのプロビジョニング</title>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node3 --ram 10000 --vcpus 6 --disk path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがネットワークブリッジを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master linux-br0 state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
3: linux-br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute linux-br0
       valid_lft forever preferred_lft forever</screen>
<para>ルーティングでブリッジが使用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # ip r
default via 192.168.122.1 dev linux-br0 proto static metric 100
192.168.122.0/24 dev linux-br0 proto static scope link metric 100
192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 metric 425</screen>
<para>静的な接続ファイルが適切に利用されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME       UUID                                  TYPE      DEVICE     FILENAME
linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  /etc/NetworkManager/system-connections/linux-br0.nmconnection
eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       /etc/NetworkManager/system-connections/eth0.nmconnection</screen>
</section>
<section xml:id="id-provisioning-the-fourth-node">
<title>4つ目のノードのプロビジョニング</title>
<para>最後に、事前定義されたどの設定ともMACアドレスが一致しないノードをプロビジョニングします。このような場合は、DHCPをデフォルトにしてネットワークインタフェースを設定します。</para>
<para>仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node4 --ram 10000 --vcpus 6 --disk path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>仮想マシンが稼働したら、このノードがそのネットワークインタフェースにランダムなIPアドレスを使用していることを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3542sec preferred_lft 3542sec
    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute
       valid_lft forever preferred_lft forever</screen>
<para>nmcがこのノードに静的な設定を適用できなかったことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl -u combustion | grep nmc
Apr 23 12:15:45 localhost.localdomain combustion[1357]: [2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the preconfigured hosts match local NICs</screen>
<para>EthernetインタフェースがDHCPを介して設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # journalctl | grep eth0
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7801] manager: (eth0): new Ethernet device (/org/freedesktop/NetworkManager/Devices/2)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7802] device (eth0): state change: unmanaged -&gt; unavailable (reason 'managed', sys-iface-state: 'external')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7929] device (eth0): carrier: link connected
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7931] device (eth0): state change: unavailable -&gt; disconnected (reason 'carrier-changed', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7944] device (eth0): Activation: starting connection 'Wired Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7945] device (eth0): state change: disconnected -&gt; prepare (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7947] device (eth0): state change: prepare -&gt; config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7953] device (eth0): state change: config -&gt; ip-config (reason 'none', sys-iface-state: 'managed')
Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout in 90 seconds)
Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: &lt;info&gt;  [1713874533.1272] dhcp4 (eth0): state changed new lease, address=192.168.122.86

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME              UUID                                  TYPE      DEVICE  FILENAME
Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    /var/run/NetworkManager/system-connections/default_connection.nmconnection</screen>
</section>
</section>
<section xml:id="id-unified-node-configurations">
<title>統合されたノード設定</title>
<para>既知のMACアドレスに依存できない場合もあります。このような場合は、いわゆる<emphasis>「統合設定」</emphasis>を選択できます。これにより、<literal>_all.yaml</literal>ファイルで設定を指定し、プロビジョニングされたノードすべてに適用することができます。</para>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、2つのEthernetインタフェース(eth0とeth1)の望ましい状態を定義します。一方ではDHCPを使用し、他方には静的IPアドレスを割り当てます。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt;- EOF &gt; $CONFIG_DIR/network/_all.yaml
interfaces:
- name: eth0
  type: ethernet
  state: up
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
- name: eth1
  type: ethernet
  state: up
  ipv4:
    address:
    - ip: 10.0.0.1
      prefix-length: 24
    enabled: true
    dhcp: false
  ipv6:
    enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric 100
10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.46) 56(84) bytes of data.
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 time=14.3 ms
64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 time=14.2 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms</screen>
<para>Ethernetインタフェースが設定され、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff
    altname enp1s0
    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0
       valid_lft 3505sec preferred_lft 3505sec
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff
    altname enp7s0
    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1
       valid_lft forever preferred_lft forever

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection
eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    /etc/NetworkManager/system-connections/eth1.nmconnection

localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70

[ipv4]
dhcp-client-id=mac
dhcp-send-hostname=true
dhcp-timeout=2147483647
ignore-auto-dns=false
ignore-auto-routes=false
method=auto
never-default=false

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled

localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
id=eth1
interface-name=eth1
type=802-3-ethernet
uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e

[ipv4]
address0=10.0.0.1/24
dhcp-timeout=2147483647
method=manual

[ipv6]
addr-gen-mode=0
dhcp-timeout=2147483647
method=disabled</screen>
</section>
<section xml:id="id-custom-network-configurations">
<title>カスタムネットワーク設定</title>
<para>ここまでは、NetworkManager Configuratorを利用した、Edge Image
Builderのデフォルトのネットワーク設定について説明してきました。一方で、カスタムスクリプトを使用してネットワーク設定を変更するオプションもあります。このオプションは非常に柔軟性が高く、MACアドレスにも依存しませんが、1つのイメージで複数のノードをブートストラップする場合に使用してもあまり便利ではないという制限があります。</para>
<blockquote>
<note>
<para><literal>/network</literal>ディレクトリにある、望ましいネットワーク状態を記述したファイルを介して、デフォルトのネットワーク設定を使用することをお勧めします。カスタムスクリプトを選択するのは、デフォルト設定の動作がユースケースに当てはまらない場合のみにしてください。</para>
</note>
</blockquote>
<para>異なる設定構造を使用して、エッジノードを構築およびプロビジョニングします。<xref
linkend="image-config-dir-creation"/>から<xref
linkend="default-network-definition"/>のすべての手順に従います。</para>
<para>この例では、プロビジョニングされたすべてのノードで<literal>eth0</literal>インタフェースに静的設定を適用し、NetworkManagerによって自動的に作成された有線接続を削除して無効にするカスタムスクリプトを作成します。これは、クラスタ内のすべてのノードに同一のネットワーキング設定を確実に適用したい場合に便利です。その結果、イメージの作成前に各ノードのMACアドレスを気にする必要がなくなります。</para>
<para>まず、<literal>/custom/files</literal>ディレクトリに接続ファイルを保存しましょう。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/custom/files

cat &lt;&lt; EOF &gt; $CONFIG_DIR/custom/files/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled
EOF</screen>
<para>静的設定が作成されたので、カスタムネットワークスクリプトも作成します。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p $CONFIG_DIR/network

cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/configure-network.sh
#!/bin/bash
set -eux

# Remove and disable wired connections
mkdir -p /etc/NetworkManager/conf.d/
printf "[main]\nno-auto-default=*\n" &gt; /etc/NetworkManager/conf.d/no-auto-default.conf
rm -f /var/run/NetworkManager/system-connections/* || true

# Copy pre-configured network configuration files into NetworkManager
mkdir -p /etc/NetworkManager/system-connections/
cp eth0.nmconnection /etc/NetworkManager/system-connections/
chmod 600 /etc/NetworkManager/system-connections/*.nmconnection
EOF

chmod a+x $CONFIG_DIR/network/configure-network.sh</screen>
<blockquote>
<note>
<para>nmcのバイナリはこれまで同様にデフォルトで含まれるため、必要に応じて<literal>configure-network.sh</literal>スクリプトで使用することもできます。</para>
</note>
</blockquote>
<warning>
<para>カスタムスクリプトは常に設定ディレクトリの<literal>/network/configure-network.sh</literal>で提供する必要があります。このファイルが存在する場合、他のファイルはすべて無視されます。YAML形式の静的設定とカスタムスクリプトの両方を同時に使用してネットワークを設定することはできません。</para>
</warning>
<para>この時点では、設定ディレクトリは次のようになっているはずです。</para>
<screen language="console" linenumbering="unnumbered">├── definition.yaml
├── custom/
│   └── files/
│       └── eth0.nmconnection
├── network/
│   └── configure-network.sh
└── base-images/
    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file definition.yaml</screen>
<para>イメージが正常に構築されたら、それを使用して仮想マシンを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">virt-install --name node1 --ram 10000 --vcpus 6 --disk path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics none --console pty,target_type=serial --network default --virt-type kvm --import</screen>
<para>プロビジョニングプロセスには数分かかる場合があります。終了したら、指定された資格情報でシステムにログインします。</para>
<para>ルーティングが適切に設定されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip r
default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric 100
192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric 100</screen>
<para>インターネット接続が利用できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ping google.com
PING google.com (142.250.72.78) 56(84) bytes of data.
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 time=13.6 ms
64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 time=13.6 ms
^C
--- google.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms</screen>
<para>接続ファイルを使用してEthernetインタフェースが静的に設定されていて、アクティブであることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">localhost:~ # ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff
    altname enp0s2
    altname ens2
    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic noprefixroute eth0

localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show
NAME  UUID                                  TYPE      DEVICE  FILENAME
eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    /etc/NetworkManager/system-connections/eth0.nmconnection

localhost:~ # cat  /etc/NetworkManager/system-connections/eth0.nmconnection
[connection]
autoconnect=true
autoconnect-slaves=-1
autoconnect-retries=1
id=eth0
interface-name=eth0
type=802-3-ethernet
uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70
wait-device-timeout=60000

[ipv4]
dhcp-timeout=2147483647
method=auto

[ipv6]
addr-gen-mode=eui64
dhcp-timeout=2147483647
method=disabled</screen>
</section>
</section>
</chapter>
<chapter xml:id="components-elemental">
<title>Elemental</title>
<para>Elementalは、Kubernetesを使用した完全にクラウドネイティブな集中型のOS管理を可能にするソフトウェアスタックです。Elementalスタックは、Rancher自体またはエッジノード上に存在する多数のコンポーネントで構成されます。中核となるコンポーネントは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">elemental-operator</emphasis> -
Rancher上に存在し、クライアントからの登録リクエストを処理するコアオペレータ。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-register</emphasis> -
エッジノード上で動作し、<literal>elemental-operator</literal>を介して登録できるようにするクライアント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">elemental-system-agent</emphasis> -
エッジノードに存在するエージェント。その設定は<literal>elemental-register</literal>から提供され、<literal>rancher-system-agent</literal>を設定するための<literal>plan</literal>を受け取ります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">rancher-system-agent</emphasis> -
エッジノードが完全に登録された後に、<literal>elemental-system-agent</literal>から処理を引き継ぎ、Rancher
Managerからの他の<literal>plans</literal>を待機します(Kubernetesのインストールなど)。</para>
</listitem>
</itemizedlist>
<para>Elemental、およびElementalとRancherとの関係の詳細については、<link
xl:href="https://elemental.docs.rancher.com/">Elementalのアップストリームドキュメント</link>を参照してください。</para>
<section xml:id="id-how-does-suse-edge-use-elemental">
<title>SUSE EdgeでのElementalの用途</title>
<para>SUSEでは、Metal<superscript>3</superscript>を選択できないリモート
デバイス(たとえば、BMCがない、デバイスがNATゲートウェイの背後にあるなど)の管理にElementalの一部を使用しています。このツールにより、オペレータは、デバイスがいつどこに配置されるかがわかる前に、ラボでデバイスをブートストラップできます。すなわち、<literal>elemental-register</literal>と<literal>elemental-system-agent</literal>コンポーネントを利用して、「Phone
Home」ネットワークプロビジョニングのユースケースでSLE MicroホストをRancherにオンボードできます。Edge Image Builder
(EIB)を使用してデプロイメントイメージを作成する場合、EIBの設定ディレクトリで登録設定を指定することで、Rancherを使用してElemental経由で自動登録を行うことができます。</para>
<note>
<para>SUSE Edge 3.0では、Elementalのオペレーティング システム管理の側面を利用して<emphasis
role="strong">「いない」</emphasis>ため、Rancher経由でオペレーティング
システムのパッチを管理することはできません。SUSE Edgeでは、Elementalツールを使用してデプロイメント
イメージを構築する代わりに、登録設定を使用するEdge Image Builderツールを使用します。</para>
</note>
</section>
<section xml:id="id-best-practices-3">
<title>ベストプラクティス</title>
<section xml:id="id-installation-media-2">
<title>インストールメディア</title>
<para>「Phone
Homeネットワークプロビジョニング」のデプロイメントフットプリントでElementalを利用してRancherに登録可能なデプロイメントイメージを構築する場合、SUSE
Edgeでは、Elementalを使用したリモートホストのオンボーディング(<xref
linkend="quickstart-elemental"/>)のクイックスタートで詳しく説明されている手順に従う方法をお勧めします。</para>
</section>
<section xml:id="id-labels">
<title>ラベル</title>
<para>Elementalは、<literal>MachineInventory</literal>
CRDを使用してインベントリを追跡し、インベントリを選択する方法を提供します。たとえば、Kubernetesクラスタのデプロイ先のマシンをラベルに基づいて選択できます。これにより、ユーザはハードウェアを購入する前に、インフラストラクチャのニーズの(すべてではないにしても)ほとんどを事前に定義しておくことができます。また、ノードはその各インベントリオブジェクトのラベルを追加/削除できるので(<literal>elemental-register</literal>を、追加のフラグ<literal>--label
"FOO=BAR "</literal>を指定して再実行する)、ノードがブートされた場所を検出してRancherに知らせるスクリプトを作成できます。</para>
</section>
</section>
<section xml:id="id-known-issues-5">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>現在のところ、Elemental UIは、インストールメディアの構築方法を認識したり、「Elemental
Teal」以外のオペレーティングシステムを更新したりすることはできません。これは将来のリリースで対応予定です。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-akri">
<title>Akri</title>
<para>Akriは、リーフデバイスを検出してKubernetesネイティブリソースとして提供することを目的としたCNCF-Sandboxプロジェクトです。また、検出された各デバイスに対してPodやジョブをスケジュールすることもできます。デバイスはノードローカルでもネットワーク接続されていてもよく、さまざまなプロトコルを使用できます。</para>
<para>Akriのアップストリームドキュメントについては、<link
xl:href="https://docs.akri.sh">https://docs.akri.sh</link>を参照してください。</para>
<section xml:id="id-how-does-suse-edge-use-akri">
<title>SUSE EdgeでのAkriの用途</title>
<warning>
<para>Akriは現在、SUSE Edgeスタックでの技術プレビュー中です。</para>
</warning>
<para>Akriは、リーフデバイスに対するワークロードの検出とスケジューリングが必要な場合はいつでも、Edgeスタックの一部として利用できます。</para>
<section xml:id="id-installing-akri">
<title>Akriのインストール</title>
<para>AkriはEdge
Helmリポジトリ内でHelmチャートとして利用できます。Akriを設定するための推奨方法は、指定したHelmチャートを使用してさまざまなコンポーネント(エージェント、コントローラ、ディスカバリハンドラ)をデプロイし、好みのデプロイメントメカニズムを使用してAkriの設定CRDをデプロイすることです。</para>
</section>
<section xml:id="id-configuring-akri">
<title>Akriの設定</title>
<para>Akriは、<literal>akri.sh/Configuration</literal>オブジェクトを使用して設定します。このオブジェクトは、デバイスの検出方法、および一致するデバイスが検出されたときの処理に関するすべての情報を取得します。</para>
<para>以下に、設定例の内訳を示し、すべてのフィールドについて説明します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: akri.sh/v0
kind: Configuration
metadata:
  name: sample-configuration
spec:</screen>
<para>次の部分では、ディスカバリハンドラの設定を記述しています。ディスカバリハンドラの名前を指定する必要があります(Akriのチャートの一部として利用可能なハンドラは、<literal>udev</literal>、<literal>opcua</literal>、<literal>onvif</literal>です)。<literal>discoveryDetails</literal>はハンドラ固有です。設定方法については、ハンドラのドキュメントを参照してください。</para>
<screen language="yaml" linenumbering="unnumbered">  discoveryHandler:
    name: debugEcho
    discoveryDetails: |+
      descriptions:
        - "foo"
        - "bar"</screen>
<para>次のセクションでは、検出された各デバイスに対してデプロイするワークロードを定義します。この例では、<literal>brokerPodSpec</literal>で<literal>Pod</literal>設定の最小バージョンが示されています。ここでは、Podの仕様の通常のフィールドをすべて使用できます。また、<literal>resources</literal>セクションに、デバイスを要求するためのAkri固有の構文も示されています。</para>
<para>または、Podの代わりにJobを使用することもできます。その場合は、代わりに<literal>brokerJobSpec</literal>キーを使用し、そこにJobの仕様部分を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerSpec:
    brokerPodSpec:
      containers:
      - name: broker-container
        image: rancher/hello-world
        resources:
          requests:
            "{{PLACEHOLDER}}" : "1"
          limits:
            "{{PLACEHOLDER}}" : "1"</screen>
<para>次の2つのセクションは、ブローカごとにサービスをデプロイするようにAkriを設定するか(<literal>instanceService</literal>)、またはすべてのブローカを指すようにAkriを設定する(<literal>configurationService</literal>)方法を示しています。これらには、通常のサービスに関連する要素がすべて含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">  instanceServiceSpec:
    type: ClusterIp
    ports:
    - name: http
      port: 80
      protocol: tcp
      targetPort: 80
  configurationServiceSpec:
    type: ClusterIp
    ports:
    - name: https
      port: 443
      protocol: tcp
      targetPort: 443</screen>
<para><literal>brokerProperties</literal>フィールドは、検出されたデバイスを要求するPodに追加の環境変数として公開されるキー/値ストアです。</para>
<para>capacityは、検出されたデバイスの許容される同時ユーザ数です。</para>
<screen language="yaml" linenumbering="unnumbered">  brokerProperties:
    key: value
  capacity: 1</screen>
</section>
<section xml:id="id-writing-and-deploying-additional-discovery-handlers">
<title>追加のディスカバリハンドラの記述とデプロイ</title>
<para>デバイスで使用されているプロトコルが既存のディスカバリハンドラでカバーされていない場合は、<link
xl:href="https://docs.akri.sh/development/handler-development">こちらのガイド</link>を使用して、独自に記述できます。</para>
</section>
<section xml:id="akri-dashboard-extension">
<title>Akri Rancher Dashboard拡張機能</title>
<para>Akriダッシュボード拡張機能を使用すると、Rancher
Dashboardユーザインタフェースを使用して、リーフデバイスを管理および監視し、デバイスが検出されたらワークロードを実行できます。</para>
<para>拡張機能をインストールしたら、クラスタエクスプローラを使用してAkri対応の管理対象クラスタに移動できます。［<emphasis
role="strong">Akri</emphasis>］ナビゲーショングループの下に［Configurations
(設定)］セクションと［Instances (インスタンス)］セクションがあります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configurations.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定</phrase></textobject>
</mediaobject>
</informalfigure>
<para>［Configurations
(設定)］リストには、設定ディスカバリハンドラとインスタンス数に関する情報が表示されます。名前をクリックすると、設定の詳細ページが開きます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-detail.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定の詳細</phrase></textobject>
</mediaobject>
</informalfigure>
<para>設定の編集や新規作成も行うことができます。拡張機能を使用すると、ディスカバリハンドラの選択、ブローカPodやブローカJobの設定、設定サービスやインスタンスサービスの設定、および設定容量の設定を行うことができます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-configuration-edit.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能の設定の編集</phrase></textobject>
</mediaobject>
</informalfigure>
<para>検出されたデバイスが <emphasis role="strong">［Instances
(インスタンス)］</emphasis>リストに一覧にされます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instances-list.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能インスタンスリスト</phrase></textobject>
</mediaobject>
</informalfigure>
<para>インスタンス名をクリックすると、詳細ページが開き、ワークロードおよびインスタンスサービスを表示できます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="akri-extension-instance-detail.png"
width=""/> </imageobject>
<textobject><phrase>Akri拡張機能のインスタンス詳細</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</chapter>
<chapter xml:id="components-k3s">
<title>K3s</title>
<para><link
xl:href="https://k3s.io/">K3s</link>は、リソースに制約のあるリモートの無人の場所やIoTアプライアンス内の運用ワークロード向けに設計された、高可用性のKubernetes認定ディストリビューションです。</para>
<para>単一の小さなバイナリとしてパッケージ化されているため、迅速かつ簡単にインストールおよび更新できます。</para>
<section xml:id="id-how-does-suse-edge-use-k3s">
<title>SUSE EdgeでのK3sの用途</title>
<para>K3sは、SUSE Edgeスタックを支えるKubernetesディストリビューションとして使用できます。K3sはSLE
Microオペレーティングシステムにインストールすることが意図されています。</para>
<para>K3sをSUSE
EdgeスタックのKubernetesディストリビューションとして使用することは、etcdをバックエンドとして使用したのでは制約に合わない場合にのみ推奨します。etcdをバックエンドとして使用できる場合は、RKE2
(<xref linkend="components-rke2"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-best-practices-4">
<title>ベストプラクティス</title>
<section xml:id="id-installation-2">
<title>インストール</title>
<para>K3sをSUSE Edgeスタックの一部としてインストールする場合に推奨する方法は、Edge Image Builder
(EIB)を使用することです。K3sをデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>この方法では、自動的にHAセットアップとElementalセットアップがサポートされます。</para>
</section>
<section xml:id="id-fleet-for-gitops-workflow">
<title>GitOpsワークフローでのFleet</title>
<para>SUSE Edgeスタックでは、GitOpsの推奨ツールとしてFleetを使用します
。Fleetのインストールと使用の詳細については、このドキュメントの「Fleet」のセクション(<xref
linkend="components-fleet"/>)を参照してください。</para>
</section>
<section xml:id="id-storage-management">
<title>ストレージ管理</title>
<para>K3sではローカルパスストレージが事前設定されており、これはシングルノードクラスタに適しています。複数のノードにまたがるクラスタの場合は、Longhorn
(<xref linkend="components-longhorn"/>)を使用することをお勧めします。</para>
</section>
<section xml:id="id-load-balancing-and-ha">
<title>負荷分散とHA</title>
<para>EIBを使用してK3sをインストールした場合、ここで説明する部分は、EIBのドキュメントの「HA」のセクションで説明済みです。</para>
<para>EIBを使用しないでK3sをインストールした場合は、MetalLBのドキュメント(<xref
linkend="guides-metallb-k3s"/>)に従ってMetalLBをインストールおよび設定する必要があります。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-rke2">
<title>RKE2</title>
<para><link xl:href="https://docs.rke2.io/">RKE2の公式ドキュメント</link>を参照してください。</para>
<para>RKE2は、以下によってセキュリティとコンプライアンスに重点を置いた、完全準拠のKubernetesディストリビューションです。</para>
<itemizedlist>
<listitem>
<para>クラスタがCIS Kubernetes Benchmark
v1.6またはv1.23に合格できるデフォルト値と設定オプションを、オペレータの介入を最小限に抑えながら提供する</para>
</listitem>
<listitem>
<para>FIPS 140-2準拠を可能にする</para>
</listitem>
<listitem>
<para><link
xl:href="https://trivy.dev">trivy</link>を使用し、コンポーネントを定期的にスキャンしてRKE2ビルドパイプラインにCVEがないかどうかを確認する</para>
</listitem>
</itemizedlist>
<para>RKE2は、コントロールプレーンコンポーネントを、kubeletによって管理される静的Podとして起動します。組み込みコンテナランタイムはcontainerdです。</para>
<para>メモ: RKE2はRKE Governmentとしても知られます。これは、RKE2が現在ターゲットにしている別のユースケースと分野を表すためです。</para>
<section xml:id="id-rke2-vs-k3s">
<title>RKE2とK3s</title>
<para>K3sは完全準拠の軽量なKubernetesディストリビューションであり、エッジ、IoT、ARMや、K8sのクラスタ技術では博士号を取得できそうにない状況に焦点を合わせています。</para>
<para>RKE2は、RKEの1.xバージョン(以下「RKE1」)とK3sの両方の長所を兼ね備えています。</para>
<para>RKE2は、K3sから使いやすさ、操作のしやすさ、およびデプロイメントモデルを継承しています。</para>
<para>RKE1から継承しているのは、アップストリームのKubernetesとの緊密な連携です。K3sはエッジデプロイメントに合わせて最適化されているため、アップストリームのKubernetesとは各所で異なりますが、RKE1とRKE2はアップストリームと緊密な連携を保つことができます。</para>
</section>
<section xml:id="id-how-does-suse-edge-use-rke2">
<title>SUSE EdgeでのRKE2の用途</title>
<para>RKE2はSUSE Edgeスタックの基礎を成す部分です。RKE2はSUSE Linux Micro (<xref
linkend="components-slmicro"/>)上に位置し、Edgeワークロードを最小限のフットプリントでデプロイするために必要な標準Kubernetesインタフェースを提供します。</para>
</section>
<section xml:id="id-best-practices-5">
<title>ベストプラクティス</title>
<section xml:id="id-installation-3">
<title>インストール</title>
<para>RKE2をSUSE Edgeスタックの一部としてインストールする場合に推奨される方法は、Edge Image Builder
(EIB)を使用することです。RKE2をデプロイするようにEIBを設定する方法の詳細については、EIBのドキュメント(<xref
linkend="components-eib"/>)を参照してください。</para>
<para>EIBは十分な柔軟性を備えているため、RKE2のバージョン、<link
xl:href="https://docs.rke2.io/reference/server_config">サーバ</link>、または<link
xl:href="https://docs.rke2.io/reference/linux_agent_config">エージェント</link>設定の指定など、RKE2で要求されるあらゆるパラメータをサポートすることができ、Edgeのすべてのユースケースに対応できます。</para>
<para>Metal<superscript>3</superscript>に関連する他のユースケースでも、RKE2が使用およびインストールされます。このような特定のケースでは、<link
xl:href="https://github.com/rancher-sandbox/cluster-api-provider-rke2">Cluster
API Provider
RKE2</link>によって、Edgeスタックを使用してMetal<superscript>3</superscript>でプロビジョニングされるクラスタにRKE2が自動的にデプロイされます。</para>
<para>このような場合、関係する各種のCRDにRKE2設定を適用する必要があります。<literal>RKE2ControlPlane</literal>
CRDを使用して異なるCNIを提供する方法の例は、次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  serverConfig:
    cni: calico
    cniMultusEnable: true
...</screen>
<para>Metal<superscript>3</superscript>のユースケースの詳細については、<xref
linkend="components-metal3"/>を参照してください。</para>
</section>
<section xml:id="id-high-availability">
<title>高可用性</title>
<para>HAデプロイメントの場合、EIBはMetalLB (<xref linkend="components-metallb"/>)と<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link>を自動的にデプロイして設定し、RKE2 APIエンドポイントを外部に公開します。</para>
</section>
<section xml:id="id-networking">
<title>ネットワーキング</title>
<para>EdgeスタックでサポートされているCNIは<link
xl:href="https://docs.cilium.io/en/stable/">Cilium</link>で、オプションでmeta-plugin
<link
xl:href="https://github.com/k8snetworkplumbingwg/multus-cni">Multus</link>を追加できますが、RKE2では<link
xl:href="https://docs.rke2.io/install/network_options">ほかにもいくつかのプラグイン</link>がサポートされています。</para>
</section>
<section xml:id="id-storage">
<title>ストレージ</title>
<para>RKE2は、どのような種類の永続ストレージクラスやオペレータも提供していません。複数のノードにまたがるクラスタの場合は、Longhorn (<xref
linkend="components-longhorn"/>)を使用することをお勧めします。</para>
</section>
</section>
</chapter>
<chapter xml:id="components-longhorn">
<title>Longhorn</title>
<para>Longhornは、Kubernetes向けに設計された、信頼性が高くユーザフレンドリな軽量の分散ブロックストレージシステムです。オープンソースプロジェクトとして、当初はRancher
Labsによって開発されていましたが、現在はCNCFの下でインキュベートされています。</para>
<section xml:id="id-prerequisites-5">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 5.5がインストールされた1つ以上のホスト(物理または仮想)</para>
</listitem>
<listitem>
<para>インストール済みのKubernetesクラスタ1つ(K3sまたはRKE2)</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-longhorn">
<title>Longhornの手動インストール</title>
<section xml:id="id-installing-open-iscsi">
<title>Open-iSCSIのインストール</title>
<para>Longhornをデプロイして使用するための中心的な要件は、<literal>open-iscsi</literal>パッケージをインストールすることと、<literal>iscsid</literal>デーモンをすべてのKubernetesノード上で実行することです。これは、Longhornがホスト上の<literal>iscsiadm</literal>を利用してKubernetesに永続ボリュームを提供するために必要です。</para>
<para>インストールしてみましょう。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install open-iscsi</screen>
<para>SLE
Microはイミュータブルオペレーティングシステムであるため、操作が完了すると、パッケージは新しいスナップショットにのみインストールされることに注意することが重要です。パッケージをロードし、<literal>iscsid</literal>デーモンの実行を開始するには、作成した新しいスナップショットで再起動する必要があります。準備が整ったら、rebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<tip>
<para>open-iscsiのインストールに関する追加のヘルプについては、<link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/#installing-open-iscsi">Longhorn公式ドキュメント</link>を参照してください。</para>
</tip>
</section>
<section xml:id="id-installing-longhorn">
<title>Longhornのインストール</title>
<para>LonghornをKubernetesクラスタにインストールする方法はいくつかあります。このガイドではHelmによるインストール手順に従いますが、別のアプローチが必要な場合は、<link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/">公式ドキュメント</link>に従ってください。</para>
<orderedlist numeration="arabic">
<listitem>
<para>LonghornのHelmリポジトリを追加します。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add longhorn https://charts.longhorn.io</screen>
</listitem>
<listitem>
<para>リポジトリから最新のチャートをフェッチします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update</screen>
</listitem>
<listitem>
<para>Longhornをlonghorn-systemネームスペースにインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.6.1</screen>
</listitem>
<listitem>
<para>デプロイメントが成功したことを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n longhorn-system get pods</screen>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl -n longhorn-system get pod
NAMESPACE         NAME                                                READY   STATUS      RESTARTS        AGE
longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     Running     0               90s
longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     Running     0               90s
longhorn-system   longhorn-manager-wts2v                              1/1     Running     1 (77s ago)     90s
longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     Running     0               90s
longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     Running     0               70s
longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     Running     0               70s
longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     Running     0               50s
longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     Running     0               50s
longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     Running     0               50s
longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     Running     0               50s
longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     Running     0               50s
longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     Running     0               50s</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-creating-longhorn-volumes">
<title>Longhornボリュームの作成</title>
<para>Longhornは、<literal>StorageClass</literal>というKubernetesリソースを利用して、Podの<literal>PersistentVolume</literal>オブジェクトを自動的にプロビジョニングします。<literal>StorageClass</literal>は、管理者が、自身が提供する<emphasis>クラス</emphasis>または<emphasis>プロファイル</emphasis>を記述する方法だと考えてください。</para>
<para>デフォルトのオプションをいくつか使用して<literal>StorageClass</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: longhorn-example
provisioner: driver.longhorn.io
allowVolumeExpansion: true
parameters:
  numberOfReplicas: "3"
  staleReplicaTimeout: "2880" # 48 hours in minutes
  fromBackup: ""
  fsType: "ext4"
EOF</screen>
<para><literal>StorageClass</literal>を作成したので、それを参照する<literal>PersistentVolumeClaim</literal>が必要です。<literal>PersistentVolumeClaim</literal>
(PVC)は、ユーザによるストレージの要求です。PVCは<literal>PersistentVolume</literal>リソースを使用します。クレームでは、特定のサイズとアクセスモードを要求できます(たとえば、1つのノードで読み取り/書き込み可能でマウントすることも、複数のノードで読み取り専用でマウントすることもできます)。</para>
<para><literal>PersistentVolumeClaim</literal>を作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: longhorn-volv-pvc
  namespace: longhorn-system
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: longhorn-example
  resources:
    requests:
      storage: 2Gi
EOF</screen>
<para>完了です。<literal>PersistentVolumeClaim</literal>を作成したら、それを<literal>Pod</literal>にアタッチする手順に進むことができます。<literal>Pod</literal>がデプロイされると、KubernetesはLonghornボリュームを作成し、ストレージが利用可能な場合は<literal>Pod</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
  namespace: longhorn-system
spec:
  containers:
  - name: volume-test
    image: nginx:stable-alpine
    imagePullPolicy: IfNotPresent
    volumeMounts:
    - name: volv
      mountPath: /data
    ports:
    - containerPort: 80
  volumes:
  - name: volv
    persistentVolumeClaim:
      claimName: longhorn-volv-pvc
EOF</screen>
<tip>
<para>Kubernetesにおけるストレージの概念は複雑であると同時に重要なトピックです。最も一般的なKubernetesリソースのいくつかを簡単に説明しましたが、Longhornが提供している<link
xl:href="https://longhorn.io/docs/1.6.1/terminology/">用語のドキュメント</link>をよく理解しておくことをお勧めします。</para>
</tip>
<para>この例では、結果は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # kubectl get storageclass
NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
longhorn (default)   driver.longhorn.io   Delete          Immediate           true                   12m
longhorn-example     driver.longhorn.io   Delete          Immediate           true                   24s

localhost:~ # kubectl get pvc -n longhorn-system
NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS       AGE
longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        RWO            longhorn-example   54s

localhost:~ # kubectl get pods -n longhorn-system
NAME                                                READY   STATUS    RESTARTS      AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             14m
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             14m
csi-provisioner-667796df57-fmz2d                    1/1     Running   0             14m
csi-provisioner-667796df57-p7rjr                    1/1     Running   0             14m
csi-provisioner-667796df57-w9fdq                    1/1     Running   0             14m
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             14m
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             14m
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             14m
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             14m
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             14m
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             14m
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             14m
longhorn-csi-plugin-gd2jx                           3/3     Running   0             14m
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             15m
longhorn-manager-z4lnl                              1/1     Running   0             15m
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m ago)   15m
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m ago)   15m
volume-test                                         1/1     Running   0             26s</screen>
</section>
<section xml:id="id-accessing-the-ui">
<title>UIへのアクセス</title>
<para>kubectlまたはHelmを使用してLonghornをインストールした場合は、クラスタへの外部トラフィックを許可するようにIngressコントローラを設定する必要があります。認証はデフォルトでは有効になっていません。Rancherカタログアプリを使用していた場合、IngressコントローラはRancherによって自動的に作成され、アクセス制御が設定されています(rancher-proxy)。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Longhornの外部サービスのIPアドレスを取得します。</para>
<screen language="console" linenumbering="unnumbered">kubectl -n longhorn-system get svc</screen>
</listitem>
<listitem>
<para><literal>longghorn-frontend</literal>のIPアドレスを取得したら、ブラウザでそのアドレスに移動してUIの使用を開始できます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-2">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、<xref linkend="components-eib"/>を使用して、ベースとなるSLE Micro
OSイメージをカスタマイズしています。ここでは、イメージをカスタマイズしてRKE2クラスタとLonghornをSLE
Micro上にプロビジョニングする方法について説明します。</para>
<para>定義ファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/eib
mkdir -p $CONFIG_DIR

cat &lt;&lt; EOF &gt; $CONFIG_DIR/iso-definition.yaml
apiVersion: 1.0
image:
  imageType: iso
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  arch: x86_64
  outputImageName: eib-image.iso
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: longhorn
        version: 1.6.1
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: longhorn
        url: https://charts.longhorn.io
operatingSystem:
  packages:
    sccRegistrationCode: &lt;reg-code&gt;
    packageList:
      - open-iscsi
  users:
  - username: root
    encryptedPassword: \$6\$jHugJNNd3HElGsUZ\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
EOF</screen>
<note>
<para>Helmチャートの値のカスタマイズは、<literal>helm.charts[].valuesFile</literal>で提供されている別個のファイルを使用して実行できます。詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes">アップストリームドキュメント</link>を参照してください。</para>
</note>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $CONFIG_DIR:/eib registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file $CONFIG_DIR/iso-definition.yaml</screen>
<para>イメージが構築されたら、それを使用して物理ホストまたは仮想ホストにOSをインストールできます。プロビジョニングが完了すると、<literal>root:eib</literal>の資格情報ペアを使用してシステムにログインできます。</para>
<para>Longhornが正常にデプロイされていることを確認します。</para>
<screen language="console" linenumbering="unnumbered">localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml -n longhorn-system get pods
NAME                                                READY   STATUS    RESTARTS        AGE
csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               103s
csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               103s
csi-provisioner-667796df57-fmz2d                    1/1     Running   0               103s
csi-provisioner-667796df57-p7rjr                    1/1     Running   0               103s
csi-provisioner-667796df57-w9fdq                    1/1     Running   0               103s
csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               103s
csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               103s
csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               103s
csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               103s
csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               103s
engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               109s
instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               109s
longhorn-csi-plugin-gd2jx                           3/3     Running   0               103s
longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               2m28s
longhorn-manager-z4lnl                              1/1     Running   0               2m28s
longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (2m7s ago)    2m28s
longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (2m10s ago)   2m28s</screen>
<note>
<para>このインストールは、完全なエアギャップ環境では動作しません。このような場合は、<xref
linkend="longhorn-install"/>を参照してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="components-neuvector">
<title>NeuVector</title>
<para>NeuVectorはKubernetes向けのセキュリティソリューションであり、L7ネットワークセキュリティ、ランタイムセキュリティ、サプライチェーンセキュリティ、およびコンプライアンスチェックを1つの統合パッケージで提供します。</para>
<para>NeuVectorは、複数のコンテナから成るプラットフォームとしてデプロイされ、各コンテナはさまざまなポートとインタフェースで相互に通信します。デプロイされる各種のコンテナは以下のとおりです。</para>
<itemizedlist>
<listitem>
<para>Manager
。Webベースのコンソールを提供するステートレスコンテナです。通常、マネージャは1つだけ必要で、どこでも実行できます。Managerにエラーが発生しても、ControllerやEnforcerの動作には影響しません。ただし、特定の通知(イベント)と最近の接続データはManagerによってメモリ内にキャッシュされているため、これらの表示には影響があります。</para>
</listitem>
<listitem>
<para>Controller。NeuVectorの「コントロールプレーン」は必ずHA設定でデプロイされるため、ノードのエラーで設定が失われることはありません。Controllerはどこでも実行できますが、その重要性から、顧客はほとんどの場合、「管理」ノード、マスタノード、またはインフラノードに配置することを選択します。</para>
</listitem>
<listitem>
<para>Enforcer。このコンテナはDaemonSetとしてデプロイされるため、保護する各ノードに1つのEnforcerが存在します。通常はすべてのワーカーノードにデプロイされますが、スケジュールを有効にしてマスタノードやインフラノードにデプロイすることもできます。メモ:
Enforcerがクラスタノードに存在しない状況で、そのノード上のPodから接続が行われた場合、その接続はNeuVectorによって「unmanaged」ワークロードとしてラベル付けされます。</para>
</listitem>
<listitem>
<para>Scanner。コントローラの指示に従って、ビルトインCVEデータベースを使用して脆弱性スキャンを実行します。複数のScannerをデプロイしてスキャン能力を拡張できます。Scannerはどこでも実行できますが、コントローラが実行されるノードで実行されることがほとんどです。Scannerノードのサイジングに関する考慮事項については、以下を参照してください。ビルドフェーズのスキャンに使用する場合、Scannerを独立して呼び出すこともできます。たとえば、スキャンをトリガし、結果を取得してScannerを停止するパイプライン内で使用する場合などです。Scannerには最新のCVEデータベースが含まれているため、毎日更新する必要があります。</para>
</listitem>
<listitem>
<para>Updater。Updaterは、CVEデータベースの更新が必要な場合に、Kubernetes
cronジョブを通じてScannerの更新をトリガします。必ず使用環境に合わせて設定してください。</para>
</listitem>
</itemizedlist>
<para>NeuVectorのオンボーディングの詳細とベストプラクティスのドキュメントについては、<link
xl:href="https://open-docs.neuvector.com/deploying/production/NV_Onboarding_5.0.pdf">こちら</link>をご覧ください。</para>
<section xml:id="id-how-does-suse-edge-use-neuvector">
<title>SUSE EdgeでのNeuVectorの用途</title>
<para>SUSE Edgeは、エッジデプロイメントの開始点として簡潔なNeuVector設定を提供します。</para>
<para>NeuVectorの設定の変更については、<link
xl:href="https://github.com/suse-edge/charts/blob/main/packages/neuvector-core/generated-changes/patch/values.yaml.patch">こちら</link>を参照してください。</para>
</section>
<section xml:id="id-important-notes">
<title>重要なメモ</title>
<itemizedlist>
<listitem>
<para><literal>Scanner</literal>コンテナには、スキャンするイメージをメモリに取り込んで解凍するのに十分なメモリが必要です。1GBを超えるイメージをスキャンするには、Scannerのメモリを、予想される最大イメージサイズをわずかに上回るサイズまで増やしてください。</para>
</listitem>
<listitem>
<para>保護モードでは、大量のネットワーク接続が予想されます。保護(インラインファイアウォールでのブロック)モードの場合、<literal>Enforcer</literal>では、接続および想定されるペイロードを保持して検査(DLP)するためにCPUとメモリが必要です。メモリを増やし、1つのCPUコアを<literal>Enforcer</literal>専用にすることで、十分なパケットフィルタリング容量を確保できます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-with-edge-image-builder-3">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeでは、<xref linkend="components-eib"/>を使用して、ベースとなるSLE Micro
OSイメージをカスタマイズします。EIBでプロビジョニングしたKubernetesクラスタ上にNeuVectorをエアギャップインストールする場合は、<xref
linkend="neuvector-install"/>に従ってください。</para>
</section>
</chapter>
<chapter xml:id="components-metallb">
<title>MetalLB</title>
<para><link
xl:href="https://metallb.universe.tf/">MetalLBの公式ドキュメント</link>を参照してください。</para>
<blockquote>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>ベアメタル環境では、ネットワークロードバランサの設定がクラウドセットアップよりも著しく複雑になります。クラウド設定でのわかりやすいAPIコールとは異なり、ベアメタルでは、高可用性(HA)を管理したり、シングルノードのロードバランサに特有の潜在的な単一障害点(SPOF)に対処したりするために、専用のネットワークアプライアンス、またはロードバランサと仮想IP
(VIP)設定の組み合わせが必要になります。このような設定は自動化しにくく、コンポーネントが動的にスケールアップ/ダウンするKubernetesのデプロイメントでは課題となります。</para>
<para>MetalLBでは、こうした課題に対処するために、Kubernetesモデルを利用してLoadBalancerタイプのサービスを作成し、ベアメタルセットアップであってもクラウド環境であるかのように動作させます。</para>
<para>2つの異なるアプローチがあります。<link
xl:href="https://metallb.universe.tf/concepts/layer2/">L2モード</link>(ARP<emphasis>「トリック」</emphasis>を使用する)アプローチか、<link
xl:href="https://metallb.universe.tf/concepts/bgp/">BGP</link>を使用するアプローチです。主にL2では特別なネットワーク機器は必要ありませんが、一般的にはBGPのほうが優れています。これはユースケースによって異なります。</para>
</blockquote>
<section xml:id="id-how-does-suse-edge-use-metallb">
<title>SUSE EdgeでのMetalLBの用途</title>
<para>SUSE Edgeでは、主に次の2つの方法でMetalLBを使用します。</para>
<itemizedlist>
<listitem>
<para>ロードバランサソリューションとして: MetalLBは、ベアメタルマシン用のロードバランサソリューションとして機能します。</para>
</listitem>
<listitem>
<para>HA K3s/RKE2セットアップの場合: MetalLBでは、仮想IPアドレスを使用してKubernetes APIを負荷分散できます。</para>
</listitem>
</itemizedlist>
<note>
<para>APIを公開できるようにするには、<literal>endpoint-copier-operator</literal>を使用して、「kubernetes」サービスから「kubernetes-vip」LoadBalancerサービスへのK8s
APIエンドポイントの同期を保ちます。</para>
</note>
</section>
<section xml:id="id-best-practices-6">
<title>ベストプラクティス</title>
<para>L2モードでのMetalLBのインストールの詳細については、MetalLBガイド(<xref
linkend="guides-metallb-k3s"/>)を参照してください。</para>
<para>MetalLBをkube-api-serverの前面にインストールしてHAセットアップを実現する方法のガイドについては、「Kubernetes
APIサーバの前面のMetalLB」(<xref
linkend="guides-metallb-kubernetes"/>)のチュートリアルを参照してください。</para>
</section>
<section xml:id="id-known-issues-6">
<title>既知の問題</title>
<itemizedlist>
<listitem>
<para>K3S LoadBalancerソリューション:
K3Sには、<literal>Klipper</literal>というロードバランサソリューションが付属しています。MetalLBを使用するには、Klipperを無効にする必要があります。このためには、<link
xl:href="https://docs.k3s.io/networking">K3sのドキュメント</link>で説明されているように、<literal>--disable
servicelb</literal>オプションを指定してK3sサーバを起動します。</para>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="components-kubevirt">
<title>Edge Virtualization</title>
<para>このセクションでは、Edge Virtualizationを使用してエッジノードで仮想マシンを実行する方法について説明します。Edge
Virtualizationは包括的なソリューションではなく、機能も限られていることを指摘しておくことが重要です。Edge
Virtualizationは、基本的な仮想マシン機能が要求される軽量な仮想化の要件を解決しようとするものです。より包括的な仮想化(およびハイパーコンバージドインフラストラクチャ)ソリューションは、<link
xl:href="https://harvesterhci.io/">Harvester</link>で提供されています。</para>
<para>SUSE Edge Virtualizationでは、仮想マシンの実行方法として次の2つをサポートしています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>libvirt+qemu-kvmを使用してホストレベルで手動で仮想マシンをデプロイする</para>
</listitem>
<listitem>
<para>KubeVirtオペレータをデプロイし、Kubernetesベースで仮想マシンを管理する</para>
</listitem>
</orderedlist>
<para>どちらのオプションも有効ですが、以下では2番目のオプションのみを説明しています。SLE
Microで提供されている、すぐに使用できる標準の仮想化メカニズムを使用する場合は、<link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/chap-virtualization-introduction.html">こちら</link>で包括的なガイドを参照してください。このガイドは主にSUSE
Linux Enterprise Server用に記載されていますが、概念はほぼ同じです。</para>
<para>このガイドではまず、事前にデプロイ済みのシステムに追加の仮想化コンポーネントをデプロイする方法について説明しますが、その後に続くセクションでは、Edge
Image
Builderを使用してこの設定を最初のデプロイメントに組み込む方法を説明しています。基本手順を実行して環境を手動で設定する必要がない場合は、そちらのセクションに進んでください。</para>
<section xml:id="id-kubevirt-overview">
<title>KubeVirtの概要</title>
<para>KubeVirtでは、仮想マシンと他のコンテナ化ワークロードを併せてKubernetesで管理できます。これを実現するために、Linux仮想化スタックのユーザスペース部分をコンテナ内で実行します。これにより、ホストシステムの要件が最小限に抑えられ、セットアップと管理が容易になります。</para>
<informalexample>
<para>KubeVirtのアーキテクチャの詳細については、<link
xl:href="https://kubevirt.io/user-guide/architecture/">アップストリームドキュメント</link>を参照してください。</para>
</informalexample>
</section>
<section xml:id="id-prerequisites-6">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 5.5+がインストールされ、BIOSで仮想化拡張機能が有効になっている物理ホスト1台以上(詳細については、<link
xl:href="https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware">こちら</link>を参照)。</para>
</listitem>
<listitem>
<para>ノード全体で、K3s/RKE2
Kubernetesクラスタがすでにデプロイされており、クラスタへのスーパーユーザアクセスを可能にする適切な<literal>kubeconfig</literal>が設定されている。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>「いない」</emphasis>ことを想定しています。</para>
</listitem>
<listitem>
<para><link
xl:href="https://helm.sh/docs/intro/install/">Helm</link>がローカルで利用可能で、適切なネットワーク接続を備えていて、Kubernetesクラスタに設定をプッシュし、必要なイメージをダウンロードできる。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation-of-edge-virtualization">
<title>Edge Virtualizationの手動インストール</title>
<para>このガイドでは、Kubernetesのデプロイメント手順については説明しませんが、SUSE Edgeに適したバージョンの<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>がインストールされていること、およびkubeconfigが適切に設定されていて標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを想定しています。また、シングルノードクラスタを形成することを想定していますが、マルチノードのデプロイメントでも大きな違いはないと考えられます。</para>
<para>SUSE Edge Virtualizationは、3つの別個のHelmチャートを使用してデプロイします。具体的には次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">KubeVirt</emphasis>:
中心的な仮想化コンポーネント。つまり、Kubernetesが仮想マシンをデプロイおよび管理できるようにするために必要なKubernetes
CRD、オペレータ、およびその他のコンポーネント。</para>
</listitem>
<listitem>
<para><emphasis role="strong">KubeVirtダッシュボード拡張機能</emphasis>:
仮想マシンの起動/停止やコンソールへのアクセスなど、基本的な仮想マシン管理を実行できるオプションのRancher UI拡張機能。</para>
</listitem>
<listitem>
<para><emphasis role="strong">Containerized Data Importer (CDI)</emphasis>:
KubeVirtの永続ストレージの統合を可能にする追加コンポーネント。仮想マシンが既存のKubernetesストレージバックエンドをデータ用に使用する機能を提供するだけでなく、ユーザが仮想マシンのデータボリュームのインポートまたはクローンの作成を行うことも可能にします。</para>
</listitem>
</itemizedlist>
<para>これらの各Helmチャートは、現在使用しているSUSE
Edgeのリリースに従ってバージョン管理されています。運用での使用/サポートされる使用のためには、SUSEレジストリにあるアーティファクトを使用してください。</para>
<para>まず、 <literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                   STATUS   ROLES                       AGE     VERSION
node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   v1.28.9+rke2r1
node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.28.9+rke2r1
node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   v1.28.9+rke2r1</screen>
<para>これで、<emphasis role="strong">KubeVirt</emphasis>および<emphasis
role="strong">Containerized Data Importer
(CDI)</emphasis>のHelmチャートのインストールに進むことができます。</para>
<screen language="shell" linenumbering="unnumbered">$ helm install kubevirt oci://registry.suse.com/edge/kubevirt-chart --namespace kubevirt-system --create-namespace
$ helm install cdi oci://registry.suse.com/edge/cdi-chart --namespace cdi-system --create-namespace</screen>
<para>数分ですべてのKubeVirtおよびCDIコンポーネントがデプロイされるはずです。これを検証するには、<literal>kubevirt-system</literal>および<literal>cdi-system</literal>のネームスペース内にデプロイされたすべてのリソースを確認します。</para>
<para>KubeVirtリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n kubevirt-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS      AGE
pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             2m24s
pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             2m24s
pod/virt-handler-t594x                 1/1     Running   0             93s
pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s
pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s
pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   118s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m1s
service/virt-api                      ClusterIP   10.43.56.140    &lt;none&gt;        443/TCP   2m1s
service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   &lt;none&gt;        443/TCP   2m1s
service/virt-exportproxy              ClusterIP   10.43.83.23     &lt;none&gt;        443/TCP   2m1s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   93s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-operator     2/2     2            2           2m24s
deployment.apps/virt-controller   2/2     2            2           93s
deployment.apps/virt-api          1/1     1            1           118s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-operator-5fbcf48d58     2         2         2       2m24s
replicaset.apps/virt-controller-5f84c69884   2         2         2       93s
replicaset.apps/virt-api-7dfc54cf95          1         1         1       118s

NAME                            AGE     PHASE
kubevirt.kubevirt.io/kubevirt   2m24s   Deployed</screen>
<para>CDIリソースを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get all -n cdi-system</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s
pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s
pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s
pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/cdi-uploadproxy          ClusterIP   10.43.117.7    &lt;none&gt;        443/TCP    2m22s
service/cdi-api                  ClusterIP   10.43.20.101   &lt;none&gt;        443/TCP    2m22s
service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   &lt;none&gt;        8080/TCP   2m21s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-operator      1/1     1            1           2m24s
deployment.apps/cdi-apiserver     1/1     1            1           2m22s
deployment.apps/cdi-deployment    1/1     1            1           2m21s
deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       2m24s
replicaset.apps/cdi-apiserver-db465b888      1         1         1       2m21s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       2m21s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       2m22s</screen>
<para><literal>VirtualMachine</literal>カスタムリソース定義(CRD)がデプロイされていることを確認するには、次のコマンドで検証できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl explain virtualmachine</screen>
<para><literal>VirtualMachine</literal>オブジェクトの定義が出力され、次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">GROUP:      kubevirt.io
KIND:       VirtualMachine
VERSION:    v1

DESCRIPTION:
    VirtualMachine handles the VirtualMachines that are not running or are in a
    stopped state The VirtualMachine contains the template to create the
    VirtualMachineInstance. It also mirrors the running state of the created
    VirtualMachineInstance in its status.
(snip)</screen>
</section>
<section xml:id="id-deploying-virtual-machines">
<title>仮想マシンのデプロイ</title>
<para>KubeVirtとCDIがデプロイされたので、<link
xl:href="https://get.opensuse.org/tumbleweed/">openSUSE
Tumbleweed</link>に基づくシンプルな仮想マシンを定義してみましょう。この仮想マシンは最もシンプルな設定であり、標準の「Podネットワーキング」を使用して、他のPodと同じネットワーキング設定を行います。また、非永続ストレージを使用するため、<link
xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">PVC</link>を持たないコンテナと同様に、ストレージは一時的なものになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: tumbleweed
  namespace: default
spec:
  runStrategy: Always
  template:
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK
        name: cloudinitdisk
EOF</screen>
<para>これにより、<literal>VirtualMachine</literal>が作成されたことを示すメッセージが出力されます。</para>
<screen language="shell" linenumbering="unnumbered">virtualmachine.kubevirt.io/tumbleweed created</screen>
<para>この<literal>VirtualMachine</literal>定義は最小限であり、設定はほとんど指定されていません。この定義は単に、この仮想マシンが、一時的な<literal><link
xl:href="https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk">containerDisk</link></literal>に基づくディスクイメージ(つまり、リモートイメージリポジトリからのコンテナイメージに保存されるディスクイメージ)を使用する、2GBのメモリを備えたマシンタイプ「<link
xl:href="https://wiki.qemu.org/Features/Q35">q35</link>」であることを示しています。また、base64でエンコードされたcloudInitディスクを指定しており、このディスクはブート時にユーザを作成してパスワードを適用する目的にのみ使用します(デコードには<literal>base64
-d</literal>を使用します)。</para>
<blockquote>
<note>
<para>この仮想マシンイメージはテスト専用です。このイメージは公式にサポートされておらず、ドキュメントの例としてのみ使用されています。</para>
</note>
</blockquote>
<para>このマシンは、openSUSE
Tumbleweedのディスクイメージをダウンロードする必要があるためブートに数分かかりますが、ブートが完了したら、次のコマンドで仮想マシンの情報をチェックして、仮想マシンの詳細を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi</screen>
<para>これにより、仮想マシンが起動されたノードと、仮想マシンのIPアドレスが出力されます。Podネットワーキングを使用しているため、報告されるIPアドレスは他のPodと同様であり、ルーティング可能であることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">NAME         AGE     PHASE     IP           NODENAME               READY
tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True</screen>
<para>これらのコマンドをKubernetesクラスタノード自体で実行する場合は、トラフィックをPodに直接ルーティングするCNI
(Ciliumなど)を使用して、マシン自体に直接<literal>ssh</literal>で接続できるはずです。次のIPアドレスを、仮想マシンに割り当てられているIPアドレスに置き換えます。</para>
<screen language="shell" linenumbering="unnumbered">$ ssh suse@10.42.2.98
(password is "suse")</screen>
<para>この仮想マシンに接続すると、さまざまな操作を試すことができますが、リソースの点で制限があり、ディスク容量は1GBしかないことに注意してください。終了したら、<literal>Ctrl-D</literal>または<literal>exit</literal>でSSHセッションを切断します。</para>
<para>仮想マシンプロセスは、依然として標準のKubernetes
Podでラップされています。<literal>VirtualMachine</literal>
CRDは目的の仮想マシンを表していますが、仮想マシンが実際に起動されるプロセスは、他のアプリケーションと同様に、標準のKubernetes
Podである<literal><link
xl:href="https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher">virt-launcher</link></literal>
Podを介して行われます。起動されたすべての仮想マシンに対して、<literal>virt-launcher</literal>
Podが存在することがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods</screen>
<para>次に、定義したTumbleweedマシンの1つの<literal>virt-launcher</literal> Podが表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m</screen>
<para>この<literal>virt-launcher</literal>
Podを調べてみると、<literal>libvirt</literal>プロセスと<literal>qemu-kvm</literal>プロセスを実行していることがわかります。このPod自体を起動して詳細を確認できます。次のコマンドは、使用しているPodの名前に合わせて調整する必要があることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash</screen>
<para>Podが起動したら、<literal>virsh</literal>コマンドを実行するのと併せて、プロセスを確認してみます。<literal>qemu-system-x86_64</literal>バイナリに加え、仮想マシンを監視するための特定のプロセスも実行されていることがわかります。
また、ディスクイメージの場所と、ネットワーキングが(タップデバイスとして)どのように接続されているかもわかります。</para>
<screen language="shell" linenumbering="unnumbered">qemu@tumbleweed:/&gt; ps ax
  PID TTY      STAT   TIME COMMAND
    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kube
   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default --kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir /var/run/kubevirt/con
   24 ?        Sl     0:00 /usr/sbin/virtlogd -f /etc/libvirt/virtlogd.conf
   25 ?        Sl     0:01 /usr/sbin/virtqemud -f /var/run/libvirt/virtqemud.conf
   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name guest=default_tumbleweed,debug-threads=on -S -object {"qom-type":"secret","id":"masterKey0","format":"raw","file":"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes"} -machine pc-q35-7.1,usb
  286 pts/0    Ss     0:00 bash
  320 pts/0    R+     0:00 ps ax

qemu@tumbleweed:/&gt; virsh list --all
 Id   Name                 State
------------------------------------
 1    default_tumbleweed   running

qemu@tumbleweed:/&gt; virsh domblklist 1
 Target   Source
---------------------------------------------------------------------------------------------
 sda      /var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2
 sdb      /var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso

qemu@tumbleweed:/&gt; virsh domiflist 1
 Interface   Type       Source   Model                     MAC
------------------------------------------------------------------------------
 tap0        ethernet   -        virtio-non-transitional   e6:e9:1a:05:c0:92

qemu@tumbleweed:/&gt; exit
exit</screen>
<para>最後に、この仮称マシンを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/tumbleweed
virtualmachine.kubevirt.io "tumbleweed" deleted</screen>
</section>
<section xml:id="id-using-virtctl">
<title>virtctlの使用</title>
<para>KubeVirtには、標準のKubernetes
CLIツールである<literal>kubectl</literal>とともに、仮想化の世界とKubernetesが設計された世界との間のギャップを埋める方法でクラスタとのインタフェースを可能にするCLIユーティリティが付属しています。たとえば、
<literal>virtctl</literal>ツールは、APIやCRDを直接使用することなく、仮想マシンのライフサイクル(起動、停止、再起動など)の管理、仮想コンソールへのアクセスの提供、仮想マシンイメージのアップロード、サービスなどのKubernetesコンストラクトとのインタフェースを行う機能を提供します。</para>
<para><literal>virtctl</literal>ツールの最新の安定バージョンをダウンロードしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ export VERSION=v1.1.0
$ wget https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64</screen>
<para>別のアーキテクチャまたはLinux以外のマシンを使用している場合は、他のリリースを<link
xl:href="https://github.com/kubevirt/kubevirt/releases">こちら</link>で見つけることができます。続行する前に、この実行可能ファイルを作成する必要があります。また、実行可能ファイルを<literal>$PATH</literal>内の特定の場所に移動すると便利な場合があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mv virtctl-${VERSION}-linux-amd64 /usr/local/bin/virtctl
$ chmod a+x /usr/local/bin/virtctl</screen>
<para>その後、<literal>virtctl</literal>コマンドラインツールを使用して、仮想マシンを作成できます。出力を<literal>kubectl
apply</literal>に直接パイプしていることに注意して、前の仮想マシンを複製してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl create vm --name virtctl-example --memory=1Gi \
    --volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest \
    --cloud-init-user-data "I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK" | kubectl apply -f -</screen>
<para>これで、仮想マシンが実行されているのがわかります(コンテナイメージがキャッシュされるため、今回はかなり早く起動するはずです)。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vmi
NAME              AGE   PHASE     IP           NODENAME               READY
virtctl-example   52s   Running   10.42.2.29   node3.edge.rdo.wales   True</screen>
<para>これで、 <literal>virtctl</literal>を使用して仮想マシンに直接接続できるようになりました。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
(password is "suse" - Ctrl-D to exit)</screen>
<para><literal>virtctl</literal>で使用可能なコマンドはほかにも多数あります。たとえば、 <literal>virtctl
console</literal>を使用すると、ネットワーキングが機能していない場合にシリアルコンソールにアクセスでき、<literal>virtctl
guestosinfo</literal>を使用すると、ゲストに<literal>qemu-guest-agent</literal>がインストールされていて実行されていれば、包括的なOS情報を取得できます。</para>
<para>最後に、仮想マシンを一時停止し、再開してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl pause vm virtctl-example
VMI virtctl-example was scheduled to pause</screen>
<para><literal>VirtualMachine</literal>オブジェクトが「<emphasis
role="strong">Paused</emphasis>」と表示され、<literal>VirtualMachineInstance</literal>オブジェクトは「<emphasis
role="strong">Running</emphasis>」ですが「<emphasis
role="strong">READY=False</emphasis>」と表示されているのがわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get vm
NAME              AGE     STATUS   READY
virtctl-example   8m14s   Paused   False

$ kubectl get vmi
NAME              AGE     PHASE     IP           NODENAME               READY
virtctl-example   8m15s   Running   10.42.2.29   node3.edge.rdo.wales   False</screen>
<para>また、仮想マシンに接続できなくなっていることもわかります。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
can't access VMI virtctl-example: Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "virtctl-example": VMI is paused</screen>
<para>仮想マシンを再開して、もう一度試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl unpause vm virtctl-example
VMI virtctl-example was scheduled to unpause</screen>
<para>これで、接続を再確立できるはずです。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl ssh suse@virtctl-example
suse@vmi/virtctl-example.default's password:
suse@virtctl-example:~&gt; exit
logout</screen>
<para>最後に、仮想マシンを削除しましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/virtctl-example
virtualmachine.kubevirt.io "virtctl-example" deleted</screen>
</section>
<section xml:id="id-simple-ingress-networking">
<title>シンプルなIngressネットワーキング</title>
<para>このセクションでは、仮想マシンを標準のKubernetesサービスとして公開し、<link
xl:href="https://docs.rke2.io/networking/networking_services#nginx-ingress-controller">NGINXとRKE2</link>、<link
xl:href="https://docs.k3s.io/networking/networking-services#traefik-ingress-controller">TraefikとK3s</link>などのKubernetes
Ingressサービスを介して利用可能にする方法を示します。このドキュメントでは、これらのコンポーネントがすでに適切に設定されていること、およびKubernetesサーバノードまたはIngress仮想IPを指す適切なDNSポインタが設定されていて(ワイルドカードを使用するなど)、Ingressを適切に解決できることを前提としています。</para>
<blockquote>
<note>
<para>SUSE Edge
3.0以降では、K3sをマルチサーバノード設定で使用している場合、MetalLBベースのVIPをIngress用に設定しなければならなかった可能性があります。これはRKE2では必要ありません。</para>
</note>
</blockquote>
<para>この例の環境では、別のopenSUSE
Tumbleweed仮想マシンをデプロイし、cloud-initを使用して、ブート時にNGINXをシンプルなWebサーバとしてインストールしています。また、呼び出しの実行時に期待どおりに動作することを確認するためにシンプルなメッセージを返すように設定しています。この処理を確認するには、以下の出力のcloud-initのセクションに対して<literal>base64
-d</literal>を実行するだけです。</para>
<para>では、この仮想マシンを作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f - &lt;&lt;EOF
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: ingress-example
  namespace: default
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        app: nginx
    spec:
      domain:
        devices: {}
        machine:
          type: q35
        memory:
          guest: 2Gi
        resources: {}
      volumes:
      - containerDisk:
          image: registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest
        name: tumbleweed-containerdisk-0
      - cloudInitNoCloud:
          userDataBase64: I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=
        name: cloudinitdisk
EOF</screen>
<para>この仮想マシンが正常に起動したら、<literal>virtctl</literal>コマンドを使用して、外部ポート<literal>8080</literal>とターゲットポート<literal>80</literal>
(NGINXがデフォルトでリスンするポート)で<literal>VirtualMachineInstance</literal>を公開できます。<literal>virtctl</literal>コマンドは、仮想マシンオブジェクトとPodのマッピングを理解しているため、ここではこのコマンドを使用します。これにより、新しいサービスが作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ virtctl expose vmi ingress-example --port=8080 --target-port=80 --name=ingress-example
Service ingress-example successfully exposed for vmi ingress-example</screen>
<para>これで、適切なサービスが自動的に作成されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc/ingress-example
NAME              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)                         AGE
ingress-example   ClusterIP      10.43.217.19    &lt;none&gt;            8080/TCP                        9s</screen>
<para>次に、<literal>kubectl create
ingress</literal>を使用すると、このサービスを指すIngressオブジェクトを作成できます。ここでURL (<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/">ingress</link>オブジェクトの「ホスト」として知られている)をDNS設定に合わせて調整し、ポート<literal>8080</literal>を指すようにします。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl create ingress ingress-example --rule=ingress-example.suse.local/=ingress-example:8080</screen>
<para>DNSが正しく設定されたら、URLに対してすぐにcurlを実行できます。</para>
<screen language="shell" linenumbering="unnumbered">$ curl ingress-example.suse.local
It works!</screen>
<para>この仮想マシンとそのサービス、およびIngressリソースを削除して、クリーンアップしましょう。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl delete vm/ingress-example svc/ingress-example ingress/ingress-example
virtualmachine.kubevirt.io "ingress-example" deleted
service "ingress-example" deleted
ingress.networking.k8s.io "ingress-example" deleted</screen>
</section>
<section xml:id="id-using-the-rancher-ui-extension">
<title>Rancher UI拡張機能の使用</title>
<para>SUSE Edge VirtualizationはRancher Manager用のUI拡張機能を提供しており、Rancher Dashboard
UIを使用して基本的な仮想マシン管理を行うことができます。</para>
<section xml:id="id-installation-4">
<title>インストール</title>
<para>インストールのガイダンスについては、<link xl:href="rancher-dashboard-extensions.xml">Rancher
Dashboard拡張機能に関するセクション</link>を参照してください。</para>
</section>
<section xml:id="kubevirt-dashboard-extension">
<title>KubeVirt Rancher Dashboard拡張機能の使用</title>
<para>この拡張機能により、Cluster Explorerに新たに［<emphasis
role="strong">KubeVirt</emphasis>］セクションが導入されます。このセクションは、KubeVirtがインストールされている管理対象クラスタに追加されます。</para>
<para>この拡張機能を使用すると、次の2つのKubeVirtリソースを直接操作できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Virtual Machine Instances</literal> — 実行中の1つの仮想マシンインスタンスを表すリソース。</para>
</listitem>
<listitem>
<para><literal>Virtual Machines</literal> — 仮想マシンのライフサイクルを管理するために使用されるリソース。</para>
</listitem>
</orderedlist>
<section xml:id="id-creating-a-virtual-machine">
<title>仮想マシンの作成</title>
<orderedlist numeration="arabic">
<listitem>
<para>左側のナビゲーションでKubeVirtが有効な管理対象クラスタをクリックして、［<emphasis role="strong">Cluster
Explorer</emphasis>］に移動します。</para>
</listitem>
<listitem>
<para>［<emphasis role="strong">KubeVirt </emphasis>］ &gt; ［Virtual Machines
(仮想マシン)］ページで、画面の右上にある［ <literal>Create from YAML
(YAMLから作成)</literal>］をクリックします。</para>
</listitem>
<listitem>
<para>仮想マシンの定義を入力するか貼り付けて、［<literal>Create (作成)</literal>］を押します。「仮想マシンのデプロイ
」セクションの仮想マシンの定義を参考にしてください。</para>
</listitem>
</orderedlist>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="virtual-machines-page.png" width=""/>
</imageobject>
<textobject><phrase>［Virtual Machines (仮想マシン)］ページ</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-starting-and-stopping-virtual-machines">
<title>仮想マシンの起動と停止</title>
<para>仮想マシンを起動および停止するには、各仮想マシンの右側にある<emphasis
role="strong">⋮</emphasis>ドロップダウンリストからアクセスできるアクションメニューを使用するか、アクションを実行する仮想マシンを選択してリストの上部にあるグループアクションを使用します。</para>
<para><literal>spec.running</literal>プロパティが定義されている仮想マシンに対してのみ、起動および停止アクションを実行できます。<literal>spec.runStrategy</literal>が使用されている場合、そのようなマシンは直接起動および停止できません。詳細については、<link
xl:href="https://kubevirt.io/user-guide/virtual_machines/run_strategies/#run-strategies">KubeVirtのドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-accessing-virtual-machine-console">
<title>仮想マシンコンソールへのアクセス</title>
<para>［Virtual Machines (仮想マシン)］リストには［<literal>Console</literal>
(コンソール)］ドロップダウンリストがあり、ここから<emphasis
role="strong">VNCまたはシリアルコンソール</emphasis>を使用してマシンに接続できます。このアクションは、実行中のマシンでのみ使用できます。</para>
<para>新しく起動した仮想マシンでは、コンソールにアクセスできるようになるまでにしばらく時間がかかることがあります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="vnc-console-ui.png" width=""/>
</imageobject>
<textobject><phrase>VNCコンソールUI</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
</section>
</section>
<section xml:id="id-installing-with-edge-image-builder-4">
<title>Edge Image Builderを使用したインストール</title>
<para>SUSE Edgeは、ベースとなるSLE Micro OSイメージをカスタマイズするために<xref
linkend="components-eib"/>を使用しています。EIBによってプロビジョニングされたKubernetesクラスタ上にKubeVirtとCDIの両方をエアギャップインストールするには、<xref
linkend="kubevirt-install"/>に従ってください。</para>
</section>
</chapter>
</part>
<part xml:id="id-how-to-guides">
<title>ハウツーガイド</title>
<partintro>
<para>ハウツーガイドとベストプラクティス</para>
</partintro>
<chapter xml:id="guides-metallb-k3s">
<title>K3s上のMetalLB (L2を使用)</title>
<para>MetalLBは、標準のルーティングプロトコルを使用する、ベアメタルKubernetesクラスタ用のロードバランサの実装です。</para>
<para>このガイドでは、MetalLBをレイヤ2モードでデプロイする方法について説明します。</para>
<section xml:id="id-why-use-this-method-2">
<title>この方法を使用する理由</title>
<para>レイヤ2モードでは、1つのノードがローカルネットワークにサービスをアドバタイズする責任を負います。ネットワークの視点からは、マシンのネットワークインタフェースに複数のIPアドレスが割り当てられているように見えます。</para>
<para>レイヤ2モードの主な利点は、その汎用性です。あらゆるEthernetネットワークで動作し、特別なハードウェアも、高価なルータも必要ありません。</para>
</section>
<section xml:id="id-metallb-on-k3s-using-l2">
<title>K3s上のMetalLB (L2を使用)</title>
<para>このクイックスタートではL2モードを使用するので、特別なネットワーク機器は必要なく、ネットワーク範囲内の空きIPをいくつか用意するだけで十分です。DHCPプール外のIPであれば割り当てられることがないため理想的です。</para>
<para>この例では、DHCPプールは、<literal>192.168.122.0/24</literal>のネットワークに対して<literal>192.168.122.100-192.168.122.200</literal>です(IPは3つです。余分なIPの理由については、「TraefikとMetalLB」
(<xref
linkend="traefik-and-metallb"/>)を参照してください)。そのため、この範囲外であれば何でも構いません(ゲートウェイと、すでに実行されている可能性のある他のホストは除きます)。</para>
</section>
<section xml:id="id-prerequisites-7">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>MetalLBがデプロイされるK3sクラスタ。</para>
</listitem>
</itemizedlist>
<warning>
<para>K3sには、Klipperという名前の独自のサービスロードバランサが付属しています。<link
xl:href="https://metallb.universe.tf/configuration/k3s/">MetalLBを実行するにはKlipperを無効にする必要があります</link>。Klipperを無効にするには、<literal>--disable=servicelb</literal>フラグを使用してK3sをインストールする必要があります。</para>
</warning>
<itemizedlist>
<listitem>
<para>Helm</para>
</listitem>
<listitem>
<para>ネットワーク範囲内の数個の空きIP (ここでは<literal>192.168.122.10-192.168.122.12</literal>)</para>
</listitem>
</itemizedlist>
<section xml:id="id-deployment">
<title>デプロイメント</title>
<para>MetalLBはHelm (および他の方法)を利用するため、次のようになります。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add metallb https://metallb.github.io/metallb
helm install --create-namespace -n metallb-system metallb metallb/metallb

while ! kubectl wait --for condition=ready -n metallb-system $(kubectl get\
 pods -n metallb-system -l app.kubernetes.io/component=controller -o name)\
 --timeout=10s; do
 sleep 2
done</screen>
</section>
<section xml:id="id-configuration">
<title>設定</title>
<para>この時点でインストールは完了しています。次に、サンプル値を使用して<link
xl:href="https://metallb.universe.tf/configuration/">設定</link>を行います。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ip-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.122.10/32
  - 192.168.122.11/32
  - 192.168.122.12/32
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
EOF</screen>
<para>これで、MetalLBを使用する準備ができました。L2モードでは、次のようにさまざまな設定をカスタマイズできます。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services">IPv6とデュアルスタックサービス</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation">アドレスの自動割り当てを制御する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service">アドレス割り当ての範囲を特定のネームスペースとサービスに縮小する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from">サービスをアナウンスできるノードのセットを制限する</link></para>
</listitem>
<listitem>
<para><link
xl:href="https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from">LBのIPをアナウンスできるネットワークインタフェースを指定する</link></para>
</listitem>
</itemizedlist>
<para>また、<link
xl:href="https://metallb.universe.tf/configuration/_advanced_bgp_configuration/">BGP</link>についても、さらに多くのカスタマイズが可能です。</para>
</section>
<section xml:id="traefik-and-metallb">
<title>TraefikとMetalLB</title>
<para>Traefikは、デフォルトではK3sとともにデプロイされます(<literal>--disable=traefik</literal>を使用して<link
xl:href="https://docs.k3s.io/networking#traefik-ingress-controller">K3sを無効にできます</link>)。また、デフォルトで<literal>LoadBalancer</literal>として公開されます(Klipperで使用するため)。ただし、Klipperを無効にする必要があるため、Ingress用Traefikサービスは<literal>LoadBalancer</literal>タイプのままです。そのため、MetalLBをデプロイした時点では、最初のIPは自動的にTraefik
Ingressに割り当てられます。</para>
<screen language="console" linenumbering="unnumbered"># Before deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   &lt;pending&gt;     80:31093/TCP,443:32095/TCP   28s
# After deploying MetalLB
kubectl get svc -n kube-system traefik
NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      AGE
traefik   LoadBalancer   10.43.44.113   192.168.122.10   80:31093/TCP,443:32095/TCP   3m10s</screen>
<para>これは、このプロセスの後半(<xref linkend="ingress-with-metallb"/>)で適用されます。</para>
</section>
<section xml:id="id-usage">
<title>使用法</title>
<para>デプロイメントの例を作成してみましょう。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
---
apiVersion: v1
kind: Namespace
metadata:
  name: hello-kubernetes
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: hello-kubernetes
  template:
    metadata:
      labels:
        app.kubernetes.io/name: hello-kubernetes
    spec:
      serviceAccountName: hello-kubernetes
      containers:
        - name: hello-kubernetes
          image: "paulbouwer/hello-kubernetes:1.10"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          env:
          - name: HANDLER_PATH_PREFIX
            value: ""
          - name: RENDER_PATH_PREFIX
            value: ""
          - name: KUBERNETES_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: KUBERNETES_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: KUBERNETES_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: CONTAINER_IMAGE
            value: "paulbouwer/hello-kubernetes:1.10"
EOF</screen>
<para>最終的にサービスは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: hello-kubernetes
  namespace: hello-kubernetes
  labels:
    app.kubernetes.io/name: hello-kubernetes
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: hello-kubernetes
EOF</screen>
<para>実際の動作を見てみましょう。</para>
<screen language="console" linenumbering="unnumbered">kubectl get svc -n hello-kubernetes
NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        AGE
hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   80:31461/TCP   8s

curl http://192.168.122.11
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-2c6ps&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
</section>
</section>
<section xml:id="ingress-with-metallb">
<title>IngressとMetalLB</title>
<para>TraefikはすでにIngressコントローラとして機能しているため、次のような<literal>Ingress</literal>オブジェクトを介してHTTP/HTTPSトラフィックを公開できます。</para>
<screen language="yaml" linenumbering="unnumbered">IP=$(kubectl get svc -n kube-system traefik -o jsonpath="{.status.loadBalancer.ingress[0].ip}")
cat &lt;&lt;- EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: hello-kubernetes-ingress
  namespace: hello-kubernetes
spec:
  rules:
  - host: hellok3s.${IP}.sslip.io
    http:
      paths:
        - path: "/"
          pathType: Prefix
          backend:
            service:
              name: hello-kubernetes
              port:
                name: http
EOF</screen>
<para>これにより、次のような結果が返されます。</para>
<screen language="console" linenumbering="unnumbered">curl http://hellok3s.${IP}.sslip.io
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
    &lt;title&gt;Hello Kubernetes!&lt;/title&gt;
    &lt;link rel="stylesheet" type="text/css" href="/css/main.css"&gt;
    &lt;link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300" &gt;
&lt;/head&gt;
&lt;body&gt;

  &lt;div class="main"&gt;
    &lt;img src="/images/kubernetes.png"/&gt;
    &lt;div class="content"&gt;
      &lt;div id="message"&gt;
  Hello world!
&lt;/div&gt;
&lt;div id="info"&gt;
  &lt;table&gt;
    &lt;tr&gt;
      &lt;th&gt;namespace:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;pod:&lt;/th&gt;
      &lt;td&gt;hello-kubernetes-7c8575c848-fvqm2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;node:&lt;/th&gt;
      &lt;td&gt;allinone (Linux 5.14.21-150400.24.46-default)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/table&gt;
&lt;/div&gt;
&lt;div id="footer"&gt;
  paulbouwer/hello-kubernetes:1.10 (linux/amd64)
&lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;

&lt;/body&gt;
&lt;/html&gt;</screen>
<para>また、MetalLB が正しく動作していることを確認するため、<literal>arping</literal>を次のように使用できます:</para>
<para><literal>arping hellok3s.${IP}.sslip.io</literal></para>
<para>予期される結果は次のとおりです。</para>
<screen language="console" linenumbering="unnumbered">ARPING 192.168.64.210
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec
60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec</screen>
<para>上記の例では、トラフィックは次のように流れます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>hellok3s.${IP}.sslip.io</literal>が実際のIPに解決されます。</para>
</listitem>
<listitem>
<para>続いて、トラフィックが<literal>metallb-speaker</literal> Podによって処理されます。</para>
</listitem>
<listitem>
<para><literal>metallb-speaker</literal>がトラフィックを<literal>traefik</literal>コントローラにリダイレクトします。</para>
</listitem>
<listitem>
<para>最後に、Traefikが要求を<literal>hello-kubernetes</literal>サービスに転送します。</para>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="guides-metallb-kubernetes">
<title>Kubernetes APIサーバの前面のMetalLB</title>
<para>このガイドでは、MetalLBサービスを使用して、3つのコントロールプレーンノードを持つHA K3sクラスタ上でK3s
APIを外部に公開する方法を示します。これを実現するために、<literal>LoadBalancer</literal>タイプのKubernetes
ServiceとEndpointsを手動で作成します。Endpointsは、クラスタで使用可能なすべてのコントロールプレーンノードのIPを保持します。Endpointsをクラスタで発生するイベント(ノードの追加/削除やノードのオフライン化)と継続的に同期するために<link
xl:href="https://github.com/suse-edge/endpoint-copier-operator">Endpoint
Copier Operator</link>がデプロイされます。Operatorはデフォルトの<literal>kubernetes</literal>
Endpointで発生するイベントを監視し、管理対象を自動的に更新して同期を維持します。管理対象のServiceは<literal>LoadBalancer</literal>タイプであるため、<literal>MetalLB</literal>は静的な<literal>ExternalIP</literal>を割り当てます。この<literal>ExternalIP</literal>はAPI
Serverとの通信に使用されます。</para>
<section xml:id="id-prerequisites-8">
<title>前提条件</title>
<itemizedlist>
<listitem>
<para>K3sをデプロイするための3つのホスト。</para>
<itemizedlist>
<listitem>
<para>ホスト名は各ホストで違う名前にしてください。</para>
</listitem>
<listitem>
<para>テストの場合は仮想マシンを使用できます。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>ネットワーク内で2つ以上のIPが使用可能(Traefik用に1つ、管理対象サービス用に1つ)。</para>
</listitem>
<listitem>
<para>Helm</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-installing-k3s">
<title>K3sのインストール</title>
<note>
<para>新しいクラスタが必要なく、既存のクラスタを使用する場合は、この手順をスキップして次の手順に進んでください。</para>
</note>
<para>まず、ネットワーク内の空きIPを、後で管理対象のServiceの<literal>ExternalIP</literal>で使用できるように予約する必要があります。</para>
<para>最初のホストにSSHで接続して、次のようにクラスタモードで<literal>K3s</literal>をインストールします。</para>
<screen language="bash" linenumbering="unnumbered"># Export the free IP mentioned above
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --cluster-init \
 --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} \
 --tls-san=https://${VIP_SERVICE_IP}.sslip.io" K3S_TOKEN=foobar sh -</screen>
<note>
<para>必ず、<literal>k3s
server</literal>コマンドで<literal>--disable=servicelb</literal>フラグを指定してください。</para>
</note>
<important>
<para>これ以降、コマンドはローカルマシンで実行する必要があります。</para>
</important>
<para>APIサーバに外部からアクセスするには、K3s VMのIPを使用します。</para>
<screen language="bash" linenumbering="unnumbered"># Replace &lt;node-ip&gt; with the actual IP of the machine
export NODE_IP=&lt;node-ip&gt;
scp ${NODE_IP}:/etc/rancher/k3s/k3s.yaml ~/.kube/config &amp;&amp; sed \
 -i '' "s/127.0.0.1/${NODE_IP}/g" ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config</screen>
</section>
<section xml:id="id-configuring-an-existing-k3s-cluster">
<title>既存のK3sクラスタの設定</title>
<note>
<para>この手順は、既存のK3sクラスタを使用する 場合にのみ有効です。</para>
</note>
<para>既存のK3sクラスタを使用するには、<literal>servicelb</literal>
LBを無効にし、<literal>tls-san</literal>フラグも変更する必要があります。</para>
<para>K3sフラグを変更するには、クラスタのすべてのVMで<literal>/etc/systemd/system/k3s.service</literal>を変更する必要があります。</para>
<para>フラグは<literal>ExecStart</literal>に挿入する必要があります。例:</para>
<screen language="shell" linenumbering="unnumbered"># Replace the &lt;vip-service-ip&gt; with the actual ip
ExecStart=/usr/local/bin/k3s \
    server \
        '--cluster-init' \
        '--write-kubeconfig-mode=644' \
        '--disable=servicelb' \
        '--tls-san=&lt;vip-service-ip&gt;' \
        '--tls-san=https://&lt;vip-service-ip&gt;.sslip.io' \</screen>
<para>次に、次のコマンドを実行して、K3sに新しい設定をロードする必要があります。</para>
<screen language="bash" linenumbering="unnumbered">systemctl daemon-reload
systemctl restart k3s</screen>
</section>
<section xml:id="id-installing-metallb">
<title>MetalLBのインストール</title>
<para><literal>MetalLB</literal>をデプロイするには、「<link
xl:href="https://suse-edge.github.io/docs/quickstart/metallb">K3s上のMetalLB</link>」のガイドを使用できます。</para>
<para><emphasis role="strong">メモ: </emphasis><literal>ip-pool</literal>
IPAddressPoolのIPアドレスが、以前に<literal>LoadBalancer</literal>サービスに対して選択したIPアドレスと重複していないことを確認してください。</para>
<para>管理対象サービスにのみ使用する<literal>IpAddressPool</literal>を別途作成します。</para>
<screen language="yaml" linenumbering="unnumbered"># Export the VIP_SERVICE_IP on the local machine
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;

cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
  - ${VIP_SERVICE_IP}/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
EOF</screen>
<screen language="yaml" linenumbering="unnumbered">cat &lt;&lt;-EOF | kubectl apply -f -
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ip-pool
  - kubernetes-vip-ip-pool
EOF</screen>
</section>
<section xml:id="id-installing-the-endpoint-copier-operator">
<title>Endpoint Copier Operatorのインストール</title>
<screen language="bash" linenumbering="unnumbered">helm repo add endpoint-copier-operator \
 https://suse-edge.github.io/endpoint-copier-operator

helm install --create-namespace -n endpoint-copier-operator \
 endpoint-copier-operator endpoint-copier-operator/endpoint-copier-operator</screen>
<para>上記のコマンドは、クラスタに3つの異なるリソースをデプロイします。</para>
<orderedlist numeration="arabic">
<listitem>
<para>2つのレプリカを持つ<literal>endpoint-copier-operator</literal>オペレータのデプロイメント。一方がリーダーとなり、他方は必要に応じてリーダーの役割を引き継ぎます。</para>
</listitem>
<listitem>
<para><literal>default</literal>ネームスペース内の<literal>kubernetes-vip</literal>という名前のKubernetesサービス。これは<literal>kubernetes</literal>サービスのコピーですが、<literal>LoadBalancer</literal>タイプです。</para>
</listitem>
<listitem>
<para><literal>default</literal>ネームスペース内の<literal>kubernetes-vip</literal>という名前のEndpointリソース。これは<literal>kubernetes</literal>
Endpointのコピーです。</para>
</listitem>
</orderedlist>
<para><literal>kubernetes-vip</literal>サービスのIPアドレスが正しいことを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get service kubernetes-vip -n default \
 -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'</screen>
<para><literal>default</literal>ネームスペースの<literal>kubernetes-vip</literal>および<literal>kubernetes</literal>
のEndpointsリソースが同じIPを指していることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get endpoints kubernetes kubernetes-vip</screen>
<para>すべて問題なければ、最後に<literal>Kubeconfig</literal>で<literal>VIP_SERVICE_IP</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' "s/${NODE_IP}/${VIP_SERVICE_IP}/g" ~/.kube/config</screen>
<para>これ以降、<literal>kubectl</literal>はすべて<literal>kubernetes-vip</literal>サービスを経由するようになります。</para>
</section>
<section xml:id="id-adding-control-plane-nodes">
<title>コントロールプレーンノードの追加</title>
<para>プロセス全体を監視するため、端末タブを2つ以上開くことができます。</para>
<para>最初の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get nodes</screen>
<para>2つ目の端末:</para>
<screen language="bash" linenumbering="unnumbered">watch kubectl get endpoints</screen>
<para>次に、以下のコマンドを2つ目のノードと3つ目のノードで実行します。</para>
<screen language="bash" linenumbering="unnumbered"># Export the VIP_SERVICE_IP in the VM
# Replace with the actual IP
export VIP_SERVICE_IP=&lt;ip&gt;
export INSTALL_K3S_SKIP_START=false

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \
 --write-kubeconfig-mode=644" K3S_TOKEN=foobar sh -</screen>
</section>
</chapter>
<chapter xml:id="id-air-gapped-deployments-with-edge-image-builder">
<title>Edge Image Builderを使用したエアギャップデプロイメント</title>
<section xml:id="id-intro">
<title>概要</title>
<para>このガイドでは、Edge Image Builder(EIB) (<xref
linkend="components-eib"/>)を使用し、完全にエアギャップされた環境で複数のSUSE EdgeコンポーネントをSLE Micro
5.5上にデプロイする方法を示します。これにより、EIBで作成したCustomized, Ready to Boot
(CRB)イメージでブートし、指定したコンポーネントをインターネット接続や手動手順なしにRKE2クラスタまたはK3sクラスタにデプロイできます。この設定は、デプロイメントに必要なアーティファクトをすべてOSイメージにプリベイクし、ブート後すぐに利用できるようにしたいお客様にとって非常に便利です。</para>
<para>ここでは、以下のエアギャップインストールについて説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="components-rancher"/></para>
</listitem>
<listitem>
<para><xref linkend="components-neuvector"/></para>
</listitem>
<listitem>
<para><xref linkend="components-longhorn"/></para>
</listitem>
<listitem>
<para><xref linkend="components-kubevirt"/></para>
</listitem>
</itemizedlist>
<warning>
<para>EIBは、指定したHelmチャートとKubernetesマニフェストで参照されているイメージをすべて解析し、事前にダウンロードします。ただし、その一部がコンテナイメージをプルし、そのイメージに基づいて実行時にKubernetesリソースを作成しようとする場合があります。このような場合、完全なエアギャップ環境を設定するには、必要なイメージを定義ファイルに手動で指定する必要があります。</para>
</warning>
</section>
<section xml:id="id-prerequisites-9">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、すでにEIB (<xref
linkend="components-eib"/>)に精通していることを想定しています。まだEIBに精通していない場合は、クイックスタートガイド(<xref
linkend="quickstart-eib"/>)に従って、以下の演習で示されている概念の理解を深めてください。</para>
</section>
<section xml:id="id-libvirt-network-configuration">
<title>Libvirtのネットワーク設定</title>
<note>
<para>エアギャップデプロイメントのデモを示すため、このガイドはシミュレートされたエアギャップ<literal>libvirt</literal>ネットワークを使用して実施し、それに合わせて以下の設定を調整します。ご自身のデプロイメントでは、<literal>host1.local.yaml</literal>の設定の変更が必要になる場合があります。これについては、次の手順で説明します。</para>
</note>
<para>同じ<literal>libvirt</literal>ネットワーク設定を使用する場合は、このまま読み進めてください。そうでない場合は、<xref
linkend="config-dir-creation"/>までスキップしてください。</para>
<para>DHCPのIPアドレス範囲<literal>192.168.100.2/24</literal>で、分離されたネットワーク設定を作成してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; isolatednetwork.xml
&lt;network&gt;
  &lt;name&gt;isolatednetwork&lt;/name&gt;
  &lt;bridge name='virbr1' stp='on' delay='0'/&gt;
  &lt;ip address='192.168.100.1' netmask='255.255.255.0'&gt;
    &lt;dhcp&gt;
      &lt;range start='192.168.100.2' end='192.168.100.254'/&gt;
    &lt;/dhcp&gt;
  &lt;/ip&gt;
&lt;/network&gt;
EOF</screen>
<para>あとはネットワークを作成して起動するだけです。</para>
<screen language="shell" linenumbering="unnumbered">virsh net-define isolatednetwork.xml
virsh net-start isolatednetwork</screen>
</section>
<section xml:id="config-dir-creation">
<title>ベースディレクトリの設定</title>
<para>ベースディレクトリの設定は、各種のコンポーネントすべてで同じであるため、ここで設定します。</para>
<para>まず、必要なサブディレクトリを作成します。</para>
<screen language="shell" linenumbering="unnumbered">export CONFIG_DIR=$HOME/config
mkdir -p $CONFIG_DIR/base-images
mkdir -p $CONFIG_DIR/network
mkdir -p $CONFIG_DIR/kubernetes/helm/values</screen>
<para>必ず、使用する予定のゴールデンイメージを<literal>base-images</literal>ディレクトリに追加してください。このガイドでは、<link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>にあるセルフインストールISOに焦点を当てて説明します。</para>
<para>ダウンロードしたイメージをコピーしましょう。</para>
<screen language="shell" linenumbering="unnumbered">cp SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso $CONFIG_DIR/base-images/slemicro.iso</screen>
<note>
<para>EIBは、ゴールデンイメージの入力を変更することはありません。</para>
</note>
<para>目的のネットワーク設定を含むファイルを作成しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/network/host1.local.yaml
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: 192.168.100.1
    next-hop-interface: eth0
    table-id: 254
  - destination: 192.168.100.0/24
    metric: 100
    next-hop-address:
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - 192.168.100.1
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: 34:8A:B1:4B:16:E7
  ipv4:
    address:
    - ip: 192.168.100.50
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
EOF</screen>
<para>この設定により、プロビジョニングされたシステムに以下が確実に存在するようになります(指定されたMACアドレスを使用)。</para>
<itemizedlist>
<listitem>
<para>静的IPアドレスを持つEthernetインタフェース</para>
</listitem>
<listitem>
<para>ルーティング</para>
</listitem>
<listitem>
<para>DNS</para>
</listitem>
<listitem>
<para>ホスト名(<literal>host1.local</literal>)</para>
</listitem>
</itemizedlist>
<para>結果のファイル構造は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">├── kubernetes/
│   └── helm/
│       └── values/
├── base-images/
│   └── slemicro.iso
└── network/
    └── host1.local.yaml</screen>
</section>
<section xml:id="id-base-definition-file">
<title>ベース定義ファイル</title>
<para>Edge Image Builderでは、<emphasis>定義ファイル</emphasis>を使用してSLE
Microイメージを変更します。定義ファイルには、設定可能なオプションの大部分が含まれています。これらのオプションの多くは、異なるコンポーネントのセクションで繰り返し使用されるため、ここで一覧にして説明します。</para>
<tip>
<para>定義ファイルのカスタマイズオプションの全リストについては、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#image-definition-file">アップストリームドキュメント</link>を参照してください。</para>
</tip>
<para>すべての定義ファイルに存在する次のフィールドを見てみましょう。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
embeddedArtifactRegistry:
  images:
    - ...</screen>
<para><literal>image</literal>セクションは必須であり、入力イメージ、そのアーキテクチャとタイプ、および出力イメージの名前を指定します。</para>
<para><literal>operatingSystem</literal>セクションはオプションであり、プロビジョニングされたシステムに<literal>root/eib</literal>のユーザ名/パスワードでログインできるようにするための設定が含まれます。</para>
<para><literal>kubernetes</literal>セクションはオプションであり、
Kubernetesのタイプとバージョンを定義します。デフォルトではKubernetes
1.28.9とRKE2を使用します。代わりにK3sが必要な場合は、<literal>kubernetes.version:
v1.28.9+k3s1</literal>を使用します。<literal>kubernetes.nodes</literal>フィールドで明示的に設定しない限り、このガイドでブートストラップするすべてのクラスタはシングルノードクラスタになります。</para>
<para><literal>embeddedArtifactRegistry</literal>セクションには、実行時に特定のコンポーネントでのみ参照されてプルされるイメージがすべて含まれます。</para>
</section>
<section xml:id="rancher-install">
<title>Rancherのインストール</title>
<note>
<para>デモで示すRancher (<xref
linkend="components-rancher"/>)のデプロイメントは、デモのために非常にスリム化されています。実際のデプロイメントでは、設定に応じて追加のアーティファクトが必要な場合があります。</para>
</note>
<para><link
xl:href="https://github.com/rancher/rancher/releases/tag/v2.8.4">Rancher
v2.8.4</link>リリースアセットには、エアギャップインストールに必要なすべてのイメージを一覧にした<literal>rancher-images.txt</literal>ファイルが含まれています。</para>
<para>コンテナイメージは合計で約602個あります。つまり、生成されるCRBイメージは約28GB以上になります。ここでのRancherのインストールでは、そのリストを最小の動作設定に減らします。そこから、自身のデプロイメントに必要なイメージを追加し直すことができます。</para>
<para>定義ファイルを作成し、必要最小限のイメージリストを含めます。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  network:
    apiVIP: 192.168.100.151
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.14.2/cert-manager.crds.yaml
  helm:
    charts:
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        valuesFile: rancher-values.yaml
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cert-manager
        installationNamespace: kube-system
        createNamespace: true
        repositoryName: jetstack
        targetNamespace: cert-manager
        version: 1.14.2
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-prime
        url:  https://charts.rancher.com/server-charts/prime
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.13
    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7
    - name: registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1
    - name: registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1
    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9
    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4
    - name: registry.rancher.com/rancher/fleet:v0.9.4
    - name: registry.rancher.com/rancher/gitjob:v0.9.7
    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410
    - name: registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10
    - name: registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1
    - name: registry.rancher.com/rancher/kubectl:v1.28.7
    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4
    - name: registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1
    - name: registry.rancher.com/rancher/pause:3.6
    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0
    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1
    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2
    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1
    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5
    - name: registry.rancher.com/rancher/rancher:v2.8.4
    - name: registry.rancher.com/rancher/rke-tools:v0.1.96
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412
    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.15
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1
    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0
    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1
    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5
    - name: registry.rancher.com/rancher/kubectl:v1.20.2</screen>
<para>602個のコンテナイメージがすべて含まれるリストと比較すると、この縮小バージョンには62個しか含まれていません。その結果、新しいCRBイメージは約7GBほどになります。</para>
<para>RancherのHelm値も作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml
hostname: 192.168.100.50.sslip.io
replicas: 1
bootstrapPassword: "adminadminadmin"
systemDefaultRegistry: registry.rancher.com
useBundledSystemChart: true
EOF</screen>
<warning>
<para><literal>systemDefaultRegistry</literal>を<literal>registry.rancher.com</literal>に設定することで、Rancherは、ブート時にCRBイメージ内で起動される組み込みのアーティファクトレジストリ内でイメージを自動的に検索できます。このフィールドを省略すると、ノードでコンテナイメージを見つけられない場合があります。</para>
</warning>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 17 MB/s)
Populating Embedded Artifact Registry... 100% (69/69, 26 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (780/780 MB, 115 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 108 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 117 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 34 MB/s)
Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 106 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Rancherのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -A --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAMESPACE                         NAME                                                        READY   STATUS      RESTARTS   AGE
cattle-fleet-local-system         pod/fleet-agent-68f4d5d5f7-tdlk7                            1/1     Running     0          34s
cattle-fleet-system               pod/fleet-controller-85564cc978-pbtvk                       1/1     Running     0          5m51s
cattle-fleet-system               pod/gitjob-9dc58fb5b-7cwsw                                  1/1     Running     0          5m51s
cattle-provisioning-capi-system   pod/capi-controller-manager-5c57b4b8f7-wlp5k                1/1     Running     0          4m52s
cattle-system                     pod/helm-operation-4fk5c                                    0/2     Completed   0          37s
cattle-system                     pod/helm-operation-6zgbq                                    0/2     Completed   0          4m54s
cattle-system                     pod/helm-operation-cjds5                                    0/2     Completed   0          5m37s
cattle-system                     pod/helm-operation-kt5c2                                    0/2     Completed   0          5m21s
cattle-system                     pod/helm-operation-ppgtw                                    0/2     Completed   0          5m30s
cattle-system                     pod/helm-operation-tvcwk                                    0/2     Completed   0          5m54s
cattle-system                     pod/helm-operation-wpxd4                                    0/2     Completed   0          53s
cattle-system                     pod/rancher-58575f9575-svrg2                                1/1     Running     0          6m34s
cattle-system                     pod/rancher-webhook-5c6556f7ff-vgmkt                        1/1     Running     0          5m19s
cert-manager                      pod/cert-manager-6c69f9f796-fkm8f                           1/1     Running     0          7m14s
cert-manager                      pod/cert-manager-cainjector-584f44558c-wg7p6                1/1     Running     0          7m14s
cert-manager                      pod/cert-manager-webhook-76f9945d6f-lv2nv                   1/1     Running     0          7m14s
endpoint-copier-operator          pod/endpoint-copier-operator-58964b659b-l64dk               1/1     Running     0          7m16s
endpoint-copier-operator          pod/endpoint-copier-operator-58964b659b-z9t9d               1/1     Running     0          7m16s
kube-system                       pod/cilium-fht55                                            1/1     Running     0          7m32s
kube-system                       pod/cilium-operator-558bbf6cfd-gwfwf                        1/1     Running     0          7m32s
kube-system                       pod/cilium-operator-558bbf6cfd-qsxb5                        0/1     Pending     0          7m32s
kube-system                       pod/cloud-controller-manager-host1.local                    1/1     Running     0          7m21s
kube-system                       pod/etcd-host1.local                                        1/1     Running     0          7m8s
kube-system                       pod/helm-install-cert-manager-fvbtt                         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-endpoint-copier-operator-5kkgw             0/1     Completed   0          8m12s
kube-system                       pod/helm-install-metallb-zfphb                              0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rancher-nc4nt                              0/1     Completed   2          8m12s
kube-system                       pod/helm-install-rke2-cilium-7wq87                          0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-coredns-nl4gc                         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-ingress-nginx-svjqd                   0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-metrics-server-gqgqz                  0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-snapshot-controller-crd-r6b5p         0/1     Completed   0          8m12s
kube-system                       pod/helm-install-rke2-snapshot-controller-ss9v4             0/1     Completed   1          8m12s
kube-system                       pod/helm-install-rke2-snapshot-validation-webhook-vlkpn     0/1     Completed   0          8m12s
kube-system                       pod/kube-apiserver-host1.local                              1/1     Running     0          7m29s
kube-system                       pod/kube-controller-manager-host1.local                     1/1     Running     0          7m30s
kube-system                       pod/kube-proxy-host1.local                                  1/1     Running     0          7m30s
kube-system                       pod/kube-scheduler-host1.local                              1/1     Running     0          7m42s
kube-system                       pod/rke2-coredns-rke2-coredns-6c8d9bb6d-qlwc8               1/1     Running     0          7m31s
kube-system                       pod/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf-j5r2z   1/1     Running     0          7m31s
kube-system                       pod/rke2-ingress-nginx-controller-4h2mm                     1/1     Running     0          7m3s
kube-system                       pod/rke2-metrics-server-544c8c66fc-lsrc6                    1/1     Running     0          7m15s
kube-system                       pod/rke2-snapshot-controller-59cc9cd8f4-4wx75               1/1     Running     0          7m14s
kube-system                       pod/rke2-snapshot-validation-webhook-54c5989b65-5kp2x       1/1     Running     0          7m15s
metallb-system                    pod/metallb-controller-5895d8446d-z54lm                     1/1     Running     0          7m15s
metallb-system                    pod/metallb-speaker-fxwgk                                   1/1     Running     0          7m15s

NAMESPACE                         NAME                                              TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)
         AGE
cattle-fleet-system               service/gitjob                                    ClusterIP      10.43.30.8      &lt;none&gt;            80/TCP
         5m51s
cattle-provisioning-capi-system   service/capi-webhook-service                      ClusterIP      10.43.7.100     &lt;none&gt;            443/TCP
         4m52s
cattle-system                     service/rancher                                   ClusterIP      10.43.100.229   &lt;none&gt;            80/TCP,443/TCP
         6m34s
cattle-system                     service/rancher-webhook                           ClusterIP      10.43.121.133   &lt;none&gt;            443/TCP
         5m19s
cert-manager                      service/cert-manager                              ClusterIP      10.43.140.65    &lt;none&gt;            9402/TCP
         7m14s
cert-manager                      service/cert-manager-webhook                      ClusterIP      10.43.108.158   &lt;none&gt;            443/TCP
         7m14s
default                           service/kubernetes                                ClusterIP      10.43.0.1       &lt;none&gt;            443/TCP
         8m26s
default                           service/kubernetes-vip                            LoadBalancer   10.43.138.138   192.168.100.151   9345:31006/TCP,6443:31599/TCP   8m21s
kube-system                       service/cilium-agent                              ClusterIP      None            &lt;none&gt;            9964/TCP
         7m32s
kube-system                       service/rke2-coredns-rke2-coredns                 ClusterIP      10.43.0.10      &lt;none&gt;            53/UDP,53/TCP
         7m31s
kube-system                       service/rke2-ingress-nginx-controller-admission   ClusterIP      10.43.157.19    &lt;none&gt;            443/TCP
         7m3s
kube-system                       service/rke2-metrics-server                       ClusterIP      10.43.4.123     &lt;none&gt;            443/TCP
         7m15s
kube-system                       service/rke2-snapshot-validation-webhook          ClusterIP      10.43.91.161    &lt;none&gt;            443/TCP
         7m16s
metallb-system                    service/metallb-webhook-service                   ClusterIP      10.43.71.192    &lt;none&gt;            443/TCP
         7m15s

NAMESPACE        NAME                                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system      daemonset.apps/cilium                          1         1         1       1            1           kubernetes.io/os=linux   7m32s
kube-system      daemonset.apps/rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux   7m3s
metallb-system   daemonset.apps/metallb-speaker                 1         1         1       1            1           kubernetes.io/os=linux   7m15s

NAMESPACE                         NAME                                                   READY   UP-TO-DATE   AVAILABLE   AGE
cattle-fleet-local-system         deployment.apps/fleet-agent                            1/1     1            1           34s
cattle-fleet-system               deployment.apps/fleet-controller                       1/1     1            1           5m51s
cattle-fleet-system               deployment.apps/gitjob                                 1/1     1            1           5m51s
cattle-provisioning-capi-system   deployment.apps/capi-controller-manager                1/1     1            1           4m52s
cattle-system                     deployment.apps/rancher                                1/1     1            1           6m34s
cattle-system                     deployment.apps/rancher-webhook                        1/1     1            1           5m19s
cert-manager                      deployment.apps/cert-manager                           1/1     1            1           7m14s
cert-manager                      deployment.apps/cert-manager-cainjector                1/1     1            1           7m14s
cert-manager                      deployment.apps/cert-manager-webhook                   1/1     1            1           7m14s
endpoint-copier-operator          deployment.apps/endpoint-copier-operator               2/2     2            2           7m16s
kube-system                       deployment.apps/cilium-operator                        1/2     2            1           7m32s
kube-system                       deployment.apps/rke2-coredns-rke2-coredns              1/1     1            1           7m31s
kube-system                       deployment.apps/rke2-coredns-rke2-coredns-autoscaler   1/1     1            1           7m31s
kube-system                       deployment.apps/rke2-metrics-server                    1/1     1            1           7m15s
kube-system                       deployment.apps/rke2-snapshot-controller               1/1     1            1           7m14s
kube-system                       deployment.apps/rke2-snapshot-validation-webhook       1/1     1            1           7m15s
metallb-system                    deployment.apps/metallb-controller                     1/1     1            1           7m15s

NAMESPACE                         NAME                                                              DESIRED   CURRENT   READY   AGE
cattle-fleet-local-system         replicaset.apps/fleet-agent-68f4d5d5f7                            1         1         1       34s
cattle-fleet-system               replicaset.apps/fleet-controller-85564cc978                       1         1         1       5m51s
cattle-fleet-system               replicaset.apps/gitjob-9dc58fb5b                                  1         1         1       5m51s
cattle-provisioning-capi-system   replicaset.apps/capi-controller-manager-5c57b4b8f7                1         1         1       4m52s
cattle-system                     replicaset.apps/rancher-58575f9575                                1         1         1       6m34s
cattle-system                     replicaset.apps/rancher-webhook-5c6556f7ff                        1         1         1       5m19s
cert-manager                      replicaset.apps/cert-manager-6c69f9f796                           1         1         1       7m14s
cert-manager                      replicaset.apps/cert-manager-cainjector-584f44558c                1         1         1       7m14s
cert-manager                      replicaset.apps/cert-manager-webhook-76f9945d6f                   1         1         1       7m14s
endpoint-copier-operator          replicaset.apps/endpoint-copier-operator-58964b659b               2         2         2       7m16s
kube-system                       replicaset.apps/cilium-operator-558bbf6cfd                        2         2         1       7m32s
kube-system                       replicaset.apps/rke2-coredns-rke2-coredns-6c8d9bb6d               1         1         1       7m31s
kube-system                       replicaset.apps/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf   1         1         1       7m31s
kube-system                       replicaset.apps/rke2-metrics-server-544c8c66fc                    1         1         1       7m15s
kube-system                       replicaset.apps/rke2-snapshot-controller-59cc9cd8f4               1         1         1       7m14s
kube-system                       replicaset.apps/rke2-snapshot-validation-webhook-54c5989b65       1         1         1       7m15s
metallb-system                    replicaset.apps/metallb-controller-5895d8446d                     1         1         1       7m15s

NAMESPACE     NAME                                                      COMPLETIONS   DURATION   AGE
kube-system   job.batch/helm-install-cert-manager                       1/1           85s        8m21s
kube-system   job.batch/helm-install-endpoint-copier-operator           1/1           59s        8m21s
kube-system   job.batch/helm-install-metallb                            1/1           60s        8m21s
kube-system   job.batch/helm-install-rancher                            1/1           100s       8m21s
kube-system   job.batch/helm-install-rke2-cilium                        1/1           44s        8m18s
kube-system   job.batch/helm-install-rke2-coredns                       1/1           45s        8m18s
kube-system   job.batch/helm-install-rke2-ingress-nginx                 1/1           76s        8m16s
kube-system   job.batch/helm-install-rke2-metrics-server                1/1           60s        8m16s
kube-system   job.batch/helm-install-rke2-snapshot-controller           1/1           61s        8m15s
kube-system   job.batch/helm-install-rke2-snapshot-controller-crd       1/1           60s        8m16s
kube-system   job.batch/helm-install-rke2-snapshot-validation-webhook   1/1           60s        8m14s</screen>
<para>そして、<literal><link
xl:href="https://192.168.100.50.sslip.io">https://192.168.100.50.sslip.io</link></literal>にアクセスし、前に設定した<literal>adminadminadmin</literal>のパスワードでログインすると、Rancher
Dashboardが表示されます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="air-gapped-rancher.png" width=""/>
</imageobject>
<textobject><phrase>エアギャップRancher</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="neuvector-install">
<title>NeuVectorのインストール</title>
<para>Rancherのインストールとは異なり、NeuVectorのインストールではEIBで特別な処理を行う必要はありません。EIBはNeuVectorに必要なすべてのイメージを自動的にエアギャップ化します。</para>
<para>定義ファイルを作成します。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector-values.yaml
    repositories:
      - name: rancher-charts
        url: https://charts.rancher.io/</screen>
<para>NeuVector用のHelm値ファイルも作成します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml
controller:
  replicas: 1
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false
EOF</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (6/6, 20 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、NeuVectorのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                           READY   STATUS    RESTARTS   AGE
pod/neuvector-controller-pod-bc74745cf-x9fsc   1/1     Running   0          13m
pod/neuvector-enforcer-pod-vzw7t               1/1     Running   0          13m

NAME                                      TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                         AGE
service/neuvector-svc-admission-webhook   ClusterIP   10.43.240.25   &lt;none&gt;        443/TCP                         13m
service/neuvector-svc-controller          ClusterIP   None           &lt;none&gt;        18300/TCP,18301/TCP,18301/UDP   13m

NAME                                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/neuvector-enforcer-pod   1         1         1       1            1           &lt;none&gt;          13m

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/neuvector-controller-pod   1/1     1            1           13m

NAME                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/neuvector-controller-pod-bc74745cf   1         1         1       13m

NAME                                  SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cronjob.batch/neuvector-updater-pod   0 0 * * *   False     0        &lt;none&gt;          13m</screen>
</section>
<section xml:id="longhorn-install">
<title>Longhornのインストール</title>
<para>Longhornの<link
xl:href="https://longhorn.io/docs/1.6.1/deploy/install/airgap/">公式ドキュメント</link>
には、エアギャップインストールに必要なすべてのイメージを一覧にした<literal>longhorn-images.txt</literal>ファイルが含まれています。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: longhorn
        repositoryName: longhorn
        targetNamespace: longhorn-system
        createNamespace: true
        version: 1.6.1
    repositories:
      - name: longhorn
        url: https://charts.longhorn.io
embeddedArtifactRegistry:
  images:
    - name: longhornio/csi-attacher:v4.4.2
    - name: longhornio/csi-provisioner:v3.6.2
    - name: longhornio/csi-resizer:v1.9.2
    - name: longhornio/csi-snapshotter:v6.3.2
    - name: longhornio/csi-node-driver-registrar:v2.9.2
    - name: longhornio/livenessprobe:v2.12.0
    - name: longhornio/backing-image-manager:v1.6.1
    - name: longhornio/longhorn-engine:v1.6.1
    - name: longhornio/longhorn-instance-manager:v1.6.1
    - name: longhornio/longhorn-manager:v1.6.1
    - name: longhornio/longhorn-share-manager:v1.6.1
    - name: longhornio/longhorn-ui:v1.6.1
    - name: longhornio/support-bundle-kit:v0.0.36</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (13/13, 20 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 MB/s)
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, 104 MB/s)
Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)
Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、Longhornのインストールを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                                    READY   STATUS    RESTARTS      AGE
pod/csi-attacher-5c4bfdcf59-9hgvv                       1/1     Running   0             35s
pod/csi-attacher-5c4bfdcf59-dt6jl                       1/1     Running   0             35s
pod/csi-attacher-5c4bfdcf59-swpwq                       1/1     Running   0             35s
pod/csi-provisioner-667796df57-dfrzw                    1/1     Running   0             35s
pod/csi-provisioner-667796df57-tvsrt                    1/1     Running   0             35s
pod/csi-provisioner-667796df57-xszsx                    1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-6khlb                        1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-gnr45                        1/1     Running   0             35s
pod/csi-resizer-694f8f5f64-sbl4k                        1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-2k4v8                     1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-9d8wl                     1/1     Running   0             35s
pod/csi-snapshotter-959b69d4b-l2w95                     1/1     Running   0             35s
pod/engine-image-ei-5cefaf2b-cwd8f                      1/1     Running   0             43s
pod/instance-manager-f0d17f96bc92f3cc44787a2a347f6a98   1/1     Running   0             43s
pod/longhorn-csi-plugin-szv7t                           3/3     Running   0             35s
pod/longhorn-driver-deployer-9f4fc86-q8fz2              1/1     Running   0             83s
pod/longhorn-manager-zp66l                              1/1     Running   0             83s
pod/longhorn-ui-5f4b7bbf69-k645d                        1/1     Running   3 (65s ago)   83s
pod/longhorn-ui-5f4b7bbf69-t7xt4                        1/1     Running   3 (62s ago)   83s

NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/longhorn-admission-webhook    ClusterIP   10.43.74.59    &lt;none&gt;        9502/TCP   83s
service/longhorn-backend              ClusterIP   10.43.45.206   &lt;none&gt;        9500/TCP   83s
service/longhorn-conversion-webhook   ClusterIP   10.43.83.108   &lt;none&gt;        9501/TCP   83s
service/longhorn-engine-manager       ClusterIP   None           &lt;none&gt;        &lt;none&gt;     83s
service/longhorn-frontend             ClusterIP   10.43.84.55    &lt;none&gt;        80/TCP     83s
service/longhorn-recovery-backend     ClusterIP   10.43.75.200   &lt;none&gt;        9503/TCP   83s
service/longhorn-replica-manager      ClusterIP   None           &lt;none&gt;        &lt;none&gt;     83s

NAME                                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/engine-image-ei-5cefaf2b   1         1         1       1            1           &lt;none&gt;          43s
daemonset.apps/longhorn-csi-plugin        1         1         1       1            1           &lt;none&gt;          35s
daemonset.apps/longhorn-manager           1         1         1       1            1           &lt;none&gt;          83s

NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/csi-attacher               3/3     3            3           35s
deployment.apps/csi-provisioner            3/3     3            3           35s
deployment.apps/csi-resizer                3/3     3            3           35s
deployment.apps/csi-snapshotter            3/3     3            3           35s
deployment.apps/longhorn-driver-deployer   1/1     1            1           83s
deployment.apps/longhorn-ui                2/2     2            2           83s

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/csi-attacher-5c4bfdcf59            3         3         3       35s
replicaset.apps/csi-provisioner-667796df57         3         3         3       35s
replicaset.apps/csi-resizer-694f8f5f64             3         3         3       35s
replicaset.apps/csi-snapshotter-959b69d4b          3         3         3       35s
replicaset.apps/longhorn-driver-deployer-9f4fc86   1         1         1       83s
replicaset.apps/longhorn-ui-5f4b7bbf69             2         2         2       83s</screen>
</section>
<section xml:id="kubevirt-install">
<title>KubeVirtとCDIのインストール</title>
<para>KubeVirtとCDIの両方のHelmチャートでインストールされるのは、それぞれのオペレータのみです。残りのシステムのデプロイはオペレータに任されています。つまり、必要なコンテナイメージすべてを定義ファイルに含める必要があります。作成してみましょう。</para>
<screen language="console" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: slemicro.iso
  outputImageName: eib-image.iso
operatingSystem:
  users:
    - username: root
      encryptedPassword: $6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/
kubernetes:
  version: v1.28.9+rke2r1
  helm:
    charts:
      - name: kubevirt-chart
        repositoryName: suse-edge
        version: 0.2.4
        targetNamespace: kubevirt-system
        createNamespace: true
        installationNamespace: kube-system
      - name: cdi-chart
        repositoryName: suse-edge
        version: 0.2.3
        targetNamespace: cdi-system
        createNamespace: true
        installationNamespace: kube-system
    repositories:
      - name: suse-edge
        url: oci://registry.suse.com/edge
embeddedArtifactRegistry:
  images:
    - name: registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0-150500.6.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-api:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-controller:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-handler:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1-150500.8.12.1
    - name: registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1-150500.8.12.1</screen>
<para>イメージを構築してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm -it --privileged -v $CONFIG_DIR:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-iso-definition.yaml</screen>
<para>出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Rpm .......................... [SKIPPED]
Systemd ...................... [SKIPPED]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Populating Embedded Artifact Registry... 100% (13/13, 6 it/min)
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.
Downloading file: rke2_installer.sh
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building ISO image...
Kernel Params ................ [SKIPPED]
Image build complete!</screen>
<para>構築したイメージを使用するノードがプロビジョニングされたら、KubeVirtとCDIの両方のインストールを確認できます。</para>
<para>KubeVirtを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/virt-api-7c45477984-z226r          1/1     Running   0          2m4s
pod/virt-controller-664d9986b5-8p8gm   1/1     Running   0          98s
pod/virt-controller-664d9986b5-v2n4h   1/1     Running   0          98s
pod/virt-handler-2fx8c                 1/1     Running   0          98s
pod/virt-operator-5cf69867dc-hz5s8     1/1     Running   0          2m30s
pod/virt-operator-5cf69867dc-kp266     1/1     Running   0          2m30s

NAME                                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
service/kubevirt-operator-webhook     ClusterIP   10.43.210.235   &lt;none&gt;        443/TCP   2m7s
service/kubevirt-prometheus-metrics   ClusterIP   None            &lt;none&gt;        443/TCP   2m7s
service/virt-api                      ClusterIP   10.43.226.140   &lt;none&gt;        443/TCP   2m7s
service/virt-exportproxy              ClusterIP   10.43.213.201   &lt;none&gt;        443/TCP   2m7s

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/virt-handler   1         1         1       1            1           kubernetes.io/os=linux   98s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/virt-api          1/1     1            1           2m4s
deployment.apps/virt-controller   2/2     2            2           98s
deployment.apps/virt-operator     2/2     2            2           2m30s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/virt-api-7c45477984          1         1         1       2m4s
replicaset.apps/virt-controller-664d9986b5   2         2         2       98s
replicaset.apps/virt-operator-5cf69867dc     2         2         2       2m30s</screen>
<para>CDIを確認します。</para>
<screen language="shell" linenumbering="unnumbered">/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig /etc/rancher/rke2/rke2.yaml</screen>
<para>出力は次のようになり、すべてが正常にデプロイされていることがわかります。</para>
<screen language="console" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
pod/cdi-apiserver-db465b888-mdsmm      1/1     Running   0          3m6s
pod/cdi-deployment-56c7d74995-vt9sw    1/1     Running   0          3m6s
pod/cdi-operator-55c74f4b86-gkt58      1/1     Running   0          3m10s
pod/cdi-uploadproxy-7d7b94b968-msg2h   1/1     Running   0          3m6s

NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/cdi-api                  ClusterIP   10.43.161.135   &lt;none&gt;        443/TCP    3m6s
service/cdi-prometheus-metrics   ClusterIP   10.43.161.159   &lt;none&gt;        8080/TCP   3m6s
service/cdi-uploadproxy          ClusterIP   10.43.25.136    &lt;none&gt;        443/TCP    3m6s

NAME                              READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cdi-apiserver     1/1     1            1           3m6s
deployment.apps/cdi-deployment    1/1     1            1           3m6s
deployment.apps/cdi-operator      1/1     1            1           3m10s
deployment.apps/cdi-uploadproxy   1/1     1            1           3m6s

NAME                                         DESIRED   CURRENT   READY   AGE
replicaset.apps/cdi-apiserver-db465b888      1         1         1       3m6s
replicaset.apps/cdi-deployment-56c7d74995    1         1         1       3m6s
replicaset.apps/cdi-operator-55c74f4b86      1         1         1       3m10s
replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       3m6s</screen>
</section>
<section xml:id="id-troubleshooting">
<title>トラブルシューティング</title>
<para>イメージの構築中に問題が発生した場合、またはプロセスをさらにテストおよびデバッグしたい場合は、<link
xl:href="https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs">アップストリームドキュメント</link>を参照してください。</para>
</section>
</chapter>
</part>
<part xml:id="id-third-party-integration">
<title>サードパーティの統合</title>
<partintro>
<para>サードパーティツールの統合方法</para>
</partintro>
<chapter xml:id="integrations-nats">
<title>NATS</title>
<para><link
xl:href="https://nats.io/">NATS</link>は、ますますハイパーコネクテッド化が進む世界のために構築された接続テクノロジです。NATSは、クラウドベンダ、オンプレミス、エッジ、Web、モバイルデバイスがどのように組み合わさっていてもアプリケーションが安全に通信することを可能にする単一のテクノロジです。NATSはオープンソース製品ファミリで構成されており、各製品は緊密に統合されている一方で、簡単に個別にデプロイできます。NATSは世界中で数千社もの企業で使用されており、マイクロサービス、エッジコンピューティング、モバイル、IoTなどのユースケースに幅広く対応しているため、NATSを使用して従来のメッセージングの強化や置き換えを図ることができます。</para>
<section xml:id="id-architecture">
<title>アーキテクチャ</title>
<para>NATSは、メッセージの形式でアプリケーション間のデータ交換を可能にするインフラストラクチャです。</para>
<section xml:id="id-nats-client-applications">
<title>NATSクライアントアプリケーション</title>
<para>NATSクライアントライブラリを使用すると、アプリケーションが異なるインスタンス間でパブリッシュ、サブスクライブ、要求、および応答できるようになります。このようなアプリケーションを一般的に<literal>クライアントアプリケーション</literal>と呼びます。</para>
</section>
<section xml:id="id-nats-service-infrastructure">
<title>NATSサービスインフラストラクチャ</title>
<para>NATSサービスは、相互接続されてNATSサービスインフラストラクチャを提供するように設定された1つ以上のNATSサーバプロセスによって提供されます。NATSサービスインフラストラクチャは、1つのエンドデバイスで動作する単一のNATSサーバプロセスから、すべての主要クラウドプロバイダと世界のあらゆる地域にまたがる多数のクラスタからなるパブリックなグローバルスーパークラスタまで拡張可能です。</para>
</section>
<section xml:id="id-simple-messaging-design">
<title>シンプルなメッセージングデザイン</title>
<para>NATSを使用すると、アプリケーションはメッセージを送受信して簡単に通信できます。これらのメッセージはサブジェクト文字列によってアドレス指定および識別され、ネットワークの場所には依存しません。データはエンコードされてメッセージとしてフレーム化され、パブリッシャによって送信されます。メッセージは1人以上のサブスクライバによって受信、デコード、処理されます。</para>
</section>
<section xml:id="id-nats-jetstream">
<title>NATS JetStream</title>
<para>NATSにはJetStreamと呼ばれる分散型の永続化システムが組み込まれています。JetStreamは、今日のテクノロジにおけるストリーミングで明らかになった問題、すなわち複雑性、脆弱性、スケーラビリティの欠如を解決するために作成されました。また、JetStreamは、パブリッシャとサブスクライバのカップリングに関する問題(パブリッシュされたメッセージを受信するにはサブスクライバが稼働している必要がある)も解決します。NATS
JetStreamの詳細については、<link
xl:href="https://docs.nats.io/nats-concepts/jetstream">こちら</link>を参照してください。</para>
</section>
</section>
<section xml:id="id-installation-5">
<title>インストール</title>
<section xml:id="id-installing-nats-on-top-of-k3s">
<title>K3s上へのNATSのインストール</title>
<para>NATSは複数のアーキテクチャ向けに構築されているため、K3s (<xref
linkend="components-k3s"/>)上に簡単にインストールできます。</para>
<para>NATSのデフォルト値を上書きするvaluesファイルを作成しましょう。</para>
<screen language="yaml" linenumbering="unnumbered">cat &gt; values.yaml &lt;&lt;EOF
cluster:
  # Enable the HA setup of the NATS
  enabled: true
  replicas: 3

nats:
  jetstream:
    # Enable JetStream
    enabled: true

    memStorage:
      enabled: true
      size: 2Gi

    fileStorage:
      enabled: true
      size: 1Gi
      storageDirectory: /data/
EOF</screen>
<para>では、Helmを介してNATSをインストールしてみましょう。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add nats https://nats-io.github.io/k8s/helm/charts/
helm install nats nats/nats --namespace nats --values values.yaml \
 --create-namespace</screen>
<para>上記の<literal>values.yaml</literal>ファイルでは、次のコンポーネントが<literal>nats</literal>ネームスペースに配置されます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>NATS StatefulsetのHAバージョン。3つのコンテナ(NATSサーバ + ConfigリローダとMetricsサイドカー)が含まれます。</para>
</listitem>
<listitem>
<para>NATS boxコンテナ。セットアップの確認に使用できる一連の<literal>NATS</literal>ユーティリティが付属します。</para>
</listitem>
<listitem>
<para>JetStreamは、Podにバインドされた<literal>PVC</literal>が付属するKey-Valueバックエンドも利用します。</para>
</listitem>
</orderedlist>
<section xml:id="id-testing-the-setup">
<title>セットアップのテスト</title>
<screen language="bash" linenumbering="unnumbered">kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l</screen>
<orderedlist numeration="arabic">
<listitem>
<para>テストサブジェクトのサブスクリプションを作成します。</para>
<screen language="bash" linenumbering="unnumbered">nats sub test &amp;</screen>
</listitem>
<listitem>
<para>テストサブジェクトにメッセージを送信します。</para>
<screen language="bash" linenumbering="unnumbered">nats pub test hi</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-cleaning-up">
<title>クリーンアップ</title>
<screen language="bash" linenumbering="unnumbered">helm -n nats uninstall nats
rm values.yaml</screen>
</section>
</section>
<section xml:id="id-nats-as-a-back-end-for-k3s">
<title>K3sのバックエンドとしてのNATS</title>
<para>K3sが利用するコンポーネントの1つが<link
xl:href="https://github.com/k3s-io/kine">KINE</link>です。KINEは、最初からリレーショナルデータベースをターゲットとした代替ストレージバックエンドでetcdを置き換えることを可能にするシムです。JetStreamはKey
Value APIを備えているので、NATSをK3sクラスタのバックエンドとして利用することが可能です。</para>
<para>K3sのビルトインNATSが容易になるマージ済みのPRがありますが、この変更はまだK3sリリースに<link
xl:href="https://github.com/k3s-io/k3s/issues/7410#issue-1692989394">含まれていません</link>。</para>
<para>このため、K3sのバイナリを手動で構築する必要があります。</para>
<para>このチュートリアルでは、<link
xl:href="https://suse-edge.github.io/docs/quickstart/slemicro-utm-aarch64">Appleシリコン上のOSX上のSLE
Micro (UTM)</link>のVMを使用します。</para>
<note>
<para>以下のコマンドはOSX PC上で実行してください。</para>
</note>
<section xml:id="id-building-k3s">
<title>K3sの構築</title>
<screen language="bash" linenumbering="unnumbered">git clone --depth 1 https://github.com/k3s-io/k3s.git &amp;&amp; cd k3s</screen>
<para>次のコマンドは、ビルドタグに<literal>nats</literal>を追加して、K3sでNATSビルトイン機能を有効にします。</para>
<screen language="bash" linenumbering="unnumbered">sed -i '' 's/TAGS="ctrd/TAGS="nats ctrd/g' scripts/build
make local</screen>
<para>&lt;node-ip&gt;は、K3sを起動するノードの実際のIPに置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">export NODE_IP=&lt;node-ip&gt;
sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s</screen>
<note>
<para>K3sをローカルで構築するには、buildx Docker CLIプラグインが必要です。<literal>$ make
local</literal>が失敗する場合は、<link
xl:href="https://github.com/docker/buildx#manual-download">手動でインストール</link>できます。</para>
</note>
</section>
<section xml:id="id-installing-nats-cli">
<title>NATS CLIのインストール</title>
<screen language="bash" linenumbering="unnumbered">TMPDIR=$(mktemp -d)
nats_version="nats-0.0.35-linux-arm64"
curl -o "${TMPDIR}/nats.zip" -sfL https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip
unzip "${TMPDIR}/nats.zip" -d "${TMPDIR}"

sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats
rm -rf ${TMPDIR}</screen>
</section>
<section xml:id="id-running-nats-as-k3s-back-end">
<title>K3sのバックエンドとしてのNATSの実行</title>
<para>ノードで<literal>ssh</literal>を実行し、<literal>--datastore-endpoint</literal>フラグで<literal>nats</literal>を指してK3sを実行しましょう。</para>
<note>
<para>次のコマンドでは、K3sをフォアグランドプロセスとして起動するので、ログを簡単に追跡して問題がないかどうかを確認できます。現在の端末をブロックしないようにするには、コマンドの前に<literal>&amp;</literal>フラグを追加して、バックグラウンドプロセスとして起動できます。</para>
</note>
<screen language="bash" linenumbering="unnumbered">k3s server  --datastore-endpoint=nats://</screen>
<note>
<para>NATSバックエンドを使用するK3sサーバを<literal>slemicro</literal>
VM上で永続化するには、次のスクリプトを実行して、必要な設定で<literal>systemd</literal>サービスを作成します。</para>
</note>
<screen language="bash" linenumbering="unnumbered">export INSTALL_K3S_SKIP_START=false
export INSTALL_K3S_SKIP_DOWNLOAD=true

curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server \
 --datastore-endpoint=nats://"  sh -</screen>
</section>
<section xml:id="id-troubleshooting-2">
<title>トラブルシューティング</title>
<para>次のコマンドをノード上で実行して、ストリームのすべてが適切に動作していることを確認できます。</para>
<screen language="bash" linenumbering="unnumbered">nats str report -a
nats str view -a</screen>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-nvidia-gpus-on-sle-micro">
<title>SLE Micro上のNVIDIA GPU</title>
<section xml:id="id-intro-2">
<title>概要</title>
<para>このガイドでは、事前構築済みの<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules">オープンソースドライバ</link>を使用してホストレベルのNVIDIA
GPUサポートをSLE Micro 5.5に実装する方法を説明します。これらのドライバは、NVIDIAの<link
xl:href="https://github.com/NVIDIA/gpu-operator">GPU
Operator</link>によって動的にロードされるのではなく、オペレーティングシステムにベイクされているドライバです。この設定は、デプロイメントに必要なすべてのアーティファクトをあらかじめイメージにベイクしておき、ドライバのバージョンを動的に選択する必要がない(つまり、ユーザがKubernetesを介してドライバのバージョンを選択する必要がない)お客様に非常に適しています。このガイドでは最初に、すでに事前にデプロイされているシステムに追加コンポーネントをデプロイする方法を説明しますが、その後のセクションでは、Edge
Image
Builderを使用してこの設定を初期デプロイメントに組み込む方法について説明します。基本的な操作を読む必要がない場合や、手動でセットアップしたくない場合は、スキップしてそちらのセクションに進んでください。</para>
<para>これらのドライバのサポートは、SUSEとNVIDIAの両社が緊密に連携して提供しており、ドライバはパッケージリポジトリの一部としてSUSEによって構築および出荷されている点を強調しておくことが重要です。ただし、ドライバを使用する組み合わせについて不安や質問がある場合は、SUSEまたはNVIDIAのアカウントマネージャに問い合わせてサポートを受けてください。<link
xl:href="https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/">NVIDIA
AI Enterprise</link> (NVAIE)を使用する予定の場合は、<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems">NVAIE認定GPU</link>を使用していることを確認してください。NVAIE認定GPUでは、独自のNVIDIAドライバを使用する必要がある<emphasis>「場合があります」</emphasis>。不明な点がある場合は、NVIDIAの担当者に問い合わせてください。</para>
<para>NVIDIA GPU
Operatorの統合の詳細は、このガイドでは説明<emphasis>「しません」</emphasis>。Kubernetes用のNVIDIA GPU
Operatorの統合についてはここでは説明しませんが、このガイドのほとんどの手順に従って、基礎となるオペレーティングシステムをセットアップできます。そして、NVIDIA
GPU
OperatorのHelmチャートの<literal>driver.enabled=false</literal>フラグを使用して<emphasis>「プリインストール」</emphasis>されたドライバをGPU
Operatorが使用できるようにするだけで、ホスト上にインストールされたドライバが取得されます。より包括的な手順については、 NVIDIA
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options">こちら</link>)で参照できます。さらにSUSEは先日、<link
xl:href="https://documentation.suse.com/trd/kubernetes/single-html/gs_rke2-slebci_nvidia-gpu-operator/">テクニカルリファレンスドキュメント</link>
(TRD)も公開しました。このドキュメントでは、ご自身のユースケースにGPU
OperatorとNVIDIA独自のドライバが必須の場合にこれらを使用する方法を説明しています。</para>
</section>
<section xml:id="id-prerequisites-10">
<title>前提条件</title>
<para>このガイドに従って操作を進める場合、以下がすでに用意されていることを想定しています。</para>
<itemizedlist>
<listitem>
<para>SLE Micro 5.5がインストールされたホストが少なくとも1台。このホストは物理でも仮想でも構いません。</para>
</listitem>
<listitem>
<para>パッケージへのアクセスにはサブスクリプションが必要であるため、ホストがサブスクリプションに接続されていること。評価版は <link
xl:href="https://www.suse.com/download/sle-micro/">こちら</link>から入手できます。</para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">互換性のあるNVIDIA
GPU</link>がインストールされていること(またはSLE
Microが実行されている仮想マシンに<emphasis>「完全に」</emphasis> パススルーされていること)。</para>
</listitem>
<listitem>
<para>ルートユーザへのアクセス —
以下の説明では、自身がルートユーザであり、<literal>sudo</literal>を使用して特権を昇格して<emphasis>「いない」</emphasis>ことを想定しています。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-manual-installation">
<title>手動インストール</title>
<para>このセクションでは、NVIDIAドライバをSLE
Microオペレーティングシステムに直接インストールします。これはopen版NVIDIAドライバがSLE
Microのコアパッケージリポジトリの一部となったためであり、必須のRPMパッケージをインストールするのと同じように簡単にインストールできるようになりました。実行可能パッケージのコンパイルやダウンロードは必要ありません。以下では、最新のGPUをサポートする「G06」世代ドライバのデプロイについて手順を追って説明します(詳細については
<link
xl:href="https://en.opensuse.org/SDB:NVIDIA_drivers#Install">こちら</link>を参照してください)。ご使用のシステムに搭載されているNVIDIA
GPUに適切なドライバ世代を選択してください。最新のGPUでは、「G06」ドライバが最も一般的な選択肢です。</para>
<para>始める前に、SUSEがSLE
Microの一部として出荷するopen版NVIDIAドライバのほかに、ご自身のセットアップに追加のNVIDIAコンポーネントも必要な場合があることを認識しておくことが重要です。たとえば、OpenGLライブラリ、CUDAツールキット、
<literal>nvidia-smi</literal>などのコマンドラインユーティリティ、<literal>nvidia-container-toolkit</literal>などのコンテナ統合コンポーネントです。これらのコンポーネントの多くはNVIDIA独自のソフトウェアであるため、SUSEからは出荷されません。また、NVIDIAの代わりにSUSEが出荷しても意味がありません。そのため、説明の一環として、これらのコンポーネントにアクセスできるようにする追加のリポジトリを設定し、これらのツールの使用方法の例をいくつか説明し、完全に機能するシステムを作成します。SUSEのリポジトリとNVIDIAのリポジトリを区別することが重要です。これは、NVIDIAが提供するパッケージのバージョンとSUSEが構築したものが一致しない場合があるためです。これは通常、SUSEがopen版ドライバの新バージョンを利用可能にしたときに発生し、NVIDIAのリポジトリで同等のパッケージが利用可能になるまでに数日かかります。</para>
<para>以下をチェックして、選択するドライババージョンがGPUと互換性があり、CUDAの要件を満たしていることを確認することをお勧めします。</para>
<itemizedlist>
<listitem>
<para><link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">CUDAリリースノート</link></para>
</listitem>
<listitem>
<para>デプロイを予定しているドライババージョンと一致するバージョンが<link
xl:href="http://download.nvidia.com/suse/sle15sp5/x86_64/">NVIDIA
SLE15-SP5リポジトリ</link>にあり、サポートコンポーネントと同等のパッケージバージョンが利用可能であることを確認する</para>
</listitem>
</itemizedlist>
<tip>
<para>open版NVIDIAドライバのバージョンを見つけるには、ターゲットマシンで<literal>zypper se -s
nvidia-open-driver</literal>を実行するか、<emphasis>「または」</emphasis>SUSE Customer
Centerの<link
xl:href="https://scc.suse.com/packages?name=SUSE%20Linux%20Enterprise%20Micro&amp;version=5.5&amp;arch=x86_64">SLE
Micro 5.5 x86_64</link>で「nvidia-open-driver」を検索します。</para>
<para>ここでは、<emphasis>4</emphasis>つのバージョンが利用可能で、<emphasis>545.29.064</emphasis>が最新です。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="scc-packages-nvidia.png" width=""/>
</imageobject>
<textobject><phrase>SUSE Customer Centre</phrase></textobject>
</mediaobject>
</informalfigure>
</tip>
<para>NVIDIAリポジトリで同等のバージョンが利用可能であることを確認したら、ホストオペレーティングシステムにパッケージをインストールできます。そのためには、<literal>transactional-update</literal>セッションを開く必要があります。これにより、基礎となるオペレーティング
システムの読み込み/書き込みスナップショットが新しく作成され、イミュータブルプラットフォームに変更を加えることが可能になります(<literal>transactional-update</literal>の詳細については、<link
xl:href="https://documentation.suse.com/sle-micro/5.4/html/SLE-Micro-all/sec-transactional-udate.html">こちら</link>を参照してください)。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para><literal>transactional-update</literal>シェルを起動したら、NVIDIAからパッケージリポジトリを追加します。これにより、<literal>nvidia-smi</literal>などの追加ユーティリティをプルできます。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar https://download.nvidia.com/suse/sle15sp5/ nvidia-sle15sp5-main
zypper --gpg-auto-import-keys refresh</screen>
<para>その後、ドライバと、追加ユーティリティの<literal>nvidia-compute-utils</literal>をインストールできます。ユーティリティが不要の場合は省略できますが、テスト目的の場合は、この段階でインストールする価値があります。</para>
<screen language="shell" linenumbering="unnumbered">zypper install -y --auto-agree-with-licenses nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06</screen>
<note>
<para>インストールが失敗する場合、選択したドライババージョンとNVIDIAがリポジトリで配布しているバージョンとの間に依存関係の不一致があることを示している可能性があります。前のセクションを参照して、バージョンが一致していることを確認してください。また、別のドライババージョンをインストールしてみてください。たとえば、NVIDIAリポジトリに以前のバージョンがある場合、インストールコマンドに<literal>nvidia-open-driver-G06-signed-kmp=545.29.06</literal>を指定して、一致するバージョンを指定してみることができます。</para>
</note>
<para>次に、サポートされているGPUを使用して<emphasis>「いない」</emphasis>場合は(<link
xl:href="https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus">こちら</link>でリストを確認できます)、モジュールレベルでサポートを有効にすることで、ドライバが動作するかどうかを確認できますが、結果はユーザによって異なります。<emphasis>「サポートされている」</emphasis>GPUを使用している場合は、この手順はスキップしてください。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<para>これらのパッケージをインストールしたので、<literal>transactional-update</literal>セッションを終了します。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<note>
<para>次に進む前に、<literal>transactional-update</literal>セッションを終了していることを確認してください。</para>
</note>
<para>ドライバをインストールしたら、再起動します。SLE
Microはイミュータブルオペレーティングシステムであるため、前の手順で作成した新しいスナップショットで再起動する必要があります。ドライバはこの新しいスナップショットにのみインストールされるため、この新しいスナップショットで再起動しないとドライバをロードできません(新しいスナップショットでの再起動は自動的に実行されます)。準備ができたらrebootコマンドを発行します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<para>システムが正常に再起動したら、ログインし直し、
<literal>nvidia-smi</literal>ツールを使用して、ドライバが正常にロードされていて、GPUへのアクセスと列挙をどちらも実行できることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-smi</screen>
<para>このコマンドの出力は次のような出力になります。以下の例では、GPUが2つあることに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">Wed Feb 28 12:31:06 2024
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    0 |
| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    0 |
| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+</screen>
<para>これで、SLE MicroシステムへのNVIDIAドライバのインストールと検証プロセスは完了です。</para>
</section>
<section xml:id="id-further-validation-of-the-manual-installation">
<title>手動インストールの追加検証</title>
<para>この段階で確認できるのは、ホストレベルでNVIDIAデバイスにアクセスできること、およびドライバが正常にロードされていることだけです。ただし、それが機能していることを確認したい場合は、簡単なテストを実施して、GPUがユーザスペースアプリケーションから、理想的にはコンテナ経由で命令を受け取れること、および実際のワークロードが通常使用するものであるCUDAライブラリを通じて命令を受け取れることを検証します。このためには、<literal>nvidia-container-toolkit</literal>
(<link
xl:href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper">NVIDIA
Container
Toolkit</link>)をインストールしてホストOSにさらに変更を加えることができます。まず、別の<literal>transactional-update</literal>シェルを開きます。前の手順ではこれを1つのトランザクションで実行できたことに注目し、後のセクションでこれを完全に自動的に実行する方法を確認します。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update shell</screen>
<para>次に、NVIDIA Container
Toolkitリポジトリから<literal>nvidia-container-toolkit</literal>パッケージをインストールします。</para>
<itemizedlist>
<listitem>
<para>次の<literal>nvidia-container-toolkit.repo</literal>には、安定版(<literal>nvidia-container-toolkit</literal>)と実験版(<literal>nvidia-container-toolkit-experimental</literal>)のリポジトリが含まれています。運用環境での使用には、安定版リポジトリをお勧めします。実験版リポジトリはデフォルトで無効になっています。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
zypper --gpg-auto-import-keys install -y nvidia-container-toolkit</screen>
<para>準備ができたら、<literal>transactional-update</literal>シェルを終了できます。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
<para>…​そして新しいスナップショットでマシンを再起動します。</para>
<screen language="shell" linenumbering="unnumbered">reboot</screen>
<note>
<para>前述同様に、変更を有効にするには、必ず<literal>transactional-shell</literal>を終了し、マシンを再起動する必要があります。</para>
</note>
<para>マシンが再起動したら、システムがNVIDIA Container
Toolkitを使用してデバイスを正常に列挙できることを確認できます。出力は詳細で、INFOとWARNのメッセージがありますが、ERRORのメッセージはありません。</para>
<screen language="shell" linenumbering="unnumbered">nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml</screen>
<para>これにより、そのマシンで起動するコンテナはすべて、検出されたNVIDIA
GPUデバイスを使用できることが確認されます。準備ができたら、podmanベースのコンテナを実行できます。これを<literal>podman</literal>を介して行うことで、コンテナ内からNVIDIAデバイスへのアクセスを効果的に検証することができ、後の段階でKubernetesで同じ操作を実行するための自信が得られます。<literal>podman</literal>に対し、前のコマンドで<link
xl:href="https://registry.suse.com/bci/bci-base-15sp5/index.html">SLE
BCI</link>に基づいて処理したラベル付きのNVIDIAデバイスへのアクセス権を与え、バッシュコマンドを実行します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it registry.suse.com/bci/bci-base:latest bash</screen>
<para>続いて、一時的なpodmanコンテナ内からコマンドを実行します。このコンテナは基盤となるシステムにはアクセスできず一時的であるため、ここで行う操作は永続せず、基盤となるホスト上にあるものを壊すことは一切できないはずです。現在はコンテナ内で作業しているため、必要なCUDAライブラリをインストールできます。ここでもう一度、ご使用のドライバにあったCUDAバージョンを<link
xl:href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/">こちら</link>で確認してください。ただし、必要なCUDAバージョンは<literal>nvidia-smi</literal>の前の出力に表示されているはずです。以下の例では、<emphasis>CUDA
12.3</emphasis>をインストールして多数の例、デモ、開発キットをプルし、GPUを完全に検証できるようにしています。</para>
<screen language="shell" linenumbering="unnumbered">zypper ar http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ cuda-sle15-sp5
zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 cuda-demo-suite-12-3</screen>
<para>これが正常にインストールされた後にコンテナを終了しないでください。<literal>deviceQuery</literal>
CUDAの例を実行し、CUDAを介して、およびコンテナ自体からGPUアクセスを包括的に検証します。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery</screen>
<para>成功すると、次のような出力が表示されます。コマンドの最後にある「<literal>Result =
PASS</literal>」というメッセージに注意してください。また、次の出力では2つのGPUが正しく識別されていますが、ご使用の環境では1つしかない場合があることにも注意してください。</para>
<screen language="shell" linenumbering="unnumbered">/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: "NVIDIA A100-PCIE-40GB"
  CUDA Driver Version / Runtime Version          12.2 / 12.1
  CUDA Capability Major/Minor version number:    8.0
  Total amount of global memory:                 40339 MBytes (42298834944 bytes)
  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores
  GPU Max Clock rate:                            1410 MHz (1.41 GHz)
  Memory Clock rate:                             1215 Mhz
  Memory Bus Width:                              5120-bit
  L2 Cache Size:                                 41943040 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)
  Run time limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing (UVA):      Yes
  Device supports Compute Preemption:            Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0
  Compute Mode:
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;

Device 1: &lt;snip to reduce output for multiple devices&gt;
     &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU0) -&gt; NVIDIA A100-PCIE-40GB (GPU1) : Yes
&gt; Peer access from NVIDIA A100-PCIE-40GB (GPU1) -&gt; NVIDIA A100-PCIE-40GB (GPU0) : Yes

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = NVIDIA A100-PCIE-40GB
Result = PASS</screen>
<para>ここから、続いて他のCUDAワークロードを実行できます。コンパイラやCUDAエコシステムの他の側面を使用して、さらにテストを実行できます。完了したら、コンテナを終了できます。コンテナにインストールしたものはすべて一時的なものであるため(したがって失われるため)、基盤となるオペレーティングシステムには影響がないことに注意してください。</para>
<screen language="shell" linenumbering="unnumbered">exit</screen>
</section>
<section xml:id="id-implementation-with-kubernetes">
<title>Kubernetesを使用した実装</title>
<para>open版NVIDIAドライバをSLE
Microにインストールして使用できることが証明されたので、同じマシンにKubernetesを設定してみましょう。このガイドでは、Kubernetesのデプロイについては説明しませんが、<link
xl:href="https://k3s.io/">K3s</link>または<link
xl:href="https://docs.rke2.io/install/quickstart">RKE2</link>をインストール済みで、kubeconfigが適宜設定されており、標準の<literal>kubectl</literal>コマンドをスーパーユーザとして実行できることを前提としています。ここではノードがシングルノードクラスタを形成していることを想定していますが、中心となる手順はマルチノードクラスタでも同様です。まず、<literal>kubectl</literal>のアクセスが機能していることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get nodes</screen>
<para>次のような画面が表示されます。</para>
<screen language="shell" linenumbering="unnumbered">NAME       STATUS   ROLES                       AGE   VERSION
node0001   Ready    control-plane,etcd,master   13d   v1.28.9+rke2r1</screen>
<para>k3s/rke2のインストールによってホスト上のNVIDIA Container
Toolkitが検出され、NVIDIAランタイム統合が<literal>containerd</literal>
(k3s/rke2が使用するContainer Runtime
Interface)に自動設定されていることを確認する必要があります。確認するには、containerdの<literal>config.toml</literal>ファイルをチェックします。</para>
<screen language="shell" linenumbering="unnumbered">tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml</screen>
<para>次のような画面が表示される必要があります。K3sの場合に相当する場所は<literal>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</literal>です。</para>
<screen language="shell" linenumbering="unnumbered">[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia"]
  runtime_type = "io.containerd.runc.v2"
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes."nvidia".options]
  BinaryName = "/usr/bin/nvidia-container-runtime"</screen>
<note>
<para>これらのエントリが存在しない場合は、検出が失敗している可能性があります。この原因として考えられるのは、マシンまたはKubernetesサービスを再起動していないことです。必要に応じて、上記のようにこれらを手動で追加してください。</para>
</note>
<para>次に、NVIDIA
<literal>RuntimeClass</literal>を追加のKubernetesランタイムとしてデフォルト値に設定する必要があります。これにより、GPUへのアクセスが必要なPodに対するユーザ要求が、
<literal>containerd</literal>の設定に従って、NVIDIA Container
Toolkitを使用して<literal>nvidia-container-runtime</literal>を介してアクセスできるようにします。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>次の手順は、<link xl:href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA
Device Plugin</link>を設定することです。これにより、NVIDIA Container
Toolkitと連携して、クラスタ内で使用可能なリソースとしてNVIDIA
GPUを利用するようにKubernetesを設定します。このツールはまず、基盤となるホスト上のすべての機能(GPU、ドライバ、その他の機能(GLなど)を含む)を検出し、その後、ユーザがGPUリソースを要求してアプリケーションの一部として使用できるようにします。</para>
<para>まず、NVIDIA Device Plugin用のHelmリポジトリを追加して更新する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">helm repo add nvdp https://nvidia.github.io/k8s-device-plugin
helm repo update</screen>
<para>これで、NVIDIA Device Pluginをインストールできます。</para>
<screen language="shell" linenumbering="unnumbered">helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace nvidia-device-plugin --create-namespace --version 0.14.5 --set runtimeClassName=nvidia</screen>
<para>数分後、新しいPodが実行されているのがわかります。これで、利用可能なノード上での検出は完了し、検出されたGPUの数を示すタグがノードに付けられます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl get pods -n nvidia-device-plugin
NAME                              READY   STATUS    RESTARTS      AGE
nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h

kubectl get node node0001 -o json | jq .status.capacity
{
  "cpu": "128",
  "ephemeral-storage": "466889732Ki",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "32545636Ki",
  "nvidia.com/gpu": "1",                      &lt;----
  "pods": "110"
}</screen>
<para>これで、このGPUを使用するNVIDIA Podを作成する準備ができました。CUDA Benchmarkコンテナで試してみましょう。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f - &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: nbody-gpu-benchmark
  namespace: default
spec:
  restartPolicy: OnFailure
  runtimeClassName: nvidia
  containers:
  - name: cuda-container
    image: nvcr.io/nvidia/k8s/cuda-sample:nbody
    args: ["nbody", "-gpu", "-benchmark"]
    resources:
      limits:
        nvidia.com/gpu: 1
    env:
    - name: NVIDIA_VISIBLE_DEVICES
      value: all
    - name: NVIDIA_DRIVER_CAPABILITIES
      value: all
EOF</screen>
<para>すべて問題なければ、ログを見て、ベンチマーク情報を確認できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl logs nbody-gpu-benchmark
Run "nbody -benchmark [-numbodies=&lt;numBodies&gt;]" to measure performance.
	-fullscreen       (run n-body simulation in fullscreen mode)
	-fp64             (use double precision floating point values for simulation)
	-hostmem          (stores simulation data in host memory)
	-benchmark        (run benchmark to measure performance)
	-numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)
	-device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)
	-numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)
	-compare          (compares simulation results running once on the default GPU and once on the CPU)
	-cpu              (run n-body simulation on the CPU)
	-tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

&gt; Windowed mode
&gt; Simulation data stored in video memory
&gt; Single precision floating point simulation
&gt; 1 Devices used for simulation
GPU Device 0: "Turing" with compute capability 7.5

&gt; Compute 7.5 CUDA device: [Tesla T4]
40960 bodies, total time for 10 iterations: 101.677 ms
= 165.005 billion interactions per second
= 3300.103 single-precision GFLOP/s at 20 flops per interaction</screen>
<para>最後に、アプリケーションでOpenGLが必要な場合は、必要なNVIDIA OpenGLライブラリをホストレベルでインストールし、 NVIDIA
Device PluginとNVIDIA Container
Toolkitを使用してそのライブラリをコンテナで利用できるようにすることができます。これを行うには、次のようにパッケージをインストールします。</para>
<screen language="shell" linenumbering="unnumbered">transactional-update pkg install nvidia-gl-G06</screen>
<note>
<para>このパッケージをアプリケーションで使用できるようにするには再起動が必要です。NVIDIA Device Pluginは、NVIDIA Container
Toolkitを介してこれを自動的に再検出します。</para>
</note>
</section>
<section xml:id="id-bringing-it-together-via-edge-image-builder">
<title>Edge Image Builderを使用した統合</title>
<para>さて、SLE Micro上のアプリケーションとGPUの全機能をデモで示したので、 <xref
linkend="components-eib"/>を使用してすべてをまとめ、デプロイ可能/使用可能なISOまたはRAWディスクイメージで提供したいと思います。このガイドでは、Edge
Image
Builderの使用方法は説明せずに、このようなイメージを構築するために必要な設定について説明します。以下に、必要なすべてのコンポーネントを追加設定なしにデプロイするためのイメージ定義の例と、必要なKubernetes設定ファイルを示します。以下に示す例では、Edge
Image Builderディレクトリは次のようなディレクトリ構造になっています。</para>
<screen language="shell" linenumbering="unnumbered">.
├── base-images
│   └── SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
├── eib-config-iso.yaml
├── kubernetes
│   ├── config
│   │   └── server.yaml
│   ├── helm
│   │   └── values
│   │       └── nvidia-device-plugin.yaml
│   └── manifests
│       └── nvidia-runtime-class.yaml
└── rpms
    └── gpg-keys
        └── nvidia-container-toolkit.key</screen>
<para>これらのファイルを調べてみましょう。まず、K3sを実行するシングルノードクラスタのサンプルイメージ定義を次に示します。このイメージ定義では、ユーティリティとOpenGLパッケージもデプロイします(<literal>eib-config-iso.yaml</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  arch: x86_64
  imageType: iso
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: deployimage.iso
operatingSystem:
  time:
    timezone: Europe/London
    ntp:
      pools:
        - 2.suse.pool.ntp.org
  isoConfiguration:
    installDevice: /dev/sda
  users:
    - username: root
      encryptedPassword: $6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1
  packages:
    packageList:
      - nvidia-open-driver-G06-signed-kmp-default
      - nvidia-compute-utils-G06
      - nvidia-gl-G06
      - nvidia-container-toolkit
    additionalRepos:
      - url: https://download.nvidia.com/suse/sle15sp5/
      - url: https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64
    sccRegistrationCode: &lt;snip&gt;
kubernetes:
  version: v1.28.9+k3s1
  helm:
    charts:
      - name: nvidia-device-plugin
        version: v0.14.5
        installationNamespace: kube-system
        targetNamespace: nvidia-device-plugin
        createNamespace: true
        valuesFile: nvidia-device-plugin.yaml
        repositoryName: nvidia
    repositories:
      - name: nvidia
        url: https://nvidia.github.io/k8s-device-plugin</screen>
<note>
<para>これは単なる例です。要件や期待に合うようにカスタマイズする必要がある場合があります。また、SLE
Microを使用する場合は、パッケージの依存関係を解決してNVIDIAドライバをプルするために、独自の
<literal>sccRegistrationCode</literal>を指定する必要があります。</para>
</note>
<para>これに加えて、他のコンポーネントを追加して、ブート時にKubernetesによってロードされるようにする必要があります。EIBディレクトリにはまず<literal>kubernetes</literal>ディレクトリが必要で、その下に設定、Helmチャート値、必要な追加のマニフェスト用のサブディレクトリが必要です。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests</screen>
<para>CNIを選択し(選択しない場合はデフォルトでCiliumになります)、SELinuxを有効にして、(オプションの)Kubernetes設定を行いましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/config/server.yaml
cni: cilium
selinux: true
EOF</screen>
<para>続いて、NVIDIA RuntimeClassがKubernetesクラスタ上に作成されていることを確認します。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/manifests/nvidia-runtime-class.yaml
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF</screen>
<para>ビルトインHelmコントローラを使用して、Kubernetes自体を使用してNVIDIA Device
Pluginをデプロイします。チャートの値ファイルでランタイムクラスを指定しましょう。</para>
<screen language="shell" linenumbering="unnumbered">cat &lt;&lt; EOF &gt; kubernetes/helm/values/nvidia-device-plugin.yaml
runtimeClassName: nvidia
EOF</screen>
<para>次に進む前に、NVIDIA Container Toolkit RPMの公開鍵を取得する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">mkdir -p rpms/gpg-keys
curl -o rpms/gpg-keys/nvidia-container-toolkit.key https://nvidia.github.io/libnvidia-container/gpgkey</screen>
<para>Kubernetesバイナリ、コンテナイメージ、Helmチャート(および参照イメージ)など、必要なアーティファクトがすべて自動的にエアギャップ化されます。つまり、デプロイ時のシステムにはデフォルトでインターネット接続は不要です。ここで必要なのは<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からSLE
Micro
ISOを取得する(そしてそれを<literal>base-images</literal>ディレクトリに配置する)ことだけです。そうすれば、Edge
Image Builderツールを呼び出してISOを生成できます。この例を完了するために、イメージの構築に使用したコマンドを次に示します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v /path/to/eib-files/:/eib \
registry.suse.com/edge/edge-image-builder:1.0.2 \
build --definition-file eib-config-iso.yaml</screen>
<para>Edge Image Builderの詳細については、<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md">ドキュメント</link>を参照してください。</para>
</section>
<section xml:id="id-resolving-issues">
<title>問題の解決</title>
<section xml:id="id-nvidia-smi-does-not-find-the-gpu">
<title>nvidia-smiでGPUが検出されない</title>
<para><literal>dmesg</literal>を使用してカーネルメッセージを確認します。<literal>NvKMSKapDevice</literal>を割り当てることができないことを示している場合は、サポート対象外のGPUの回避策を適用します。</para>
<screen language="shell" linenumbering="unnumbered">sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' /etc/modprobe.d/50-nvidia-default.conf</screen>
<blockquote>
<para><emphasis>メモ</emphasis>:
上記の手順でカーネルモジュールの設定を変更した場合は、変更を有効にするために、カーネルモジュールを再ロードするか、再起動する必要があります。</para>
</blockquote>
</section>
</section>
</chapter>
</part>
<part xml:id="id-day-2-operations">
<title>Day 2操作</title>
<partintro>
<para>このセクションでは、管理者がさまざまな「Day 2」操作タスクを管理クラスタとダウンストリームクラスタの両方で処理する方法について説明します。</para>
</partintro>
<chapter xml:id="day2-mgmt-cluster">
<title>管理クラスタ</title>
<para>このセクションでは、さまざまな<literal>Day
2</literal>操作を<literal>管理クラスタ</literal>で実行する方法について説明します。</para>
<section xml:id="id-rke2-upgrade">
<title>RKE2のアップグレード</title>
<note>
<para>確実な<emphasis
role="strong">障害復旧</emphasis>のため、RKE2クラスタデータをバックアップすることをお勧めします。バックアップの実行方法については、<link
xl:href="https://docs.rke2.io/backup_restore">こちら</link>をご確認ください。<literal>rke2</literal>バイナリのデフォルトの場所は、<literal>/opt/rke2/bin</literal>です。</para>
</note>
<para>次のようなRKE2インストールスクリプトを使用して、RKE2バージョンをアップグレードできます。</para>
<screen language="bash" linenumbering="unnumbered">curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=vX.Y.Z+rke2rN sh -</screen>
<para>インストール後に<literal>rke2</literal>プロセスを再起動することを忘れないでください。</para>
<screen language="bash" linenumbering="unnumbered"># For server nodes:
systemctl restart rke2-server

# For agent nodes:
systemctl restart rke2-agent</screen>
<important>
<para>アップグレードに関する予期しない問題を避けるために、ノードのアップグレードは次の順序で行ってください。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis>サーバノード </emphasis>- 一度に<emphasis
role="strong">1</emphasis>ノードずつアップグレードする必要があります。</para>
</listitem>
<listitem>
<para><emphasis>エージェントノード </emphasis> - <emphasis
role="strong">すべて</emphasis>のサーバノードのアップグレードが完了した後でアップグレードする必要があります。並行してアップグレード可能です。</para>
</listitem>
</orderedlist>
</important>
<para><emphasis>詳細については、<link
xl:href="https://docs.rke2.io/upgrade/manual_upgrade#upgrade-rke2-using-the-installation-script">RKE2のアップグレードに関するドキュメント</link>を参照してください。</emphasis></para>
</section>
<section xml:id="id-os-upgrade">
<title>OSのアップグレード</title>
<note>
<para>このセクションでは、システムを<link
xl:href="https://scc.suse.com">https://scc.suse.com</link>に登録済みであることを想定しています。</para>
</note>
<para>SUSEでは、新しい<literal>SLE
Micro</literal>パッケージの更新を定期的にリリースしています。更新されたパッケージバージョンを取得するために、SLE
Microでは<literal>transactional-upgrade</literal>を使用します。</para>
<para><literal>transactional-upgrade</literal>では、Linuxオペレーティングシステムをトランザクション方式で更新するためのアプリケーションとライブラリを用意しています。すなわち、更新をバックグラウンドで実行しながら、システムはそのまま動作を継続します。システムを<emphasis
role="strong">再起動</emphasis>した後でのみ、更新が有効になります。詳細については、<link
xl:href="https://github.com/openSUSE/transactional-update">GitHub</link>の<literal>transactional-update</literal>のページを参照してください。</para>
<formalpara>
<title>システム内のすべてのパッケージを更新するには、次のコマンドを実行します。</title>
<para>
<screen language="bash" linenumbering="unnumbered">transactional-update</screen>
</para>
</formalpara>
<para>ノードを<emphasis
role="strong">「再起動」</emphasis>すると、ノードはしばらくの間利用不可能になるため、マルチノードクラスタを実行している場合は、ノードに対して<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">cordon</link>および<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/">drain</link>を実行してから
<emphasis role="strong">再起動</emphasis>できます。</para>
<formalpara>
<title>ノードに対してcordonを実行するには、次のコマンドを実行します。</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl cordon &lt;node&gt;</screen>
</para>
</formalpara>
<para>これにより、ノードはデフォルトのスケジューリングメカニズムから除外され、誤ってPodが割り当てられることがなくなります。</para>
<formalpara>
<title>ノードに対してdrainを実行するには、次のコマンドを実行します。</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl drain &lt;node&gt;</screen>
</para>
</formalpara>
<para>これにより、ノード上のすべてのワークロードが他の利用可能なノードに転送されるようになります。</para>
<note>
<para>ノードで実行されているワークロードによっては、追加のフラグ(<literal>--delete-emptydir-data</literal>、<literal>--ignore-daemonsets</literal>など)をコマンドに指定する必要がある場合があります。</para>
</note>
<formalpara>
<title>ノードを再起動します。</title>
<para>
<screen language="bash" linenumbering="unnumbered">sudo reboot</screen>
</para>
</formalpara>
<para>正常に再起動すると、ノードのパッケージが更新されます。残る手順は、<link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/">uncordon</link>コマンドを使用して、ノードをデフォルトのスケジューリングメカニズムに戻すことのみです。</para>
<formalpara>
<title>ノードに対してCordonを解除します。</title>
<para>
<screen language="bash" linenumbering="unnumbered">kubectl uncordon &lt;node&gt;</screen>
</para>
</formalpara>
<note>
<para>更新を元に戻す場合は、上記の手順と次の<literal>transactional-update</literal>コマンドを使用します。</para>
<screen language="bash" linenumbering="unnumbered">transactional-update rollback last</screen>
</note>
</section>
<section xml:id="id-helm-upgrade">
<title>Helmのアップグレード</title>
<note>
<para>このセクションでは、システムに<literal>helm</literal>がインストール済みであることを想定しています。<literal>helm</literal>のインストール手順については、<link
xl:href="https://helm.sh/docs/intro/install">こちら</link>をご確認ください。</para>
</note>
<para>このセクションでは、EIBでデプロイされたHelmチャート(<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade"/>)とEIB以外でデプロイされたHelmチャート(<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade"/>)の両方をアップグレードする方法について説明します。</para>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade">
<title>EIBでデプロイされたHelmチャート</title>
<para>EIBは、そのイメージ定義ファイル(<xref
linkend="quickstart-eib-definition-file"/>)で定義されたHelmチャートを、RKE2のマニフェストである<link
xl:href="https://docs.rke2.io/advanced#auto-deploying-manifests">auto-deploy</link>機能を使用してデプロイします。</para>
<para>このような方法でデプロイされたチャートをアップグレードするには、EIBが<literal>初期化</literal>ノードの<literal>/var/lib/rancher/rke2/server/manifests</literal>ディレクトリの下に作成する、チャートマニフェストファイルをアップグレードする必要があります。</para>
<note>
<para>確実な障害復旧のために、必ずチャートマニフェストファイルをバックアップするとともに、チャートで提供されている障害復旧に関連するドキュメントに従うことをお勧めします。</para>
</note>
<para>チャートマニフェストファイルをアップグレードするには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>初期化</literal>ノードを見つけます。</para>
<itemizedlist>
<listitem>
<para><literal>マルチノードクラスタ</literal>の場合 -
EIBのイメージ定義ファイルで、ノードの1つに対して<literal>initializer:
true</literal>プロパティが指定されている必要があります。このプロパティが指定されていない場合、ノードリストで最初の<emphasis
role="strong">サーバ</emphasis>ノードが初期化ノードになります。</para>
</listitem>
<listitem>
<para><literal>シングルノードクラスタ</literal>の場合 - 初期化ノードは現在実行中のノードです。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>初期化</literal>ノードにSSHで接続します。</para>
<screen language="bash" linenumbering="unnumbered">ssh root@&lt;node_ip&gt;</screen>
</listitem>
<listitem>
<para>Helmチャートを<link xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
<itemizedlist>
<listitem>
<para>HelmチャートリポジトリでホストされているHelmチャートの場合:</para>
<screen language="bash" linenumbering="unnumbered">helm repo add &lt;chart_repo_name&gt; &lt;chart_repo_urls&gt;
helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt;

# Alternatively if you want to pull a specific verison
helm pull &lt;chart_repo_name&gt;/&lt;chart_name&gt; --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>OCIベースのHelmチャートの場合:</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://&lt;chart_oci_url&gt;

# Alternatively if you want to pull a specific verison
helm pull oci://&lt;chart_oci_url&gt; --version=X.Y.Z</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>プルした<literal>.tgz</literal>アーカイブをエンコードして、<literal>HelmChart</literal>
CR設定に渡せるようにします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 &lt;chart_name&gt;-X.Y.Z.tgz  &gt; &lt;chart_name&gt;-X.Y.Z.txt</screen>
</listitem>
<listitem>
<para>編集するチャートマニフェストファイルのコピーを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/&lt;chart_name&gt;.yaml ./&lt;chart_name&gt;.yaml</screen>
</listitem>
<listitem>
<para><literal>bar.yaml</literal>ファイルの<literal>chartContent</literal>と<literal>version</literal>の設定を変更します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;chart-name-X.Y.Z.txt)|" -e "s|version:.*|version: X.Y.Z|" &lt;chart_name&gt;.yaml</screen>
<note>
<para>チャートにアップグレードの追加の変更(<emphasis
role="strong">新しい</emphasis>カスタムチャート値の追加など)を加える必要がある場合は、チャートマニフェストファイルを手動で編集する必要があります。</para>
</note>
</listitem>
<listitem>
<para>元のチャートマニフェストファイルを置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">cp &lt;chart_name&gt;.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>上記のコマンドにより、Helmチャートのアップグレードがトリガされます。アップグレードは、<link
xl:href="https://github.com/k3s-io/helm-controller#helm-controller">helm-controller</link>によって処理されます。</para>
<para>Helmチャートのアップグレードを追跡するには、<literal>helm-controller</literal>がチャートのアップグレード時に作成するPodのログを表示する必要があります。詳細については、例(<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples"/>)のセクションを参照してください。</para>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples">
<title>例</title>
<note>
<para>このセクションの例では、<literal>初期化</literal>ノードをすでに特定して接続していることを想定しています。</para>
</note>
<para>このセクションでは、以下をアップグレードする方法の例を示します。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-rancher"/>)
Helmチャート</para>
</listitem>
<listitem>
<para>Metal3 (<xref
linkend="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-metal3"/>)
Helmチャート</para>
</listitem>
</itemizedlist>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-rancher">
<title>Rancherのアップグレード</title>
<note>
<para>確実な障害復旧のために、Rancherのバックアップを作成することをお勧めします。方法については、<link
xl:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher">こちら</link>でご確認ください。</para>
</note>
<para>この例では、Rancherを<literal>2.8.4</literal>バージョンにアップグレードする方法を説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher Prime</literal> Helmリポジトリを追加します。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add rancher-prime https://charts.rancher.com/server-charts/prime</screen>
</listitem>
<listitem>
<para><literal>Rancher Prime</literal>の最新のHelmチャートバージョンをプルします。</para>
<screen language="bash" linenumbering="unnumbered">helm pull rancher-prime/rancher --version=2.8.4</screen>
</listitem>
<listitem>
<para><literal>.tgz</literal>アーカイブをエンコードし、<literal>HelmChart</literal>
CR設定に渡せるようにします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 rancher-2.8.4.tgz  &gt; rancher-2.8.4-encoded.txt</screen>
</listitem>
<listitem>
<para>編集する<literal>rancher.yaml</literal>ファイルのコピーを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/rancher.yaml ./rancher.yaml</screen>
</listitem>
<listitem>
<para><literal>rancher.yaml</literal>ファイルの<literal>chartContent</literal>と<literal>version</literal>の設定を変更します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;rancher-2.8.4-encoded.txt)|" -e "s|version:.*|version: 2.8.4|" rancher.yaml</screen>
<note>
<para>アップグレードの追加の変更( <emphasis
role="strong">新しい</emphasis>カスタムチャート値の追加など)を加える必要がある場合は、<literal>rancher.yaml</literal>ファイルを手動で編集する必要があります。</para>
</note>
</listitem>
<listitem>
<para>元の<literal>rancher.yaml</literal>ファイルを置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">cp rancher.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>更新を確認するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>default</literal>ネームスペースのPodを一覧にします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n default

# Example output
NAME                              READY   STATUS      RESTARTS   AGE
helm-install-cert-manager-7v7nm   0/1     Completed   0          88m
helm-install-rancher-p99k5        0/1     Completed   0          3m21s</screen>
</listitem>
<listitem>
<para><literal>helm-install-rancher-*</literal> Podのログを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;helm_install_rancher_pod&gt; -n default

# Example
kubectl logs helm-install-rancher-p99k5 -n default</screen>
</listitem>
<listitem>
<para><literal>Rancher</literal>のPodが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n cattle-system

# Example output
NAME                               READY   STATUS      RESTARTS   AGE
helm-operation-mccvd               0/2     Completed   0          3m52s
helm-operation-np8kn               0/2     Completed   0          106s
helm-operation-q8lf7               0/2     Completed   0          2m53s
rancher-648d4fbc6c-qxfpj           1/1     Running     0          5m27s
rancher-648d4fbc6c-trdnf           1/1     Running     0          9m57s
rancher-648d4fbc6c-wvhbf           1/1     Running     0          9m57s
rancher-webhook-649dcc48b4-zqjs7   1/1     Running     0          100s</screen>
</listitem>
<listitem>
<para><literal>Rancher</literal>のバージョンがアップグレードされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get settings.management.cattle.io server-version

# Example output
NAME             VALUE
server-version   v2.8.4</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-mgmt-cluster-eib-helm-chart-upgrade-examples-metal3">
<title>Metal<superscript>3</superscript>のアップグレード</title>
<para>この例では、Metal<superscript>3</superscript>を<literal>0.7.1</literal>バージョンにアップグレードする方法を説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>の最新のHelmチャートバージョンをプルします。</para>
<screen language="bash" linenumbering="unnumbered">helm pull oci://registry.suse.com/edge/metal3-chart --version 0.7.1</screen>
</listitem>
<listitem>
<para><literal>.tgz</literal>アーカイブをエンコードし、<literal>HelmChart</literal>
CR設定に渡せるようにします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 metal3-chart-0.7.1.tgz  &gt; metal3-chart-0.7.1-encoded.txt</screen>
</listitem>
<listitem>
<para>編集する<literal>Metal<superscript>3</superscript></literal>マニフェストファイルのコピーを作成します。</para>
<screen language="bash" linenumbering="unnumbered">cp /var/lib/rancher/rke2/server/manifests/metal3.yaml ./metal3.yaml</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>マニフェストファイルの<literal>chartContent</literal>と<literal>version</literal>の設定を変更します。</para>
<screen language="bash" linenumbering="unnumbered">sed -i -e "s|chartContent:.*|chartContent: $(&lt;metal3-chart-0.7.1-encoded.txt)|" -e "s|version:.*|version: 0.7.1|" metal3.yaml</screen>
<note>
<para>チャートにアップグレードの追加の変更( <emphasis
role="strong">新しい</emphasis>カスタムチャート値の追加など)を加える必要がある場合は、<literal>metal3.yaml</literal>ファイルを手動で編集する必要があります。</para>
</note>
</listitem>
<listitem>
<para>元の<literal>Metal<superscript>3</superscript></literal>マニフェストファイルを置き換えます。</para>
<screen language="bash" linenumbering="unnumbered">cp metal3.yaml /var/lib/rancher/rke2/server/manifests/</screen>
</listitem>
</orderedlist>
<para>更新を確認するには、次の手順に従います。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>default</literal>ネームスペースのPodを一覧にします。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n default

# Example output
NAME                              READY   STATUS      RESTARTS   AGE
helm-install-metal3-7p7bl         0/1     Completed   0          27s</screen>
</listitem>
<listitem>
<para><literal>helm-install-rancher-*</literal> Podのログを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl logs &lt;helm_install_rancher_pod&gt; -n default

# Example
kubectl logs helm-install-metal3-7p7bl -n default</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>のPodが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n metal3-system

# Example output
NAME                                                     READY   STATUS    RESTARTS      AGE
baremetal-operator-controller-manager-785f99c884-9z87p   2/2     Running   2 (25m ago)   36m
metal3-metal3-ironic-96fb66cdd-lkss2                     4/4     Running   0             3m54s
metal3-metal3-mariadb-55fd44b648-q6zhk                   1/1     Running   0             36m</screen>
</listitem>
<listitem>
<para><literal>HelmChart</literal>のリソースバージョンがアップグレードされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get helmchart metal3 -n default

# Example output
NAME     JOB                   CHART   TARGETNAMESPACE   VERSION   REPO   HELMVERSION   BOOTSTRAP
metal3   helm-install-metal3           metal3-system     0.7.1</screen>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade">
<title>EIB以外でデプロイされたHelmチャート</title>
<orderedlist numeration="arabic">
<listitem>
<para>現在実行中のHelmチャートの<literal>.yaml</literal>ファイルの値を取得し、<emphasis
role="strong">必要に応じて</emphasis>値を変更します。</para>
<screen language="bash" linenumbering="unnumbered">helm get values &lt;chart_name&gt; -n &lt;chart_namespace&gt; -o yaml &gt; &lt;chart_name&gt;-values.yaml</screen>
</listitem>
<listitem>
<para>Helmチャートを更新します。</para>
<screen language="bash" linenumbering="unnumbered"># For charts using a chart repository
helm upgrade &lt;chart_name&gt; &lt;chart_repo_name&gt;/&lt;chart_name&gt; \
  --namespace &lt;chart_namespace&gt; \
  -f &lt;chart_name&gt;-values.yaml \
  --version=X.Y.Z

# For OCI based charts
helm upgrade &lt;chart_name&gt; oci://&lt;oci_registry_url&gt;/&lt;chart_name&gt; \
  --namespace &lt;chart_namespace&gt; \
  -f &lt;chart_name&gt;-values.yaml \
  --version=X.Y.Z</screen>
</listitem>
<listitem>
<para>チャートがアップグレードされていることを確認します。チャートによっては、さまざまなリソースを確認しなければならない場合があります。チャートのアップグレードの例については、例(<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples"/>)のセクションを参照してください。</para>
</listitem>
</orderedlist>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples">
<title>例</title>
<para>このセクションでは、以下をアップグレードする方法の例を示します。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples-rancher"/>) Helmチャート</para>
</listitem>
<listitem>
<para>Metal3 (<xref
linkend="day2-mgmt-cluster-helm-chart-upgrade-examples-metal3"/>) Helmチャート</para>
</listitem>
</itemizedlist>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples-rancher">
<title>Rancher</title>
<note>
<para>確実な障害復旧のために、Rancherのバックアップを作成することをお勧めします。方法については、<link
xl:href="https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher">こちら</link>でご確認ください。</para>
</note>
<para>この例では、Rancherを<literal>2.8.4</literal>バージョンにアップグレードする方法を説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Rancherの現在のリリースの値を取得し、それらの値を<literal>rancher-values.yaml</literal>ファイルに出力します。</para>
<screen language="bash" linenumbering="unnumbered">helm get values rancher -n cattle-system -o yaml &gt; rancher-values.yaml</screen>
</listitem>
<listitem>
<para>Helmチャートを更新します。</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade rancher rancher-prime/rancher \
  --namespace cattle-system \
  -f rancher-values.yaml \
  --version=2.8.4</screen>
</listitem>
<listitem>
<para><literal>Rancher</literal>のバージョンがアップグレードされていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get settings.management.cattle.io server-version

# Example output
NAME             VALUE
server-version   v2.8.4</screen>
</listitem>
</orderedlist>
<para><emphasis>RancherのHelmチャートのアップグレードに関する追加情報については、<link
xl:href="https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades">こちら</link>をご確認ください。</emphasis></para>
</section>
<section xml:id="day2-mgmt-cluster-helm-chart-upgrade-examples-metal3">
<title>Metal<superscript>3</superscript></title>
<para>この例では、Metal<superscript>3</superscript>を<literal>0.7.1</literal>バージョンにアップグレードする方法を説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>Rancherの現在のリリースの値を取得し、それらの値を<literal>rancher-values.yaml</literal>ファイルに出力します。</para>
<screen language="bash" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml</screen>
</listitem>
<listitem>
<para>Helmチャートを更新します。</para>
<screen language="bash" linenumbering="unnumbered">helm upgrade metal3 oci://registry.suse.com/edge/metal3-chart \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.1</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>のPodが実行されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get pods -n metal3-system

# Example output
NAME                                                     READY   STATUS    RESTARTS   AGE
baremetal-operator-controller-manager-785f99c884-fvsx4   2/2     Running   0          12m
metal3-metal3-ironic-96fb66cdd-j9mgf                     4/4     Running   0          2m41s
metal3-metal3-mariadb-55fd44b648-7fmvk                   1/1     Running   0          12m</screen>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>のHelmリリースバージョンが変更されていることを確認します。</para>
<screen language="bash" linenumbering="unnumbered">helm ls -n metal3-system

# Expected output
NAME  	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART       	APP VERSION
metal3	metal3-system	2       	2024-06-17 12:43:06.774802846 +0000 UTC	deployed	metal3-0.7.1	1.16.0</screen>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="id-downstream-clusters">
<title>ダウンストリームクラスタ</title>
<para>このセクションでは、<literal>管理クラスタ</literal>を使用して、ダウンストリームクラスタのさまざまな部分に対して、各種の<literal>Day
2</literal>操作を行う方法について説明します。</para>
<section xml:id="id-introduction">
<title>はじめに</title>
<para>このセクションは、<literal>Day 2</literal>操作に関するドキュメントの<emphasis
role="strong">「出発点」</emphasis>となることを目的としています。次の情報を確認できます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>複数のダウンストリームクラスタで<literal>Day 2</literal>操作を実行するために使用するデフォルトコンポーネント(<xref
linkend="day2-downstream-components"/>)。</para>
</listitem>
<listitem>
<para>自身の特定のユースケース(<xref linkend="day2-determine-use-case"/>)にどの<literal>Day
2</literal>リソースを使用するかの判断。</para>
</listitem>
<listitem>
<para><literal>Day 2</literal>操作に推奨されるワークフローシーケンス(<xref
linkend="day2-upgrade-workflow"/>)。</para>
</listitem>
</orderedlist>
<section xml:id="day2-downstream-components">
<title>コンポーネント</title>
<para>以下に、<literal>Day
2</literal>操作を正常に実行できるように、<literal>管理クラスタ</literal>または<literal>ダウンストリームクラスタ</literal>のいずれかでセットアップする必要があるデフォルトコンポーネントについて説明します。</para>
<section xml:id="id-rancher">
<title>Rancher</title>
<note>
<para>Rancherを使用せずにFleet (<xref
linkend="components-fleet"/>)を利用するユースケースの場合は、Rancherコンポーネントはまとめてスキップできます。</para>
</note>
<para><literal>ダウンストリームクラスタ</literal>の管理を担当します。<literal>管理クラスタ</literal>上にデプロイする必要があります。</para>
<para>詳細については、<xref linkend="components-rancher"/>を参照してください。</para>
</section>
<section xml:id="id-fleet">
<title>Fleet</title>
<para>マルチクラスタリソースのデプロイメントを担当します。</para>
<para>通常は、<literal>Rancher</literal>コンポーネントによって提供されます。
<literal>Rancher</literal>を使用しないユースケースでは、スタンドアロンコンポーネントとしてデプロイできます。</para>
<para>Fleetをスタンドアロンコンポーネントとしてインストールする方法の詳細については、Fleetの<link
xl:href="https://fleet.rancher.io/installation">インストールの詳細</link>を参照してください。</para>
<para>Fleetコンポーネントに関する詳細については、<xref linkend="components-fleet"/>を参照してください。</para>
<important>
<para>このドキュメントは、GitOps方式で<literal>Day
2</literal>操作に関連するリソースのデプロイメントを自動化するために、<literal>Fleet</literal>、具体的には<literal>GitRepo</literal>と<literal>Bundle</literal>リソース(この詳細は<xref
linkend="day2-determine-use-case"/>で詳しく説明します)に大きく依存しています。</para>
<para>サードパーティのGitOpsツールの使用が必要なユースケースの場合は、以下を参照してください。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>OSパッケージの更新</literal>の場合 - <xref
linkend="os-pkg-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para><literal>Kubernetesディストリビューションの更新</literal>の場合 - <xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/></para>
</listitem>
<listitem>
<para><literal>Helmチャートのアップグレード</literal>の場合 - <xref
linkend="release-notes"/>のページから目的のEdgeリリースでサポートされているチャートバージョンを取得して、そのチャートバージョンとURLをサードパーティのGitOpsツールに入力します</para>
</listitem>
</orderedlist>
</important>
</section>
<section xml:id="id-system-upgrade-controller-suc">
<title>System-upgrade-controller (SUC)</title>
<para><emphasis role="strong">system-upgrade-controller (SUC)</emphasis>は、
カスタムリソースを通じて提供される設定データ(<literal>Plan</literal>)に基づいて、指定したノード上でタスクを実行する役割を担います。SUCは、何らかの<literal>Day
2</literal>操作が必要な各<literal>ダウンストリームクラスタ</literal>に配置する必要があります。</para>
<para><emphasis role="strong">SUC</emphasis>に関する詳細については、アップストリーム<link
xl:href="https://github.com/rancher/system-upgrade-controller">リポジトリ</link>を参照してください。</para>
<para>ダウンストリームクラスタに<emphasis
role="strong">SUC</emphasis>をデプロイする方法については、まず、ユースケース(<xref
linkend="day2-determine-use-case"/>)を決定してから、<xref
linkend="day2-suc-dep-gitrepo"/>または<xref
linkend="day2-suc-dep-bundle"/>でSUCのデプロイメントに関する情報を参照してください。</para>
</section>
</section>
<section xml:id="day2-determine-use-case">
<title>ユースケースの決定</title>
<para>前述のように、<literal>Day
2</literal>操作に関連するリソースは、Fleetの<literal>GitRepo</literal>リソースと<literal>Bundle</literal>リソースを使用してダウンストリームクラスタに伝播されます。</para>
<para>以下に、これらのリソースの機能と、 <literal>Day 2</literal>操作に使用する必要があるユースケースに関する詳細を説明します。</para>
<section xml:id="id-gitrepo">
<title>GitRepo</title>
<para><literal>GitRepo</literal>は、<literal>Fleet</literal>が<literal>バンドル</literal>の作成元として使用できるGitリポジトリを表すFleet
(<xref
linkend="components-fleet"/>)リソースです。各<literal>バンドル</literal>は、<literal>GitRepo</literal>リソースの内部で定義された設定パスに基づいて作成されます。詳細については、<link
xl:href="https://fleet.rancher.io/gitrepo-add">GitRepo</link>のドキュメントを参照してください。</para>
<para><literal>Day
2</literal>操作の観点では、<literal>GitRepo</literal>リソースは通常、<emphasis>Fleet
GitOps</emphasis>アプローチを利用する<emphasis role="strong">非エアギャップ</emphasis>環境に
<literal>SUC</literal>または<literal>SUC Plan</literal>をデプロイするために使用されます。</para>
<para>または、リポジトリのセットアップを<emphasis
role="strong">ローカルGitサーバ経由でミラーリングすると</emphasis>、<literal>GitRepo</literal>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
<section xml:id="id-bundle">
<title>Bundle</title>
<para><literal>バンドル</literal>は、ターゲットクラスタにデプロイする<emphasis role="strong">
「生」</emphasis>のKubernetesリソースを保持します。バンドルは通常、<literal>GitRepo</literal>リソースから作成されますが、手動でデプロイできるユースケースもあります。詳細については、<link
xl:href="https://fleet.rancher.io/bundle-add">バンドル</link>のドキュメントを参照してください。</para>
<para><literal>Day 2</literal>操作の観点では、<literal>バンドル</literal>リソースは通常、
何らかの形態の<emphasis>ローカルGitOps</emphasis>手法を使用しない<emphasis
role="strong">エアギャップ</emphasis>環境(<emphasis
role="strong">「ローカルGitサーバ」</emphasis>など
)で<literal>SUC</literal>または<literal>SUC Plan</literal>をデプロイするために使用されます。</para>
<para>または、ご自身のユースケースで<emphasis>GitOps</emphasis>ワークフローを使用できない場合は(Gitリポジトリを使用する場合など)、<emphasis
role="strong">バンドル</emphasis>リソースを使用して、<literal>SUC</literal>または<literal>SUC
Plan</literal>を<emphasis role="strong">非エアギャップ</emphasis>環境にデプロイすることもできます。</para>
</section>
</section>
<section xml:id="day2-upgrade-workflow">
<title>Day 2ワークフロー</title>
<para>以下に、ダウンストリームクラスタを特定のEdgeリリースにアップグレードする際に従う必要がある<literal>Day
2</literal>ワークフローを示します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>OSパッケージの更新(<xref linkend="day2-os-package-update"/>)</para>
</listitem>
<listitem>
<para>Kubernetesバージョンアップグレード(<xref linkend="day2-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helmチャートのアップグレード(<xref linkend="day2-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-deployment-guide">
<title>システムアップグレードコントローラのデプロイメントガイド</title>
<para><emphasis role="strong">system-upgrade-controller
(SUC)</emphasis>は、<emphasis
role="strong">Plan</emphasis>と呼ばれるカスタムリソースで定義された設定に基づいてクラスタの特定のノード上にリソースをデプロイする役割を担います。詳細については、<link
xl:href="https://github.com/rancher/system-upgrade-controller">アップストリーム</link>ドキュメントを参照してください。</para>
<note>
<para>このセクションは、<literal>system-upgrade-controller</literal>のデプロイにのみ焦点を当てています。<emphasis
role="strong">Plan</emphasis>リソースは次のドキュメントからデプロイする必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>OSパッケージの更新(<xref linkend="day2-os-package-update"/>)</para>
</listitem>
<listitem>
<para>Kubernetesバージョンアップグレード(<xref linkend="day2-k8s-upgrade"/>)</para>
</listitem>
<listitem>
<para>Helmチャートのアップグレード(<xref linkend="day2-helm-upgrade"/>)</para>
</listitem>
</orderedlist>
</note>
<section xml:id="id-deployment-2">
<title>デプロイメント</title>
<note>
<para>このセクションでは、 Fleet (<xref linkend="components-fleet"/>)を使用して<emphasis
role="strong">SUC</emphasis>のデプロイメントを調整することを想定しています。サードパーティのGitOpsワークフローを使用しているユーザは、<xref
linkend="day2-suc-third-party-gitops"/>で、そのワークフローでのセットアップに必要なリソースを参照してください。</para>
</note>
<para>使用するリソースを判断するには、<xref linkend="day2-determine-use-case"/>を参照してください。</para>
<section xml:id="day2-suc-dep-gitrepo">
<title>GitRepoリソースを使用したSUCのデプロイメント</title>
<para>このセクションでは、<emphasis role="strong">SUC</emphasis>を正常にデプロイするために必要な<literal>SUC
Plan</literal>を<emphasis
role="strong">ターゲット</emphasis>ダウンストリームクラスタに配布する<literal>GitRepo</literal>リソースを作成する方法について説明します。</para>
<para>Edgeチームは、<literal>suse-edge/fleet-examples</literal>の各<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>において、<emphasis
role="strong">SUC</emphasis>に対してすぐに使える<literal>GitRepo</literal>リソースを<literal>gitrepos/day2/system-upgrade-controller-gitrepo.yaml</literal>で維持しています。</para>
<important>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを使用している場合は、必ず、専用の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグのリソースを使用してください。</para>
</important>
<para><literal>GitRepo</literal>は、次の方法のいずれかで作成できます。</para>
<itemizedlist>
<listitem>
<para>Rancher UI (<xref
linkend="day2-suc-dep-gitrepo-rancher"/>)を使用する(<literal>Rancher</literal>が利用可能な場合)</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="day2-suc-dep-gitrepo-manual"/>)</para>
</listitem>
</itemizedlist>
<para>作成されると、<literal>Fleet</literal>は、リソースを取得し、<emphasis
role="strong">SUC</emphasis>リソースをすべての<emphasis
role="strong">ターゲット</emphasis>クラスタにデプロイする役割を担います。デプロイメントプロセスを追跡する方法については、<xref
linkend="monitor-suc-deployment"/>を参照してください。</para>
<section xml:id="day2-suc-dep-gitrepo-rancher">
<title>GitRepoのデプロイメント - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Git Repos (Gitリポジトリ)］→［ Add Repository
(リポジトリの追加)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを使用する場合:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL (リポジトリのURL)</emphasis> -
<literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">［Watch (ウォッチ)］→［Revision (リビジョン)］</emphasis> -
使用する<literal>suse-edge/fleet-examples</literal>リポジトリの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(<literal>release-3.0.1</literal>など)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Paths (パス)］</emphasis>の下に、<emphasis
role="strong">system-upgrade-controller</emphasis>へのパスを、リリースタグ -
<literal>fleets/day2/system-upgrade-controller</literal>のとおりに追加します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Next (次へ)］</emphasis>を選択して、<emphasis
role="strong">ターゲット</emphasis>の設定セクションに移動します。</para>
</listitem>
<listitem>
<para><emphasis
role="strong"><literal>system-upgrade-controller</literal>をデプロイするクラスタのみを選択します。</emphasis>
設定に問題がなければ、<emphasis role="strong">［Create (作成)］</emphasis>をクリックします。</para>
</listitem>
</orderedlist>
<para>または、独自のリポジトリを使用してこれらのファイルをホストする場合は、上の手順で自身のリポジトリデータを指定する必要があります。</para>
</section>
<section xml:id="day2-suc-dep-gitrepo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUC</emphasis>
<literal>GitRepo</literal>のデプロイ元にする、目的のEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>
タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o system-upgrade-controller-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/{REVISION}/gitrepos/day2/system-upgrade-controller-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>の設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはダウンストリームクラスタにマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより詳細に選択する必要がある場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f system-upgrade-controller-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo system-upgrade-controller -n fleet-default

# Example output
NAME                        REPO                                               COMMIT       BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-dep-bundle">
<title>バンドルリソースを使用したSUCのデプロイメント</title>
<para>このセクションでは、<emphasis role="strong">SUC</emphasis>を正常にデプロイするために必要な<literal>SUC
Plan</literal>を<emphasis
role="strong">ターゲット</emphasis>のダウンストリームクラスタに配布する<literal>バンドル</literal>リソースを作成する方法について説明します。</para>
<para>Edgeチームは、<literal>suse-edge/fleet-examples</literal>の各<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>において、<emphasis
role="strong">SUC</emphasis>に対してすぐに使える<literal>バンドル</literal>リソースを<literal>bundles/day2/system-upgrade-controller/controller-bundle.yaml</literal>で維持しています。</para>
<important>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを使用している場合は、必ず、専用の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグのリソースを使用してください。</para>
</important>
<para><literal>バンドル</literal>は次のいずれかの方法で作成できます。</para>
<itemizedlist>
<listitem>
<para>Rancher UI(<xref
linkend="day2-suc-dep-bundle-rancher"/>)を使用する(<literal>Rancher</literal>が利用可能な場合)</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="day2-suc-dep-bundle-manual"/>)</para>
</listitem>
</itemizedlist>
<para>作成されると、<literal>Fleet</literal>は、リソースを取得し、 <emphasis
role="strong">SUC</emphasis>リソースをすべての<emphasis
role="strong">ターゲット</emphasis>クラスタにデプロイする役割を担います。デプロイメントプロセスを追跡する方法については、<xref
linkend="monitor-suc-deployment"/>を参照してください。</para>
<section xml:id="day2-suc-dep-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<itemizedlist>
<listitem>
<para>ファイルコンテンツを<emphasis role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページに手動でコピーする。ファイルコンテンツは、<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml</link>から取得できます。ここで、<literal>${REVISION}</literal>は、目的のEdge
<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグです(<literal>release-3.0.1</literal>など)。</para>
</listitem>
<listitem>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグに複製し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。そこから<literal>bundles/day2/system-upgrade-controller</literal>ディレクトリに移動し、<literal>controller-bundle.yaml</literal>を選択します。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-suc-dep-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis
role="strong">SUC</emphasis><literal>バンドル</literal>のデプロイ元にするEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o controller-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller/controller-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="bash" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより詳細に選択する必要がある場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f controller-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles system-upgrade-controller -n fleet-default

# Example output
NAME                        BUNDLEDEPLOYMENTS-READY   STATUS
system-upgrade-controller   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="day2-suc-third-party-gitops">
<title>サードパーティのGitOpsワークフローの使用時におけるsystem-upgrade-controllerのデプロイ</title>
<para>サードパーティのGitOpsツールを使用して<literal>system-upgrade-controller</literal>をデプロイする場合、ツールによっては、<literal>system-upgrade-controller</literal>
HelmチャートまたはKubernetesリソース、あるいはその両方の情報が必要な場合があります。</para>
<para><emphasis role="strong">SUC</emphasis>を使用するための特定のEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>を選択します。</para>
<para>そこから、<emphasis
role="strong">SUC</emphasis>のHelmチャートデータをファイル<literal>fleets/day2/system-upgrade-controller/fleet.ymal</literal>の<literal>helm</literal>設定セクションで見つけることができます。</para>
<para><emphasis
role="strong">SUC</emphasis>のKubernetesリソースは、<literal>.spec.resources.content</literal>の<emphasis
role="strong">SUC</emphasis><literal>バンドル</literal>設定にあります。バンドルの場所は<literal>bundles/day2/system-upgrade-controller/controller-bundle.yaml</literal>です。</para>
<para>上記のリソースを使用して、<emphasis
role="strong">SUC</emphasis>をデプロイするためにサードパーティのGitOpsワークフローで必要なデータを入力します。</para>
</section>
</section>
<section xml:id="id-monitor-suc-resources-using-rancher">
<title>Rancherを使用したSUCリソースの監視</title>
<para>このセクションでは、Rancher UIを使用して<emphasis
role="strong">SUC</emphasis>デプロイメントのライフサイクルおよびデプロイした<emphasis
role="strong">SUC Plan</emphasis>を監視する方法を説明します。</para>
<section xml:id="monitor-suc-deployment">
<title>SUCデプロイメントの監視</title>
<para>特定のクラスタの<emphasis role="strong">SUC</emphasis> Podのログを確認するには、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → &lt;クラスタ名&gt;</emphasis>を選択します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Workloads (ワークロード)］ → ［Pods］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ネームスペースのドロップダウンメニューで、<literal>cattle-system</literal>ネームスペースを選択します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-1.png"
width=""/> </imageobject>
<textobject><phrase>day2 sucデプロイメントの監視1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［Pods］フィルタバーに、<emphasis role="strong">SUC</emphasis>の名前 -
<literal>system-upgrade-controller</literal>と入力します。</para>
</listitem>
<listitem>
<para>Podの右側で<emphasis role="strong">⋮ → ［View Logs (ログの表示)］</emphasis>を選択します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-2.png"
width=""/> </imageobject>
<textobject><phrase>day2 sucデプロイメントの監視2</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para><emphasis role="strong">SUC</emphasis>のログは次のようになります。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-3.png"
width=""/> </imageobject>
<textobject><phrase>day2 sucデプロイメントの監視3</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
</orderedlist>
</section>
<section xml:id="monitor-suc-plans">
<title>SUC Planの監視</title>
<important>
<para><emphasis role="strong">SUC Plan</emphasis>のPodの存続時間は<emphasis
role="strong">15</emphasis>分です。この時間を過ぎると、Podを作成した該当するジョブにより削除されます。<emphasis
role="strong">SUC
Plan</emphasis>のPodのログにアクセスするには、クラスタのログを有効にする必要があります。Rancherでログを有効にする方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/logging">Rancher
Integration with Logging Services (Rancherとログサービスの統合)</link>」を参照してください。</para>
</important>
<para>特定の<emphasis role="strong">SUC</emphasis> Planの<emphasis
role="strong">Pod</emphasis>のログを確認するには、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → &lt;クラスタ名&gt;</emphasis>を選択します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Workloads (ワークロード)］ → ［Pods］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ネームスペースのドロップダウンメニューで、<literal>cattle-system</literal>ネームスペースを選択します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="day2-monitor-suc-deployment-1.png"
width=""/> </imageobject>
<textobject><phrase>day2 sucデプロイメントの監視1</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<para>［Pods］フィルタバーに<emphasis role="strong">SUC Plan</emphasis>
Podの名前を入力します。名前は<literal>apply-&lt;plan_name&gt;-on-&lt;node_name&gt;</literal>のテンプレート形式に従います。</para>
<figure>
<title>KubernetesアップグレードPlanのPodの例</title>
<mediaobject>
<imageobject> <imagedata fileref="day2-k8s-plan-monitor.png" width=""/>
</imageobject>
<textobject><phrase>day2 k8s planの監視</phrase></textobject>
</mediaobject></figure>
<para><emphasis>図1</emphasis>に注目してください。一方のPodが<emphasis role="strong">［Completed
(完了)］</emphasis>、他方が<emphasis role="strong">［Unknown
(不明)］</emphasis>になっています。これは想定内であり、ノードのKubernetesバージョンアップグレードによるものです。</para>
<figure>
<title>OSパッケージ更新PlanのPodの例</title>
<mediaobject>
<imageobject> <imagedata fileref="day2-os-pkg-plan-monitor.png" width=""/>
</imageobject>
<textobject><phrase>day2 osパッケージplanの監視</phrase></textobject>
</mediaobject></figure>
<figure>
<title>EIBによってHAクラスタにデプロイされるHelmチャートのアップグレードPlanのPodの例</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_chart_upgrade_plan_monitor.png"
width=""/> </imageobject>
<textobject><phrase>day2チャートアップグレードPlanの監視</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>ログを確認するPodを選択し、<emphasis role="strong">⋮ → ［View Logs
(ログの表示)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="day2-os-package-update">
<title>OSパッケージの更新</title>
<section xml:id="id-components">
<title>コンポーネント</title>
<para>このセクションでは、<literal>OSパッケージの更新</literal>プロセスがデフォルトの<literal>Day
2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)よりも優先して使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="id-edge-update-service">
<title>edge-update.service</title>
<para><literal>OSパッケージの更新</literal>を実行するSystemdサービスです。<link
xl:href="https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html">transactional-update</link>コマンドを使用して<link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Distribution_upgrade">ディストリビューションのアップグレード</link>(<literal>dup</literal>)を実行します。</para>
<note>
<para><link
xl:href="https://en.opensuse.org/SDB:Zypper_usage#Updating_packages">通常アップグレード</link>の方法を使用する場合、各ノードの<literal>/etc/edge/</literal>にファイル<literal>edge-update.conf</literal>を作成します。このファイル内に、<literal>UPDATE_METHOD=up</literal>変数を追加します。</para>
</note>
<para><emphasis role="strong">SUC
Plan</emphasis>によって配布されます。OSパッケージの更新が必要な各<emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置する必要があります。</para>
</section>
</section>
<section xml:id="id-requirements">
<title>要件</title>
<para><emphasis>全般:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SCC登録マシン</emphasis> -
すべてのダウンストリームクラスタノードは、<literal><link
xl:href="https://scc.suse.com/">https://scc.suse.com/</link></literal>に登録する必要があります。これは、<literal>edge-update.service</literal>を必要なOS
RPMリポジトリに正しく接続するために必要です。</para>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。OSパッケージの更新に関連する<emphasis
role="strong">SUC
Plan</emphasis>は、<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal>の下の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。<emphasis
role="strong">有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para><emphasis role="strong">control-plane</emphasis> SUC
Planに対してカスタムのTolerationを定義する例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-pkg-plan-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<para><emphasis>エアギャップ:</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">SUSE RPMリポジトリのミラーリング</emphasis> - OS
RPMリポジトリをローカルにミラーリングし、<literal>edge-update.service</literal>がそのリポジトリにアクセスできるようにする必要があります。このためには、<link
xl:href="https://github.com/SUSE/rmt">RMT</link>を使用します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-update-procedure">
<title>更新手順</title>
<note>
<para>このセクションでは、Fleet (<xref
linkend="components-fleet"/>)を使用して<literal>OSパッケージの更新</literal><emphasis
role="strong">SUC Plan</emphasis>をデプロイすることを想定しています。異なる方法で<emphasis
role="strong">SUC Plan</emphasis>をデプロイする場合は、<xref
linkend="os-pkg-suc-plan-deployment-third-party"/>を参照してください。</para>
</note>
<para><literal>OSパッケージの更新手順</literal>は、<emphasis role="strong">SUC
Plan</emphasis>をダウンストリームクラスタにデプロイする手順が中心になります。その後、これらのPlanには、<literal>edge-update.service</literal>
systemd.serviceをどのような方法でどのノードにデプロイするかに関する情報が保持されます。<emphasis
role="strong">SUC Plan</emphasis>の構造の詳細については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリーム</link>ドキュメントを参照してください。</para>
<para><literal>OSパッケージの更新</literal> SUC Planは次の方法で配布されます。</para>
<itemizedlist>
<listitem>
<para><literal>GitRepo</literal>リソースを使用する - <xref
linkend="os-pkg-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para><literal>バンドル</literal>リソースを使用する - <xref
linkend="os-pkg-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref linkend="day2-determine-use-case"/>を参照してください。</para>
<para><emphasis>更新手順</emphasis>中の処理の概要については、<xref
linkend="os-pkg-update-overview"/>のセクションを参照してください。</para>
<section xml:id="os-pkg-update-overview">
<title>概要</title>
<para>このセクションは、<emphasis
role="strong"><emphasis>OSパッケージの更新プロセス</emphasis></emphasis>が開始から終了までに経由するワークフロー全体について説明することを目的としています。</para>
<figure>
<title>OSパッケージの更新ワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_os_pkg_update_diagram.png" width=""/>
</imageobject>
<textobject><phrase>day2 osパッケージ更新の図</phrase></textobject>
</mediaobject></figure>
<para>OSパッケージの更新手順:</para>
<orderedlist numeration="arabic">
<listitem>
<para>ユーザは、<literal>OSパッケージの更新SUC
Plan</literal>を目的のダウンストリームクラスタにデプロイするために、<emphasis
role="strong">GitRepo</emphasis>リソースを使用するか、それとも<emphasis
role="strong">バンドル</emphasis>リソースを使用するかをそのユースケースに基づいて判断します。<emphasis
role="strong">GitRepo/バンドル</emphasis>を特定のダウンストリームクラスタセットにマップする方法の詳細については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">SUC Plan</emphasis>のデプロイメントに<emphasis
role="strong">GitRepo</emphasis>リソースまたは<emphasis
role="strong">バンドル</emphasis>リソースのどちらを使用すべきかわからない場合は、<xref
linkend="day2-determine-use-case"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo/バンドル</emphasis>の設定オプションについては、<xref
linkend="os-pkg-suc-plan-deployment-git-repo"/>または<xref
linkend="os-pkg-suc-plan-deployment-bundle"/>を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ユーザは、設定した<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースを<literal>管理クラスタ</literal>の<literal>fleet-default</literal>のネームスペースにデプロイします。これは、<emphasis
role="strong">手動</emphasis>で実行するか、<emphasis role="strong">Rancher
UI</emphasis> (利用可能な場合)を使用して実行します。</para>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)は、<literal>fleet-default</literal>ネームスペースを絶えず監視し、新しくデプロイされた<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースをすぐに検出します。Fleetが監視するネームスペースの詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/namespaces">Namespaces
(ネームスペース)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>ユーザが<emphasis
role="strong">GitRepo</emphasis>リソースをデプロイした場合、<literal>Fleet</literal>は、<emphasis
role="strong">GitRepo</emphasis>を調整し、その<emphasis
role="strong">パス</emphasis>と<emphasis
role="strong">fleet.yaml</emphasis>の設定に基づいて、<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-default</literal>ネームスペースにデプロイします。詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリの内容)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><literal>Fleet</literal>は続いて、<literal>Kubernetesリソース</literal>をこの<emphasis
role="strong">バンドル</emphasis>から、ターゲットとするすべての<literal>ダウンストリームクラスタ</literal>にデプロイします。<literal>OSパッケージの更新</literal>のコンテキストでは、Fleetは、次のリソースを<emphasis
role="strong">バンドル</emphasis>からデプロイします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>os-pkg-plan-agent</literal> <emphasis role="strong">SUC
Plan</emphasis> - クラスタの<emphasis
role="strong"><emphasis>エージェント</emphasis></emphasis>ノード上でパッケージの更新を行う方法を<emphasis
role="strong">SUC</emphasis>に指示します。
クラスタが<emphasis>control-plane</emphasis>ノードのみで構成されている場合は、解釈<emphasis
role="strong">「されません」</emphasis>。</para>
</listitem>
<listitem>
<para><literal>os-pkg-plan-control-plane</literal> <emphasis role="strong">SUC
Plan</emphasis> - クラスタの<emphasis
role="strong"><emphasis>control-plane</emphasis></emphasis>ノードに対してパッケージの更新を実行する方法を<emphasis
role="strong">SUC</emphasis>に指示します。</para>
</listitem>
<listitem>
<para><literal>os-pkg-update</literal> <emphasis role="strong">シークレット</emphasis> -
各<emphasis role="strong">SUC
Plan</emphasis>で参照され、実際にパッケージの更新を実行する<literal>edge-update.service</literal>
<emphasis
role="strong"><emphasis>sustemd.service</emphasis></emphasis>を作成する<literal>update.sh</literal>スクリプトを配布します。</para>
<note>
<para>上記のリソースは、各ダウンストリームクラスタの<literal>cattle-system</literal>ネームスペースにデプロイされます。</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ダウンストリームクラスタで、<emphasis role="strong">SUC</emphasis>は、新しくデプロイされた<emphasis
role="strong">SUC Plan</emphasis>を検出し、 <emphasis role="strong">SUC
Plan</emphasis>で定義された<emphasis
role="strong">ノードセレクタ</emphasis>と一致する各ノードに<emphasis
role="strong"><emphasis>更新Pod</emphasis></emphasis>をデプロイします。<emphasis
role="strong">SUC Plan Pod</emphasis>を監視する方法については、<xref
linkend="monitor-suc-plans"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">更新Pod</emphasis>
(各ノードにデプロイ)は、<literal>os-pkg-update</literal>シークレットを<emphasis
role="strong">マウント</emphasis>し、シークレットによって配布される<literal>update.sh</literal>スクリプトを<emphasis
role="strong">実行</emphasis>します。</para>
</listitem>
<listitem>
<para><literal>update.sh</literal>は、処理を進めて以下を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>edge-update.service</literal>の作成 - 作成されるサービスは<emphasis
role="strong">ワンショット</emphasis>のタイプで、次のワークフローを採用します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>以下を実行して、ノードのOS上のすべてのパッケージバージョンを更新します。</para>
<screen language="bash" linenumbering="unnumbered">transactional-update cleanup dup</screen>
</listitem>
<listitem>
<para><literal>transactional-update</literal>が正常に実行されたら、システムの<emphasis
role="strong">再起動</emphasis>をスケジュールして、パッケージバージョンの更新を有効にできるようにします。</para>
<note>
<para>システムの再起動は、<literal>transactional-update</literal>が正常に実行されてから<emphasis
role="strong">1分</emphasis>の間にスケジュールされます。</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>edge-update.service</literal>を起動し、完了するまで待機します。</para>
</listitem>
<listitem>
<para><literal>edge-update.service</literal>のクリーンアップ - <emphasis
role="strong"><emphasis>systemd.service</emphasis></emphasis>は、ジョブの完了後にシステムから削除されます。これは今後、誤って実行/再起動されないようにするためです。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>OSパッケージの更新手順は、<emphasis
role="strong"><emphasis>システムの再起動</emphasis></emphasis>で完了します。再起動後、すべてのOSパッケージバージョンを、利用可能なOS
RPMリポジトリに表示されている対応する最新バージョンに更新する必要があります。</para>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment">
<title>OSパッケージの更新 - SUC Planのデプロイメント</title>
<para>このセクションでは、Fleetの<emphasis role="strong">GitRepo</emphasis>リソースおよび<emphasis
role="strong">バンドル</emphasis>リソースを使用して、<emphasis role="strong">SUC
Plan</emphasis>に関連するOSパッケージの更新のデプロイメントを調整する方法について説明します。</para>
<section xml:id="os-pkg-suc-plan-deployment-git-repo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>OSパッケージの更新</literal> <emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">GitRepo</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="os-pkg-suc-plan-deployment-git-repo-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="os-pkg-suc-plan-deployment-git-repo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSパッケージの更新プロセスを監視するには、<xref
linkend="monitor-suc-plans"/>のドキュメントを参照してください。</para>
<section xml:id="os-pkg-suc-plan-deployment-git-repo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Git Repos (Gitリポジトリ)］→［ Add Repository
(リポジトリの追加)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを使用する場合:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL (リポジトリのURL)</emphasis> -
<literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">［Watch (ウォッチ)］ → ［Revision (リビジョン)］</emphasis> -
使用する<literal>suse-edge/fleet-examples</literal>リポジトリの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Paths
(パス)］</emphasis>に、使用するOSパッケージ更新用のFleetのパス<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal>を追加します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Next (次へ)］</emphasis>を選択して<emphasis
role="strong">ターゲット</emphasis>の設定セクションに移動します。<emphasis
role="strong">アップグレードするノードのパッケージを含むクラスタのみを選択してください。</emphasis></para>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
<para>または、独自のリポジトリを使用してこれらのファイルをホストする場合は、上の手順で自身のリポジトリデータを指定する必要があります。</para>
</section>
<section xml:id="os-pkg-suc-plan-deployment-git-repo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para>OSの<emphasis role="strong">SUC更新Plan</emphasis>の適用元にするEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o os-pkg-update-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/os-pkg-update-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f os-pkg-update-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get gitrepo os-pkg-update -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   https://github.com/suse-edge/fleet-examples.git   release-3.0.1  0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>OSパッケージの更新</literal><emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">バンドル</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="os-pkg-suc-plan-deployment-bundle-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="os-pkg-suc-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのOSパッケージの更新プロセスを監視するには、<xref
linkend="monitor-suc-plans"/>のドキュメントを参照してください。</para>
<section xml:id="os-pkg-suc-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、［<emphasis role="strong">☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>コンテンツを<emphasis role="strong">［Create
from YAML (YAMLから作成)］</emphasis>ページに手動コピーする。コンテンツは、<link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml</link>から取得できます。ここで、<literal>${REVISION}</literal>は、使用しているEdgeの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>です。</para>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリを目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグに複製し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページで<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択する。そこから、<literal>bundles/day2/system-upgrade-controller-plans/os-pkg-update</literal>ディレクトリに移動し、<literal>pkg-update-bundle.yaml</literal>を選択します。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Create (作成)］</emphasis>を選択します。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="os-pkg-suc-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para>OSパッケージの更新<emphasis role="strong">SUC Plan</emphasis>の適用元にするEdgeの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<screen language="bash" linenumbering="unnumbered">curl -o pkg-update-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl apply -f pkg-update-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered">kubectl get bundles os-pkg-update -n fleet-default

# Example output
NAME            BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="os-pkg-suc-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザがOSパッケージの更新<emphasis role="strong">SUC
Plan</emphasis>を独自のサードパーティGitOpsワークフロー(<literal>Flux</literal>など)に統合したいユースケースが存在する場合があります。</para>
<para>必要なOSパッケージの更新リソースを取得するには、まず、使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを特定します。</para>
<para>その後、リソースを<literal>fleets/day2/system-upgrade-controller-plans/os-pkg-update</literal>で特定できます。</para>
<itemizedlist>
<listitem>
<para><literal>plan-control-plane.yaml</literal> - <emphasis
role="strong">control-plane</emphasis>ノードの<literal>system-upgrade-controller</literal>のPlanリソース</para>
</listitem>
<listitem>
<para><literal>plan-agent.yaml</literal> -
<literal>system-upgrade-controller</literal> <emphasis
role="strong">agent</emphasis>ノードのPlanリソース</para>
</listitem>
<listitem>
<para><literal>secret.yaml</literal> - <literal>edge-update.service</literal>
<link
xl:href="https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html">systemd.service</link>を作成するスクリプトを配布するシークレット</para>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、<literal>system-upgrade-controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。<literal>system-upgrade-controller</literal>をデプロイする方法については、<xref
linkend="day2-suc-third-party-gitops"/>を参照してください。</para>
</important>
<para>GitOpsワークフローを使用してOSパッケージの更新用の<emphasis role="strong">SUC
Plan</emphasis>をデプロイする方法について理解を深めるには、<literal>Fleet</literal>を使用した更新手順の概要(<xref
linkend="os-pkg-update-overview"/>)を確認すると役に立ちます。</para>
</section>
</section>
</section>
<section xml:id="day2-k8s-upgrade">
<title>Kubernetesバージョンアップグレード</title>
<important>
<para>このセクションでは、Rancher (<xref
linkend="components-rancher"/>)インスタンスを使用して作成されて<emphasis
role="strong">「いない」</emphasis>ダウンストリームクラスタのアップグレードについて説明します。<literal>Rancher</literal>で作成したクラスタのKubernetesバージョンをアップグレードする方法については、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/getting-started/installation-and-upgrade/upgrade-and-roll-back-kubernetes#upgrading-the-kubernetes-version">Upgrading
and Rolling Back Kubernetes (Kubernetesのアップグレードとロールバック)</link>」を参照してください。</para>
</important>
<section xml:id="id-components-2">
<title>コンポーネント</title>
<para>このセクションでは、<literal>Kubernetesアップグレード</literal>プロセスがデフォルトの<literal>Day
2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)よりも優先して使用するカスタムコンポーネントについて説明します。</para>
<section xml:id="id-rke2-upgrade-2">
<title>rke2-upgrade</title>
<para>特定ノードのRKE2バージョンのアップグレードを行うイメージです。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、RKE2のアップグレードが必要な各<emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置する必要があります。</para>
<para><literal>rke2-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade/tree/master">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
<section xml:id="id-k3s-upgrade">
<title>k3s-upgrade</title>
<para>特定のノードのK3sバージョンをアップグレードするイメージです。</para>
<para><emphasis role="strong">SUC Plan</emphasis>に基づいて<emphasis
role="strong">SUC</emphasis>によって作成されたPodを通じて配布されます。このPlanは、K3sのアップグレードが必要な各<emphasis
role="strong">ダウンストリームクラスタ</emphasis>に配置する必要があります。</para>
<para><literal>k3s-upgrade</literal>イメージによるアップグレードの実行方法の詳細については、<link
xl:href="https://github.com/k3s-io/k3s-upgrade">アップストリーム</link>ドキュメントを参照してください。</para>
</section>
</section>
<section xml:id="id-requirements-2">
<title>要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Kubernetesディストリビューションをバックアップします。</emphasis></para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">インポートしたRKE2クラスタ</emphasis>については、<link
xl:href="https://docs.rke2.io/backup_restore">RKE2のバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">インポートしたK3sクラスタ</emphasis>については、<link
xl:href="https://docs.k3s.io/datastore/backup-restore">K3sのバックアップと復元</link>に関するドキュメントを参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">SUC PlanのTolerationがノードのTolerationと一致すること</emphasis>
- Kubernetesクラスタノードにカスタムの<emphasis
role="strong">Taint</emphasis>が設定されている場合は、<emphasis role="strong">SUC
Plan</emphasis>にそのTaintに対する<link
xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">Toleration</link>を追加してください。デフォルトでは、<emphasis
role="strong">SUC Plan</emphasis>には、<emphasis
role="strong">control-plane</emphasis>ノードのTolerationのみが含まれます。デフォルトのTolerationは次のとおりです。</para>
<itemizedlist>
<listitem>
<para><emphasis>CriticalAddonsOnly=true:NoExecute</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/control-plane:NoSchedule</emphasis></para>
</listitem>
<listitem>
<para><emphasis>node-role.kubernetes.io/etcd:NoExecute</emphasis></para>
<note>
<para>追加のTolerationは、各Planの<literal>.spec.tolerations</literal>セクションに追加する必要があります。Kubernetesバージョンアップグレードに関する<emphasis
role="strong">SUC Plan</emphasis>は、次の場所の<link
xl:href="https://github.com/suse-edge/fleet-examples">suse-edge/fleet-examples</link>リポジトリにあります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis> -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">必ず、有効なリポジトリ<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグからのPlanを使用してください。</emphasis></para>
<para>RKE2 <emphasis role="strong">control-plane</emphasis> SUC
PlanのカスタムTolerationの定義例は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: rke2-plan-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...</screen>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="id-upgrade-procedure">
<title>アップグレード手順</title>
<note>
<para>このセクションでは、<emphasis role="strong">SUC Plan</emphasis>をFleet (<xref
linkend="components-fleet"/>)を使用してデプロイすることを想定しています。別の方法で<emphasis
role="strong">SUC Plan</emphasis>をデプロイする場合は、<xref
linkend="k8s-upgrade-suc-plan-deployment-third-party"/>を参照してください。</para>
</note>
<para><literal>Kubernetesバージョンアップグレード手順</literal>は、<emphasis role="strong">SUC
Plan</emphasis>をダウンストリームクラスタにデプロイする手順が中心になります。その後、これらのPlanには、<emphasis
role="strong">SUC</emphasis>に対し、<literal>rke2/k3s-upgrade</literal>イメージを実行するPodをどのノード上に作成するかを指示する情報が保持されます。<emphasis
role="strong">SUC Plan</emphasis>の構造については、<link
xl:href="https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans">アップストリーム</link>ドキュメントを参照してください。</para>
<para><literal>Kubernetesアップグレード</literal>Planは次のように配布されます。</para>
<itemizedlist>
<listitem>
<para><literal>GitRepo</literal>リソースを使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo"/></para>
</listitem>
<listitem>
<para><literal>バンドル</literal>リソースを使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/></para>
</listitem>
</itemizedlist>
<para>どのリソースを使用すべきかを判断するには、<xref linkend="day2-determine-use-case"/>を参照してください。</para>
<para><emphasis>更新手順</emphasis>中の処理の概要については、<xref
linkend="k8s-version-upgrade-overview"/>のセクションを参照してください。</para>
<section xml:id="k8s-version-upgrade-overview">
<title>概要</title>
<para>このセクションは、<emphasis
role="strong"><emphasis>Kubernetesバージョンアップグレードプロセス</emphasis></emphasis>が開始から終了までに経由するワークフロー全体について説明することを目的としています。</para>
<figure>
<title>Kubernetesバージョンアップグレードのワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_k8s_version_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 k8sバージョンアップグレードの図</phrase></textobject>
</mediaobject></figure>
<para>Kubernetesバージョンアップグレードの手順:</para>
<orderedlist numeration="arabic">
<listitem>
<para>ユーザは、<literal>KubernetesアップグレードSUC
Plan</literal>を目的のダウンストリームクラスタにデプロイするために、<emphasis
role="strong">GitRepo</emphasis>リソースを使用するか、それとも<emphasis
role="strong">バンドル</emphasis>リソースを使用するかをそのユースケースに基づいて判断します。<emphasis
role="strong">GitRepo/バンドル</emphasis>を特定のダウンストリームクラスタセットにマップする方法の詳細については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">SUC Plan</emphasis>のデプロイメントに<emphasis
role="strong">GitRepo</emphasis>リソースまたは<emphasis
role="strong">バンドル</emphasis>リソースのどちらを使用すべきかわからない場合は、<xref
linkend="day2-determine-use-case"/>を参照してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo/バンドル</emphasis>の設定オプションについては、<xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo"/>または<xref
linkend="k8s-upgrade-suc-plan-deployment-bundle"/>を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ユーザは、設定した<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースを<literal>管理クラスタ</literal>の<literal>fleet-default</literal>のネームスペースにデプロイします。これは、<emphasis
role="strong">手動</emphasis>で実行するか、<emphasis role="strong">Rancher
UI</emphasis> (利用可能な場合)を使用して実行します。</para>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)は、<literal>fleet-default</literal>ネームスペースを絶えず監視し、新しくデプロイされた<emphasis
role="strong">GitRepo/バンドル</emphasis>リソースをすぐに検出します。Fleetが監視するネームスペースの詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/namespaces">Namespaces
(ネームスペース)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para>ユーザが<emphasis
role="strong">GitRepo</emphasis>リソースをデプロイした場合、<literal>Fleet</literal>は、<emphasis
role="strong">GitRepo</emphasis>を調整し、その<emphasis
role="strong">パス</emphasis>と<emphasis
role="strong">fleet.yaml</emphasis>の設定に基づいて、<emphasis
role="strong">バンドル</emphasis>リソースを<literal>fleet-default</literal>ネームスペースにデプロイします。詳細については、Fleetの「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリの内容)</link>」のドキュメントを参照してください。</para>
</listitem>
<listitem>
<para><literal>Fleet</literal>は続いて、<literal>Kubernetesリソース</literal>をこの<emphasis
role="strong">バンドル</emphasis>から、ターゲットとするすべての<literal>ダウンストリームクラスタ</literal>にデプロイします。<literal>Kubernetesバージョンアップグレード</literal>のコンテキストでは、Fleetは、次のリソースを<emphasis
role="strong">バンドル</emphasis>からデプロイします(Kubernetesディストリビューションによって異なる)。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>rke2-plan-agent</literal>/<literal>k3s-plan-agent</literal> -
クラスタ<emphasis
role="strong"><emphasis>エージェント</emphasis></emphasis>ノードでKubernetesをアップグレードする方法を<emphasis
role="strong">SUC</emphasis>に指示します。クラスタが<emphasis>control-plane</emphasis>ノードのみで構成されている場合は、解釈<emphasis
role="strong">「されません」</emphasis>。</para>
</listitem>
<listitem>
<para><literal>rke2-plan-control-plane</literal>/<literal>k3s-plan-control-plane</literal>
- クラスタの<emphasis
role="strong"><emphasis>control-plane</emphasis></emphasis>ノードでKubernetesをアップグレードする方法を<emphasis
role="strong">SUC</emphasis>に指示します。</para>
<note>
<para>上記の<emphasis role="strong">SUC
Plan</emphasis>は、各ダウンストリームクラスタの<literal>cattle-system</literal>ネームスペースにデプロイされます。</para>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>ダウンストリームクラスタで、<emphasis role="strong">SUC</emphasis>は、新しくデプロイされた<emphasis
role="strong">SUC Plan</emphasis>を検出し、 <emphasis role="strong">SUC
Plan</emphasis>で定義された<emphasis
role="strong">ノードセレクタ</emphasis>と一致する各ノードに<emphasis
role="strong"><emphasis>更新Pod</emphasis></emphasis>をデプロイします。<emphasis
role="strong">SUC Plan Pod</emphasis>を監視する方法については、<xref
linkend="monitor-suc-plans"/>を参照してください。</para>
</listitem>
<listitem>
<para>デプロイした<emphasis role="strong">SUC Plan</emphasis>に応じて、<emphasis
role="strong">更新Pod</emphasis>は、<link
xl:href="https://hub.docker.com/r/rancher/rke2-upgrade/tags">rke2-upgrade</link>イメージまたは<link
xl:href="https://hub.docker.com/r/rancher/k3s-upgrade/tags">k3s-upgrade</link>イメージのいずれかを実行して、<emphasis
role="strong">それぞれの</emphasis>クラスタノードで次のワークフローを実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/">Cordon</link>クラスタノード
-
ノードのアップグレード時にこのノードにPodが誤ってスケジュールされないようにするために、このノードを<literal>unschedulable</literal>とマークします。</para>
</listitem>
<listitem>
<para>ノードOSにインストールされている<literal>rke2/k3s</literal>バイナリを、Podが現在実行している<literal>rke2-upgrade/k3s-upgrade</literal>イメージによって配布されるバイナリに置き換えます。</para>
</listitem>
<listitem>
<para>ノードOSで実行されている<literal>rke2/k3s</literal>プロセスを強制終了します -
これにより、新しいバージョンを使用して<literal>rke2/k3s</literal>プロセスを自動的に再起動するように<emphasis
role="strong">スーパーバイザ</emphasis>に指示します。</para>
</listitem>
<listitem>
<para><link
xl:href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/">Uncordon</link>クラスタノード
-
Kubernetesディストリビューションが正常にアップグレードされると、このノードは再び<literal>scheduable</literal>とマークされます。</para>
<note>
<para><literal>rke2-upgrade</literal>イメージおよび<literal>k3s-upgrade</literal>イメージの機能の詳細については、<link
xl:href="https://github.com/rancher/rke2-upgrade">rke2-upgrade</link>および<link
xl:href="https://github.com/k3s-io/k3s-upgrade">k3s-upgrade</link>のアップストリームプロジェクトを参照してください。</para>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>上記の手順を実行すると、各クラスタノードのKubernetesバージョンが目的のEdge互換<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>にアップグレードされているはずです。</para>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment">
<title>Kubernetesバージョンアップグレード - SUC Planのデプロイメント</title>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo">
<title>SUC Planのデプロイメント - GitRepoリソース</title>
<para>必要な<literal>Kubernetesアップグレード</literal><emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">GitRepo</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="k8s-upgrade-suc-plan-deployment-git-repo-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesのアップグレードプロセスを監視するには、<xref
linkend="monitor-suc-plans"/>のドキュメントを参照してください。</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-rancher">
<title>GitRepoの作成 - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Git Repos (Gitリポジトリ)］→［ Add Repository
(リポジトリの追加)］</emphasis>に移動します。</para>
</listitem>
</orderedlist>
<para><literal>suse-edge/fleet-examples</literal>リポジトリを使用する場合:</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">Repository URL (リポジトリのURL)</emphasis> -
<literal><link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link></literal></para>
</listitem>
<listitem>
<para><emphasis role="strong">［Watch (ウォッチ)］ → ［Revision (リビジョン)］</emphasis> -
使用する<literal>suse-edge/fleet-examples</literal>リポジトリの<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Paths
(パス)］</emphasis>の下に、KubernetesディストリビューションのアップグレードFleetへのパスを、リリースタグに表示されているとおりに追加します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>RKE2の場合 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade</literal></para>
</listitem>
<listitem>
<para>K3sの場合 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade</literal></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis role="strong">［Next (次へ)］</emphasis>を選択し、<emphasis
role="strong">ターゲット</emphasis>の設定セクションに移動します。<emphasis
role="strong">目的のKubernetesディストリビューションをアップグレードするクラスタのみを選択します</emphasis></para>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
<para>または、独自のリポジトリを使用してこれらのファイルをホストする場合は、上の手順で自身のリポジトリデータを指定する必要があります。</para>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-git-repo-manual">
<title>GitRepoの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para>Kubernetes <emphasis
role="strong">SUCアップグレードPlan</emphasis>を適用する目的のEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">GitRepo</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/rke2-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-upgrade-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/k3s-upgrade-gitrepo.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>設定を編集し、<literal>spec.targets</literal>で目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>GitRepo</literal>リソースはどのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタ変更に一致させるには、デフォルトの<literal>GitRepo</literal><emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">GitRepo</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl apply -f rke2-upgrade-gitrepo.yaml

# K3s
kubectl apply -f k3s-upgrade-gitrepo.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した <emphasis
role="strong">GitRepo</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># RKE2
kubectl get gitrepo rke2-upgrade -n fleet-default

# K3s
kubectl get gitrepo k3s-upgrade -n fleet-default

# Example output
NAME           REPO                                              COMMIT          BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0
rke2-upgrade   https://github.com/suse-edge/fleet-examples.git   release-3.0.1   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle">
<title>SUC Planのデプロイメント - バンドルリソース</title>
<para>必要な<literal>Kubernetesアップグレード</literal><emphasis role="strong">SUC
Plan</emphasis>を配布する<emphasis
role="strong">バンドル</emphasis>リソースは、次のいずれかの方法でデプロイできます。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>Rancher UI</literal>を使用する - <xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-rancher"/>
(<literal>Rancher</literal>が利用可能な場合)。</para>
</listitem>
<listitem>
<para>リソースを<literal>管理クラスタ</literal>に手動でデプロイする(<xref
linkend="k8s-upgrade-suc-plan-deployment-bundle-manual"/>)。</para>
</listitem>
</orderedlist>
<para>デプロイ後に、ターゲットクラスタのノードのKubernetesのアップグレードプロセスを監視するには、<xref
linkend="monitor-suc-plans"/>のドキュメントを参照してください。</para>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-rancher">
<title>バンドルの作成 - Rancher UI</title>
<orderedlist numeration="arabic">
<listitem>
<para>左上隅で、［<emphasis role="strong">☰］ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Advanced (詳細)］</emphasis>&gt;<emphasis
role="strong">［Bundles (バンドル)］ </emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Create from YAML (YAMLから作成)］</emphasis>を選択します。</para>
</listitem>
<listitem>
<para>ここから次のいずれかの方法でバンドルを作成できます。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><emphasis role="strong">バンドル</emphasis>コンテンツを<emphasis role="strong">［Create
from YAML (YAMLから作成)］</emphasis>ページに手動でコピーする。コンテンツは次の場所から取得できます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>RKE2 - <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</link></para>
</listitem>
<listitem>
<para>K3s - <link
xl:href="https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml">https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</link></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリを目的の<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグに複製し、<emphasis
role="strong">［Create from YAML (YAMLから作成)］</emphasis>ページの<emphasis
role="strong">［Read from File
(ファイルから読み取り)］</emphasis>オプションを選択します。そこから、必要なバンドルに移動します(RKE2の場合は<literal>/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</literal>、K3sの場合は<literal>/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</literal>)。<emphasis
role="strong">［Create from YAML
(YAMLから作成)］</emphasis>ページにバンドルコンテンツが自動的に入力されます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>クラスタを次のように変更します。</para>
<itemizedlist>
<listitem>
<para>すべてのダウンストリームクラスタに一致させるには、デフォルトのバンドル<literal>.spec.targets</literal>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>より細かくダウンストリームクラスタにマッピングするには、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-bundle-manual">
<title>バンドルの作成 - 手動</title>
<orderedlist numeration="arabic">
<listitem>
<para>Kubernetes <emphasis
role="strong">SUCアップグレードPlan</emphasis>を適用する目的のEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを選択します(以下では<literal>${REVISION}</literal>として参照)。</para>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースをプルします。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">RKE2</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o rke2-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/rke2-upgrade/plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><emphasis role="strong">K3s</emphasis>クラスタの場合:</para>
<screen language="bash" linenumbering="unnumbered">curl -o k3s-plan-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/k3s-upgrade/plan-bundle.yaml</screen>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>設定を編集し、<literal>spec.targets</literal>に目的のターゲットリストを指定します。デフォルトでは、<literal>suse-edge/fleet-examples</literal>の<literal>バンドル</literal>リソースは、どのダウンストリームクラスタにもマップ<emphasis
role="strong">「されません」</emphasis>。</para>
<itemizedlist>
<listitem>
<para>すべてのクラスタに一致させるには、デフォルトの<literal>バンドル</literal>の<emphasis
role="strong">ターゲット</emphasis>を次のように変更します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  targets:
  - clusterSelector: {}</screen>
</listitem>
<listitem>
<para>または、クラスタをより細かく選択したい場合は、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Cluster (ダウンストリームクラスタへのマップ)</link>」を参照してください</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">バンドル</emphasis>リソースを<literal>管理クラスタ</literal>に適用します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl apply -f rke2-plan-bundle.yaml

# For K3s
kubectl apply -f k3s-plan-bundle.yaml</screen>
</listitem>
<listitem>
<para><literal>fleet-default</literal>ネームスペースで、作成した<emphasis
role="strong">バンドル</emphasis>リソースを表示します。</para>
<screen language="bash" linenumbering="unnumbered"># For RKE2
kubectl get bundles rke2-upgrade -n fleet-default

# For K3s
kubectl get bundles k3s-upgrade -n fleet-default

# Example output
NAME           BUNDLEDEPLOYMENTS-READY   STATUS
k3s-upgrade    0/0
rke2-upgrade   0/0</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="k8s-upgrade-suc-plan-deployment-third-party">
<title>SUC Planのデプロイメント - サードパーティのGitOpsワークフロー</title>
<para>ユーザがKubernetesアップグレードリソースを独自のサードパーティGitOpsワークフロー(<literal>Flux</literal>など)に統合したいユースケースが存在する場合があります。</para>
<para>必要なアップグレードリソースを取得するには、まず、使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを特定します。</para>
<para>その後、次の場所でリソースを確認できます。</para>
<itemizedlist>
<listitem>
<para>RKE2クラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>エージェント</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/rke2-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>K3sクラスタのアップグレードの場合:</para>
<itemizedlist>
<listitem>
<para><literal>control-plane</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-control-plane.yaml</literal></para>
</listitem>
<listitem>
<para><literal>agent</literal>ノード用 -
<literal>fleets/day2/system-upgrade-controller-plans/k3s-upgrade/plan-agent.yaml</literal></para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは、<literal>system-upgrade-controller</literal>によって解釈され、アップグレードする各ダウンストリームクラスタにデプロイする必要があります。<literal>system-upgrade-controller</literal>をデプロイする方法については、<xref
linkend="day2-suc-third-party-gitops"/>を参照してください。</para>
</important>
<para>GitOpsワークフローを使用してKubernetesバージョンアップグレード用の<emphasis role="strong">SUC
Plan</emphasis>をデプロイする方法について理解を深めるには、<literal>Fleet</literal>を使用した更新手順の概要(<xref
linkend="k8s-version-upgrade-overview"/>)を確認すると役に立ちます。</para>
</section>
</section>
</section>
<section xml:id="day2-helm-upgrade">
<title>Helmチャートのアップグレード</title>
<note>
<para>以下の各セクションでは、<literal>Fleet</literal>の機能を使用してHelmチャートの更新を実現する方法を中心に説明します。</para>
<para>サードパーティのGitOpsワークフローを採用するユーザは、<literal>fleets/day2/chart-templates/&lt;chart-name&gt;</literal>にある<literal>fleet.yaml</literal>から目的のHelmチャートの設定を取得する必要があります。<emphasis
role="strong">有効な「Day 2」Edge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>からチャートデータを取得してください。</emphasis></para>
</note>
<section xml:id="id-components-3">
<title>コンポーネント</title>
<para>この操作には、デフォルトの<literal>Day 2</literal>コンポーネント(<xref
linkend="day2-downstream-components"/>)以外のカスタムコンポーネントは不要です。</para>
</section>
<section xml:id="id-preparation-for-air-gapped-environments">
<title>エアギャップ環境の準備</title>
<section xml:id="id-ensure-that-you-have-access-to-your-helm-charts-upgrade-fleet-yaml-file">
<title>Helmチャートのアップグレード<literal>fleet.yaml</literal>ファイルにアクセスできることの確認</title>
<para>必要なリソースを<literal>管理クラスタ</literal>がアクセスできるローカルGitサーバにホストします。</para>
</section>
<section xml:id="id-find-the-required-assets-for-your-edge-release-version">
<title>Edgeリリースバージョンに必要なアセットの検索</title>
<orderedlist numeration="arabic">
<listitem>
<para>Day 2<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>のページに移動し、チャートのアップグレード先のEdge
3.X.Yリリースを見つけ、<emphasis role="strong">［Assets (アセット)］</emphasis>をクリックします。</para>
</listitem>
<listitem>
<para>そのリリースの<emphasis role="strong">［Assets
(アセット)］</emphasis>セクションから、次のファイルをダウンロードします。これは、SUSEがサポートするHelmチャートのエアギャップアップグレードに必要です。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<tbody>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">リリースファイル</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">説明</emphasis></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>このスクリプトは、ファイル<literal>edge-release-images.txt</literal>に記載されたイメージをプルし、エアギャップ環境で使用できる「.tar.gz」アーカイブに保存します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-save-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>このスクリプトは、ファイル<literal>edge-release-helm-oci-artefacts.txt</literal>に記載されたSUSE
OCIチャートアーティファクトを取得し、その他すべてのチャートOCIアーカイブを含むディレクトリの「.tar.gz」アーカイブを作成します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-images.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>このスクリプトは、<literal>edge-save-images.sh</literal>によって生成された「.tar.gz」アーカイブのイメージを読み込み、そのイメージに再度タグを付けて、プライベートレジストリにプッシュします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-load-oci-artefacts.sh</emphasis></para></entry>
<entry align="left" valign="top"><para>このスクリプトは、「.tgz」SUSE
OCIチャートを含むディレクトリを取得し、すべてのOCIチャートをプライベートレジストリに読み込みます。ディレクトリは、<literal>edge-save-oci-artefacts.sh</literal>スクリプトによって生成された「.tar.gz」アーカイブから取得されます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-helm-oci-artefacts.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>このファイルには、SUSE EdgeリリースHelmチャートのOCIアーティファクトが含まれています。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis>edge-release-images.txt</emphasis></para></entry>
<entry align="left" valign="top"><para>このファイルには、EdgeリリースHelmチャートに必要なイメージのリストが含まれています。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-the-suse-edge-release-images-archive">
<title>SUSE Edgeリリースイメージのアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-images.sh</screen>
</listitem>
<listitem>
<para><literal>edge-save-images.sh</literal>スクリプトを使用して、<emphasis>Docker</emphasis>のインポート可能な「.tar.gz」アーカイブを作成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-images.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para><literal>-i|--images</literal>オプションを指定していない場合、すぐに読み込める<literal>edge-images.tar.gz</literal>アーカイブが必要なイメージとともに作成されます。</para>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします</para>
<screen language="bash" linenumbering="unnumbered">scp edge-images.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-a-suse-edge-helm-chart-oci-images-archive">
<title>SUSE Edge HelmチャートOCIイメージのアーカイブの作成</title>
<para><emphasis>インターネットにアクセスできるマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>edge-save-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-save-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para><literal>edge-save-oci-artefacts.sh</literal>スクリプトを使用して、すべてのSUSE Edge
HelmチャートOCIイメージの「.tar.gz」アーカイブを作成します。</para>
<screen language="bash" linenumbering="unnumbered">./edge-save-oci-artefacts.sh --source-registry registry.suse.com</screen>
</listitem>
<listitem>
<para>すべてのSUSE Edge
HelmチャートOCIイメージを含む<literal>oci-artefacts.tar.gz</literal>アーカイブが作成されます。</para>
</listitem>
<listitem>
<para>このアーカイブを<emphasis role="strong">エアギャップ</emphasis>マシンにコピーします</para>
<screen language="bash" linenumbering="unnumbered">scp oci-artefacts.tar.gz &lt;user&gt;@&lt;machine_ip&gt;:/path</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-suse-edge-release-images-to-your-air-gapped-machine">
<title>エアギャップマシンへのSUSE Edgeリリースイメージの読み込み</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-images.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-images.sh</screen>
</listitem>
<listitem>
<para><literal>edge-load-images.sh</literal>を使用して、<emphasis
role="strong">コピーした</emphasis>
<literal>edge-images.tar.gz</literal>アーカイブからイメージを読み込み、そのイメージに再度タグを付けて、プライベートレジストリにプッシュします。</para>
<screen language="bash" linenumbering="unnumbered">./edge-load-images.sh --source-registry registry.suse.com --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --images edge-images.tar.gz</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-load-suse-edge-helm-chart-oci-images-to-your-air-gapped-machine">
<title>エアギャップマシンへのSUSE Edge HelmチャートOCIイメージの読み込み</title>
<para><emphasis>エアギャップマシンで次の手順を実行します。</emphasis></para>
<orderedlist numeration="arabic">
<listitem>
<para>プライベートレジストリにログインします(必要な場合)。</para>
<screen language="bash" linenumbering="unnumbered">podman login &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt;</screen>
</listitem>
<listitem>
<para><literal>edge-load-oci-artefacts.sh</literal>を実行可能にします。</para>
<screen language="bash" linenumbering="unnumbered">chmod +x edge-load-oci-artefacts.sh</screen>
</listitem>
<listitem>
<para>コピーした<literal>oci-artefacts.tar.gz</literal>アーカイブをuntarします。</para>
<screen language="bash" linenumbering="unnumbered">tar -xvf oci-artefacts.tar.gz</screen>
</listitem>
<listitem>
<para>命名テンプレート<literal>edge-release-oci-tgz-&lt;date&gt;</literal>を含むディレクトリが生成されます。</para>
</listitem>
<listitem>
<para>このディレクトリを<literal>edge-load-oci-artefacts.sh</literal>スクリプトに渡し、SUSE Edge
HelmチャートOCIイメージをプライベートレジストリに読み込みます。</para>
<note>
<para>このスクリプトは、<literal>Helm</literal>
CLIが環境にプリインストールされていることを想定しています。Helmのインストール手順については、「<link
xl:href="https://helm.sh/docs/intro/install/">Helmのインストール</link>」を参照してください。</para>
</note>
<screen language="bash" linenumbering="unnumbered">./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-&lt;date&gt; --registry &lt;REGISTRY.YOURDOMAIN.COM:PORT&gt; --source-registry registry.suse.com</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="id-create-registry-mirrors-pointing-to-your-private-registry-for-your-kubernetes-distribution">
<title>Kubernetesディストリビューションのプライベートレジストリを指すレジストリミラーの作成</title>
<para>RKE2の場合は、「<link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration">Containerd
Registry Configuration (Containerdレジストリの設定)</link>」を参照してください。</para>
<para>K3sの場合は、「<link
xl:href="https://docs.k3s.io/installation/registry-mirror">埋め込みレジストリミラー</link>」を参照してください。</para>
</section>
</section>
<section xml:id="id-upgrade-procedure-2">
<title>アップグレード手順</title>
<note>
<para>以下のアップグレード手順は、RancherのFleet (<xref
linkend="components-fleet"/>)機能を利用します。サードパーティのGitOpsワークフローを使用するユーザは、<xref
linkend="release-notes"/>から各Edgeリリースでサポートされるチャートバージョンを取得し、そのバージョンをサードパーティのGitOpsワークフローに入力する必要があります。</para>
</note>
<para>このセクションでは、Helmアップグレード手順の次のユースケースを中心に説明します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>新しいクラスタがあり、SUSE Helmチャートをデプロイして管理したい(<xref
linkend="day2-helm-upgrade-new-cluster"/>)</para>
</listitem>
<listitem>
<para>Fleetで管理されているHelmチャートをアップグレードしたい(<xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>)</para>
</listitem>
<listitem>
<para>EIBで作成されたHelmチャートをアップグレードしたい(<xref linkend="day2-helm-upgrade-eib-chart"/>)</para>
</listitem>
</orderedlist>
<important>
<para>手動でデプロイしたHelmチャートを確実にアップグレードすることはできません。<xref
linkend="day2-helm-upgrade-new-cluster"/>で説明する方法を使用してHelmチャートを再デプロイすることをお勧めします。</para>
</important>
<section xml:id="day2-helm-upgrade-new-cluster">
<title>新しいクラスタがあり、SUSE Helmチャートをデプロイして管理したい場合</title>
<para>Fleetを使用してHelmチャートのライフサイクルを管理したいユーザが対象です。</para>
<section xml:id="id-prepare-your-fleet-resources">
<title>Fleetリソースの準備</title>
<orderedlist numeration="arabic">
<listitem>
<para>チャートのFleetリソースを、使用するEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグから取得します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>選択したEdgeリリースタグのリビジョンから、HelmチャートのFleet
(<literal>fleets/day2/chart-templates/&lt;chart&gt;</literal>)に移動します。</para>
</listitem>
<listitem>
<para>チャートのFleetディレクトリを、GitOpsワークフローで使用するGitリポジトリにコピーします。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis> Helmチャートで<emphasis
role="strong">値</emphasis>を設定する必要がある場合は、コピーしたディレクトリの<literal>fleet.yaml</literal>ファイルに含まれる設定<literal>.helm.values</literal>を編集します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">(任意)</emphasis>
環境に合わせて、チャートのFleetにリソースを追加しなければならないユースケースがあります。Fleetディレクトリを拡張する方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-content">Git Repository Contents
(Gitリポジトリのコンテンツ)</link>」を参照してください。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para><literal>longhorn</literal> Helmチャートの<emphasis
role="strong">例</emphasis>は次のようになります。</para>
<itemizedlist>
<listitem>
<para>ユーザのGitリポジトリの構造:</para>
<screen language="bash" linenumbering="unnumbered">&lt;user_repository_root&gt;
└── longhorn
    └── fleet.yaml</screen>
</listitem>
<listitem>
<para>ユーザの<literal>longhorn</literal>データが入力された<literal>fleet.yaml</literal> の内容:</para>
<screen language="yaml" linenumbering="unnumbered">defaultNamespace: longhorn-system

helm:
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.longhorn.io"
  version: "1.6.1"
  takeOwnership: true
  # custom chart value overrides
  values:
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}</screen>
<note>
<para>これらは値の例であり、<literal>longhorn</literal>チャートのカスタム設定を示すために使用しているだけです。<literal>longhorn</literal>チャートのデプロイメントのガイドラインと<emphasis
role="strong">「みなさない」</emphasis>でください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-create-the-gitrepo">
<title>GitRepoの作成</title>
<para>リポジトリにチャートのFleetリソースを入力した後、<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo</link>リソースを作成する必要があります。このリソースは、チャートのFleetリソースへのアクセス方法と、これらのリソースを適用する必要があるクラスタに関する情報を保持します。</para>
<para><literal>GitRepo</literal>リソースは、Rancher
UIを使用するか、リソースを<literal>管理クラスタ</literal>に手動でデプロイすることによって作成できます。</para>
<para>GitRepoリソースを<emphasis role="strong">手動</emphasis>で作成してデプロイする方法については、「<link
xl:href="https://fleet.rancher.io/tut-deployment">Creating a Deployment
(デプロイメントの作成)</link>」を参照してください。</para>
<para><emphasis role="strong">Rancher
UI</emphasis>を使用して<literal>GitRepo</literal>リソースを作成するには、「<link
xl:href="https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui">Accessing
Fleet in the Rancher UI (Rancher UIでのFleetへのアクセス)</link>」を参照してください。</para>
<para><emphasis><emphasis role="strong">手動</emphasis>デプロイメントの<emphasis
role="strong">Longhorn</emphasis>
<literal>GitRepo</literal>リソースの例</emphasis></para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: &lt;user_repository_tag&gt;
  #
  # If using a branch
  # branch: &lt;user_repository_branch&gt;
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  repo: &lt;user_repository_url&gt;
  targets:
  # Match all clusters
  - clusterSelector: {}</screen>
</section>
<section xml:id="id-managing-the-deployed-helm-chart">
<title>デプロイされたHelmチャートの管理</title>
<para>Fleetでデプロイした後のHelmチャートのアップグレードについては、<xref
linkend="day2-helm-upgrade-fleet-managed-chart"/>を参照してください。</para>
</section>
</section>
<section xml:id="day2-helm-upgrade-fleet-managed-chart">
<title>Fleetで管理されているHelmチャートをアップグレードしたい場合</title>
<orderedlist numeration="arabic">
<listitem>
<para>チャートのアップグレード後のバージョンを確認し、Edge
3.X.Yリリースとの互換性を確保します。各Edgeリリースに対応するHelmチャートのバージョンは、<xref
linkend="release-notes"/>で確認できます。</para>
</listitem>
<listitem>
<para>Fleetで監視されているGitリポジトリで、Helmチャートの<literal>fleet.yaml</literal>ファイルを、<xref
linkend="release-notes"/>から取得した正しいチャート<emphasis
role="strong">バージョン</emphasis>と<emphasis
role="strong">リポジトリ</emphasis>で編集します。</para>
</listitem>
<listitem>
<para>リポジトリの変更をコミットしてプッシュすると、目的のHelmチャートのアップグレードがトリガされます。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart">
<title>EIBで作成されたHelmチャートをアップグレードしたい場合</title>
<note>
<para>このセクションでは、system-upgrade-controller
(SUC)を事前にデプロイ済みであることを想定しています。まだデプロイしていないか、SUCが必要な理由がわからない場合は、デフォルトのDay
2コンポーネント(<xref linkend="day2-downstream-components"/>)のリストを参照してください。</para>
</note>
<para>EIBでは、Helmチャートをデプロイするために、<link
xl:href="https://docs.rke2.io/advanced#auto-deploying-manifests">rke2</link>/<link
xl:href="https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons">k3s</link>のマニフェストの自動デプロイ機能を利用します。具体的には、<link
xl:href="https://github.com/k3s-io/helm-controller#helm-controller">Helmチャート</link>のリソース定義マニフェストを初期化ノードの<literal>/var/lib/rancher/&lt;rke2/k3s&gt;/server/manifests</literal>に作成し、<literal>rke2/k3s</literal>がこれを検出してクラスタ内に自動デプロイします。</para>
<para><literal>Day
2</literal>の観点から見ると、これは、特定のチャートの<literal>HelmChart</literal>マニフェストファイルを編集してHelmチャートをアップグレードする必要があることを意味します。このプロセスを複数のクラスタで自動化するために、このセクションでは<emphasis
role="strong">SUC Plan</emphasis>を使用します。</para>
<para>以下に関する情報が提供されています。</para>
<itemizedlist>
<listitem>
<para>Helmチャートのアップグレードワークフローの概要(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)。</para>
</listitem>
<listitem>
<para>Helmチャートのアップグレードを正常に完了するために必要なアップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)。</para>
</listitem>
<listitem>
<para>説明されていた方法を使用して<link
xl:href="https://longhorn.io">Longhorn</link>チャートをアップグレードする方法を示す例(<xref
linkend="day2-helm-upgrade-eib-chart-example"/>)。</para>
</listitem>
<listitem>
<para>異なるGitOpsツールでアップグレードプロセスを使用する方法(<xref
linkend="day2-helm-upgrade-eib-chart-third-party"/>)。</para>
</listitem>
</itemizedlist>
<section xml:id="day2-helm-upgrade-eib-chart-overview">
<title>概要</title>
<para>このセクションは、1つまたは複数のHelmチャートをアップグレードするためにユーザが行うワークフローの概要を説明することを意図しています。Helmチャートのアップグレードに必要な手順の詳細な説明については、<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>を参照してください。</para>
<figure>
<title>Helmチャートのアップグレードワークフロー</title>
<mediaobject>
<imageobject> <imagedata fileref="day2_helm_chart_upgrade_diagram.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレードの図</phrase></textobject>
</mediaobject></figure>
<orderedlist numeration="arabic">
<listitem>
<para>このワークフローでは最初に、チャートのアップグレード先にする新しいHelmチャートアーカイブをユーザが<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
</listitem>
<listitem>
<para>続いて、アーカイブを<emphasis>エンコード</emphasis>し、関連するSUC
PlanのFleetディレクトリにある<literal>eib-chart-upgrade-user-data.yaml</literal>ファイルに設定として渡す必要があります。これは、アップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)のセクションで詳しく説明しています。</para>
</listitem>
<listitem>
<para>ユーザは次の操作に進み、必要なすべてのリソース(SUC
Plan、シークレットなど)をダウンストリームクラスタに配布する<literal>GitRepo</literal>リソースを設定してデプロイします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>リソースは<literal>fleet-default</literal>ネームスペースの<literal>管理クラスタ</literal>にデプロイされます。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>Fleet (<xref
linkend="components-fleet"/>)はデプロイされたリソースを検出し、設定されているすべてのリソースを指定のダウンストリームクラスタにデプロイします。デプロイされたリソースには次のようなものがあります。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>eib-chart-upgrade</literal> SUC Plan。各ノードに<emphasis
role="strong">アップグレードPod</emphasis>を作成するためにSUCで使用されます。</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-script</literal>シークレット。<emphasis
role="strong">アップグレードPod</emphasis>が初期化ノードで<literal>HelmChart</literal>マニフェストをアップグレードするために使用する<literal>アップグレードスクリプト</literal>を配布します。</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-user-data</literal>シークレット。アップグレードする必要があるチャートマニフェストを理解するために<literal>アップグレードスクリプト</literal>で使用するチャートデータを配布します。</para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade</literal> SUC Planがデプロイされると、SUCは、<emphasis
role="strong">アップグレードPod</emphasis>をデプロイするジョブを選択して作成します。</para>
</listitem>
<listitem>
<para>デプロイされると、<emphasis
role="strong">アップグレードPod</emphasis>は、<literal>eib-chart-upgrade-script</literal>シークレットと<literal>eib-chart-upgrade-user-data</literal>シークレットをマウントし、<literal>eib-chart-upgrade-script</literal>シークレットによって配布される<literal>アップグレードスクリプト</literal>を実行します。</para>
</listitem>
<listitem>
<para><literal>アップグレードスクリプト</literal>は以下を実行します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>スクリプトが実行されているPodが<literal>初期化</literal>ノードにデプロイされているかどうかを判断します。<literal>初期化</literal>ノードは、<literal>HelmChart</literal>マニフェストをホストしているノードです。シングルノードクラスタの場合は、単一のコントロールプレーンノードです。HAクラスタの場合は、EIBでクラスタを作成したときに<literal>initializer</literal>とマークされたノードです。<literal>initializer</literal>プロパティを指定しなかった場合は、<literal>nodes</literal>リストの最初のノードが<literal>initializer</literal>とマークされます。詳細については、EIBの<link
xl:href="https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes">アップストリーム</link>ドキュメントを参照してください。</para>
<note>
<para><literal>アップグレードスクリプト</literal>が初期化ノード以外で動作している場合、スクリプトは直ちに実行を停止し、以下の手順には進みません。</para>
</note>
</listitem>
<listitem>
<para>編集するマニフェストをバックアップし、障害復旧を確実に行えるようにします。</para>
<note>
<para>デフォルトでは、マニフェストのバックアップは、<literal>/tmp/eib-helm-chart-upgrade-&lt;date&gt;</literal>ディレクトリに保存されます。カスタムの場所を使用する場合、<literal>MANIFEST_BACKUP_DIR</literal>環境変数をHelmチャートアップグレードSUC
Planに渡すことができます(Planの例)。</para>
</note>
</listitem>
<listitem>
<para><literal>HelmChart</literal>マニフェストを編集します。このバージョンの時点で、チャートアップグレードをトリガするために、次のプロパティが変更されます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para><literal>chartContent</literal>プロパティの内容は、<literal>eib-chart-upgrade-user-data</literal>シークレットで指定されているエンコードされたアーカイブに置き換えられます。</para>
</listitem>
<listitem>
<para><literal>version</literal>プロパティの値は、<literal>eib-chart-upgrade-user-data</literal>シークレットで指定されているバージョンに置き換えられます。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><literal>アップグレードスクリプト</literal>が正常に実行されると、<link
xl:href="https://docs.rke2.io/helm">RKE2</link>/<link
xl:href="https://docs.k3s.io/helm">K3s</link>のHelm統合が変更を検出し、Helmチャートのアップグレードを自動的にトリガします。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-upgrade-steps">
<title>アップグレード手順</title>
<orderedlist numeration="arabic">
<listitem>
<para>Helmチャートアップグレードのロジックのコピー元のEdge <link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリースタグ</link>を特定します。</para>
</listitem>
<listitem>
<para><literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>
Fleetを、FleetでGitOpsを行うために使用するリポジトリにコピーします。</para>
</listitem>
<listitem>
<para>アップグレード先のHelmチャートアーカイブを<link
xl:href="https://helm.sh/docs/helm/helm_pull/">プル</link>します。</para>
<screen language="bash" linenumbering="unnumbered">helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0</screen>
</listitem>
<listitem>
<para>プルしたチャートアーカイブをエンコードします。</para>
<screen language="bash" linenumbering="unnumbered"># Encode the archive and disable line wrapping
base64 -w 0 &lt;chart-archive&gt;.tgz</screen>
</listitem>
<listitem>
<para>手順2でコピーした<literal>eib-chart-upgrade</literal>
Fleetにある<literal>eib-chart-upgrade-user-data.yaml</literal>シークレットを設定します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>このシークレットは、<literal>chart_upgrade_data.txt</literal>というファイルを配布します。このファイルにはチャートアップグレードのデータが保持されており、<literal>アップグレードスクリプト</literal>ではこのデータを使用して、アップグレードする必要があるチャートを把握します。このファイルではチャートエントリにつき1行が必要で、形式は「<emphasis
role="strong">&lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;</emphasis>」です。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para><literal>name</literal> -
EIB定義ファイルの<literal>kubernetes.helm.charts.name[]</literal>プロパティに表示される、Helmチャートの名前です。</para>
</listitem>
<listitem>
<para><literal>version</literal> -
Helmチャートの新しいバージョンを保持する必要があります。アップグレード時に、この値を使用して<literal>HelmChart</literal>マニフェストの古い<literal>version</literal>を置き換えます。</para>
</listitem>
<listitem>
<para><literal>base64_encoded_archive</literal> - ここで<literal>base64 -w 0
&lt;chart-archive&gt;.tgz</literal>の出力を渡します。アップグレード時に、この値を使用して<literal>HelmChart</literal>マニフェストの古い<literal>chartContent</literal>値を置き換えます。</para>
<note>
<para><emphasis
role="strong">&lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;</emphasis>の行は、データの追加を始める前にファイルから削除する必要があります。この行は、チャートデータをどこで、どのように設定する必要があるかの例を示すためのものです。</para>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>チャートアップグレードの<literal>fleet</literal>を配布する<literal>GitRepo</literal>リソースを設定します。<literal>GitRepo</literal>の詳細については、「<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo Resource
(GitRepoリソース)</link>」を参照してください。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>Rancher UIを使用して<literal>GitRepo</literal>を設定します。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>左上隅で、<emphasis role="strong">☰ → ［Continuous Delivery
(継続的デリバリ)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Git Repos (Gitリポジトリ)］→［ Add Repository
(リポジトリの追加)］</emphasis>に移動します。</para>
</listitem>
<listitem>
<para>ここで、<emphasis role="strong">リポジトリデータ</emphasis>と<emphasis
role="strong">パス</emphasis>をチャートの<literal>アップグレードFleet</literal>に渡します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">［Next
(次へ)］</emphasis>を選択し、設定されたチャートをアップグレードする<emphasis
role="strong">ターゲット</emphasis>クラスタを指定します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">作成します</emphasis></para>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>お使いのセットアップで<literal>Rancher</literal>を使用できない場合は、<literal>管理クラスタ</literal>で<literal>GitRepo</literal>を手動で設定できます。</para>
<orderedlist numeration="lowerroman">
<listitem>
<para>次のテンプレートにデータを入力します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: CHANGE_ME
  namespace: fleet-default
spec:
  # if running from a tag
  # revision: CHANGE_ME
  # if running from a branch
  # branch: CHANGE_ME
  paths:
  # path to your chart upgrade fleet relative to your repository
  - CHANGE_ME
  # your repository URL
  repo: CHANGE_ME
  targets:
  # Select target clusters
  - clusterSelector: CHANGE_ME
  # To match all clusters:
  # - clusterSelector: {}</screen>
<para><literal>GitRepo</literal>リソースのセットアップとデプロイの方法の詳細については、「<link
xl:href="https://fleet.rancher.io/ref-gitrepo">GitRepo Resource
(GitRepoリソース)</link>」および「<link
xl:href="https://fleet.rancher.io/gitrepo-add">Create a GitRepo Resource
(GitRepoリソースの作成)</link>」を参照してください。</para>
<para>より細かいレベルで<emphasis role="strong">ターゲット</emphasis>クラスタに一致させる方法については、「<link
xl:href="https://fleet.rancher.io/gitrepo-targets">Mapping to Downstream
Clusters (ダウンストリームクラスタへのマップ)</link>」を参照してください。</para>
</listitem>
<listitem>
<para>設定した<literal>GitRepo</literal>リソースを<literal>管理クラスタ</literal>の<literal>fleet-default</literal>ネームスペースにデプロイします。</para>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>この手順を実行すると、<literal>GitRepo</literal>リソースが正常に作成されるはずです。続いてこのリソースがFleetによって検出されて、バンドルが作成されます。このバンドルには、<literal>GitRepo</literal>がFleetディレクトリの下に設定した<emphasis
role="strong">「生」</emphasis>のKubernetesリソースが保持されます。</para>
<para>次にFleetは、すべてのKubernetesリソースをバンドルから指定のダウンストリームクラスタにデプロイします。このリソースの1つは、チャートのアップグレードをトリガするSUC
Planです。デプロイされるリソースのリストとアップグレードプロセスのワークフローについては、「概要」(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)のセクションを参照してください。</para>
<para>アップグレードプロセス自体を追跡するには、「SUC Planの監視」(<xref
linkend="monitor-suc-plans"/>)のセクションを参照してください。</para>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-example">
<title>例</title>
<para>次のセクションは、<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>セクションに実際の例を提供することを目的としています。</para>
<para>EIBでデプロイされた次の2つのクラスタがあります。</para>
<itemizedlist>
<listitem>
<para><literal>longhorn-single-k3s</literal> - シングルノードK3sクラスタ</para>
</listitem>
<listitem>
<para><literal>longhorn-ha-rke2</literal> - HA RKE2クラスタ</para>
</listitem>
</itemizedlist>
<para>どちらのクラスタでも<link
xl:href="https://longhorn.io">Longhorn</link>が実行されており、次のイメージ定義<emphasis>スニペット</emphasis>を使用してEIBによってデプロイされています。</para>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  # HA RKE2 cluster specific snippet
  # nodes:
  # - hostname: cp1rke2.example.com
  #   initializer: true
  #   type: server
  # - hostname: cp2rke2.example.com
  #   type: server
  # - hostname: cp3rke2.example.com
  #   type: server
  # - hostname: agent1rke2.example.com
  #   type: agent
  # - hostname: agent2rke2.example.com
  #   type: agent
  # version depending on the distribution
  version: v1.28.9+k3s1/v1.28.9+rke2r1
  helm:
    charts:
    - name: longhorn
      repositoryName: longhorn
      targetNamespace: longhorn-system
      createNamespace: true
      version: 1.5.5
    repositories:
    - name: longhorn
      url: https://charts.longhorn.io
...</screen>
<figure>
<title>longhorn-single-k3sにインストールされているLonghornバージョン</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_k3s_old.png" width=""/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例 k3s 古い</phrase></textobject>
</mediaobject></figure>
<figure>
<title>longhorn-ha-rke2にインストールされているLonghornバージョン</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_rke2_old.png" width=""/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例rke2古い</phrase></textobject>
</mediaobject></figure>
<para>ここでの問題は、現在のところ<literal>longhorn-single-k3s</literal>と<literal>longhorn-ha-rke2</literal>がどのEdgeリリースとも互換性のないLonghornバージョンで実行されていることです。</para>
<para>両方のクラスタのチャートをEdgeでサポートされるLonghornバージョンにアップグレードする必要があります。</para>
<para>これを実行するには、次の手順を実行します。</para>
<orderedlist numeration="arabic">
<listitem>
<para>アップグレードロジックを取得するEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリースタグ</link>を特定します。たとえば、この例では、サポートされているLonghornバージョンが<literal>1.6.1</literal>である<literal>release-3.0.1</literal>リリースタグを使用します。</para>
</listitem>
<listitem>
<para><literal>release-3.0.1</literal>リリースタグのクローンを作成し、<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>ディレクトリを自身のリポジトリにコピーします。</para>
<para>単純化するために、このセクションでは<literal>suse-edge/fleet-examples</literal>リポジトリのブランチから作業します。したがって、ディレクトリ構造は同じですが、<literal>eib-chart-upgrade</literal>
Fleetはリポジトリ内の任意の場所に配置できます。</para>
<formalpara>
<title>ディレクトリ構造の例</title>
<para>
<screen language="bash" linenumbering="unnumbered">.
...
|-- fleets
|   `-- day2
|       `-- system-upgrade-controller-plans
|           `-- eib-chart-upgrade
|               |-- eib-chart-upgrade-script.yaml
|               |-- eib-chart-upgrade-user-data.yaml
|               |-- fleet.yaml
|               `-- plan.yaml
...</screen>
</para>
</formalpara>
</listitem>
<listitem>
<para>Longhornチャートリポジトリを追加します。</para>
<screen language="bash" linenumbering="unnumbered">helm repo add longhorn https://charts.longhorn.io</screen>
</listitem>
<listitem>
<para>Longhornチャートバージョン<literal>1.6.1</literal>をプルします。</para>
<screen language="bash" linenumbering="unnumbered">helm pull longhorn/longhorn --version 1.6.1</screen>
<para>Longhornが<literal>longhorn-1.6.1.tgz</literal>という名前のアーカイブとしてプルされます。</para>
</listitem>
<listitem>
<para>Longhornアーカイブをエンコードします。</para>
<screen language="bash" linenumbering="unnumbered">base64 -w 0 longhorn-1.6.1.tgz</screen>
<para>アーカイブがbase64でエンコードされた長い1行の文字列で出力されます。</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-user-data.yaml</literal>ファイルを設定するために必要なデータがすべて揃いました。ファイル設定は次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # &lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;
  chart_upgrade_data.txt: |
    longhorn|1.6.1|H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV...</screen>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>longhorn</literal>は、EIB定義ファイルにあるチャートの名前です。</para>
</listitem>
<listitem>
<para><literal>1.6.1</literal>は、Longhorn
<literal>HelmChart</literal>マニフェストの<literal>version</literal>プロパティのアップグレード先のバージョンです。</para>
</listitem>
<listitem>
<para><literal>H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV…​</literal>は、エンコードされたLonghorn
<literal>1.6.1</literal>アーカイブのスニペットです。<emphasis
role="strong">スニペットがここに追加されているのは読みやすくするためです。ここには必ず、base64エンコードアーカイブの文字列全体を追加してください。</emphasis></para>
<note>
<para>この例は、1つのチャートのアップグレードの設定を示していますが、複数のクラスタで複数のチャートをアップグレードする必要がある場合、以下に示すように追加のチャートデータを付加できます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # &lt;name&gt;|&lt;version&gt;|&lt;base64_encoded_archive&gt;
  chart_upgrade_data.txt: |
    chartA|0.0.0|&lt;chartA_base64_archive&gt;
    chartB|0.0.0|&lt;chartB_base64_archive&gt;
    chartC|0.0.0|&lt;chartC_base64_archive&gt;
    ...</screen>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>また、マニフェストのバックアップを<literal>/tmp</literal>に保持しないことにしたため、次の設定が<literal>plan.yaml</literal>ファイルに追加されました。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: eib-chart-upgrade
spec:
  ...
  upgrade:
    ...
    # For when you want to backup your chart
    # manifest data under a specific directory
    #
    envs:
    - name: MANIFEST_BACKUP_DIR
      value: "/root"</screen>
<para>これにより、マニフェストのバックアップは、<literal>/tmp</literal>ではなく<literal>/root</literal>ディレクトリに保存されます。</para>
</listitem>
<listitem>
<para>これで必要な設定はすべて完了したので、後は<literal>GitRepo</literal>リソースを作成するだけです。この例では、<literal>Rancher
UI</literal>を使用して<literal>GitRepo</literal>リソースを作成します。</para>
</listitem>
<listitem>
<para>アップグレード手順(<xref
linkend="day2-helm-upgrade-eib-chart-upgrade-steps"/>)で説明されている手順に従い、次の項目を実行しました。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>GitRepo</literal>に「longhorn-upgrade」という名前を付けました。</para>
</listitem>
<listitem>
<para>使用するリポジトリにURLを渡しました - <link
xl:href="https://github.com/suse-edge/fleet-examples.git">https://github.com/suse-edge/fleet-examples.git</link>。</para>
</listitem>
<listitem>
<para>リポジトリのブランチを渡しました - 「doc-example」。</para>
</listitem>
<listitem>
<para>リポジトリのとおりに<literal>eib-chart-upgrade</literal>フリートへのパスを渡しました -
<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>。</para>
</listitem>
<listitem>
<para>ターゲットクラスタを選択し、リソースを作成しました。</para>
<figure>
<title>正常にデプロイされたSUCとlonghorn GitRepo</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_gitrepo.png" width=""/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例gitrepo</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>続いてクラスタでアップグレード手順を監視する必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>「SUC Planの監視」(<xref linkend="monitor-suc-plans"/>)のセクションの指示に従い、<emphasis
role="strong">アップグレードPod</emphasis>のステータスを確認します。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para><literal>初期化</literal>ノードで動作している、正常に完了した<emphasis
role="strong">アップグレードPod</emphasis>には、次のようなログが保持されているはずです。</para>
<figure>
<title>初期化ノードで実行されているアップグレードPod</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_initialiser_logs.png" width=""/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例の初期化ノードログ</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para><literal>非初期化</literal>ノードで動作している、正常に完了した<emphasis
role="strong">アップグレードPod</emphasis>には、次のようなログが保持されているはずです。</para>
<figure>
<title>非初期化ノードで実行されているアップグレードPod</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_non_initialiser_logs.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例の非初期化ノードログ</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para><emphasis
role="strong">アップグレードPod</emphasis>が正常に完了した後は、Helmコントローラによって作成されるPodについても待機して監視する必要があります。これらのPodは、<emphasis
role="strong">アップグレードPod</emphasis>が<literal>HelmChart</literal>マニフェストファイルに加えたファイルの変更に基づいて、実際のアップグレードを行います。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>クラスタ内で<emphasis role="strong">［Workloads (ワークロード)］ →
［Pods］</emphasis>にアクセスし、文字列<literal>longhorn</literal>を含むPodを<literal>default</literal>ネームスペース内で検索します。これにより、命名テンプレート<literal>helm-install-longhorn-*</literal>を使用してPodが生成されます。このPodのログを確認します。</para>
<figure>
<title>正常に完了したhelm-install Pod</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_helm_install.png" width=""/>
</imageobject>
<textobject><phrase>day2 helmチャートアップグレード例のhelmインストール</phrase></textobject>
</mediaobject></figure>
</listitem>
<listitem>
<para>ログは次のようになります。</para>
<figure>
<title>正常に完了したhelm-install Podのログ</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_successfully_upgraded_pod.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例の正常にアップグレードされたPod</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<para>すべてが正常に完了したことを確認したので、次にバージョン変更を検証する必要があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para>クラスタ上で、<emphasis role="strong">［More Resources (追加のリソース)］ → ［Helm］ →
［HelmCharts］</emphasis>に移動し、<literal>longhorn</literal>
HelmChartリソースを<literal>default</literal>ネームスペース内で検索する必要があります。</para>
<figure>
<title>longhorn-single-k3のアップグレードされたLonghornバージョン</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_k3s_longhorn_upgrade.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例のk3s longhornアップグレード</phrase></textobject>
</mediaobject></figure>
<figure>
<title>longhorn-ha-rke2のアップグレードされたLonghornバージョン</title>
<mediaobject>
<imageobject> <imagedata
fileref="day2_helm_chart_upgrade_example_rke2_longhorn_upgrade.png"
width=""/> </imageobject>
<textobject><phrase>day2 helmチャートアップグレード例のrke2 longhornアップグレード</phrase></textobject>
</mediaobject></figure>
</listitem>
</orderedlist>
<para>これにより、<literal>Longhorn</literal>
helmチャートが正常にアップグレードされたことを確認します。以上でこの例は完了です。</para>
<para>何かの理由でLonghornの以前のチャートバージョンに戻す必要がある場合、以前のLonghornマニフェストは、初期化ノードの<literal>/root/longhorn.yaml</literal>に保存されています。これは、SUC
Planで<literal>MANIFEST_BACKUP_DIR</literal>を指定したためです。</para>
</section>
<section xml:id="day2-helm-upgrade-eib-chart-third-party">
<title>サードパーティのGitOpsツールを使用したHelmチャートのアップグレード</title>
<para>ユーザがこのアップグレード手順をFleet以外のGitOpsワークフロー(<literal>Flux</literal>など)で実行したいユースケースが存在する場合があります。</para>
<para>EIBでデプロイされたHelmチャートのアップグレードに関連するリソースを取得するには、まず、使用する<link
xl:href="https://github.com/suse-edge/fleet-examples.git">suse-edge/fleet-examples</link>リポジトリのEdge<link
xl:href="https://github.com/suse-edge/fleet-examples/releases">リリース</link>タグを特定する必要があります。</para>
<para>その後、<literal>fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade</literal>でリソースを見つけることができます。</para>
<itemizedlist>
<listitem>
<para><literal>plan.yaml</literal> - アップグレード手順に関連するsystem-upgrade-controller Plan。</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-script.yaml</literal> -
<literal>HelmChart</literal>マニフェストファイルを編集してアップグレードする<literal>アップグレードスクリプト</literal>を保持しているシークレット。</para>
</listitem>
<listitem>
<para><literal>eib-chart-upgrade-user-data.yaml</literal> -
<literal>アップグレードスクリプト</literal>が利用するファイルを保持しているシークレット。関連するチャートアップグレードデータがあらかじめユーザによって入力されています。</para>
</listitem>
</itemizedlist>
<important>
<para>これらの<literal>Plan</literal>リソースは<literal>system-upgrade-controller</literal>によって解釈され、アップグレードが必要なチャートを保持する各ダウンストリームクラスタにデプロイする必要があります。<literal>system-upgrade-controller</literal>をデプロイする方法については、<xref
linkend="day2-suc-third-party-gitops"/>を参照してください。</para>
</important>
<para>GitOpsワークフローを使用してアップグレードプロセス用の<emphasis role="strong">SUC
Plan</emphasis>をデプロイする方法の理解を深めるには、
<literal>Fleet</literal>を使用したプロセスの概要(<xref
linkend="day2-helm-upgrade-eib-chart-overview"/>)を確認すると役に立ちます。</para>
</section>
</section>
</section>
</section>
</chapter>
</part>
<part xml:id="id-product-documentation">
<title>製品マニュアル</title>
<partintro>
<para>ATIPのマニュアルはここにあります</para>
</partintro>
<chapter xml:id="atip">
<title>SUSE Adaptive Telco Infrastructure Platform (ATIP)</title>
<para>SUSE Adaptive Telco Infrastructure Platform
(<literal>ATIP</literal>)は、通信事業者向けに最適化されたエッジコンピューティングプラットフォームであり、通信事業者はそのネットワークを刷新し、その最新化を加速できます。</para>
<para>ATIPは、5G Packet CoreやCloud RANなどのCNFをホストするための、通信事業者向けの充実したクラウドスタックです。</para>
<itemizedlist>
<listitem>
<para>エッジスタックの複雑な設定を通信事業者の規模で自動的にゼロタッチでロールアウトし、ライフサイクルを管理します。</para>
</listitem>
<listitem>
<para>通信事業者に固有の設定とワークロードを使用して、通信事業者グレードのハードウェアの品質を継続的に保証します。</para>
</listitem>
<listitem>
<para>エッジ専用に設計されたコンポーネントで構成されているため、フットプリントが小さく、ワットパフォーマンスが高くなっています。</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIを備え、100%オープンソースであるため、柔軟なプラットフォーム戦略を維持します。</para>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="atip-architecture">
<title>コンセプトとアーキテクチャ</title>
<para>SUSE
ATIPは、クラウドネイティブな最新の通信事業者向けアプリケーションをコアからエッジまで大規模にホストするために設計されたプラットフォームです。</para>
<para>このページでは、ATIPで使用されるアーキテクチャとコンポーネントについて説明します。これを理解しておくと、ATIPをデプロイおよび使用する際に役立ちます。</para>
<section xml:id="id-atip-architecture">
<title>ATIPのアーキテクチャ</title>
<para>次の図は、ATIPのアーキテクチャの概要を示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture1.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ1</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="id-components-4">
<title>コンポーネント</title>
<para>異なる2つのブロックがあります。管理スタックとランタイムスタックです。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理スタック</emphasis>:
ATIP内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用される部分です。次のコンポーネントが含まれます。</para>
<itemizedlist>
<listitem>
<para>Rancher (<xref
linkend="components-rancher"/>)を使用した、パブリッククラウド環境とプライベートクラウド環境のマルチクラスタ管理</para>
</listitem>
<listitem>
<para>Metal3 (<xref linkend="components-metal3"/>)、MetalLB (<xref
linkend="components-metallb"/>)、および<literal>CAPI</literal> (Cluster
API)インフラストラクチャプロバイダを使用したベアメタルサポート</para>
</listitem>
<listitem>
<para>包括的なテナント分離と<literal>IDP</literal> (IDプロバイダ)の統合</para>
</listitem>
<listitem>
<para>サードパーティ統合と拡張機能の大規模なマーケットプレイス</para>
</listitem>
<listitem>
<para>ベンダに依存しないAPIと充実したプロバイダエコシステム</para>
</listitem>
<listitem>
<para>SLE Microのトランザクション更新の制御</para>
</listitem>
<listitem>
<para>GitリポジトリとFleet (<xref
linkend="components-fleet"/>)を使用してクラスタのライフサイクルを管理するGitOpsエンジン</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ランタイムスタック</emphasis>: ATIP内でワークロードを実行するために使用される要素です。</para>
<itemizedlist>
<listitem>
<para>Kubernetesと、K3s (<xref linkend="components-k3s"/>)やRKE2 (<xref
linkend="components-rke2"/>)などの安全で軽量なディストリビューション(<literal>RKE2</literal>は、政府機関での使用や規制対象産業向けに強化、認証、最適化されています)。</para>
</listitem>
<listitem>
<para>NeuVector (<xref
linkend="components-neuvector"/>)。イメージの脆弱性スキャン、ディープパケットインスペクション、クラスタ内の自動トラフィック制御などのセキュリティ機能を実現します。</para>
</listitem>
<listitem>
<para>ブロックストレージとLonghorn (<xref
linkend="components-longhorn"/>)。クラウドネイティブのストレージソリューションをシンプルかつ簡単に使用できます。</para>
</listitem>
<listitem>
<para>SLE Micro(<xref
linkend="components-slmicro"/>)で最適化されたオペレーティングシステム。コンテナ運用のための、安全、軽量でイミュータブルな(トランザクショナルファイルシステムを備えた)
OSを実現します。SLE
Microは<literal>aarch64</literal>アーキテクチャと<literal>x86_64</literal>アーキテクチャで利用でき、通信事業者およびエッジのユースケース向けに<literal>リアルタイムカーネル</literal>もサポートしています。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-example-deployment-flows">
<title>デプロイメントフローの例</title>
<para>管理コンポーネントとランタイムコンポーネントの関係を理解できるように、以下にワークフローの概要の例を示します。</para>
<para>ダイレクトネットワークプロビジョニングは、すべてのコンポーネントを事前設定した状態で新しいダウンストリームクラスタをデプロイできるワークフローであり、手動操作なしですぐにワークロードを実行できます。</para>
<section xml:id="id-example-1-deploying-a-new-management-cluster-with-all-components-installed">
<title>例1: すべてのコンポーネントがインストールされた新しい管理クラスタをデプロイする</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、管理スタックが含まれる新しい<literal>ISO</literal>イメージを作成します。その後、この<literal>ISO</literal>イメージを使用して、新しい管理クラスタをVMまたはベアメタルにインストールできます。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture2.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ2</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>新しい管理クラスタをデプロイする方法については、ATIPの管理クラスタに関するガイド(<xref
linkend="atip-management-cluster"/>)を参照してください。</para>
</note>
<note>
<para>Edge Image Builderの使用方法については、Edge Image Builderのガイド(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-2-deploying-a-single-node-downstream-cluster-with-telco-profiles-to-enable-it-to-run-telco-workloads">
<title>例2: 通信事業者プロファイルを使用してシングルノードのダウンストリームクラスタをデプロイして通信ワークロードを実行可能にする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、すべての通信機能が有効化および設定された状態でシングルノードのダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture3.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ3</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタをデプロイする方法については、ATIPの自動プロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para>通信機能の詳細については、ATIPの通信機能に関するガイド(<xref linkend="atip-features"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-example-3-deploying-a-high-availability-downstream-cluster-using-metallb-as-a-load-balancer">
<title>例3: MetalLBをロードバランサとして使用して高可用性ダウンストリームクラスタをデプロイする</title>
<para>管理クラスタが稼働したら、その管理クラスタを使用して、ダイレクトネットワークプロビジョニングワークフローにより、<literal>MetalLB</literal>をロードバランサとして使用する高可用性ダウンストリームクラスタをデプロイすることができます。</para>
<para>次の図に、これをデプロイするワークフローの概要を示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-architecture4.png" width=""/>
</imageobject>
<textobject><phrase>製品atipアーキテクチャ4</phrase></textobject>
</mediaobject>
</informalfigure>
<note>
<para>ダウンストリームクラスタをデプロイする方法については、ATIPの自動プロビジョニングに関するガイド(<xref
linkend="atip-automated-provisioning"/>)を参照してください。</para>
</note>
<note>
<para><literal>MetalLB</literal>の詳細については、<xref
linkend="components-metallb"/>を参照してください。</para>
</note>
</section>
</section>
</chapter>
<chapter xml:id="atip-requirements">
<title>要件と前提</title>
<section xml:id="id-hardware">
<title>ハードウェア</title>
<para>ATIPノードのハードウェア要件は次のコンポーネントに基づきます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理クラスタ</emphasis>: 管理クラスタには、<literal>SLE
Micro</literal>、<literal>RKE2</literal>、<literal>Rancher
Prime</literal>、<literal>Metal<superscript>3</superscript></literal>などのコンポーネントが含まれ、管理クラスタを使用して複数のダウンストリームクラスタを管理します。管理するダウンストリームクラスタの数によっては、サーバのハードウェア要件は変わる場合があります。</para>
<itemizedlist>
<listitem>
<para>サーバ(<literal>VM</literal>または<literal>ベアメタル</literal>)の最小要件は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>RAM: 8GB以上(16GB以上を推奨)</para>
</listitem>
<listitem>
<para>CPU: 2個以上(4個以上を推奨)</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">ダウンストリームクラスタ</emphasis>:
ダウンストリームクラスタは、ATIPノードにデプロイされて通信ワークロードを実行するクラスタです。<literal>SR-IOV</literal>、<literal>CPUパフォーマンス最適化</literal>などの特定の通信機能を有効にするには、固有の要件が必要になります。</para>
<itemizedlist>
<listitem>
<para>SR-IOV: VF
(仮想関数)をCNF/VNFにパススルーモードでアタッチするには、NICがSR-IOVをサポートしていて、BIOSでVT-d/AMD-Viが有効化されている必要があります。</para>
</listitem>
<listitem>
<para>CPUプロセッサ: 特定の通信ワークロードを実行するには、こちらの参照表(<xref
linkend="atip-features"/>)に記載されているほとんどの機能を利用できるようにCPUプロセッサモデルを適応させる必要があります。</para>
</listitem>
<listitem>
<para>仮想メディアでインストールするためのファームウェア要件:</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>サーバハードウェア</para></entry>
<entry align="left" valign="top"><para>BMCモデル</para></entry>
<entry align="left" valign="top"><para>管理</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Dell製ハードウェア</para></entry>
<entry align="left" valign="top"><para>第15世代</para></entry>
<entry align="left" valign="top"><para>iDRAC9</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Supermicro製ハードウェア</para></entry>
<entry align="left" valign="top"><para>01.00.25</para></entry>
<entry align="left" valign="top"><para>Supermicro SMC - redfish</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>HPE製ハードウェア</para></entry>
<entry align="left" valign="top"><para>1.50</para></entry>
<entry align="left" valign="top"><para>iLO6</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="id-network">
<title>ネットワーク</title>
<para>ネットワークアーキテクチャの参考として、次の図に、通信事業者環境の一般的なネットワークアーキテクチャを示します。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-requirement1.png" width=""/>
</imageobject>
<textobject><phrase>製品atip要件1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>このネットワークアーキテクチャは次のコンポーネントに基づきます。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">管理ネットワーク</emphasis>:
このネットワークは、ATIPノードの管理や帯域外管理に使用されます。通常は独立した管理スイッチに接続しますが、同じサービススイッチに接続し、VLANを使ってトラフィックを分離することもできます。</para>
</listitem>
<listitem>
<para><emphasis role="strong">コントロールプレーンネットワーク</emphasis>:
このネットワークは、ATIPノードと、そこで実行されているサービスとの間の通信に使用されます。また、ATIPノードと外部サービス(<literal>DHCP</literal>サーバや<literal>DNS</literal>サーバなど)との間の通信にも使用されます。接続環境では、スイッチやルータでインターネット経由のトラフィックを処理できる場合もあります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">その他のネットワーク</emphasis>:
場合によっては、お客様の特定の目的に合わせてATIPノードを他のネットワークに接続できます。</para>
</listitem>
</itemizedlist>
<note>
<para>ダイレクトネットワークプロビジョニングワークフローを使用するには、管理クラスタがダウンストリームクラスタサーバのBaseboard Management
Controller (BMC)とネットワークで接続されていて、ホストの準備とプロビジョニングを自動化できる必要があります。</para>
</note>
</section>
<section xml:id="id-services-dhcp-dns-etc">
<title>サービス(DHCP、DNSなど)</title>
<para>デプロイ先の環境の種類によっては、<literal>DHCP</literal>、<literal>DNS</literal>などの外部サービスが必要な場合があります。</para>
<itemizedlist>
<listitem>
<para><emphasis role="strong">接続環境</emphasis>:
この場合、ATIPノードはインターネットに接続され(L3ルーティングプロトコルを使用)、外部サービスはお客様が提供します。</para>
</listitem>
<listitem>
<para><emphasis role="strong">非接続/エアギャップ環境</emphasis>:
この場合、ATIPノードはインターネットにIPで接続されないため、サービスを追加して、ATIPのダイレクトネットワークプロビジョニングワークフローに必要なコンテンツをローカルにミラーリングする必要があります。</para>
</listitem>
<listitem>
<para><emphasis role="strong">ファイルサーバ</emphasis>:
ファイルサーバは、ダイレクトネットワークプロビジョニングワークフローの中で、ATIPノードにプロビジョニングするISOイメージを保存するために使用されます。<literal>metal<superscript>3</superscript></literal>
HelmチャートでメディアサーバをデプロイしてISOイメージを保存できます。次のセクション(<xref
linkend="metal3-media-server"/>)を確認してください。ただし、既存のローカルWebサーバを使用することもできます。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-disabling-rebootmgr">
<title>rebootmgrの無効化</title>
<para><literal>rebootmgr</literal>は、システムに保留中の更新がある場合の再起動方針を設定できるサービスです。通信ワークロードでは、システムによってスケジュールされた更新がある場合、<literal>rebootmgr</literal>サービスを無効にするか正しく設定してノードの再起動を回避し、ノードで実行中のサービスへの影響を避けることが非常に重要です。</para>
<note>
<para><literal>rebootmgr</literal>の詳細については、<link
xl:href="https://github.com/SUSE/rebootmgr">rebootmgrのGitHubリポジトリ</link>を参照してください。</para>
</note>
<para>次のコマンドを実行して、使用する方針を検証します。</para>
<screen language="shell" linenumbering="unnumbered">cat /etc/rebootmgr.conf
[rebootmgr]
window-start=03:30
window-duration=1h30m
strategy=best-effort
lock-group=default</screen>
<para>また、次のコマンドを実行すると無効にすることができます。</para>
<screen language="shell" linenumbering="unnumbered">sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf</screen>
<para>または、<literal>rebootmgrctl</literal>コマンドを次のように使用できます。</para>
<screen language="shell" linenumbering="unnumbered">rebootmgrctl strategy off</screen>
<note>
<para><literal>rebootmgr</literal>の方針を設定するこの設定は、ダイレクトネットワークプロビジョニングワークフローを使用して自動化できます。詳細については、ATIPの自動化されたプロビジョニングに関するドキュメント(<xref
linkend="atip-automated-provisioning"/>)を確認してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="atip-management-cluster">
<title>管理クラスタの設定</title>
<section xml:id="id-introduction-2">
<title>はじめに</title>
<para>管理クラスタは、ATIP内でランタイムスタックのプロビジョニングとライフサイクルを管理するために使用されます。技術的観点からは、管理クラスタには次のコンポーネントが含まれています。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの一部の設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。ユースケースに応じて、<literal>Multus</literal>、<literal>Cilium</literal>などの特定のCNIプラグインを使用するように設定できます。</para>
</listitem>
<listitem>
<para><literal>Rancher</literal> (管理プラットフォーム)。クラスタのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>Metal<superscript>3</superscript></literal>
(コンポーネント)。ベアメタルノードのライフサイクルを管理します。</para>
</listitem>
<listitem>
<para><literal>CAPI</literal> (コンポーネント)。Kubernetesクラスタ
(ダウンストリームクラスタ)のライフサイクルを管理します。ATIPでは、RKE2クラスタ(ダウンストリームクラスタ)のライフサイクルを管理するために<literal>RKE2
CAPI Provider</literal>も使用します。</para>
</listitem>
</itemizedlist>
<para>上記のコンポーネントをすべて使用すると、管理クラスタは、宣言型アプローチを使用してインフラストラクチャやアプリケーションを管理し、ダウンストリームクラスタのライフサイクルを管理できます。</para>
<note>
<para><literal>SUSE Linux Enterprise Micro</literal>の詳細については、「SLE Micro」(<xref
linkend="components-slmicro"/>)を参照してください。</para>
<para><literal>RKE2</literal>の詳細については、「RKE2」(<xref
linkend="components-rke2"/>)を参照してください。</para>
<para><literal>Rancher</literal>の詳細については、「Rancher」(<xref
linkend="components-rancher"/>)を参照してください。</para>
<para><literal>Metal<superscript>3</superscript></literal>の詳細については、「Metal3」(<xref
linkend="components-metal3"/>)を参照してください。</para>
</note>
</section>
<section xml:id="id-steps-to-set-up-the-management-cluster">
<title>管理クラスタの設定手順</title>
<para>管理クラスタを設定するには、次の手順が必要です(シングルノードを使用)。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="product-atip-mgmtcluster1.png" width=""/>
</imageobject>
<textobject><phrase>製品atip管理クラスタ1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>宣言型アプローチを使用して管理クラスタを設定するには、主に3つの手順があります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><emphasis role="strong">接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>)</emphasis>:
最初の手順では、接続環境で使用する必要がある設定をすべて含むマニフェストとファイルを準備します。</para>
<itemizedlist>
<listitem>
<para>接続環境のディレクトリ構造(<xref linkend="mgmt-cluster-directory-structure"/>):
この手順では、Edge Image Builderで使用するディレクトリ構造を作成し、設定ファイルとイメージそのものを保存します。</para>
</listitem>
<listitem>
<para>管理クラスタ定義ファイル(<xref linkend="mgmt-cluster-image-definition-file"/>):
<literal>mgmt-cluster.yaml</literal>ファイルが管理クラスタのメイン定義ファイルです。このファイルには、作成するイメージに関する次の情報が含まれています。</para>
<itemizedlist>
<listitem>
<para>イメージ情報: ゴールデンイメージを使用して作成するイメージに関する情報。</para>
</listitem>
<listitem>
<para>オペレーティングシステム: イメージで使用するオペレーティングシステムの設定。</para>
</listitem>
<listitem>
<para>Kubernetes: Helmチャートとリポジトリ、Kubernetesのバージョン、ネットワーク設定、およびクラスタで使用するノード。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Customフォルダ(<xref linkend="mgmt-cluster-custom-folder"/>):
<literal>custom</literal>フォルダには設定ファイルとスクリプトが含まれ、Edge Image
Builderはこれらを使用して完全に機能する管理クラスタをデプロイします。</para>
<itemizedlist>
<listitem>
<para>ファイル: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
<listitem>
<para>スクリプト: 管理クラスタが使用するスクリプトが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Kubernetesフォルダ(<xref linkend="mgmt-cluster-kubernetes-folder"/>):
<literal>kubernetes</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれています。</para>
<itemizedlist>
<listitem>
<para>Manifests: 管理クラスタが使用するマニフェストが含まれています。</para>
</listitem>
<listitem>
<para>Helm: 管理クラスタが使用するHelmチャートが含まれています。</para>
</listitem>
<listitem>
<para>Config: 管理クラスタが使用する設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>Networkフォルダ(<xref linkend="mgmt-cluster-network-folder"/>):
<literal>network</literal>フォルダには、管理クラスタノードが使用するネットワーク設定ファイルが含まれています。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">エアギャップ環境でのイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-airgap"/>)</emphasis>:
この手順では、エアギャップシナリオで使用するマニフェストとファイルを準備する際の相違点を示します。</para>
<itemizedlist>
<listitem>
<para>エアギャップ環境のディレクトリ構造(<xref
linkend="mgmt-cluster-directory-structure-airgap"/>):
ディレクトリ構造を変更して、管理クラスタをエアギャップ環境で実行するために必要なリソースを含める必要があります。</para>
</listitem>
<listitem>
<para>定義ファイルの変更(<xref linkend="mgmt-cluster-image-definition-file-airgap"/>):
<literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIBの出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para>customeフォルダの変更(<xref linkend="mgmt-cluster-custom-folder-airgap"/>):
<literal>custom</literal>フォルダを変更し、管理クラスタをエアギャップ環境で実行するために必要なリソースを含める必要があります。</para>
<itemizedlist>
<listitem>
<para>登録スクリプト:
エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。</para>
</listitem>
<listitem>
<para>エアギャップリソース:
<literal>custom/files/airgap-resources.tar.gz</literal>ファイルを、管理クラスタをエアギャップ環境で運用するために必要なすべてのリソースとともに<literal>custom/files</literal>フォルダに含める必要があります。</para>
</listitem>
<listitem>
<para>スクリプト:
<literal>custom/scripts/99-mgmt-setup.sh</literal>スクリプトを変更し、<literal>airgap-resources.tar.gz</literal>ファイルを抽出して最終的な場所にコピーするようにする必要があります。また、<literal>custom/files/metal3.sh</literal>スクリプトを変更し、インターネットからリソースをダウンロードするのではなく、<literal>airgap-resources.tar.gz</literal>ファイルに含まれるローカルリソースを使用するようにする必要があります。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><emphasis role="strong">イメージの作成(<xref
linkend="mgmt-cluster-image-creation"/>)</emphasis>: この手順では、Edge Image
Builderツールを使用してイメージを作成します(接続環境とエアギャップ環境の両方が対象です)。ご使用のシステムでEdge Image
Builderツールを実行するための前提条件(<xref linkend="components-eib"/>)を確認してください。</para>
</listitem>
<listitem>
<para><emphasis role="strong">管理クラスタのプロビジョニング(<xref
linkend="mgmt-cluster-provision"/>)</emphasis>:
この手順では、前の手順で作成したイメージを使用して管理クラスタをプロビジョニングする方法について説明します(接続シナリオとエアギャップシナリオの両方が対象です)。この手順は、ラップトップ、サーバ、VM、またはUSBポートを搭載した他の任意のx86_64システムを使用して実行できます。</para>
</listitem>
</orderedlist>
<note>
<para>Edge Image Builderの詳細については、「Edge Image Builder」(<xref
linkend="components-eib"/>)およびEdge Image Builderのクイックスタート(<xref
linkend="quickstart-eib"/>)を参照してください。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-preparation-connected">
<title>接続環境用のイメージの準備</title>
<para>Edge Image
Builderを使用して管理クラスタのイメージを作成すると、多くの設定をカスタマイズできますが、このドキュメントでは、管理クラスタの設定に必要な最小設定について説明します。Edge
Image Builderは通常、コンテナ内から実行されるため、まだコンテナを実行する手段がない場合は、まずコンテナランタイム(<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher
Desktop</link>)などのコンテナランタイムをインストールする必要があります。このガイドでは、コンテナランタイムをすでに使用できる状況であることを想定しています。</para>
<para>また、高可用性管理クラスタをデプロイするための前提条件として、ネットワークで次の3つのIPを予約する必要があります。-
<literal>apiVIP</literal> (API VIPアドレス用(Kubernetes APIサーバへのアクセスに使用))、-
<literal>ingressVIP</literal> (Ingress VIPアドレス(Rancher UIなどで使用))、-
<literal>metal3VIP</literal> (Metal3 VIPアドレス用)。</para>
<section xml:id="mgmt-cluster-directory-structure">
<title>ディレクトリ構造</title>
<para>EIBを実行する場合、ディレクトリはホストからマウントされます。したがって、最初に実行する手順は、EIBが設定ファイルとイメージ自体を保存するために使用するディレクトリ構造を作成することです。このディレクトリの構造は次のとおりです。</para>
<screen language="console" linenumbering="unnumbered">eib
├── mgmt-cluster.yaml
├── network
│ └── mgmt-cluster-node1.yaml
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
└── base-images</screen>
<note>
<para>イメージ<literal>SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso</literal>は、<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードして、<literal>base-images</literal>フォルダに配置する必要があります。</para>
<para>イメージのSHA256チェックサムを確認し、イメージが改ざんされていないことを確認する必要があります。このチェックサムは、イメージをダウンロードした場所と同じ場所にあります。</para>
<para>ディレクトリ構造の例は、 <link xl:href="https://github.com/suse-edge/atip">SUSE Edge
GitHubリポジトリの「telco-examples」フォルダ</link>にあります。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file">
<title>管理クラスタ定義ファイル</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルは管理クラスタのメイン定義ファイルで、次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster-node1
        initializer: true
        type: server
#     - hostname: mgmt-cluster-node2
#       initializer: true
#       type: server
#     - hostname: mgmt-cluster-node3
#       initializer: true
#       type: server</screen>
<para><literal>mgmt-cluster.yaml</literal>定義ファイルのフィールドと値について説明するために、ここではこのファイルを次のセクションに分割しています。</para>
<itemizedlist>
<listitem>
<para>イメージセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso</screen>
<para>ここで、<literal>baseImage</literal>は、SUSE Customer
CenterまたはSUSEダウンロードページからダウンロードした元のイメージです。<literal>outputImageName</literal>は、管理クラスタのプロビジョニングに使用する新しいイメージの名前です。</para>
<itemizedlist>
<listitem>
<para>オペレーティングシステムセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>ここで、<literal>installDevice</literal>はオペレーティングシステムのインストールに使用するデバイス、<literal>username</literal>および<literal>encryptedPassword</literal>はシステムへのアクセスに使用する資格情報、<literal>packageList</literal>はインストールするパッケージのリスト(<literal>jq</literal>はインストールプロセス中に内部的に必要)です。<literal>sccRegistrationCode</literal>は構築時にパッケージと依存関係を取得するために使用する登録コードで、SUSE
Customer
Centerから取得できます。暗号化パスワードは次のように<literal>openssl</literal>コマンドを使用して生成できます。</para>
<screen language="shell" linenumbering="unnumbered">openssl passwd -6 MyPassword!123</screen>
<para>この出力は次のようになります。</para>
<screen language="console" linenumbering="unnumbered">$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31</screen>
<itemizedlist>
<listitem>
<para>Kubernetesセクション(定義ファイル):</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster1
        initializer: true
        type: server
#      - hostname: mgmt-cluster2
#        type: server
#      - hostname: mgmt-cluster3
#        type: server</screen>
<para>ここで、<literal>version</literal>はインストールするKubernetesのバージョンです。ここでは、RKE2クラスタを使用しているため、バージョンは1.29未満のマイナーバージョンにして<literal>Rancher</literal>との互換性を保つ必要があります(<literal>v1.28.9+rke2r1</literal>など)。</para>
<para><literal>helm</literal>セクションには、インストールするHelmチャートのリスト、使用するリポジトリ、およびこれらすべてのバージョン設定が含まれます。</para>
<para><literal>network</literal>セクションには、<literal>RKE2</literal>コンポーネントが使用する<literal>apiHost</literal>や<literal>apiVIP</literal>などのネットワーク設定が含まれます。<literal>apiVIP</literal>は、ネットワーク内で使用されていないIPアドレスにし、DHCPを使用する場合はDHCPプールから除外してください。マルチノードクラスタでは、<literal>apiVIP</literal>がKubernetes
APIサーバへのアクセスに使用されます。<literal>apiHost</literal>は、<literal>RKE2</literal>コンポーネントが使用する<literal>apiVIP</literal>への名前解決として機能します。</para>
<para><literal>nodes</literal>セクションには、クラスタで使用するノードのリストが含まれています。<literal>nodes</literal>セクションには、クラスタで使用するノードのリストが含まれています。この例では、シングルノードクラスタを使用していますが、リストにノードを追加する(行のコメントを解除する)ことによってマルチノードクラスタに拡張できます。</para>
<note>
<para>ノードの名前はクラスタ内で一意にし、リストの最初のノードの<literal>initializer</literal>フィールドは<literal>true</literal>に設定する必要があります。ノードの名前は、<literal>network</literal>セクションで定義したホスト名と同じにする必要があります。これは<literal>network</literal>セクションのファイル名と直接照合されます。</para>
</note>
</section>
<section xml:id="mgmt-cluster-custom-folder">
<title>Customフォルダ</title>
<para><literal>custom</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── custom
│ ├── scripts
│ │ ├── 99-register.sh
│ │ ├── 99-mgmt-setup.sh
│ │ └── 99-alias.sh
│ └── files
│     ├── rancher.sh
│     ├── mgmt-stack-setup.service
│     ├── metal3.sh
│     └── basic-setup.sh
...</screen>
<itemizedlist>
<listitem>
<para><literal>custom/files</literal>フォルダには、管理クラスタが使用する設定ファイルが含まれます。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>フォルダには、管理クラスタが使用するスクリプトが含まれます。</para>
</listitem>
</itemizedlist>
<para><literal>custom/files</literal>フォルダには、次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>basic-setup.sh</literal>:
使用する<literal>Metal<superscript>3</superscript></literal>バージョンに関する設定パラメータ、および<literal>Rancher</literal>と<literal>MetalLB</literal>の基本パラメータが含まれます。このファイルを変更するのは、使用するコンポーネントのバージョンまたはネームスペースを変更する場合のみにしてください。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
# Pre-requisites. Cluster already running
export KUBECTL="/var/lib/rancher/rke2/bin/kubectl"
export KUBECONFIG="/etc/rancher/rke2/rke2.yaml"

##################
# METAL3 DETAILS #
##################
export METAL3_CHART_TARGETNAMESPACE="metal3-system"
export METAL3_CLUSTERCTLVERSION="1.6.2"
export METAL3_CAPICOREVERSION="1.6.2"
export METAL3_CAPIMETAL3VERSION="1.6.0"
export METAL3_CAPIRKE2VERSION="0.2.6"
export METAL3_CAPIPROVIDER="rke2"
export METAL3_CAPISYSTEMNAMESPACE="capi-system"
export METAL3_RKE2BOOTSTRAPNAMESPACE="rke2-bootstrap-system"
export METAL3_CAPM3NAMESPACE="capm3-system"
export METAL3_RKE2CONTROLPLANENAMESPACE="rke2-control-plane-system"
export METAL3_CAPI_IMAGES="registry.suse.com/edge"
# Or registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse for the upstream ones

###########
# METALLB #
###########
export METALLBNAMESPACE="metallb-system"

###########
# RANCHER #
###########
export RANCHER_CHART_TARGETNAMESPACE="cattle-system"
export RANCHER_FINALPASSWORD="adminadminadmin"

die(){
  echo ${1} 1&gt;&amp;2
  exit ${2}
}</screen>
</listitem>
<listitem>
<para><literal>metal3.sh</literal>:
使用する<literal>Metal<superscript>3</superscript></literal>コンポーネントの設定が含まれます(変更不要)。今後のバージョンでは、このスクリプトは代わりに<literal>Rancher
Turtles</literal>を使用するように置き換えられて、使いやすくなる予定です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# Get the ironic IP
IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic-bmo -o jsonpath='{.data.IRONIC_IP}')

# If LoadBalancer, use metallb, else it is NodePort
if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} metal3-metal3-ironic -o jsonpath='{.spec.type}') == "LoadBalancer" ]; then
  # Wait for metallb
  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} $(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; done

  # Do not create the ippool if already created
  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  metadata:
    name: ironic-ip-pool
    namespace: ${METALLBNAMESPACE}
  spec:
    addresses:
    - ${IRONICIP}/32
    serviceAllocation:
      priority: 100
      serviceSelectors:
      - matchExpressions:
        - {key: app.kubernetes.io/name, operator: In, values: [metal3-ironic]}
	EOF

  # Same for L2 Advs
  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} ironic-ip-pool-l2-adv -o name || cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
  apiVersion: metallb.io/v1beta1
  kind: L2Advertisement
  metadata:
    name: ironic-ip-pool-l2-adv
    namespace: ${METALLBNAMESPACE}
  spec:
    ipAddressPools:
    - ironic-ip-pool
	EOF
fi

# If clusterctl is not installed, install it
if ! command -v clusterctl &gt; /dev/null 2&gt;&amp;1; then
  LINUXARCH=$(uname -m)
  case $(uname -m) in
    "x86_64")
      export GOARCH="amd64" ;;
    "aarch64")
      export GOARCH="arm64" ;;
    "*")
      echo "Arch not found, asumming amd64"
      export GOARCH="amd64" ;;
  esac

  # Clusterctl bin
  # Maybe just use the binary from hauler if available
  curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v${METAL3_CLUSTERCTLVERSION}/clusterctl-linux-${GOARCH} -o /usr/local/bin/clusterctl
  chmod +x /usr/local/bin/clusterctl
fi

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Deploy CAPI
if [ $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -o name | wc -l) -lt 1 ]; then

  # https://github.com/rancher-sandbox/cluster-api-provider-rke2#setting-up-clusterctl
  mkdir -p ~/.cluster-api
  cat &lt;&lt;-EOF &gt; ~/.cluster-api/clusterctl.yaml
	images:
	  all:
	    repository: ${METAL3_CAPI_IMAGES}
	EOF

  # Try this command 3 times just in case, stolen from https://stackoverflow.com/a/33354419
  if ! (r=3; while ! clusterctl init \
    --core "cluster-api:v${METAL3_CAPICOREVERSION}"\
    --infrastructure "metal3:v${METAL3_CAPIMETAL3VERSION}"\
    --bootstrap "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --control-plane "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}" ; do
            ((--r))||exit
            echo "Something went wrong, let's wait 10 seconds and retry"
            sleep 10;done) ; then
      echo "clusterctl failed"
      exit 1
  fi

  # Wait for capi-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPISYSTEMNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -l cluster.x-k8s.io/provider=cluster-api -o name) --timeout=10s; do sleep 2 ; done

  # Wait for capm3-controller-manager, there are two pods, the ipam and the capm3 one, just wait for the first one
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPM3NAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPM3NAMESPACE} -l cluster.x-k8s.io/provider=infrastructure-metal3 -o name | head -n1 ) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-bootstrap-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} -l cluster.x-k8s.io/provider=bootstrap-rke2 -o name) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-control-plane-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2CONTROLPLANENAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2CONTROLPLANENAMESPACE} -l cluster.x-k8s.io/provider=control-plane-rke2 -o name) --timeout=10s; do sleep 2 ; done

fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
<itemizedlist>
<listitem>
<para><literal>rancher.sh</literal>:
使用する<literal>Rancher</literal>コンポーネントの設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

RANCHERLOCKNAMESPACE="default"
RANCHERLOCKCMNAME="rancher-lock"

if [ -z "${RANCHER_FINALPASSWORD}" ]; then
  # If there is no final password, then finish the setup right away
  exit 0
fi

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for rancher to be deployed
while ! ${KUBECTL} wait --for condition=ready -n ${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do sleep 2 ; done
until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher &gt; /dev/null 2&gt;&amp;1; do sleep 10; done

RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n ${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher -o jsonpath='{.spec.rules[0].host}')

# Skip the whole process if things have been set already
if [ -z $(${KUBECTL} get settings.management.cattle.io first-login -ojsonpath='{.value}') ]; then
  # Add the protocol
  RANCHERHOSTNAME="https://${RANCHERHOSTNAME}"
  TOKEN=""
  while [ -z "${TOKEN}" ]; do
    # Get token
    sleep 2
    TOKEN=$(curl -sk -X POST ${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H 'content-type: application/json' -d "{\"username\":\"admin\",\"password\":\"${RANCHERBOOTSTRAPPASSWORD}\"}" | jq -r .token)
  done

  # Set password
  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H 'content-type: application/json' -H "Authorization: Bearer $TOKEN" -d "{\"currentPassword\":\"${RANCHERBOOTSTRAPPASSWORD}\",\"newPassword\":\"${RANCHER_FINALPASSWORD}\"}"

  # Create a temporary API token (ttl=60 minutes)
  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: application/json' -H "Authorization: Bearer ${TOKEN}" -d '{"type":"token","description":"automation","ttl":3600000}' | jq -r .token)

  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: application/json' -H "Authorization: Bearer ${APITOKEN}" -X PUT -d "{\"name\":\"server-url\",\"value\":\"${RANCHERHOSTNAME}\"}"
  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H 'content-type: application/json' -H 'accept: application/json' -H "Authorization: Bearer ${APITOKEN}" -d '{"value":"out"}'
fi

# Clean up the lock cm
${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n ${RANCHERLOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>mgmt-stack-setup.service</literal>:
systemdサービスを作成して初回ブート時にスクリプトを実行するための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">[Unit]
Description=Setup Management stack components
Wants=network-online.target
# It requires rke2 or k3s running, but it will not fail if those services are not present
After=network.target network-online.target rke2-server.service k3s.service
# At least, the basic-setup.sh one needs to be present
ConditionPathExists=/opt/mgmt/bin/basic-setup.sh

[Service]
User=root
Type=forking
# Metal3 can take A LOT to download the IPA image
TimeoutStartSec=1800

ExecStartPre=/bin/sh -c "echo 'Setting up Management components...'"
# Scripts are executed in StartPre because Start can only run a single on
ExecStartPre=/opt/mgmt/bin/rancher.sh
ExecStartPre=/opt/mgmt/bin/metal3.sh
ExecStart=/bin/sh -c "echo 'Finished setting up Management components'"
RemainAfterExit=yes
KillMode=process
# Disable &amp; delete everything
ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh
ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh
ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-stack-setup.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service

[Install]
WantedBy=multi-user.target</screen>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<para><literal>custom/scripts</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>99-alias.sh</literal>スクリプト:
管理クラスタが初回ブート時にkubeconfigファイルを読み込むために使用するエイリアスが含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
echo "alias k=kubectl" &gt;&gt; /etc/profile.local
echo "alias kubectl=/var/lib/rancher/rke2/bin/kubectl" &gt;&gt; /etc/profile.local
echo "export KUBECONFIG=/etc/rancher/rke2/rke2.yaml" &gt;&gt; /etc/profile.local</screen>
</listitem>
<listitem>
<para><literal>99-mgmt-setup.sh</literal>スクリプト:
初回ブート時にスクリプトをコピーするための設定が含まれます(変更不要)。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service</screen>
</listitem>
<listitem>
<para><literal>99-register.sh</literal>スクリプト:
SCC登録コードを使用してシステムを登録するための設定が含まれます。アカウントにシステムを登録するには、<literal>${SCC_ACCOUNT_EMAIL}</literal>および<literal>${SCC_REGISTRATION_CODE}</literal>が正しく設定されている必要があります。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

# Registration https://www.suse.com/support/kb/doc/?id=000018564
if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then
	zypper --non-interactive install suseconnect-ng
fi
SUSEConnect --email "${SCC_ACCOUNT_EMAIL}" --url "https://scc.suse.com" --regcode "${SCC_REGISTRATION_CODE}"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-kubernetes-folder">
<title>Kubernetesフォルダ</title>
<para><literal>kubernetes</literal>フォルダには次のサブフォルダが含まれます。</para>
<screen language="console" linenumbering="unnumbered">...
├── kubernetes
│ ├── manifests
│ │ ├── rke2-ingress-config.yaml
│ │ ├── neuvector-namespace.yaml
│ │ ├── ingress-l2-adv.yaml
│ │ └── ingress-ippool.yaml
│ ├── helm
│ │ └── values
│ │     ├── rancher.yaml
│ │     ├── neuvector.yaml
│ │     ├── metal3.yaml
│ │     └── certmanager.yaml
│ └── config
│     └── server.yaml
...</screen>
<para><literal>kubernetes/config</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>server.yaml</literal>:
デフォルトでは、デフォルトでインストールされている<literal>CNI</literal>プラグインは<literal>Cilium</literal>であるため、このフォルダとファイルを作成する必要はありません。<literal>CNI</literal>プラグインをカスタマイズする必要がある場合に備えて、<literal>kubernetes/config</literal>フォルダにある<literal>server.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
</listitem>
</itemizedlist>
<note>
<para>これは、使用するCNIプラグインなどの特定のKubernetesカスタマイズを定義する任意のファイルです。さまざまなオプションについては、<link
xl:href="https://docs.rke2.io/install/configuration">公式ドキュメント</link>で確認できます。</para>
</note>
<para><literal>kubernetes/manifests</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-ingress-config.yaml</literal>:
管理クラスタ用の<literal>Ingress</literal>サービスを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local</screen>
</listitem>
<listitem>
<para><literal>neuvector-namespace.yaml</literal>:
<literal>NeuVector</literal>ネームスペースを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
  name: neuvector</screen>
</listitem>
<listitem>
<para><literal>ingress-l2-adv.yaml</literal>:
<literal>MetalLB</literal>コンポーネントの<literal>L2Advertisement</literal>を作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - ingress-ippool</screen>
</listitem>
<listitem>
<para><literal>ingress-ippool.yaml</literal>:
<literal>rke2-ingress-nginx</literal>コンポーネントの<literal>IPAddressPool</literal>を作成するための設定が含まれます。<literal>${INGRESS_VIP}</literal>を正しく設定し、<literal>rke2-ingress-nginx</literal>コンポーネントで使用するために予約するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
    - ${INGRESS_VIP}/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
      - matchExpressions:
          - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}</screen>
</listitem>
</itemizedlist>
<para><literal>kubernetes/helm/values</literal>フォルダには次のファイルが含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>rancher.yaml</literal>:
<literal>Rancher</literal>コンポーネントを作成するための設定が含まれます。<literal>${INGRESS_VIP}</literal>を正しく設定して、<literal>Rancher</literal>コンポーネントで使用するIPアドレスを定義する必要があります。<literal>Rancher</literal>コンポーネントにアクセスするためのURLは、<literal><link
xl:href="https://rancher-${INGRESS_VIP}.sslip.io">https://rancher-${INGRESS_VIP}.sslip.io</link></literal>です。</para>
<screen language="yaml" linenumbering="unnumbered">hostname: rancher-${INGRESS_VIP}.sslip.io
bootstrapPassword: "foobar"
replicas: 1
global.cattle.psp.enabled: "false"</screen>
</listitem>
<listitem>
<para><literal>neuvector.yaml</literal>:
<literal>NeuVector</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">controller:
  replicas: 1
  ranchersso:
    enabled: true
manager:
  enabled: false
cve:
  scanner:
    enabled: false
    replicas: 1
k3s:
  enabled: true
crdwebhook:
  enabled: false</screen>
</listitem>
<listitem>
<para><literal>metal3.yaml</literal>:
<literal>Metal<superscript>3</superscript></literal>コンポーネントを作成するための設定が含まれます。<literal>${METAL3_VIP}</literal>を正しく設定して、<literal>Metal<superscript>3</superscript></literal>コンポーネントで使用するIPアドレスを定義する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">global:
  ironicIP: ${METAL3_VIP}
  enable_vmedia_tls: false
  additionalTrustedCAs: false
metal3-ironic:
  global:
    predictableNicNames: "true"
  persistence:
    ironic:
      size: "5Gi"</screen>
</listitem>
</itemizedlist>
<note xml:id="metal3-media-server">
<para>メディアサーバは、Metal<superscript>3</superscript>に含まれるオプションの機能です(デフォルトでは無効になっています)。このMetal3の機能を使用するには、以前のマニフェストで設定する必要があります。Metal<superscript>3</superscript>メディアサーバを使用するには、次の変数を指定します。</para>
<itemizedlist>
<listitem>
<para>メディアサーバ機能を有効にするために、globalセクションに<literal>enable_metal3_media_server</literal>を追加して<literal>true</literal>に設定します。</para>
</listitem>
<listitem>
<para>メディアサーバ設定に次の内容を含めます。${MEDIA_VOLUME_PATH}はメディアボリュームのパスです(例:
<literal>/home/metal3/bmh-image-cache</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}</screen>
</listitem>
</itemizedlist>
<para>外部のメディアサーバを使用してイメージを保存できます。外部のメディアサーバをTLSで使用する場合は、次の設定を変更する必要があります。</para>
<itemizedlist>
<listitem>
<para>前の<literal>metal3.yaml</literal>ファイルで<literal>additionalTrustedCAs</literal>を<literal>true</literal>に設定し、外部のメディアサーバから、信頼できる追加のCAを有効にします。</para>
</listitem>
<listitem>
<para><literal>kubernetes/manifests/metal3-cacert-secret.yaml</literal>フォルダに次のシークレット設定を含めて、外部のメディアサーバのCA証明書を保存します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: metal3-system
---
apiVersion: v1
kind: Secret
metadata:
  name: tls-ca-additional
  namespace: metal3-system
type: Opaque
data:
  ca-additional.crt: {{ additional_ca_cert | b64encode }}</screen>
</listitem>
</itemizedlist>
<para><literal>additional_ca_cert</literal>は、外部のメディアサーバのbase64エンコードCA証明書です。次のコマンドを使用し、証明書をエンコードして手動でシークレットを生成できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl -n meta3-system create secret generic tls-ca-additional --from-file=ca-additional.crt=./ca-additional.crt</screen>
</note>
<itemizedlist>
<listitem>
<para><literal>certmanager.yaml</literal>:
<literal>Cert-Manager</literal>コンポーネントを作成するための設定が含まれます(変更不要)。</para>
<screen language="yaml" linenumbering="unnumbered">installCRDs: "true"</screen>
</listitem>
</itemizedlist>
</section>
<section xml:id="mgmt-cluster-network-folder">
<title>ネットワーキングフォルダ</title>
<para><literal>network</literal>フォルダには、管理クラスタのノードと同じ数のファイルが含まれます。ここでは、ノードは1つのみであるため、<literal>mgmt-cluster-node1.yaml</literal>というファイルが1つあるだけです。ファイルの名前は、上述のnetwork/nodeセクションで<literal>mgmt-cluster.yaml</literal>定義ファイルに定義されているホスト名と一致させる必要があります。</para>
<para>ネットワーキング設定をカスタマイズして特定の静的IPアドレスを使用する必要がある場合(たとえばDHCPを使用しないシナリオの場合)、<literal>network</literal>フォルダにある<literal>mgmt-cluster-node1.yaml</literal>ファイルを使用できます。このファイルには次の情報が含まれます。</para>
<itemizedlist>
<listitem>
<para><literal>${MGMT_GATEWAY}</literal>: ゲートウェイのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_DNS}</literal>: DNSサーバのIPアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_MAC}</literal>: ネットワークインタフェースのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${MGMT_NODE_IP}</literal>: 管理クラスタのIPアドレス。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_NODE_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false</screen>
<para>DHCPを使用してIPアドレスを取得する場合、次の設定を使用できます(<literal>${MGMT_MAC}</literal>変数を使用して、<literal>MAC</literal>アドレスを正しく設定する必要があります)。</para>
<screen language="yaml" linenumbering="unnumbered">## This is an example of a dhcp network configuration for a management cluster
## interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false</screen>
<note>
<itemizedlist>
<listitem>
<para>管理クラスタのノード数に応じて、<literal>mgmt-cluster-node2.yaml</literal>、<literal>mgmt-cluster-node3.yaml</literal>などのように追加のファイルを作成して残りのノードを設定できます。</para>
</listitem>
<listitem>
<para><literal>routes</literal>セクションは、管理クラスタのルーティングテーブルを定義するために使用します。</para>
</listitem>
</itemizedlist>
</note>
</section>
</section>
<section xml:id="mgmt-cluster-image-preparation-airgap">
<title>エアギャップ環境のイメージの準備</title>
<para>このセクションでは、エアギャップ環境を準備する方法について説明し、前の各セクションとの相違点のみを示します。エアギャップ環境のイメージを準備するには、前のセクション(接続環境のイメージの準備(<xref
linkend="mgmt-cluster-image-preparation-connected"/>))を次のように変更する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIB出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。</para>
</listitem>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトは削除する必要があります。</para>
</listitem>
<listitem>
<para><literal>custom/files/airgap-resources.tar.gz</literal>ファイルを、管理クラスタをエアギャップ環境で実行するために必要なすべてのリソースとともに<literal>custom/files</literal>フォルダに含める必要があります。</para>
</listitem>
<listitem>
<para><literal>custom/scripts/99-mgmt-setup.sh</literal>スクリプトを変更し、<literal>airgap-resources.tar.gz</literal>ファイルを抽出して最終的な場所にコピーするようにする必要があります。</para>
</listitem>
<listitem>
<para><literal>custom/files/metal3.sh</literal>スクリプトを変更し、インターネットからリソースをダウンロードするのではなく、<literal>airgap-resources.tar.gz</literal>ファイルに含まれるローカルリソースを使用するように必要があります。</para>
</listitem>
</itemizedlist>
<section xml:id="mgmt-cluster-directory-structure-airgap">
<title>エアギャップ環境のディレクトリ構造</title>
<para>エアギャップ環境のディレクトリ構造は接続環境とほぼ同じです。相違点を次に説明します。</para>
<screen language="console" linenumbering="unnumbered">eib
|-- base-images
|   |-- SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
|-- custom
|   |-- files
|   |   |-- airgap-resources.tar.gz
|   |   |-- basic-setup.sh
|   |   |-- metal3.sh
|   |   |-- mgmt-stack-setup.service
|   |   |-- rancher.sh
|   |-- scripts
|       |-- 99-alias.sh
|       |-- 99-mgmt-setup.sh
|-- kubernetes
|   |-- config
|   |   |-- server.yaml
|   |-- helm
|   |   |-- values
|   |       |-- certmanager.yaml
|   |       |-- metal3.yaml
|   |       |-- neuvector.yaml
|   |       |-- rancher.yaml
|   |-- manifests
|       |-- neuvector-namespace.yaml
|-- mgmt-cluster.yaml
|-- network
    |-- mgmt-cluster-network.yaml</screen>
<note>
<para>イメージ<literal>SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso</literal>を<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードし、プロセスを開始する前に<literal>base-images</literal>フォルダに配置する必要があります。</para>
<para>イメージのSHA256チェックサムを確認し、イメージが改ざんされていないことを確認する必要があります。このチェックサムは、イメージをダウンロードした場所と同じ場所にあります。</para>
<para>ディレクトリ構造の例は、 <link xl:href="https://github.com/suse-edge/atip">SUSE Edge
GitHubリポジトリの「telco-examples」フォルダ</link>にあります。</para>
</note>
</section>
<section xml:id="mgmt-cluster-image-definition-file-airgap">
<title>定義ファイルの変更</title>
<para><literal>mgmt-cluster.yaml</literal>ファイルを変更して<literal>embeddedArtifactRegistry</literal>セクションを含め、<literal>images</literal>フィールドに、EIB出力イメージに組み込むすべてのコンテナイメージを設定する必要があります。<literal>images</literal>フィールドには、出力イメージに含めるすべてのコンテナイメージのリストを含める必要があります。次に、<literal>embeddedArtifactRegistry</literal>セクションが含まれる<literal>mgmt-cluster.yaml</literal>ファイルの例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - jq
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
  helm:
    charts:
      - name: cert-manager
        repositoryName: jetstack
        version: 1.14.2
        targetNamespace: cert-manager
        valuesFile: certmanager.yaml
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn-crd
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: longhorn
        version: 103.3.0+up1.6.1
        repositoryName: rancher-charts
        targetNamespace: longhorn-system
        createNamespace: true
        installationNamespace: kube-system
      - name: metal3-chart
        version: 0.7.1
        repositoryName: suse-edge-charts
        targetNamespace: metal3-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: metal3.yaml
      - name: neuvector-crd
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: neuvector
        version: 103.0.3+up2.7.6
        repositoryName: rancher-charts
        targetNamespace: neuvector
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: neuvector.yaml
      - name: rancher
        version: 2.8.4
        repositoryName: rancher-prime
        targetNamespace: cattle-system
        createNamespace: true
        installationNamespace: kube-system
        valuesFile: rancher.yaml
    repositories:
      - name: jetstack
        url: https://charts.jetstack.io
      - name: rancher-charts
        url: https://charts.rancher.io/
      - name: suse-edge-charts
        url: oci://registry.suse.com/edge
      - name: rancher-prime
        url: https://charts.rancher.com/server-charts/prime
    network:
      apiHost: ${API_HOST}
      apiVIP: ${API_VIP}
    nodes:
      - hostname: mgmt-cluster-node1
        initializer: true
        type: server
#     - hostname: mgmt-cluster-node2
#       initializer: true
#       type: server
#     - hostname: mgmt-cluster-node3
#       initializer: true
#       type: server
embeddedArtifactRegistry:
  images:
    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2
    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1
    - name: registry.rancher.com/rancher/cis-operator:v1.0.13
    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7
    - name: registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1
    - name: registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1
    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9
    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4
    - name: registry.rancher.com/rancher/fleet:v0.9.4
    - name: registry.rancher.com/rancher/gitjob:v0.9.7
    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5
    - name: registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410
    - name: registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423
    - name: registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124
    - name: registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325
    - name: registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305
    - name: registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125
    - name: registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418
    - name: registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423
    - name: registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401
    - name: registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416
    - name: registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208
    - name: registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926
    - name: registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208
    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1
    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10
    - name: registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0
    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228
    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7
    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1
    - name: registry.rancher.com/rancher/kubectl:v1.28.7
    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine
    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26
    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112
    - name: registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4
    - name: registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1
    - name: registry.rancher.com/rancher/pause:3.6
    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0
    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1
    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2
    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2
    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4
    - name: registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client
    - name: registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy
    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4
    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1
    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5
    - name: registry.rancher.com/rancher/rancher:v2.8.4
    - name: registry.rancher.com/rancher/rke-tools:v0.1.96
    - name: registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412
    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/security-scan:v0.2.15
    - name: registry.rancher.com/rancher/shell:v0.1.24
    - name: registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1
    - name: registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1
    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc
    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1
    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0
    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1
    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5
    - name: registry.rancher.com/rancher/kubectl:v1.20.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.4.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v3.6.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.9.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v6.3.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.9.2
    - name: registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.12.0
    - name: registry.rancher.com/rancher/mirrored-longhornio-backing-image-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.6.1
    - name: registry.rancher.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.36
    - name: registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:v0.2.6
    - name: registry.suse.com/edge/cluster-api-provider-rke2-controlplane:v0.2.6
    - name: registry.suse.com/edge/cluster-api-controller:v1.6.2
    - name: registry.suse.com/edge/cluster-api-provider-metal3:v1.6.0
    - name: registry.suse.com/edge/ip-address-manager:v1.6.0</screen>
</section>
<section xml:id="mgmt-cluster-custom-folder-airgap">
<title>カスタムフォルダの変更</title>
<itemizedlist>
<listitem>
<para>エアギャップ環境を使用する場合、<literal>custom/scripts/99-register.sh</literal>スクリプトを削除する必要があります。ディレクトリ構造からわかるように、<literal>99-register.sh</literal>スクリプトは<literal>custom/scripts</literal>フォルダに含まれていません。</para>
</listitem>
<listitem>
<para><literal>custom/scripts/99-mgmt-setup.sh</literal>スクリプトを変更し、<literal>airgap-resources.tar.gz</literal>ファイルを抽出して最終的な場所にコピーするようにする必要があります。次に、<literal>airgap-resources.tar.gz</literal>ファイルを抽出してコピーするように変更した<literal>99-mgmt-setup.sh</literal>スクリプトの例を示します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# Copy the scripts from combustion to the final location
mkdir -p /opt/mgmt/bin/
for script in basic-setup.sh rancher.sh metal3.sh; do
	cp ${script} /opt/mgmt/bin/
done

# Copy the systemd unit file and enable it at boot
cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service
systemctl enable mgmt-stack-setup.service

# Extract the airgap resources
tar zxf airgap-resources.tar.gz

# Copy the clusterctl binary to the final location
cp airgap-resources/clusterctl /opt/mgmt/bin/ &amp;&amp; chmod +x /opt/mgmt/bin/clusterctl

# Copy the clusterctl.yaml and override
mkdir -p /root/cluster-api
cp -r airgap-resources/clusterctl.yaml airgap-resources/overrides /root/cluster-api/</screen>
</listitem>
<listitem>
<para><literal>custom/files/metal3.sh</literal>スクリプトを変更し、インターネットからリソースをダウンロードするのではなく、<literal>airgap-resources.tar.gz</literal>ファイルに含まれるローカルリソースを使用するようにする必要があります。次に、ローカルリソースを使用するように変更した<literal>metal3.sh</literal>スクリプトの例を示します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
set -euo pipefail

BASEDIR="$(dirname "$0")"
source ${BASEDIR}/basic-setup.sh

METAL3LOCKNAMESPACE="default"
METAL3LOCKCMNAME="metal3-lock"

trap 'catch $? $LINENO' EXIT

catch() {
  if [ "$1" != "0" ]; then
    echo "Error $1 occurred on $2"
    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}
  fi
}

# Get or create the lock to run all those steps just in a single node
# As the first node is created WAY before the others, this should be enough
# TODO: Investigate if leases is better
if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o name | wc -l) -lt 1 ]; then
  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} --from-literal foo=bar
else
  exit 0
fi

# Wait for metal3
while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o name) --timeout=10s; do sleep 2 ; done

# If rancher is deployed
if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name | wc -l) -ge 1 ]; then
  cat &lt;&lt;-EOF | ${KUBECTL} apply -f -
	apiVersion: management.cattle.io/v3
	kind: Feature
	metadata:
	  name: embedded-cluster-api
	spec:
	  value: false
	EOF

  # Disable Rancher webhooks for CAPI
  ${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
  ${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi

# Deploy CAPI
if [ $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -o name | wc -l) -lt 1 ]; then

  # Try this command 3 times just in case, stolen from https://stackoverflow.com/a/33354419
  if ! (r=3; while ! /opt/mgmt/bin/clusterctl init \
    --core "cluster-api:v${METAL3_CAPICOREVERSION}"\
    --infrastructure "metal3:v${METAL3_CAPIMETAL3VERSION}"\
    --bootstrap "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --control-plane "${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}"\
    --config /root/cluster-api/clusterctl.yaml ; do
            ((--r))||exit
            echo "Something went wrong, let's wait 10 seconds and retry"
            sleep 10;done) ; then
      echo "clusterctl failed"
      exit 1
  fi

  # Wait for capi-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPISYSTEMNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -l cluster.x-k8s.io/provider=cluster-api -o name) --timeout=10s; do sleep 2 ; done

  # Wait for capm3-controller-manager, there are two pods, the ipam and the capm3 one, just wait for the first one
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPM3NAMESPACE} $(${KUBECTL} get pods -n ${METAL3_CAPM3NAMESPACE} -l cluster.x-k8s.io/provider=infrastructure-metal3 -o name | head -n1 ) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-bootstrap-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2BOOTSTRAPNAMESPACE} -l cluster.x-k8s.io/provider=bootstrap-rke2 -o name) --timeout=10s; do sleep 2 ; done

  # Wait for rke2-control-plane-controller-manager
  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_RKE2CONTROLPLANENAMESPACE} $(${KUBECTL} get pods -n ${METAL3_RKE2CONTROLPLANENAMESPACE} -l cluster.x-k8s.io/provider=control-plane-rke2 -o name) --timeout=10s; do sleep 2 ; done

fi

# Clean up the lock cm

${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}</screen>
</listitem>
<listitem>
<para><literal>custom/files/airgap-resources.tar.gz</literal>ファイルを変更し、管理クラスタをエアギャップ環境で実行するために必要なすべてのリソースとともに<literal>custom/files</literal>フォルダに含める必要があります。このファイルを準備するには、すべてのリソースを手動でダウンロードして1つのファイルに圧縮する必要があります。<literal>airgap-resources.tar.gz</literal>ファイルには次のリソースが含まれます。</para>
<screen language="console" linenumbering="unnumbered">|-- clusterctl
|-- clusterctl.yaml
|-- overrides
    |-- bootstrap-rke2
    |   |-- v0.2.6
    |       |-- bootstrap-components.yaml
    |       |-- metadata.yaml
    |-- cluster-api
    |   |-- v1.6.2
    |       |-- core-components.yaml
    |       |-- metadata.yaml
    |-- control-plane-rke2
    |   |-- v0.2.6
    |       |-- control-plane-components.yaml
    |       |-- metadata.yaml
    |-- infrastructure-metal3
        |-- v1.6.0
            |-- cluster-template.yaml
            |-- infrastructure-components.yaml
            |-- metadata.yaml</screen>
</listitem>
</itemizedlist>
<para><literal>clusterctl.yaml</literal>ファイルには、イメージの場所、および<literal>clusterctl</literal>ツールで利用する上書き設定が含まれます。<literal>overrides</literal>フォルダには、インターネットからダウンロードする代わりに使用する<literal>yaml</literal>ファイルマニフェストが含まれます。</para>
<screen language="yaml" linenumbering="unnumbered">providers:
  # override a pre-defined provider
  - name: "cluster-api"
    url: "/root/cluster-api/overrides/cluster-api/v1.6.2/core-components.yaml"
    type: "CoreProvider"
  - name: "metal3"
    url: "/root/cluster-api/overrides/infrastructure-metal3/v1.6.0/infrastructure-components.yaml"
    type: "InfrastructureProvider"
  - name: "rke2"
    url: "/root/cluster-api/overrides/bootstrap-rke2/v0.2.6/bootstrap-components.yaml"
    type: "BootstrapProvider"
  - name: "rke2"
    url: "/root/cluster-api/overrides/control-plane-rke2/v0.2.6/control-plane-components.yaml"
    type: "ControlPlaneProvider"
images:
  all:
    repository: registry.suse.com/edge</screen>
<para><literal>overrides</literal>フォルダに含まれる<literal>clusterctl</literal>と残りのファイル
は、次のcurlsコマンドを使用してダウンロードできます。</para>
<screen language="shell" linenumbering="unnumbered"># clusterctl binary
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/1.6.2/clusterctl-linux-${GOARCH} -o /usr/local/bin/clusterct

# boostrap-components (boostrap-rke2)
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/bootstrap-components.yaml
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml

# control-plane-components (control-plane-rke2)
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/control-plane-components.yaml
curl -L https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml

# cluster-api components
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/core-components.yaml
curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/metadata.yaml

# infrastructure-components (infrastructure-metal3)
curl -L https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/infrastructure-components.yaml
curl -L https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/metadata.yaml</screen>
<note>
<para>異なるバージョンのコンポーネントを使用する場合、URL内のバージョンを変更すると、特定のバージョンのコンポーネントをダウンロードできます。</para>
</note>
<para>以前のリソースをダウンロード済みの場合、次のコマンドを使用してそれらを1つのファイルに圧縮できます。</para>
<screen language="shell" linenumbering="unnumbered">tar -czvf airgap-resources.tar.gz clusterctl clusterctl.yaml overrides</screen>
</section>
</section>
<section xml:id="mgmt-cluster-image-creation">
<title>イメージの作成</title>
<para>前の各セクションに従ってディレクトリ構造を準備したら(接続シナリオとエアギャップシナリオの両方が対象です)、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file mgmt-cluster.yaml</screen>
<para>ISO出力イメージファイルが作成されます。ここでは、上記のイメージ定義に基づく<literal>eib-mgmt-cluster-image.iso</literal>という名前のファイルです。</para>
</section>
<section xml:id="mgmt-cluster-provision">
<title>管理クラスタのプロビジョニング</title>
<para>前のイメージには、上述のコンポーネントがすべて含まれています。このイメージを使って、仮想マシンまたはベアメタルサーバを使用して(仮想メディア機能を使用して)管理クラスタをプロビジョニングできます。</para>
</section>
</chapter>
<chapter xml:id="atip-features">
<title>通信機能の設定</title>
<para>このセクションでは、ATIPがデプロイされたクラスタの通信事業者固有の機能について記述および説明します。</para>
<para>ダイレクトネットワークプロビジョニングのデプロイメント方法を使用します。この方法については、ATIPの自動プロビジョニング(<xref
linkend="atip-automated-provisioning"/>)に関するセクションで説明しています。</para>
<para>このセクションでは、次のトピックについて説明します。</para>
<itemizedlist>
<listitem>
<para>リアルタイム用のカーネルイメージ(<xref linkend="kernel-image-for-real-time"/>):
リアルタイムカーネルで使用するカーネルイメージ。</para>
</listitem>
<listitem>
<para>CPU調整設定(<xref linkend="cpu-tuned-configuration"/>):
リアルタイムカーネルで使用するために調整した設定。</para>
</listitem>
<listitem>
<para>CNI設定(<xref linkend="cni-configuration"/>): Kubernetesクラスタで使用するCNI設定。</para>
</listitem>
<listitem>
<para>SR-IOV設定(<xref linkend="sriov"/>): Kubernetesワークロードで使用するSR-IOV設定。</para>
</listitem>
<listitem>
<para>DPDK設定(<xref linkend="dpdk"/>): システムで使用するDPDK設定。</para>
</listitem>
<listitem>
<para>vRANアクセラレーションカード(<xref linkend="acceleration"/>):
Kubernetesワークロードで使用するアクセラレーションカードの設定。</para>
</listitem>
<listitem>
<para>Huge Page (<xref linkend="huge-pages"/>): Kubernetesワークロードで使用するHuge Pageの設定。</para>
</listitem>
<listitem>
<para>CPUピニング設定(<xref linkend="cpu-pinning-configuration"/>):
Kubernetesワークロードで使用するCPUピニング設定。</para>
</listitem>
<listitem>
<para>NUMA対応のスケジューリング設定(<xref linkend="numa-aware-scheduling"/>):
Kubernetesワークロードで使用するNUMA対応のスケジューリング設定。</para>
</listitem>
<listitem>
<para>Metal LB設定(<xref linkend="metal-lb-configuration"/>):
Kubernetesワークロードで使用するMetal LB設定。</para>
</listitem>
<listitem>
<para>プライベートレジストリ設定(<xref linkend="private-registry"/>):
Kubernetesワークロードで使用するプライベートレジストリ設定。</para>
</listitem>
</itemizedlist>
<section xml:id="kernel-image-for-real-time">
<title>リアルタイム用のカーネルイメージ</title>
<para>リアルタイムカーネルイメージは必ずしも標準カーネルより優れているとは限りません。リアルタイムカーネルは、特定のユースケース用に調整された別のカーネルです。低レイテンシを実現するために調整されていますが、その結果、スループットが犠牲になります。リアルタイムカーネルは一般的な用途には推奨されませんが、ここでは低レイテンシが重要な要因である通信ワークロード用のカーネルとして推奨されています。</para>
<para>主に4つの機能があります。</para>
<itemizedlist>
<listitem>
<para>決定論的実行:</para>
<para>予測可能性の向上 —
高負荷状態でも重要なビジネスプロセスが期限内に確実に完了し、常に高品質なサービスを提供します。高優先度プロセスのために重要なシステムリソースを保護することで、時間に依存するアプリケーションの予測可能性を向上できます。</para>
</listitem>
<listitem>
<para>低ジッタ:</para>
<para>高度に決定論的な技術に基づいてジッタが低く抑えられているため、アプリケーションと実世界との同期を維持できます。これは、継続的に繰り返し計算を行う必要があるサービスで役立ちます。</para>
</listitem>
<listitem>
<para>優先度の継承:</para>
<para>優先度の継承とは、優先度の高いプロセスがある状況において、そのプロセスがタスクを完了するためには優先度の低いプロセスが完了するのを待つ必要がある場合に、優先度の低いプロセスが高優先度を一時的に引き受ける機能です。SUSE
Linux Enterprise Real Timeは、ミッションクリティカルなプロセスにおけるこのような優先度の逆転の問題を解決します。</para>
</listitem>
<listitem>
<para>スレッドの割り込み:</para>
<para>一般的なオペレーティングシステムでは、割り込みモードで実行中のプロセスはプリエンプト可能ではありません。SUSE Linux Enterprise
Real
Timeでは、このような割り込みをカーネルスレッドでカプセル化して割り込み可能にし、ユーザが定義した高優先度プロセスでハード割り込みとソフト割り込みをプリエンプトできます。</para>
<para>ここでは、<literal>SLE Micro
RT</literal>のようなリアルタイムイメージをインストール済みの場合、カーネルリアルタイムはすでにインストールされています。リアルタイムカーネルイメージは<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>からダウンロードできます。</para>
<note>
<para>リアルタイムカーネルの詳細については、<link
xl:href="https://www.suse.com/products/realtime/">SUSE Real
Time</link>を参照してください。</para>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="cpu-tuned-configuration">
<title>CPU調整設定</title>
<para>CPU調整設定を使用すると、リアルタイムカーネルが使用するCPUコアを分離できます。OSがリアルタイムカーネルと同じコアを使用しないようにすることが重要です。OSがそのコアを使用すると、リアルタイムカーネルの遅延が増加するためです。</para>
<para>この機能を有効にして設定するには、まず、分離するCPUコアのプロファイルを作成します。ここでは、コア<literal>1-30</literal>および<literal>33-62</literal>を分離しています。</para>
<screen language="shell" linenumbering="unnumbered">$ echo "export tuned_params" &gt;&gt; /etc/grub.d/00_tuned

$ echo "isolated_cores=1-30,33-62" &gt;&gt; /etc/tuned/cpu-partitioning-variables.conf

$ tuned-adm profile cpu-partitioning
Tuned (re)started, changes applied.</screen>
<para>次に、GRUBオプションを変更して、CPUコアと、CPUの使用法に関するその他の重要なパラメータを分離する必要があります。現在のハードウェア仕様で次のオプションをカスタマイズすることが重要です。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>コア1-30および33-62を分離します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルは分離されたCPU全体でタイマー割り込みをずらすことができます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルはシステムがアイドル状態のときに1つのCPU上でタイマーティックを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>カーネルブートパラメータは、完全なdynticksとCPU分離の設定を行うための現在の主要インタフェースです。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、カーネルはシステムがアイドル状態のときに1つのCPU上でRCUコールバックを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>kthread_cpus</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、システムがアイドル状態のときに1つのCPU上でkthreadsを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、システムがアイドル状態のときに1つのCPU上で割り込みを実行できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>processor.max_cstate</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、アイドル状態のときにCPUがスリープ状態になるのを防ぎます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_idle.max_cstate</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、intel_idleドライバが無効になり、acpi_idleを使用できるようになります。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>上記の値を使用することで、60個のコアを分離し、4個のコアをOSに使用します。</para>
<para>次のコマンドでGRUB設定を変更し、上記の変更を次回ブート時に適用します。</para>
<para><literal>/etc/default/grub</literal>ファイルを編集し、上記のパラメータを追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>GRUB設定を更新します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドを使用してカーネルコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
</section>
<section xml:id="cni-configuration">
<title>CNI設定</title>
<section xml:id="id-cilium">
<title>Cilium</title>
<para><literal>Cilium</literal>はATIPのデフォルトのCNIプラグインです。RKE2クラスタでCiliumをデフォルトのプラグインとして有効にするには、<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに次の設定が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- cilium</screen>
<para>これはコマンドライン引数でも指定できます。具体的には、<literal>/etc/systemd/system/rke2-server</literal>ファイルのサーバの行に<literal>--cni=cilium</literal>を追加します。</para>
<para>次のセクション(<xref linkend="option2-sriov-helm"/>)で説明する<literal>SR-IOV</literal>
Network
Operatorを使用するには、<literal>Multus</literal>とともに、<literal>Cilium</literal>や<literal>Calico</literal>などの別のCNIプラグインをセカンダリプラグインとして使用します。</para>
<screen language="yaml" linenumbering="unnumbered">cni:
- multus
- cilium</screen>
<note>
<para>CNIプラグインの詳細については、「<link
xl:href="https://docs.rke2.io/install/network_options">Network Options
(ネットワークオプション)</link>」を参照してください。</para>
</note>
</section>
</section>
<section xml:id="sriov">
<title>SR-IOV</title>
<para>SR-IOVを使用すると、ネットワークアダプタなどのデバイスで、そのリソースへのアクセスをさまざまな<literal>PCIe</literal>ハードウェア機能の間で分離することができます。<literal>SR-IOV</literal>をデプロイするにはさまざまな方法がありますが、ここでは2つの方法を示します。</para>
<itemizedlist>
<listitem>
<para>オプション1: <literal>SR-IOV</literal> CNIデバイスプラグインと設定マップを使用して適切に設定する。</para>
</listitem>
<listitem>
<para>オプション2 (推奨): Rancher Primeから<literal>SR-IOV</literal>
Helmチャートを使用してこのデプロイメントを簡単に行えるようにする。</para>
</listitem>
</itemizedlist>
<para xml:id="option1-sriov-deviceplugin"><emphasis role="strong">オプション1 - SR-IOV
CNIデバイスプラグインと設定マップをインストールして適切に設定する</emphasis></para>
<itemizedlist>
<listitem>
<para>デバイスプラグインの設定マップを準備する</para>
</listitem>
</itemizedlist>
<para>設定マップに入力する情報を<literal>lspci</literal>コマンドから取得します。</para>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c

$ lspci | grep -i net
19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)
51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C for QSFP (rev 02)
51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)
51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual Function (rev 02)</screen>
<para>設定マップは<literal>JSON</literal>ファイルで構成され、このファイルで、フィルタを使用して検出を行うデバイスを記述し、インタフェースのグループを作成します。フィルタとグループを理解することが重要です。フィルタはデバイスを検出するために使用され、グループはインタフェースを作成するために使用されます。</para>
<para>フィルタを設定することもできます。</para>
<itemizedlist>
<listitem>
<para>vendorID: <literal>8086</literal> (Intel)</para>
</listitem>
<listitem>
<para>deviceID: <literal>0d5c</literal> (アクセラレータカード)</para>
</listitem>
<listitem>
<para>driver: <literal>vfio-pci</literal> (ドライバ)</para>
</listitem>
<listitem>
<para>pfNames: <literal>p2p1</literal> (物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>フィルタを設定して、より複雑なインタフェース構文に一致させることもできます。次に例を示します。</para>
<itemizedlist>
<listitem>
<para>pfNames:
<literal>["eth1#1,2,3,4,5,6"]</literal>または<literal>[eth1#1-6]</literal>
(物理インタフェース名)</para>
</listitem>
</itemizedlist>
<para>グループに関連して、<literal>FEC</literal>カード用のグループを1つと、<literal>Intel</literal>カード用のグループを1つ作成し、さらに、ユースケースに応じてプレフィックスを作成することもできます。</para>
<itemizedlist>
<listitem>
<para>resourceName: <literal>pci_sriov_net_bh_dpdk</literal></para>
</listitem>
<listitem>
<para>resourcePrefix: <literal>Rancher.io</literal></para>
</listitem>
</itemizedlist>
<para>リソースグループを検出して作成し、一部の<literal>VF</literal>をPodに割り当てる組み合わせは多数あります。</para>
<note>
<para>フィルタとグループの詳細については、「<link
xl:href="https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin">sriov-network-device-plugin
(sr-iovネットワークデバイスプラグイン)</link>」を参照してください。</para>
</note>
<para>フィルタとグループを設定して、ハードウェアとユースケースに応じたインタフェースに一致させると、使用する例が次の設定マップに表示されます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: sriovdp-config
  namespace: kube-system
data:
  config.json: |
    {
        "resourceList": [
            {
                "resourceName": "intel_fec_5g",
                "devicetype": "accelerator",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["0d5d"]
                }
            },
            {
                "resourceName": "intel_sriov_odu",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p1"]
                }
            },
            {
                "resourceName": "intel_sriov_oru",
                "selectors": {
                    "vendors": ["8086"],
                    "devices": ["1889"],
                    "drivers": ["vfio-pci"],
                    "pfNames": ["p2p2"]
                }
            }
        ]
    }</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>ファイルを準備して、デバイスプラグインをデプロイします。</para>
</listitem>
</itemizedlist>
<para>このデバイスプラグインは、複数のアーキテクチャ(<literal>arm</literal>、<literal>amd</literal>、<literal>ppc64le</literal>)をサポートしています。したがって、同じファイルを異なるアーキテクチャに使用して、各アーキテクチャに複数の<literal>daemonset</literal>をデプロイできます。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ServiceAccount
metadata:
  name: sriov-device-plugin
  namespace: kube-system
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-sriov-device-plugin-amd64
  namespace: kube-system
  labels:
    tier: node
    app: sriovdp
spec:
  selector:
    matchLabels:
      name: sriov-device-plugin
  template:
    metadata:
      labels:
        name: sriov-device-plugin
        tier: node
        app: sriovdp
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/arch: amd64
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      serviceAccountName: sriov-device-plugin
      containers:
      - name: kube-sriovdp
        image: rancher/hardened-sriov-network-device-plugin:v3.5.1-build20231009-amd64
        imagePullPolicy: IfNotPresent
        args:
        - --log-dir=sriovdp
        - --log-level=10
        securityContext:
          privileged: true
        resources:
          requests:
            cpu: "250m"
            memory: "40Mi"
          limits:
            cpu: 1
            memory: "200Mi"
        volumeMounts:
        - name: devicesock
          mountPath: /var/lib/kubelet/
          readOnly: false
        - name: log
          mountPath: /var/log
        - name: config-volume
          mountPath: /etc/pcidp
        - name: device-info
          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp
      volumes:
        - name: devicesock
          hostPath:
            path: /var/lib/kubelet/
        - name: log
          hostPath:
            path: /var/log
        - name: device-info
          hostPath:
            path: /var/run/k8s.cni.cncf.io/devinfo/dp
            type: DirectoryOrCreate
        - name: config-volume
          configMap:
            name: sriovdp-config
            items:
            - key: config.json
              path: config.json</screen>
<itemizedlist>
<listitem>
<para>設定マップと<literal>daemonset</literal>を適用すると、デバイスプラグインがデプロイされ、インタフェースが検出されてPodで使用できるようになります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get pods -n kube-system | grep sriov
kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m</screen>
</listitem>
<listitem>
<para>Podで使用するノードでインタフェースが検出されて利用可能であることを確認します。</para>
<screen>$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "40Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "intel.com/intel_sriov_odu": "4",
  "intel.com/intel_sriov_oru": "4",
  "memory": "221396384Ki",
  "pods": "110"
}</screen>
</listitem>
<listitem>
<para><literal>FEC</literal>は<literal>intel.com/intel_fec_5g</literal>で、値は1です。</para>
</listitem>
<listitem>
<para>Helmチャートを使用せずに、デバイスプラグインと設定マップを使用してデプロイした場合、<literal>VF</literal>は、<literal>intel.com/intel_sriov_odu</literal>または<literal>intel.com/intel_sriov_oru</literal>です。</para>
</listitem>
</itemizedlist>
<important>
<para>ここにインタフェースがない場合、そのインタフェースをPodで使用することはできないため、続行しても意味がありません。まず、設定マップとフィルタを確認して問題を解決してください。</para>
</important>
<para xml:id="option2-sriov-helm"><emphasis role="strong">オプション2 (推奨) - Rancherを使用し、SR-IOV
CNIおよびデバイスプラグイン用のHelmチャートを使用したインストール</emphasis></para>
<itemizedlist>
<listitem>
<para>Helmがない場合は入手します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash</screen>
<itemizedlist>
<listitem>
<para>SR-IOVをインストールします。</para>
</listitem>
</itemizedlist>
<para>この部分は2つの方法で実行できます。<literal>CLI</literal>を使用する方法と、<literal>Rancher
UI</literal>を使用する方法です。</para>
<variablelist>
<varlistentry>
<term>CLIからのオペレータのインストール</term>
<listitem>
<screen>helm repo add suse-edge https://suse-edge.github.io/charts
helm install sriov-crd suse-edge/sriov-crd -n sriov-network-operator
helm install install sriov-network-operator suse-edge/sriov-network-operator -n sriov-network-operator</screen>
</listitem>
</varlistentry>
<varlistentry>
<term>Rancher UIからのオペレータのインストール</term>
<listitem>
<para>クラスタがインストールされたら、<literal>Rancher UI</literal>にアクセスできるようになり、［Apps
(アプリ)］タブで<literal>Rancher
UI</literal>から<literal>SR-IOVオペレータ</literal>をインストールできます。</para>
</listitem>
</varlistentry>
</variablelist>
<note>
<para>必ず、正しいネームスペースを選択してオペレータをインストールしてください(例:
<literal>sriov-network-operator</literal>)。</para>
</note>
<para>+ image::features_sriov.png[sriov.png]</para>
<itemizedlist>
<listitem>
<para>デプロイしたリソースのcrdとPodを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get crd
$ kubectl -n sriov-network-operator get pods</screen>
<itemizedlist>
<listitem>
<para>ノードのラベルを確認します。</para>
</listitem>
</itemizedlist>
<para>すべてのリソースが実行されていると、ラベルがノードに自動的に表示されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get nodes -oyaml | grep feature.node.kubernetes.io/network-sriov.capable

feature.node.kubernetes.io/network-sriov.capable: "true"</screen>
<itemizedlist>
<listitem>
<para><literal>daemonset</literal>を確認し、新しい<literal>sriov-network-config-daemon</literal>および<literal>sriov-rancher-nfd-worker</literal>がアクティブで準備できていることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get daemonset -A
NAMESPACE             NAME                            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           AGE
calico-system            calico-node                     1         1         1       1            1           kubernetes.io/os=linux                                  15h
sriov-network-operator   sriov-network-config-daemon     1         1         1       1            1           feature.node.kubernetes.io/network-sriov.capable=true   45m
sriov-network-operator   sriov-rancher-nfd-worker        1         1         1       1            1           &lt;none&gt;                                                  45m
kube-system              rke2-ingress-nginx-controller   1         1         1       1            1           kubernetes.io/os=linux                                  15h
kube-system              rke2-multus-ds                  1         1         1       1            1           kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h</screen>
<para>数分後(更新に最大で10分かかる可能性があります)、ノードが検出されて、<literal>SR-IOV</literal>の機能が設定されます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A
NAMESPACE             NAME     AGE
sriov-network-operator   xr11-2   83s</screen>
<itemizedlist>
<listitem>
<para>検出されたインタフェースを確認します。</para>
</listitem>
</itemizedlist>
<para>検出されたインタフェースはネットワークデバイスのPCIアドレスである必要があります。この情報は、ホストで<literal>lspci</literal>コマンドを使用して確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n kube-system -oyaml
apiVersion: v1
items:
- apiVersion: sriovnetwork.openshift.io/v1
  kind: SriovNetworkNodeState
  metadata:
    creationTimestamp: "2023-06-07T09:52:37Z"
    generation: 1
    name: xr11-2
    namespace: sriov-network-operator
    ownerReferences:
    - apiVersion: sriovnetwork.openshift.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: SriovNetworkNodePolicy
      name: default
      uid: 80b72499-e26b-4072-a75c-f9a6218ec357
    resourceVersion: "356603"
    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad
  spec:
    dpConfigVersion: "356507"
  status:
    interfaces:
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f0
      mtu: 1500
      name: p2p1
      pciAddress: "0000:51:00.0"
      totalvfs: 128
      vendor: "8086"
    - deviceID: "1592"
      driver: ice
      eSwitchMode: legacy
      linkType: ETH
      mac: 40:a6:b7:9b:35:f1
      mtu: 1500
      name: p2p2
      pciAddress: "0000:51:00.1"
      totalvfs: 128
      vendor: "8086"
    syncStatus: Succeeded
kind: List
metadata:
  resourceVersion: ""</screen>
<note>
<para>ここでインタフェースが検出されていない場合は、インタフェースが次の設定マップに存在することを確認してください。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator</screen>
<para>ここにデバイスがない場合は、設定マップを編集して、検出すべき適切な値を追加します(<literal>sriov-network-config-daemon</literal>デーモンセットの再起動が必要になります)。</para>
</note>
<itemizedlist>
<listitem>
<para><literal>NetworkNodeポリシー</literal>を作成して<literal>VF</literal>を設定します。</para>
</listitem>
</itemizedlist>
<para><literal>VF</literal>
(<literal>numVfs</literal>)がデバイス(<literal>rootDevices</literal>)から作成され、ドライバ<literal>deviceType</literal>と<literal>MTU</literal>が設定されます。</para>
<note>
<para><literal>resourceName</literal>フィールドには特殊文字を含めないでください。また、このフィールドはクラスタ全体で一意である必要があります。この例では、<literal>dpdk</literal>を<literal>sr-iov</literal>と組み合わせて使用するため、<literal>deviceType:
vfio-pci</literal>を使用しています。<literal>dpdk</literal>を使用しない場合は、deviceTypeを<literal>deviceType:
netdevice</literal> (デフォルト値)にする必要があります。</para>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-dpdk
  namespace: sriov-network-operator
spec:
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  resourceName: intelnicsDpdk
  deviceType: vfio-pci
  numVfs: 8
  mtu: 1500
  nicSelector:
    deviceID: "1592"
    vendor: "8086"
    rootDevices:
    - 0000:51:00.0</screen>
<itemizedlist>
<listitem>
<para>設定を検証します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get $(kubectl get nodes -oname) -o jsonpath='{.status.allocatable}' | jq
{
  "cpu": "64",
  "ephemeral-storage": "256196109726",
  "hugepages-1Gi": "60Gi",
  "hugepages-2Mi": "0",
  "intel.com/intel_fec_5g": "1",
  "memory": "200424836Ki",
  "pods": "110",
  "rancher.io/intelnicsDpdk": "8"
}</screen>
<itemizedlist>
<listitem>
<para>sr-iovネットワークを作成します(別のネットワークが必要な場合のオプション)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: network-dpdk
  namespace: sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "192.168.0.0/24",
      "rangeStart": "192.168.0.20",
      "rangeEnd": "192.168.0.60",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "192.168.0.1"
    }
  vlan: 500
  resourceName: intelnicsDpdk</screen>
<itemizedlist>
<listitem>
<para>作成されたネットワークを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml

apiVersion: v1
items:
- apiVersion: k8s.cni.cncf.io/v1
  kind: NetworkAttachmentDefinition
  metadata:
    annotations:
      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk
    creationTimestamp: "2023-06-08T11:22:27Z"
    generation: 1
    name: network-dpdk
    namespace: sriov-network-operator
    resourceVersion: "13124"
    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15
  spec:
    config: '{ "cniVersion":"0.3.1", "name":"network-dpdk","type":"sriov","vlan":500,"vlanQoS":0,"ipam":{"type":"host-local","subnet":"192.168.0.0/24","rangeStart":"192.168.0.10","rangeEnd":"192.168.0.60","routes":[{"dst":"0.0.0.0/0"}],"gateway":"192.168.0.1"}
      }'
kind: List
metadata:
  resourceVersion: ""</screen>
</section>
<section xml:id="dpdk">
<title>DPDK</title>
<para><literal>DPDK</literal>
(データプレーン開発キット)は、パケットの高速処理用の一連のライブラリとドライバです。DPDKは、広範なCPUアーキテクチャ上で実行されるパケット処理ワークロードを高速化するために使用されます。DPDKには、データプレーンライブラリと、以下のために最適化されたネットワークインタフェースコントローラ(<literal>NIC</literal>)ドライバが含まれています。</para>
<orderedlist numeration="arabic">
<listitem>
<para>キューマネージャはロックなしのキューを実装します。</para>
</listitem>
<listitem>
<para>バッファマネージャは固定サイズのバッファを事前割り当てします。</para>
</listitem>
<listitem>
<para>メモリマネージャは、メモリ内にオブジェクトのプールを割り当て、リングを使用してフリーオブジェクトを格納します。オブジェクトがすべての<literal>DRAM</literal>チャンネルに均等に分散されるようにします。</para>
</listitem>
<listitem>
<para>ポールモードドライバ(<literal>PMD</literal>)は、非同期通知なしで動作するように設計されているため、オーバーヘッドが軽減されます。</para>
</listitem>
<listitem>
<para>パケット処理を開発するためのヘルパである一連のライブラリとしてのパケットフレームワーク。</para>
</listitem>
</orderedlist>
<para>次の手順では、<literal>DPDK</literal>を有効にする方法と、<literal>DPDK</literal>インタフェースが使用する<literal>NIC</literal>から<literal>VF</literal>を作成する方法を示します。</para>
<itemizedlist>
<listitem>
<para><literal>DPDK</literal>パッケージをインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ transactional-update pkg install dpdk22 dpdk22-tools libdpdk-23
$ reboot</screen>
<itemizedlist>
<listitem>
<para>カーネルパラメータ:</para>
</listitem>
</itemizedlist>
<para>DPDKを使用するには、ドライバをいくつか使用して、カーネルの特定のパラメータを有効にします。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースに<literal>vfio</literal>ドライバを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、<literal>VF</literal>に<literal>vfio</literal>を使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>これらのパラメータを有効にするには、各パラメータを<literal>/etc/default/grub</literal>ファイルに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<itemizedlist>
<listitem>
<para><literal>vfio-pci</literal>カーネルモジュールを読み込み、<literal>NIC</literal>で<literal>SR-IOV</literal>を有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para><literal>NIC</literal>から仮想機能(<literal>VF</literal>)をいくつか作成します。</para>
</listitem>
</itemizedlist>
<para>たとえば、2つの異なる<literal>NIC</literal>に対して<literal>VF</literal>を作成するには、次のコマンドが必要です。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs
$ echo 4 &gt; /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs</screen>
<itemizedlist>
<listitem>
<para>新しいVFを<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 0000:51:01.3 \
                              0000:51:11.0 0000:51:11.1 0000:51:11.2 0000:51:11.3</screen>
<itemizedlist>
<listitem>
<para>設定が正しく適用されたことを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s

Network devices using DPDK-compatible driver
============================================
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio
0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci unused=iavf,igb_uio

Network devices using kernel driver
===================================
0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*
0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci
0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet 1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci
0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice unused=igb_uio,vfio-pci
0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice unused=igb_uio,vfio-pci</screen>
</section>
<section xml:id="acceleration">
<title>vRANアクセラレーション(<literal>Intel ACC100/ACC200</literal>)</title>
<para>4Gから5Gネットワークへの移行に伴い、多くの通信サービスプロバイダが仮想化無線アクセスネットワーク(<literal>vRAN</literal>)アーキテクチャを採用して、チャンネル容量を増やし、エッジベースのサービスとアプリケーションのデプロイメントを容易にしようとしています。vRANソリューションは、ネットワーク上のリアルタイムのトラフィックと需要の量に応じて容量を柔軟に増減できるため、低レイテンシのサービスを提供するのに理想的です。</para>
<para>4Gおよび5Gで最も計算負荷が高いワークロードの1つがRANレイヤ1
(<literal>L1</literal>)の<literal>FEC</literal>です。これは、信頼性の低い通信チャンネルやノイズの多い通信チャンネルでのデータ伝送エラーを解消するものです。<literal>FEC</literal>技術は、4Gまたは5Gデータの一定数のエラーを検出して訂正することで、再送信の必要性を解消します。<literal>FEC</literal>アクセラレーショントランザクションにはセルの状態情報が含まれないため、簡単に仮想化でき、プールするメリットとセルの容易な移行が実現します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para><literal>vRAN</literal>アクセラレーションを有効にするには、次のカーネルパラメータを有効にする必要があります(まだ存在しない場合)。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、DPDKインタフェースにvfioを使用できます。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、VFにvfioを使用できます。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータをカーネルコマンドラインに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを確認するには、コマンドラインを確認します。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>vfio-pciカーネルモジュールを読み込み、<literal>vRAN</literal>アクセラレーションを有効にします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1</screen>
<itemizedlist>
<listitem>
<para>インタフェース情報Acc100を取得します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ lspci | grep -i acc
8a:00.0 Processing accelerators: Intel Corporation Device 0d5c</screen>
<itemizedlist>
<listitem>
<para>物理インタフェース(<literal>PF</literal>)を<literal>vfio-pci</literal>ドライバにバインドします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0</screen>
<itemizedlist>
<listitem>
<para>仮想機能(<literal>VF</literal>)を物理インタフェース(<literal>PF</literal>)から作成します。</para>
</listitem>
</itemizedlist>
<para>2つの<literal>VF</literal>を<literal>PF</literal>から作成し、次の手順に従って<literal>vfio-pci</literal>にバインドします。</para>
<screen language="shell" linenumbering="unnumbered">$ echo 2 &gt; /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs
$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0</screen>
<itemizedlist>
<listitem>
<para>提案された設定ファイルを使用してacc100を設定します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg
Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL
Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode
Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92
Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != 10092020
Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete
Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration complete!</screen>
<itemizedlist>
<listitem>
<para>FEC PFから作成した新しいVFを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ dpdk-devbind.py -s
Baseband devices using DPDK-compatible driver
=============================================
0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=
0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=

Other Baseband devices
======================
0000:8b:00.1 'Device 0d5d' unused=</screen>
</section>
<section xml:id="huge-pages">
<title>Huge Page</title>
<para>プロセスが<literal>RAM</literal>を使用すると、<literal>CPU</literal>はそのメモリ領域をプロセスが使用中であるとマークします。効率を高めるために、<literal>CPU</literal>は<literal>RAM</literal>をチャンクで割り当てます。多くのプラットフォームでは<literal>4K</literal>バイトがチャンクのデフォルト値です。これらのチャンクをページと呼び、ディスクなどにスワップできます。</para>
<para>プロセスのアドレススペースは仮想であるため、<literal>CPU</literal>とオペレーティングシステムは、どのページがどのプロセスに属していて、各ページがどこに保管されているかを覚えておく必要があります。ページ数が多いほど、メモリマッピングの検索に時間がかかります。プロセスが<literal>1GB</literal>のメモリを使用する場合、検索するエントリは262,144個になります(<literal>1GB</literal>
/ <literal>4K</literal>)。1つのページテーブルエントリが8バイトを消費する場合、<literal>2MB</literal>
(262,144 * 8)を検索することになります。</para>
<para>最新の<literal>CPU</literal>アーキテクチャはデフォルトより大きいページをサポートしているので、<literal>CPU/OS</literal>が検索するエントリが減少します。</para>
<itemizedlist>
<listitem>
<para>カーネルパラメータ</para>
</listitem>
</itemizedlist>
<para>Huge Pageを有効にするには、次のカーネルパラメータを追加する必要があります。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">パラメータ</entry>
<entry align="left" valign="top">値</entry>
<entry align="left" valign="top">説明</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>このオプションを使用すると、Huge Pageを1Gに設定できます</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数です</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを取得するためのデフォルト値です</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para>GRUBファイル<literal>/etc/default/grub</literal>を変更して、これらのパラメータをカーネルコマンドラインに追加します。</para>
<screen language="shell" linenumbering="unnumbered">GRUB_CMDLINE_LINUX="intel_iommu=on intel_pstate=passive processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 rcu_nocbs=1-30,33-62 rcu_nocb_poll"</screen>
<para>GRUBの設定を更新し、システムを再起動して変更を適用します。</para>
<screen language="shell" linenumbering="unnumbered">$ transactional-update grub.cfg
$ reboot</screen>
<para>再起動後にパラメータが適用されていることを検証するには、次のコマンドラインを確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ cat /proc/cmdline</screen>
<itemizedlist>
<listitem>
<para>Huge Pageの使用</para>
</listitem>
</itemizedlist>
<para>Huge Pageを使用するには、Huge Pageをマウントする必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ mkdir -p /hugepages
$ mount -t hugetlbfs nodev /hugepages</screen>
<para>Kubernetesワークロードをデプロイし、リソースとボリュームを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">...
 resources:
   requests:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
   limits:
     memory: "24Gi"
     hugepages-1Gi: 16Gi
     intel.com/intel_sriov_oru: '4'
...</screen>
<screen language="yaml" linenumbering="unnumbered">...
volumeMounts:
  - name: hugepage
    mountPath: /hugepages
...
volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
...</screen>
</section>
<section xml:id="cpu-pinning-configuration">
<title>CPUピニング設定</title>
<itemizedlist>
<listitem>
<para>要件</para>
<orderedlist numeration="arabic">
<listitem>
<para>こちらのセクション(<xref
linkend="cpu-tuned-configuration"/>)で説明したパフォーマンスプロファイルに合わせて<literal>CPU</literal>が調整されていること。</para>
</listitem>
<listitem>
<para>次のブロック(例)を<literal>/etc/rancher/rke2/config.yaml</literal>ファイルに追加して、<literal>RKE2</literal>クラスタのkubeletにCPU管理の引数が設定されていること。</para>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">kubelet-arg:
- "cpu-manager=true"
- "cpu-manager-policy=static"
- "cpu-manager-policy-options=full-pcpus-only=true"
- "cpu-manager-reconcile-period=0s"
- "kubelet-reserved=cpu=1"
- "system-reserved=cpu=1"</screen>
<itemizedlist>
<listitem>
<para>KubernetesでのCPUピニングの使用</para>
</listitem>
</itemizedlist>
<para>kubeletで定義された<literal>静的ポリシー</literal>を使ってCPUピニング機能を使用する方法は、ワークロードに対して定義した要求と制限に応じて3つあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>BestEffort</literal> QoSクラス:
<literal>CPU</literal>に対して要求または制限を定義していない場合、Podはシステムで使用できる最初の<literal>CPU</literal>でスケジュールされます。</para>
<para><literal>BestEffort</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</screen>
</listitem>
<listitem>
<para><literal>Burstable</literal> QoSクラス:
CPUに対して要求を定義し、その要求が制限と同じではない場合、またはCPUの要求がない場合。</para>
<para><literal>Burstable</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</screen>
<para>または</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "100Mi"
        cpu: "1"</screen>
</listitem>
<listitem>
<para><literal>Guaranteed</literal> QoSクラス: CPUに対して要求を定義し、その要求が制限と同じである場合。</para>
<para><literal>Guaranteed</literal> QoSクラスを使用する例を次に示します。</para>
<screen language="yaml" linenumbering="unnumbered">spec:
  containers:
    - name: nginx
      image: nginx
      resources:
        limits:
          memory: "200Mi"
          cpu: "2"
        requests:
          memory: "200Mi"
          cpu: "2"</screen>
</listitem>
</orderedlist>
</section>
<section xml:id="numa-aware-scheduling">
<title>NUMA対応のスケジューリング</title>
<para>Non-Uniform Memory AccessまたはNon-Uniform Memory Architecture
(<literal>NUMA</literal>)は、<literal>SMP</literal>
(マルチプロセッサ)アーキテクチャにおいて使用される物理メモリ設計であり、メモリアクセス時間がプロセッサからのメモリの相対的な位置によって異なります。<literal>NUMA</literal>では、プロセッサは専用のローカルメモリに、非ローカルメモリ、つまり別のプロセッサにローカルなメモリや複数のプロセッサで共有されているメモリよりも高速にアクセスできます。</para>
<section xml:id="id-identifying-numa-nodes">
<title>NUMAノードの特定</title>
<para><literal>NUMA</literal>ノードを特定するには、システムで次のコマンドを使用します。</para>
<screen language="shell" linenumbering="unnumbered">$ lscpu | grep NUMA
NUMA node(s):                       1
NUMA node0 CPU(s):                  0-63</screen>
<note>
<para>この例では、<literal>NUMA</literal>ノードが1つだけあり、64個の<literal>CPU</literal>が表示されています。</para>
<para><literal>NUMA</literal>は<literal>BIOS</literal>で有効にする必要があります。<literal>dmesg</literal>にブート時のNUMA初期化レコードがない場合、カーネルリングバッファ内の<literal>NUMA</literal>関連のメッセージが上書きされた可能性があります。</para>
</note>
</section>
</section>
<section xml:id="metal-lb-configuration">
<title>MetalLB</title>
<para><literal>MetalLB</literal>は、ベアメタルKubernetesクラスタ用のロードバランサの実装であり、<literal>L2</literal>や<literal>BGP</literal>などの標準ルーティングプロトコルをアドバタイズプロトコルとして使用します。ベアメタル環境ではKubernetesサービスタイプ<literal>LoadBalancer</literal>を使用する必要があるため、Kubernetesクラスタ内のサービスを外部に公開するために使用できるのは、ネットワークロードバランサです。</para>
<para><literal>RKE2</literal>クラスタで<literal>MetalLB</literal>を有効にするには、次の手順を実行する必要があります。</para>
<itemizedlist>
<listitem>
<para>次のコマンドを使用して<literal>MetalLB</literal>をインストールします。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply &lt;&lt;EOF -f
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: metallb
  namespace: kube-system
spec:
  repo: https://metallb.github.io/metallb/
  chart: metallb
  targetNamespace: metallb-system
---
apiVersion: helm.cattle.io/v1
kind: HelmChart
metadata:
  name: endpoint-copier-operator
  namespace: kube-system
spec:
  repo: https://suse-edge.github.io/endpoint-copier-operator
  chart: endpoint-copier-operator
  targetNamespace: endpoint-copier-operator
EOF</screen>
<itemizedlist>
<listitem>
<para><literal>IpAddressPool</literal>および<literal>L2advertisement</literal>の設定を作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: kubernetes-vip-ip-pool
  namespace: metallb-system
spec:
  addresses:
    - 10.168.200.98/32
  serviceAllocation:
    priority: 100
    namespaces:
      - default
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ip-pool-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - kubernetes-vip-ip-pool</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>を公開するためのエンドポイントサービスを作成します。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: kubernetes-vip
  namespace: default
spec:
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rke2-api
    port: 9345
    protocol: TCP
    targetPort: 9345
  - name: k8s-api
    port: 6443
    protocol: TCP
    targetPort: 6443
  sessionAffinity: None
  type: LoadBalancer</screen>
<itemizedlist>
<listitem>
<para><literal>VIP</literal>が作成され、<literal>MetalLB</literal>のPodが実行中であることを確認します。</para>
</listitem>
</itemizedlist>
<screen language="shell" linenumbering="unnumbered">$ kubectl get svc -n default
$ kubectl get pods -n default</screen>
</section>
<section xml:id="private-registry">
<title>プライベートレジストリ設定</title>
<para><literal>Containerd</literal>をプライベートレジストリに接続するように設定し、そのプライベートレジストリを使用して各ノードにプライベートイメージをプルできます。</para>
<para>起動時に、<literal>RKE2</literal>は、<literal>registries.yaml</literal>ファイルが<literal>/etc/rancher/rke2/</literal>に存在するかどうかを確認し、このファイルで定義されたレジストリを使用するように<literal>containerd</literal>に指示します。プライベートレジストリを使用するには、このファイルを、レジストリを使用する各ノードにルートとして作成します。</para>
<para>プライベートレジストリを追加するには、ファイル<literal>/etc/rancher/rke2/registries.yaml</literal>を作成して次の内容を設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    auth:
      username: xxxxxx # this is the registry username
      password: xxxxxx # this is the registry password
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>または、認証を使用しない場合は次のように設定します。</para>
<screen language="yaml" linenumbering="unnumbered">mirrors:
  docker.io:
    endpoint:
      - "https://registry.example.com:5000"
configs:
  "registry.example.com:5000":
    tls:
      cert_file:            # path to the cert file used to authenticate to the registry
      key_file:             # path to the key file for the certificate used to authenticate to the registry
      ca_file:              # path to the ca file used to verify the registry's certificate
      insecure_skip_verify: # may be set to true to skip verifying the registry's certificate</screen>
<para>レジストリの変更を有効にするには、ノード上でRKE2を起動する前にこのファイルを設定するか、または設定した各ノードでRKE2を再起動します。</para>
<note>
<para>詳細については、「<link
xl:href="https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file">Containerd
Registry Configuration | RKE2 (Containerdのレジストリ設定 | RKE2)</link>」を確認してください。</para>
</note>
</section>
</chapter>
<chapter xml:id="atip-automated-provisioning">
<title>完全に自動化されたダイレクトネットワークプロビジョニング</title>
<section xml:id="id-introduction-3">
<title>はじめに</title>
<para>ダイレクトネットワークプロビジョニングは、ダウンストリームクラスタのプロビジョニングを自動化できる機能です。この機能は、プロビジョニングするダウンストリームクラスタが多数あり、そのプロセスを自動化したい場合に便利です。</para>
<para>管理クラスタ(<xref linkend="atip-management-cluster"/>)は、次のコンポーネントのデプロイメントを自動化します。</para>
<itemizedlist>
<listitem>
<para><literal>SUSE Linux Enterprise Micro RT</literal>
(OS)。ユースケースに応じて、ネットワーキング、ストレージ、ユーザ、カーネル引数などの設定をカスタマイズできます。</para>
</listitem>
<listitem>
<para><literal>RKE2</literal>
(Kubernetesクラスタ)。デフォルトの<literal>CNI</literal>プラグインは<literal>Cilium</literal>です。ユースケースに応じて、特定の<literal>CNI</literal>プラグイン(<literal>Cilium+Multus</literal>など)を使用できます。</para>
</listitem>
<listitem>
<para><literal>Longhorn</literal> (ストレージソリューション)。</para>
</listitem>
<listitem>
<para><literal>NeuVector</literal> (セキュリティソリューション)。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>。高可用性マルチノードクラスタのロードバランサとして使用できます。</para>
</listitem>
</itemizedlist>
<note>
<para><literal>SUSE Linux Enterprise Micro</literal>の詳細については<xref
linkend="components-slmicro"/>を、<literal>RKE2</literal>の詳細については<xref
linkend="components-rke2"/>を、<literal>Longhorn</literal>の詳細については<xref
linkend="components-longhorn"/>を、<literal>NeuVector</literal>の詳細については<xref
linkend="components-neuvector"/>をそれぞれ参照してください。</para>
</note>
<para>以降のセクションでは、さまざまなダイレクトネットワークプロビジョニングワークフローと、プロビジョニングプロセスに追加できる機能について説明します。</para>
<itemizedlist>
<listitem>
<para><xref linkend="eib-edge-image-connected"/></para>
</listitem>
<listitem>
<para><xref linkend="eib-edge-image-airgap"/></para>
</listitem>
<listitem>
<para><xref linkend="single-node"/></para>
</listitem>
<listitem>
<para><xref linkend="multi-node"/></para>
</listitem>
<listitem>
<para><xref linkend="advanced-network-configuration"/></para>
</listitem>
<listitem>
<para><xref linkend="add-telco"/></para>
</listitem>
<listitem>
<para><xref linkend="atip-private-registry"/></para>
</listitem>
<listitem>
<para><xref linkend="airgap-deployment"/></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="eib-edge-image-connected">
<title>接続シナリオのダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>ほとんどの設定はEdge Image
Builderを使用して行うことができますが、このガイドではダウンストリームクラスタをセットアップするために必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-connected-scenarios">
<title>接続シナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージ<literal>SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</literal>を<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-connected-scenarios">
<title>接続シナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>custom/scriptsディレクトリには、初回ブート時に実行するスクリプトが含まれます。現在、デプロイメントのOSルートパーティションのサイズを変更するには、<literal>01-fix-growfs.sh</literal>スクリプトが必要です。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ scripts/
        └ 01-fix-growfs.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-2">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-config.yaml</literal>ファイルは、ダウンストリームクラスタイメージの主要な設定ファイルです。次に、Metal<superscript>3</superscript>を介したデプロイメントの最小例を示します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${USERKEY1}</screen>
<para><literal>${ROOT_PASSWORD}</literal>はルートユーザの暗号化パスワードで、テスト/デバッグに役立ちます。このパスワードは、<literal>openssl
passwd -6 PASSWORD</literal>コマンドで生成できます。</para>
<para>運用環境では、<literal>${USERKEY1}</literal>を実際のSSHキーに置き換えて、usersブロックに追加できるSSHキーを使用することをお勧めします。</para>
<note>
<para><literal>net.ifnames=1</literal>は、<link
xl:href="https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html">Predictable
Network Interface Naming</link>を有効にします。</para>
<para>これはmetal3チャートのデフォルト設定と一致しますが、この設定は、設定されたチャートの<literal>predictableNicNames</literal>の値と一致する必要があります。</para>
<para>また、<literal>ignition.platform.id=openstack</literal>は必須であり、この引数がないと、Metal<superscript>3</superscript>の自動化フローでIgnitionによるSLEMicroの設定が失敗することにも注意してください。</para>
</note>
</section>
<section xml:id="add-custom-script-growfs">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト<literal>custom/scripts/01-fix-growfs.sh</literal>が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
<note>
<para>同じアプローチを使用して、プロビジョニングプロセス中に実行する独自のカスタムスクリプトを追加します。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
<para>この回避策に関連するバグは、<link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1217430">https://bugzilla.suse.com/show_bug.cgi?id=1217430</link>です。</para>
</note>
</section>
<section xml:id="add-telco-feature-eib">
<title>通信ワークロードの追加設定</title>
<para><literal>dpdk</literal>、<literal>sr-iov</literal>、<literal>FEC</literal>などの通信機能を有効にするには、次の例に示すように追加のパッケージが必要な場合があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: 1.0
image:
  imageType: RAW
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
  outputImageName: eibimage-slemicro55rt-telco.raw
operatingSystem:
  kernelArgs:
    - ignition.platform.id=openstack
    - net.ifnames=1
  systemd:
    disable:
      - rebootmgr
  users:
    - username: root
      encryptedPassword: ${ROOT_PASSWORD}
      sshKeys:
      - ${user1Key1}
  packages:
    packageList:
      - jq
      - dpdk22
      - dpdk22-tools
      - libdpdk-23
      - pf-bb-config
    additionalRepos:
      - url: https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SLEMicro5.5/
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}</screen>
<para>ここで、<literal>${SCC_REGISTRATION_CODE}</literal>は<link
xl:href="https://scc.suse.com/">SUSE Customer
Center</link>からコピーした登録コードです。また、パッケージリストには通信事業者プロファイル用の最小限のパッケージが含まれています。<literal>pf-bb-config</literal>パッケージを使用するには(<literal>FEC</literal>機能とドライバへのバインドを有効にするには)、<literal>additionalRepos</literal>ブロックを含めて<literal>SUSE
Edge Telco</literal>リポジトリを追加する必要があります。</para>
</section>
<section xml:id="add-network-eib">
<title>高度なネットワーク設定のための追加スクリプト</title>
<para><xref
linkend="advanced-network-configuration"/>で説明されている静的IPや、より高度なネットワーキングシナリオを設定する必要がある場合、次の追加設定が必要です。</para>
<para><literal>network</literal>フォルダに、次の<literal>configure-network.sh</literal>ファイルを作成します。
このファイルは、初回ブート時に設定ドライブデータを使用し、<link
xl:href="https://github.com/suse-edge/nm-configurator">NM
Configuratorツール</link>を使用してホストネットワーキングを設定します。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

set -eux

# Attempt to statically configure a NIC in the case where we find a network_data.json
# In a configuration drive

CONFIG_DRIVE=$(blkid --label config-2 || true)
if [ -z "${CONFIG_DRIVE}" ]; then
  echo "No config-2 device found, skipping network configuration"
  exit 0
fi

mount -o ro $CONFIG_DRIVE /mnt

NETWORK_DATA_FILE="/mnt/openstack/latest/network_data.json"

if [ ! -f "${NETWORK_DATA_FILE}" ]; then
  umount /mnt
  echo "No network_data.json found, skipping network configuration"
  exit 0
fi

DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\n' | grep '\"metal3-name\"' | sed 's/.*\"metal3-name\": \"\(.*\)\"/\1/')

mkdir -p /tmp/nmc/{desired,generated}
cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml
umount /mnt

./nmc generate --config-dir /tmp/nmc/desired --output-dir /tmp/nmc/generated
./nmc apply --config-dir /tmp/nmc/generated</screen>
</section>
</section>
<section xml:id="id-image-creation-2">
<title>イメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-config.yaml</screen>
<para>これにより、上記の定義に基づいて<literal>eibimage-slemicro55rt-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタのドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>と呼びます。</para>
</section>
</section>
<section xml:id="eib-edge-image-airgap">
<title>エアギャップシナリオ用のダウンストリームクラスタイメージの準備</title>
<para>Edge Image Builder (<xref
linkend="components-eib"/>)を使用して、ダウンストリームクラスタホスト上にプロビジョニングされる、変更されたSLEMicroベースイメージを準備します。</para>
<para>設定の多くはEdge Image
Builderを使用して行うことができますが、このガイドではエアギャップシナリオ用のダウンストリームクラスタの設定に必要な最小限の設定について説明します。</para>
<section xml:id="id-prerequisites-for-air-gap-scenarios">
<title>エアギャップシナリオの前提条件</title>
<itemizedlist>
<listitem>
<para>Edge Image Builderを実行するには、<link
xl:href="https://podman.io">Podman</link>や<link
xl:href="https://rancherdesktop.io">Rancher Desktop</link>などのコンテナランタイムが必要です。</para>
</listitem>
<listitem>
<para>ゴールデンイメージ<literal>SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw</literal>を<link
xl:href="https://scc.suse.com/">SUSE Customer Center</link>または<link
xl:href="https://www.suse.com/download/sle-micro/">SUSEダウンロードページ</link>からダウンロードする必要があります。</para>
</listitem>
<listitem>
<para>コンテナイメージが必要なSR-IOVなどのワークロードを使用する場合、ローカルのプライベートレジストリをデプロイして設定済みである必要があります(TLSまたは認証、あるいはその両方を使用/不使用)。このレジストリを使用して、イメージとHelmチャートOCIイメージを保存します。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-image-configuration-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの設定</title>
<para>Edge Image
Builderを実行すると、そのホストからディレクトリがマウントされるため、ターゲットイメージの定義に使用する設定ファイルを保存するディレクトリ構造を作成する必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>downstream-cluster-airgap-config.yaml</literal>はイメージ定義ファイルです。詳細については、<xref
linkend="quickstart-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para>ダウンロードされたベースイメージは<literal>xz</literal>で圧縮されているので、<literal>unxz</literal>で展開し、<literal>base-images</literal>フォルダの下にコピー/移動する必要があります。</para>
</listitem>
<listitem>
<para><literal>network</literal>フォルダはオプションです。詳細については、<xref
linkend="add-network-eib"/>を参照してください。</para>
</listitem>
<listitem>
<para><literal>custom/scripts</literal>ディレクトリには、初回ブート時に実行するスクリプトが含まれています。現在は、デプロイメントのOSルートパーティションのサイズを変更するために、スクリプト<literal>01-fix-growfs.sh</literal>が必要です。エアギャップシナリオでは、イメージ作成プロセス中にイメージを正しい場所にコピーするために、スクリプト<literal>02-airgap.sh</literal>が必要です。</para>
</listitem>
<listitem>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にそのイメージにコピーする<literal>rke2</literal>イメージと<literal>cni</literal>イメージが含まれています。</para>
</listitem>
</itemizedlist>
<screen language="console" linenumbering="unnumbered">├── downstream-cluster-airgap-config.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw
├── network/
|   └ configure-network.sh
└── custom/
    └ files/
    |   └ install.sh
    |   └ rke2-images-cilium.linux-amd64.tar.zst
    |   └ rke2-images-core.linux-amd64.tar.zst
    |   └ rke2-images-multus.linux-amd64.tar.zst
    |   └ rke2-images.linux-amd64.tar.zst
    |   └ rke2.linux-amd64.tar.zst
    |   └ sha256sum-amd64.txt
    └ scripts/
        └ 01-fix-growfs.sh
        └ 02-airgap.sh</screen>
<section xml:id="id-downstream-cluster-image-definition-file-3">
<title>ダウンストリームクラスタイメージ定義ファイル</title>
<para><literal>downstream-cluster-airgap-config.yaml</literal>ファイルは、ダウンストリームクラスタ用のメイン設定ファイルです。その内容については、前のセクション(<xref
linkend="add-telco-feature-eib"/>)で説明されています。</para>
</section>
<section xml:id="id-growfs-script-2">
<title>Growfsスクリプト</title>
<para>現在、プロビジョニング後の初回ブート時にディスクサイズに合わせてファイルシステムを拡張するには、カスタムスクリプト<literal>custom/scripts/01-fix-growfs.sh</literal>が必要です。<literal>01-fix-growfs.sh</literal>スクリプトには次の情報が含まれます。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash
growfs() {
  mnt="$1"
  dev="$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE --noheadings)"
  # /dev/sda3 -&gt; /dev/sda, /dev/nvme0n1p3 -&gt; /dev/nvme0n1
  parent_dev="/dev/$(lsblk --nodeps -rno PKNAME "${dev}")"
  # Last number in the device name: /dev/nvme0n1p42 -&gt; 42
  partnum="$(echo "${dev}" | sed 's/^.*[^0-9]\([0-9]\+\)$/\1/')"
  ret=0
  growpart "$parent_dev" "$partnum" || ret=$?
  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1
  /usr/lib/systemd/systemd-growfs "$mnt"
}
growfs /</screen>
</section>
<section xml:id="id-air-gap-script">
<title>エアギャップスクリプト</title>
<para>イメージ作成プロセス中にイメージを正しい場所にコピーするために、次のスクリプト<literal>custom/scripts/02-airgap.sh</literal>が必要です。</para>
<screen language="shell" linenumbering="unnumbered">#!/bin/bash

# create the folder to extract the artifacts there
mkdir -p /opt/rke2-artifacts
mkdir -p /var/lib/rancher/rke2/agent/images

# copy the artifacts
cp install.sh /opt/
cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt /opt/rke2-artifacts/</screen>
</section>
<section xml:id="id-custom-files-for-air-gap-scenarios">
<title>エアギャップシナリオのカスタムファイル</title>
<para><literal>custom/files</literal>ディレクトリには、イメージ作成プロセス中にそのイメージにコピーする<literal>rke2</literal>イメージと<literal>cni</literal>イメージが含まれています。イメージを簡単に生成するには、次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-rke2-images.sh">スクリプト</link>と、<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-release-rke2-images.txt">こちら</link>にあるイメージのリストを使用してローカルでイメージを準備し、<literal>custom/files</literal>に含める必要があるアーティファクトを生成します。また、最新の<literal>rke2-install</literal>スクリプトを<link
xl:href="https://get.rke2.io/">こちら</link>からダウンロードすることもできます。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-rke2-images.sh -o custom/files -l ~/edge-release-rke2-images.txt</screen>
<para>イメージをダウンロードした後、ディレクトリ構造は次のようになるはずです。</para>
<screen language="console" linenumbering="unnumbered">└── custom/
    └ files/
        └ install.sh
        └ rke2-images-cilium.linux-amd64.tar.zst
        └ rke2-images-core.linux-amd64.tar.zst
        └ rke2-images-multus.linux-amd64.tar.zst
        └ rke2-images.linux-amd64.tar.zst
        └ rke2.linux-amd64.tar.zst
        └ sha256sum-amd64.txt</screen>
</section>
<section xml:id="preload-private-registry">
<title>エアギャップシナリオおよびSR-IOV (オプション)に必要なイメージのプライベートレジストリへのプリロード</title>
<para>エアギャップシナリオまたはその他のワークロードイメージでSR-IOVを使用する場合、次の各手順に従って、ローカルのプライベートレジストリにイメージをプリロードする必要があります。</para>
<itemizedlist>
<listitem>
<para>HelmチャートOCIイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
<listitem>
<para>必要な残りのイメージをダウンロードして抽出し、プライベートレジストリにプッシュする</para>
</listitem>
</itemizedlist>
<para>次のスクリプトを使用して、イメージをダウンロードして抽出し、プライベートレジストリにプッシュできます。これからSR-IOVイメージをプリロードする例を示しますが、その他のカスタムイメージも同じ方法でプリロードすることができます。</para>
<orderedlist numeration="arabic">
<listitem>
<para>SR-IOVのHelmチャートOCIイメージのプリロード:</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>必要なHelmチャートOCIイメージが含まれるリストを作成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-helm-oci-artifacts.txt &lt;&lt;EOF
edge/sriov-network-operator-chart:1.2.2
edge/sriov-crd-chart:1.2.2
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-oci-artefacts.sh">スクリプト</link>と前の手順で作成したリストを使用して、ローカルのtarballファイルを生成します。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s registry.suse.com
Pulled: registry.suse.com/edge/sriov-network-operator-chart:1.2.2
Pulled: registry.suse.com/edge/sriov-crd-chart:1.2.2
a edge-release-oci-tgz-20240705
a edge-release-oci-tgz-20240705/sriov-network-operator-chart-1.2.2.tgz
a edge-release-oci-tgz-20240705/sriov-crd-chart-1.2.2.tgz</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-oci-artefacts.sh">スクリプト</link>を使用してtarballファイルをプライベートレジストリ(例:
<literal>myregistry:5000</literal>)にアップロードし、前の手順でダウンロードしたHelmチャートOCIイメージをレジストリにプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-oci-tgz-20240705.tgz
$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
<listitem>
<para>SR-IOVに必要な残りのイメージをプリロードします。</para>
<orderedlist numeration="loweralpha">
<listitem>
<para>ここでは、通信ワークロードのために「sr-iov」コンテナイメージを含める必要があります(例: 参考として、これは<link
xl:href="https://github.com/suse-edge/charts/blob/release-3.0/charts/sriov-network-operator/1.2.2%2Bup0.1.0/values.yaml">helmチャート値</link>から取得できます)。</para>
<screen language="shell" linenumbering="unnumbered">$ cat &gt; edge-release-images.txt &lt;&lt;EOF
rancher/hardened-sriov-network-operator:v1.2.0-build20240327
rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327
rancher/hardened-sriov-cni:v1.2.0-build20240327
rancher/hardened-ib-sriov-cni:v1.2.0-build20240327
rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327
rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327
rancher/hardened-sriov-network-webhook:v1.2.0-build20240327
EOF</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-images.sh">スクリプト</link>と前の手順で作成したリストを使用し、必要なイメージが含まれるtarballファイルをローカルで生成する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ ./edge-save-images.sh -al ./edge-release-images.txt -s registry.suse.com
Pulled: registry.suse.com/rancher/hardened-sriov-network-operator:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-cni:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-ib-sriov-cni:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327
Pulled: registry.suse.com/rancher/hardened-sriov-network-webhook:v1.2.0-build20240327
a edge-release-images-tgz-20240705
a edge-release-images-tgz-20240705/hardened-sriov-network-operator-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-config-daemon-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-cni-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-ib-sriov-cni-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-device-plugin-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-resources-injector-v1.2.0-build20240327.tar.gz
a edge-release-images-tgz-20240705/hardened-sriov-network-webhook-v1.2.0-build20240327.tar.gz</screen>
</listitem>
<listitem>
<para>次の<link
xl:href="https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-images.sh">スクリプト</link>を使用してtarballファイルをプライベートレジストリ(例:
<literal>myregistry:5000</literal>)にアップロードし、前の手順でダウンロードしたイメージをプライベートレジストリにプリロードします。</para>
<screen language="shell" linenumbering="unnumbered">$ tar zxvf edge-release-images-tgz-20240705.tgz
$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r myregistry:5000</screen>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="id-image-creation-for-air-gap-scenarios">
<title>エアギャップシナリオのイメージの作成</title>
<para>これまでのセクションに従ってディレクトリ構造を準備したら、次のコマンドを実行してイメージを構築します。</para>
<screen language="shell" linenumbering="unnumbered">podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.2 \
 build --definition-file downstream-cluster-airgap-config.yaml</screen>
<para>これにより、上記の定義に基づいて<literal>eibimage-slemicro55rt-telco.raw</literal>という名前の出力ISOイメージファイルが作成されます。</para>
<para>その後、この出力イメージをWebサーバ経由で利用できるようにする必要があります。その際、管理クラスタドキュメントを使用して有効にしたメディアサーバコンテナ(<xref
linkend="metal3-media-server"/>)か、ローカルにアクセス可能な他のサーバのいずれかを使用します。以下の例では、このサーバを<literal>imagecache.local:8080</literal>として参照します。</para>
</section>
</section>
<section xml:id="single-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローについて説明します。これは、ダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化するために用いるワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automated-singlenode1.png" width=""/>
</imageobject>
<textobject><phrase>atip自動化シングルノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<para>ダイレクトネットワークプロビジョニングを使用してシングルノードのダウンストリームクラスタのプロビジョニングを自動化する手順は2種類です。</para>
<orderedlist numeration="arabic">
<listitem>
<para>ベアメタルホストを登録して、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>ベアメタルホストをプロビジョニングして、オペレーティングシステムとKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para xml:id="enroll-bare-metal-host"><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、新しいベアメタルホストを管理クラスタに登録してプロビジョニングできるようにします。そのためには、次のファイル(<literal>bmh-example.yaml</literal>)を管理クラスタ内に作成して、使用する<literal>BMC</literal>資格情報と登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_USERNAME}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_PASSWORD}</literal> —
新しいベアメタルホストの<literal>BMC</literal>のパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_MAC}</literal> — 使用する新しいベアメタルホストの<literal>MAC</literal>アドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_ADDRESS}</literal> —
ベアメタルホストの<literal>BMC</literal>の<literal>URL</literal> (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行し、管理クラスタで新しいベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para xml:id="single-node-provision"><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>ベアメタルホストが登録されて使用可能になったら、次に、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定します。そのためには、次の情報を使用して管理クラスタで以下のファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(<literal>capi-provisioning-example.yaml</literal>は、以下のブロックを結合して生成できます)。</para>
<note>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</note>
<para>次のブロックはクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してネットワーキングを設定できます。また、ここにはコントロールプレーンオブジェクトと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)オブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: single-node-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: single-node-cluster</screen>
<para><literal>Metal3Cluster</literal>オブジェクトには、設定するコントロールプレーンのエンドポイント(<literal>${DOWNSTREAM_CONTROL_PLANE_IP}</literal>を置き換える)と、<literal>noCloudProvider</literal>(ベアメタルノードを使用しているため)を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: single-node-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${DOWNSTREAM_CONTROL_PLANE_IP}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーンの設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。また、ここには使用するレプリカの数(ここでは1)と、使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)に関する情報が含まれています。agentConfigブロックには、使用する<literal>Ignition</literal>の形式と、使用する<literal>additionalUserData</literal>が含まれます。これを使用して、<literal>RKE2</literal>ノードに<literal>rke2-preinstall.service</literal>という名前のsystemdサービスを設定し、プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。最後の情報ブロックには、使用するKubernetesバージョンが含まれています。<literal>${RKE2_VERSION}</literal>は使用する<literal>RKE2</literal>のバージョンであり、この値は置き換えます(例:
<literal>v1.28.9+rke2r1</literal>)。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行して、新しいベアメタルホストのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="multi-node">
<title>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード)</title>
<para>このセクションでは、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>をロードバランサ戦略として使用して、マルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローについて説明します。これはダウンストリームクラスタのプロビジョニングを自動化する最もシンプルな方法です。次の図は、ダイレクトネットワークプロビジョニングと<literal>MetalLB</literal>を使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するためのワークフローを示しています。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して、ダウンストリームクラスタを設定するための最小限の設定で生成されたイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上にある管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">ワークフロー</emphasis></para>
<para>次の図は、ダイレクトネットワークプロビジョニングを使用してマルチノードのダウンストリームクラスタのプロビジョニングを自動化するために使用するワークフローを示しています。</para>
<informalfigure>
<mediaobject>
<imageobject> <imagedata fileref="atip-automate-multinode1.png" width=""/>
</imageobject>
<textobject><phrase>atip自動化マルチノード1</phrase></textobject>
</mediaobject>
</informalfigure>
<orderedlist numeration="arabic">
<listitem>
<para>3つのベアメタルホストを登録し、プロビジョニングプロセスで使用できるようにする。</para>
</listitem>
<listitem>
<para>3つのベアメタルホストをプロビジョニングし、オペレーティングシステムと、<literal>MetalLB</literal>を使用するKubernetesクラスタをインストールして設定する。</para>
</listitem>
</orderedlist>
<para><emphasis role="strong">ベアメタルホストの登録</emphasis></para>
<para>最初の手順では、管理クラスタに3つのベアメタルホストを登録してプロビジョニングできるようにします。そのためには、管理クラスタにファイル<literal>bmh-example-node1.yaml</literal>、<literal>bmh-example-node2.yaml</literal>、および<literal>bmh-example-node3.yaml</literal>を作成して、使用する<literal>BMC</literal>資格情報と、管理クラスタに登録する<literal>BaremetalHost</literal>オブジェクトを指定する必要があります。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para>1つのホストのプロセスについてのみ説明します。他の2つのノードにも同じ手順が適用されます。</para>
</listitem>
</itemizedlist>
</note>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: node1-example-credentials
type: Opaque
data:
  username: ${BMC_NODE1_USERNAME}
  password: ${BMC_NODE1_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: node1-example
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_NODE1_MAC}
  bmc:
    address: ${BMC_NODE1_ADDRESS}
    disableCertificateVerification: true
    credentialsName: node1-example-credentials</screen>
<para>各項目の内容は次のとおりです。</para>
<itemizedlist>
<listitem>
<para><literal>${BMC_NODE1_USERNAME}</literal> — 最初のベアメタルホストのBMCのユーザ名。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_PASSWORD}</literal> — 最初のベアメタルホストのBMCのパスワード。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_MAC}</literal> — 使用する最初のベアメタルホストのMACアドレス。</para>
</listitem>
<listitem>
<para><literal>${BMC_NODE1_ADDRESS}</literal> — 最初のベアメタルホストのBMCのURL (例:
<literal>redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/</literal>)。ハードウェアプロバイダに応じて使用できる各種のオプションの詳細については、<link
xl:href="https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md">こちらのリンク</link>を確認してください。</para>
</listitem>
</itemizedlist>
<para>ファイルを作成したら、管理クラスタで次のコマンドを実行して、管理クラスタへのベアメタルホストの登録を開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f bmh-example-node1.yaml
$ kubectl apply -f bmh-example-node2.yaml
$ kubectl apply -f bmh-example-node3.yaml</screen>
<para>新しいベアメタルホストオブジェクトが登録され、その状態が「Registering (登録中)」から「Inspecting
(検査中)」に変わり、「Available (使用可能)」になります。この変更は次のコマンドを使用して確認できます。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl get bmh -o wide</screen>
<note>
<para><literal>BaremetalHost</literal>オブジェクトは、<literal>BMC</literal>の資格情報が検証されるまでは「<literal>Registering
(登録中)</literal>」の状態です。資格情報の検証が完了すると、<literal>BaremetalHost</literal>オブジェクトの状態が「<literal>Inspecting
(検査中)</literal>」に変わります。この手順はハードウェアによっては多少時間がかかる可能性があります(最大で20分)。「Inspecting
(検査中)」のフェーズ中に、ハードウェア情報が取得されてKubernetesオブジェクトが更新されます。<literal>kubectl get bmh
-o yaml</literal>コマンドを使用して情報を確認してください。</para>
</note>
<para><emphasis role="strong">プロビジョニング手順</emphasis></para>
<para>3つのベアメタルホストが登録されて使用可能になったら、次の手順は、ベアメタルホストをプロビジョニングしてオペレーティングシステムとKubernetesクラスタをインストールして設定し、ロードバランサを作成して3つのベアメタルホストを管理することです。そのためには、次の情報を使用して管理クラスタにファイル<literal>capi-provisioning-example.yaml</literal>を作成する必要があります(capi-provisioning-example.yamlは、次のブロックを結合して生成できます)。</para>
<note>
<itemizedlist>
<listitem>
<para>実際の値に置き換える必要があるのは、<literal>$\{…​\}</literal>の中の値だけです。</para>
</listitem>
<listitem>
<para><literal>VIP</literal>アドレスは、どのノードにも割り当てられていない予約済みのIPアドレスであり、ロードバランサを設定するために使用されます。</para>
</listitem>
</itemizedlist>
</note>
<para>以下はクラスタ定義です。ここで、<literal>pods</literal>ブロックと<literal>services</literal>ブロックを使用してクラスタのネットワークを設定できます。また、ここにはコントロールプレーンと、使用するインフラストラクチャ(<literal>Metal<superscript>3</superscript></literal>プロバイダを使用)のオブジェクトへの参照も含まれています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/18
    services:
      cidrBlocks:
        - 10.96.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
    kind: RKE2ControlPlane
    name: multinode-cluster
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3Cluster
    name: multinode-cluster</screen>
<para><literal>Metal3Cluster</literal>オブジェクトには、予約済みの<literal>VIP</literal>アドレス(<literal>${DOWNSTREAM_VIP_ADDRESS}</literal>を置き換え)を使用して設定するコントロールプレーンのエンドポイントと、<literal>noCloudProvider</literal>
(3つのベアメタルノードを使用しているため)を指定しています。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3Cluster
metadata:
  name: multinode-cluster
  namespace: default
spec:
  controlPlaneEndpoint:
    host: ${EDGE_VIP_ADDRESS}
    port: 6443
  noCloudProvider: true</screen>
<para><literal>RKE2ControlPlane</literal>オブジェクトには、使用するコントロールプレーンの設定を指定し、<literal>Metal3MachineTemplate</literal>オブジェクトには、使用するコントロールプレーンのイメージを指定します。</para>
<itemizedlist>
<listitem>
<para>使用するレプリカの数(ここでは3)。</para>
</listitem>
<listitem>
<para>ロードバランサで使用するアドバタイズモード(<literal>address</literal>ではL2実装を使用)、および使用するアドレス(<literal>${EDGE_VIP_ADDRESS}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para>使用する<literal>CNI</literal>プラグイン(ここでは<literal>Cilium</literal>)と、<literal>VIP</literal>アドレスの設定に使用する<literal>tlsSan</literal>が含まれる<literal>serverConfig</literal>。</para>
</listitem>
<listitem>
<para>agentConfigブロックには、使用する<literal>Ignition</literal>の形式と、<literal>RKE2</literal>ノードに次のような情報を設定するために使用する<literal>additionalUserData</literal>が含まれています。</para>
<itemizedlist>
<listitem>
<para>プロビジョニングプロセス中にIronicの情報を使用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換える、<literal>rke2-preinstall.service</literal>という名前のsystemdサービス。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>と<literal>endpoint-copier-operator</literal>のインストールに使用するHelmチャートが含まれている<literal>storage</literal>ブロック。</para>
</listitem>
<listitem>
<para>使用する<literal>IPaddressPool</literal>と<literal>L2Advertisement</literal>が含まれている<literal>metalLB</literal>カスタムリソースファイル(<literal>${EDGE_VIP_ADDRESS}</literal>を<literal>VIP</literal>アドレスに置き換え)。</para>
</listitem>
<listitem>
<para><literal>MetalLB</literal>が<literal>VIP</literal>アドレスを管理するために使用する<literal>kubernetes-vip</literal>サービスの設定に使用する<literal>endpoint-svc.yaml</literal>ファイル。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>最後の情報ブロックには、使用するKubernetesバージョンが含まれています。<literal>${RKE2_VERSION}</literal>は、使用する<literal>RKE2</literal>のバージョンであり、この値は置き換えます(例:
<literal>v1.28.9+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: multinode-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: multinode-cluster-controlplane
  replicas: 3
  registrationMethod: "address"
  registrationAddress: ${EDGE_VIP_ADDRESS}
  serverConfig:
    cni: cilium
    tlsSan:
      - ${EDGE_VIP_ADDRESS}
      - https://${EDGE_VIP_ADDRESS}.sslip.io
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: endpoint-copier-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/endpoint-copier-operator-chart
                    targetNamespace: endpoint-copier-operator
                    version: 0.2.0
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: metallb
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/metallb-chart
                    targetNamespace: metallb-system
                    version: 0.14.3
                    createNamespace: true

            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: metallb.io/v1beta1
                  kind: IPAddressPool
                  metadata:
                    name: kubernetes-vip-ip-pool
                    namespace: metallb-system
                  spec:
                    addresses:
                      - ${EDGE_VIP_ADDRESS}/32
                    serviceAllocation:
                      priority: 100
                      namespaces:
                        - default
                      serviceSelectors:
                        - matchExpressions:
                          - {key: "serviceType", operator: In, values: [kubernetes-vip]}
                  ---
                  apiVersion: metallb.io/v1beta1
                  kind: L2Advertisement
                  metadata:
                    name: ip-pool-l2-adv
                    namespace: metallb-system
                  spec:
                    ipAddressPools:
                      - kubernetes-vip-ip-pool
            - path: /var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: Service
                  metadata:
                    name: kubernetes-vip
                    namespace: default
                    labels:
                      serviceType: kubernetes-vip
                  spec:
                    ports:
                    - name: rke2-api
                      port: 9345
                      protocol: TCP
                      targetPort: 9345
                    - name: k8s-api
                      port: 6443
                      protocol: TCP
                      targetPort: 6443
                    type: LoadBalancer
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "Node-multinode-cluster"</screen>
<para><literal>Metal3MachineTemplate</literal>オブジェクトには次の情報を指定します。</para>
<itemizedlist>
<listitem>
<para>テンプレートへの参照として使用する<literal>dataTemplate</literal>。</para>
</listitem>
<listitem>
<para>登録プロセス中に作成されたラベルとの一致に使用する<literal>hostSelector</literal>。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で<literal>EIB</literal>を使用して生成されたイメージへの参照として使用する<literal>image</literal>、およびそのイメージを検証するために使用する<literal>checksum</literal>と<literal>checksumType</literal>。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: multinode-cluster-controlplane
  namespace: default
spec:
  template:
    spec:
      dataTemplate:
        name: multinode-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw</screen>
<para><literal>Metal3DataTemplate</literal>オブジェクトには、ダウンストリームクラスタの<literal>metaData</literal>を指定します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: single-node-cluster-controlplane-template
  namespace: default
spec:
  clusterName: single-node-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<para>前のブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行して、新しい3つのベアメタルホストのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="advanced-network-configuration">
<title>高度なネットワーク設定</title>
<para>ダイレクトネットワークプロビジョニングのワークフローでは、静的IP、ボンディング、VLANなどのダウンストリームクラスタのネットワーク設定を行うことができます。</para>
<para>次の各セクションでは、高度なネットワーク設定を使用してダウンストリームクラスタをプロビジョニングできるようにするために必要な追加手順について説明します。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>このセクション(<xref
linkend="add-network-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージにネットワークフォルダとスクリプトを含める必要があります。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>次の2つのセクションをベースとして使用し、ホストを登録してプロビジョニングします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>高度なネットワーク設定を有効にするために必要な変更は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>登録手順: 設定に使用する<literal>networkData</literal>に関する情報(例:
ダウンストリームクラスタの静的<literal>IP</literal>と<literal>VLAN</literal>)を格納したシークレットが含まれる新しいサンプルファイル</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: controlplane-0-networkdata
type: Opaque
stringData:
  networkData: |
    interfaces:
    - name: ${CONTROLPLANE_INTERFACE}
      type: ethernet
      state: up
      mtu: 1500
      mac-address: "${CONTROLPLANE_MAC}"
      ipv4:
        address:
        - ip:  "${CONTROLPLANE_IP}"
          prefix-length: "${CONTROLPLANE_PREFIX}"
        enabled: true
        dhcp: false
    - name: floating
      type: vlan
      state: up
      vlan:
        base-iface: ${CONTROLPLANE_INTERFACE}
        id: ${VLAN_ID}
    dns-resolver:
      config:
        server:
        - "${DNS_SERVER}"
    routes:
      config:
      - destination: 0.0.0.0/0
        next-hop-address: "${CONTROLPLANE_GATEWAY}"
        next-hop-interface: ${CONTROLPLANE_INTERFACE}</screen>
<para>このファイルには、ダウンストリームクラスタに高度なネットワーク設定(例:
<literal>静的IP</literal>や<literal>VLAN</literal>)を行う場合に使用する<literal>nmstate</literal>形式の<literal>networkData</literal>が含まれています。ご覧のとおり、この例は、静的IPを使用してインタフェースを有効にするための設定と、ベースインタフェースを使用してVLANを有効にするための設定を示しています。その他の<literal>nmstate</literal>の例を定義して、ダウンストリームクラスタのネットワークを特定の要件に適合するように設定できます。ここでは、次の変数を実際の値に置き換える必要があります。</para>
<itemizedlist>
<listitem>
<para><literal>${CONTROLPLANE1_INTERFACE}</literal> —
エッジクラスタに使用するコントロールプレーンインタフェース(例: <literal>eth0</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_IP}</literal> —
エッジクラスタのエンドポイントとして使用するIPアドレス(kubeapiサーバのエンドポイントと一致する必要があります)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_PREFIX}</literal> — エッジクラスタに使用するCIDR (例:
<literal>/24</literal>または<literal>255.255.255.0</literal>を使用する場合には<literal>24</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_GATEWAY}</literal> — エッジクラスタに使用するゲートウェイ(例:
<literal>192.168.100.1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${CONTROLPLANE1_MAC}</literal> — コントロールプレーンインタフェースに使用するMACアドレス(例:
<literal>00:0c:29:3e:3e:3e</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DNS_SERVER}</literal> — エッジクラスタに使用するDNS (例:
<literal>192.168.100.2</literal>)。</para>
</listitem>
<listitem>
<para><literal>${VLAN_ID}</literal> — エッジクラスタに使用するVLAN ID (例:
<literal>100</literal>)。</para>
</listitem>
</itemizedlist>
<para>また、管理クラスタに登録するには、ファイルの末尾にある<literal>BaremetalHost</literal>オブジェクトに、<literal>preprovisioningNetworkDataName</literal>を使用するシークレットへの参照が必要です。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: example-demo-credentials
type: Opaque
data:
  username: ${BMC_USERNAME}
  password: ${BMC_PASSWORD}
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: flexran-demo
  labels:
    cluster-role: control-plane
spec:
  online: true
  bootMACAddress: ${BMC_MAC}
  rootDeviceHints:
    deviceName: /dev/nvme0n1
  bmc:
    address: ${BMC_ADDRESS}
    disableCertificateVerification: true
    credentialsName: example-demo-credentials
  preprovisioningNetworkDataName: controlplane-0-networkdata</screen>
<note>
<para>マルチノードクラスタをデプロイする必要がある場合は、同じプロセスをその他のノードに対して実行する必要があります。</para>
</note>
<itemizedlist>
<listitem>
<para>プロビジョニング手順:
ネットワークデータに関連する情報のブロックを削除する必要があります。その理由は、ネットワークデータの設定は、プラットフォームによってシークレット<literal>controlplane-0-networkdata</literal>に組み込まれるためです。</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3DataTemplate
metadata:
  name: multinode-cluster-controlplane-template
  namespace: default
spec:
  clusterName: multinode-cluster
  metaData:
    objectNames:
      - key: name
        object: machine
      - key: local-hostname
        object: machine
      - key: local_hostname
        object: machine</screen>
<note>
<para><literal>Metal3DataTemplate</literal>、<literal>networkData</literal>、および<literal>Metal3
IPAM</literal>は現在サポートされていません。静的シークレットを介した設定のみが完全にサポートされています。</para>
</note>
</section>
<section xml:id="add-telco">
<title>通信機能(DPDK、SR-IOV、CPUの分離、Huge Page、NUMAなど)</title>
<para>ダイレクトネットワークプロビジョニングのワークフローでは、ダウンストリームクラスタで使用する通信機能を自動化して、そのサーバ上で通信ワークロードを実行できます。</para>
<para><emphasis role="strong">要件</emphasis></para>
<itemizedlist>
<listitem>
<para>こちらのセクション(<xref
linkend="add-telco-feature-eib"/>)に従って、<literal>EIB</literal>を使用して生成したイメージに特定の通信機能を含める必要がある。</para>
</listitem>
<listitem>
<para>前のセクション(<xref
linkend="eib-edge-image-connected"/>)で説明されているように、<literal>EIB</literal>を使用して生成したイメージが、こちらのセクション(<xref
linkend="metal3-media-server"/>)で設定した正確なパス上の管理クラスタに配置されている。</para>
</listitem>
<listitem>
<para>管理サーバが作成されていて、次の各セクションで使用できるようになっている。詳細については、管理クラスタに関するセクション<xref
linkend="atip-management-cluster"/>を参照してください。</para>
</listitem>
</itemizedlist>
<para><emphasis role="strong">設定</emphasis></para>
<para>次の2つのセクションをベースとして使用し、ホストを登録してプロビジョニングします。</para>
<itemizedlist>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(シングルノード) (<xref
linkend="single-node"/>)</para>
</listitem>
<listitem>
<para>ダイレクトネットワークプロビジョニングを使用したダウンストリームクラスタのプロビジョニング(マルチノード) (<xref
linkend="multi-node"/>)</para>
</listitem>
</itemizedlist>
<para>このセクションで説明する通信機能を次に示します。</para>
<itemizedlist>
<listitem>
<para>DPDKとVFの作成</para>
</listitem>
<listitem>
<para>ワークロードで使用されるSR-IOVとVFの割り当て</para>
</listitem>
<listitem>
<para>CPUの分離とパフォーマンスの調整</para>
</listitem>
<listitem>
<para>Huge Pageの設定</para>
</listitem>
<listitem>
<para>カーネルパラメータの調整</para>
</listitem>
</itemizedlist>
<note>
<para>通信機能の詳細については、<xref linkend="atip-features"/>を参照してください。</para>
</note>
<para>上記の通信機能を有効にするために必要な変更はすべて、プロビジョニングファイル<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロック内にあります。ファイル<literal>capi-provisioning-example.yaml</literal>内の残りの情報は、プロビジョニングに関するセクション(<xref
linkend="single-node-provision"/>)で指定した情報と同じです。</para>
<para>このプロセスを明確にするために、通信機能を有効にするためにそのブロック(<literal>RKE2ControlPlane</literal>)で必要な変更を次に示します。</para>
<itemizedlist>
<listitem>
<para><literal>RKE2</literal>インストールプロセスの前にコマンドを実行するために使用する<literal>preRKE2Commands</literal>。ここでは、<literal>modprobe</literal>コマンドを使用して、<literal>vfio-pci</literal>と<literal>SR-IOV</literal>のカーネルモジュールを有効にします。</para>
</listitem>
<listitem>
<para>作成してワークロードに公開するインタフェース、ドライバ、および<literal>VFs</literal>の数を定義するために使用するIgnitionファイル<literal>/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml</literal>。</para>
<itemizedlist>
<listitem>
<para>実際の値に置き換える値は、設定マップ<literal>sriov-custom-auto-config</literal>内の値のみです。</para>
<itemizedlist>
<listitem>
<para><literal>${RESOURCE_NAME1}</literal> —
最初の<literal>PF</literal>インタフェースに使用するリソース名(例:
<literal>sriov-resource-du1</literal>)。このリソース名はプレフィックス<literal>rancher.io</literal>に追加されてラベルとして使用され、ワークロードで使用されます(例:
<literal>rancher.io/sriov-resource-du1</literal>)。</para>
</listitem>
<listitem>
<para><literal>${SRIOV-NIC-NAME1}</literal> —
使用する最初の<literal>PF</literal>インタフェースの名前(例: <literal>eth0</literal>)。</para>
</listitem>
<listitem>
<para><literal>${PF_NAME1}</literal> —
使用する最初の物理機能<literal>PF</literal>の名前。これを使用して、より複雑なフィルタを生成します(例:
<literal>eth0#2-5</literal>)。</para>
</listitem>
<listitem>
<para><literal>${DRIVER_NAME1}</literal> —
最初の<literal>VF</literal>インタフェースに使用するドライバ名(例: <literal>vfio-pci</literal>)。</para>
</listitem>
<listitem>
<para><literal>${NUM_VFS1}</literal> —
最初の<literal>PF</literal>インタフェース用に作成する<literal>VF</literal>の数(例:
<literal>8</literal>)。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>/var/sriov-auto-filler.sh</literal>は、高レベルの設定マップ<literal>sriov-custom-auto-config</literal>と、低レベルのハードウェア情報を含む<literal>sriovnetworknodepolicy</literal>との間で情報を変換するために使用されます。このスクリプトは、ユーザがハードウェア情報を事前に把握する手間をなくすために作成されています。このファイルを変更する必要はありませんが、<literal>sr-iov</literal>を有効にして<literal>VF</literal>を作成する必要がある場合は、このファイルが存在する必要があります。</para>
</listitem>
<listitem>
<para>次の機能を有効にするために使用するカーネル引数:</para>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>パラメータ</para></entry>
<entry align="left" valign="top"><para>値</para></entry>
<entry align="left" valign="top"><para>説明</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>isolcpus</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>コア1-30および33-62を分離します。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>skew_tick</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>分離されたCPU間でカーネルがタイマー割り込みをずらすことができるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでタイマーティックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>nohz_full</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>カーネルブートパラメータは、完全なdynticksとCPU分離の設定を行うための現在の主要インタフェースです。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>rcu_nocbs</para></entry>
<entry align="left" valign="top"><para>1-30、33-62</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでRCUコールバックを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>kthread_cpus</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUでkthreadsを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>irqaffinity</para></entry>
<entry align="left" valign="top"><para>0、31、32、63</para></entry>
<entry align="left" valign="top"><para>システムがアイドル状態のときにカーネルが1つのCPUで割り込みを実行できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>processor.max_cstate</para></entry>
<entry align="left" valign="top"><para>1</para></entry>
<entry align="left" valign="top"><para>アイドル状態のときにCPUがスリープ状態にならないようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_idle.max_cstate</para></entry>
<entry align="left" valign="top"><para>0</para></entry>
<entry align="left" valign="top"><para>intel_idleドライバを無効にし、acpi_idleを使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>iommu</para></entry>
<entry align="left" valign="top"><para>pt</para></entry>
<entry align="left" valign="top"><para>vfioをdpdkインタフェースに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>intel_iommu</para></entry>
<entry align="left" valign="top"><para>on</para></entry>
<entry align="left" valign="top"><para>vfioをVFに使用できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageのサイズを1Gに設定できるようにします。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>hugepages</para></entry>
<entry align="left" valign="top"><para>40</para></entry>
<entry align="left" valign="top"><para>前に定義したHuge Pageの数。</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>default_hugepagesz</para></entry>
<entry align="left" valign="top"><para>1G</para></entry>
<entry align="left" valign="top"><para>Huge Pageを有効にする場合のデフォルト値。</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist>
<listitem>
<para>次のsystemdサービスは、以下のサービスを有効にするために使用します。</para>
<itemizedlist>
<listitem>
<para><literal>rke2-preinstall.service</literal> -
プロビジョニングプロセス中にIronicの情報を利用して<literal>BAREMETALHOST_UUID</literal>と<literal>node-name</literal>を自動的に置き換えます。</para>
</listitem>
<listitem>
<para><literal>cpu-performance.service</literal> -
CPUパフォーマンスの調整を有効にします。<literal>${CPU_FREQUENCY}</literal>は実際の値に置き換える必要があります(例:
CPUの周波数を<literal>2.5GHz</literal>に設定する場合、<literal>2500000</literal>)。</para>
</listitem>
<listitem>
<para><literal>cpu-partitioning.service</literal> -
<literal>CPU</literal>の分離コアを有効にします(例: <literal>1-30,33-62</literal>)。</para>
</listitem>
<listitem>
<para><literal>sriov-custom-auto-vfs.service</literal> - <literal>sriov</literal>
Helmチャートをインストールし、カスタムリソースが作成されるまで待機し、<literal>/var/sriov-auto-filler.sh</literal>を実行して設定マップ<literal>sriov-custom-auto-config</literal>の値を置き換えてワークロードが使用する<literal>sriovnetworknodepolicy</literal>を作成します。</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para><literal>${RKE2_VERSION}</literal>は、この値の置き換えに使用する<literal>RKE2</literal>のバージョンです(例:
<literal>v1.28.9+rke2r1</literal>)。</para>
</listitem>
</itemizedlist>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: kube-system
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/sriov-crd-chart
                    targetNamespace: sriov-network-operator
                    version: 1.2.2
                    createNamespace: true
            - path: /var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://registry.suse.com/edge/sriov-network-operator-chart
                    targetNamespace: sriov-network-operator
                    version: 1.2.2
                    createNamespace: true
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: sriov-network-operator
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n kube-system -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nohz=on
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-performance.service
              enabled: true
              contents: |
                [Unit]
                Description=CPU perfomance
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "cpupower frequency-set -g performance; cpupower frequency-set -u ${CPU_FREQUENCY}; cpupower frequency-set -d ${CPU_FREQUENCY}"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>前の各ブロックを結合してファイルを作成したら、管理クラスタで次のコマンドを実行し、通信機能を使用する新しいダウンストリームクラスタのプロビジョニングを開始する必要があります。</para>
<screen language="shell" linenumbering="unnumbered">$ kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
<section xml:id="atip-private-registry">
<title>プライベートレジストリ</title>
<para>ワークロードで使用するイメージのミラーとしてプライベートレジストリを設定できます。</para>
<para>そのために、ダウンストリームクラスタで使用するプライベートレジストリに関する情報を含むシークレットを作成します。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_CERTIFICATE}
  tls.key: ${TLS_KEY}
  ca.crt: ${CA_CERTIFICATE}
type: kubernetes.io/tls
---
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}</screen>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、および<literal>ca.crt</literal>は、プライベートレジストリを認証するために使用する証明書です。<literal>username</literal>および<literal>password</literal>は、プライベートレジストリを認証するために使用する資格情報です。</para>
<note>
<para><literal>tls.crt</literal>、<literal>tls.key</literal>、<literal>ca.crt</literal>、<literal>username</literal>、および<literal>password</literal>は、シークレットで使用する前にbase64形式でエンコードする必要があります。</para>
</note>
<para>これらの変更をすべて行うと、<literal>capi-provisioning-example.yaml</literal>の<literal>RKE2ControlPlane</literal>ブロックは次のようになります。</para>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:
    mirrors:
      "registry.example.com":
        endpoint:
          - "https://registry.example.com:5000"
    configs:
      "registry.example.com":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
  serverConfig:
    cni: cilium
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
<para>ここで、<literal>registry.example.com</literal>は、ダウンストリームクラスタで使用するプライベートレジストリの名前の例です。これは実際の値に置き換える必要があります。</para>
</section>
<section xml:id="airgap-deployment">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>ダイレクトネットワークプロビジョニングワークフローでは、エアギャップシナリオでのダウンストリームクラスタのプロビジョニングを自動化できます。</para>
<section xml:id="id-requirements-for-air-gapped-scenarios">
<title>エアギャップシナリオの要件</title>
<orderedlist numeration="arabic">
<listitem>
<para><literal>EIB</literal>を使用して生成された<literal>生</literal>のイメージには、エアギャップシナリオでダウンストリームクラスタを実行するために必要な特定のコンテナイメージ(HelmチャートOCIイメージとコンテナイメージ)を含める必要があります。詳細については、こちらのセクション(<xref
linkend="eib-edge-image-airgap"/>)を参照してください。</para>
</listitem>
<listitem>
<para>SR-IOVまたはその他のカスタムワークロードを使用する場合、プライベートレジストリへのプリロードに関するセクション(<xref
linkend="preload-private-registry"/>)に従って、ワークロードを実行するために必要なイメージをプライベートレジストリにプリロードする必要があります。</para>
</listitem>
</orderedlist>
</section>
<section xml:id="id-enroll-the-bare-metal-hosts-in-air-gap-scenarios">
<title>エアギャップシナリオでのベアメタルホストの登録</title>
<para>管理クラスタにベアメタルホストを登録するプロセスは、前のセクション(<xref
linkend="enroll-bare-metal-host"/>)で説明したプロセスと同じです。</para>
</section>
<section xml:id="id-provision-the-downstream-cluster-in-air-gap-scenarios">
<title>エアギャップシナリオでのダウンストリームクラスタのプロビジョニング</title>
<para>エアギャップシナリオでダウンストリームクラスタをプロビジョニングするために必要となる重要な変更がいくつかあります。</para>
<orderedlist numeration="arabic">
<listitem>
<para><literal>capi-provisioning-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに<literal>spec.agentConfig.airGapped:
true</literal>ディレクティブを含める必要があります。</para>
</listitem>
<listitem>
<para>プライベートレジストリに関するセクション(<xref
linkend="atip-private-registry"/>)に従って、プライベートレジストリの設定を<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>RKE2ControlPlane</literal>ブロックに含める必要があります。</para>
</listitem>
<listitem>
<para>SR-IOV、またはHelmチャートをインストールする必要があるその他の<literal>AdditionalUserData</literal>設定(combustionスクリプト)を使用している場合、内容を変更して、パブリックレジストリを使用するのではなくプライベートレジストリを参照する必要があります。</para>
</listitem>
</orderedlist>
<para>次の例は、プライベートレジストリを参照するために必要な変更を行った、<literal>capi-provisioning-airgap-example.yaml</literal>ファイルの<literal>AdditionalUserData</literal>ブロックのSR-IOVの設定を示しています。</para>
<itemizedlist>
<listitem>
<para>プライベートレジストリのシークレットの参照</para>
</listitem>
<listitem>
<para>パブリックOCIイメージではなくプライベートレジストリを使用するHelmチャートの定義</para>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered"># secret to include the private registry certificates
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-cert
  namespace: default
data:
  tls.crt: ${TLS_BASE64_CERT}
  tls.key: ${TLS_BASE64_KEY}
  ca.crt: ${CA_BASE64_CERT}
type: kubernetes.io/tls
---
# secret to include the private registry auth credentials
apiVersion: v1
kind: Secret
metadata:
  name: private-registry-auth
  namespace: default
data:
  username: ${REGISTRY_USERNAME}
  password: ${REGISTRY_PASSWORD}
---
apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  privateRegistriesConfig:       # Private registry configuration to add your own mirror and credentials
    mirrors:
      docker.io:
        endpoint:
          - "https://$(PRIVATE_REGISTRY_URL)"
    configs:
      "192.168.100.22:5000":
        authSecret:
          apiVersion: v1
          kind: Secret
          namespace: default
          name: private-registry-auth
        tls:
          tlsConfigSecret:
            apiVersion: v1
            kind: Secret
            namespace: default
            name: private-registry-cert
          insecureSkipVerify: false
  serverConfig:
    cni: cilium
    cniMultusEnable: true
  preRKE2Commands:
    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1
  agentConfig:
    airGapped: true       # Airgap true to enable airgap mode
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        storage:
          files:
            - path: /var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    name: sriov-custom-auto-config
                    namespace: sriov-network-operator
                  data:
                    config.json: |
                      [
                         {
                           "resourceName": "${RESOURCE_NAME1}",
                           "interface": "${SRIOV-NIC-NAME1}",
                           "pfname": "${PF_NAME1}",
                           "driver": "${DRIVER_NAME1}",
                           "numVFsToCreate": ${NUM_VFS1}
                         },
                         {
                           "resourceName": "${RESOURCE_NAME2}",
                           "interface": "${SRIOV-NIC-NAME2}",
                           "pfname": "${PF_NAME2}",
                           "driver": "${DRIVER_NAME2}",
                           "numVFsToCreate": ${NUM_VFS2}
                         }
                      ]
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml
              overwrite: true
              contents:
                inline: |
                  apiVersion: v1
                  data:
                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}
                  kind: Secret
                  metadata:
                    name: privregauth
                    namespace: kube-system
                  type: kubernetes.io/dockerconfigjson
                  ---
                  apiVersion: v1
                  kind: ConfigMap
                  metadata:
                    namespace: kube-system
                    name: example-repo-ca
                  data:
                    ca.crt: |-
                      -----BEGIN CERTIFICATE-----
                      ${CA_BASE64_CERT}
                      -----END CERTIFICATE-----
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-crd
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_CRD_VERSION}
                  ---
                  apiVersion: helm.cattle.io/v1
                  kind: HelmChart
                  metadata:
                    name: sriov-network-operator
                    namespace: kube-system
                  spec:
                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator
                    dockerRegistrySecret:
                      name: privregauth
                    repoCAConfigMap:
                      name: example-repo-ca
                    createNamespace: true
                    set:
                      global.clusterCIDR: 192.168.0.0/18
                      global.clusterCIDRv4: 192.168.0.0/18
                      global.clusterDNS: 10.96.0.10
                      global.clusterDomain: cluster.local
                      global.rke2DataDir: /var/lib/rancher/rke2
                      global.serviceCIDR: 10.96.0.0/12
                    targetNamespace: sriov-network-operator
                    version: ${SRIOV_OPERATOR_VERSION}
              mode: 0644
              user:
                name: root
              group:
                name: root
            - path: /var/sriov-auto-filler.sh
              overwrite: true
              contents:
                inline: |
                  #!/bin/bash
                  cat &lt;&lt;- EOF &gt; /var/sriov-networkpolicy-template.yaml
                  apiVersion: sriovnetwork.openshift.io/v1
                  kind: SriovNetworkNodePolicy
                  metadata:
                    name: atip-RESOURCENAME
                    namespace: sriov-network-operator
                  spec:
                    nodeSelector:
                      feature.node.kubernetes.io/network-sriov.capable: "true"
                    resourceName: RESOURCENAME
                    deviceType: DRIVER
                    numVfs: NUMVF
                    mtu: 1500
                    nicSelector:
                      pfNames: ["PFNAMES"]
                      deviceID: "DEVICEID"
                      vendor: "VENDOR"
                      rootDevices:
                        - PCIADDRESS
                  EOF

                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export KUBECTL=/var/lib/rancher/rke2/bin/kubectl
                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r '.items[].status.syncStatus') != "Succeeded" ]; do sleep 1; done
                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm sriov-custom-auto-config -n sriov-network-operator -ojson | jq -r '.data."config.json"')
                  jq -c '.[]' &lt;&lt;&lt; $input | while read i; do
                    interface=$(echo $i | jq -r '.interface')
                    pfname=$(echo $i | jq -r '.pfname')
                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.pciAddress")
                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.vendor")
                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator -ojson | jq -r ".items[].status.interfaces[]|select(.name==\"$interface\")|.deviceID")
                    resourceName=$(echo $i | jq -r '.resourceName')
                    driver=$(echo $i | jq -r '.driver')
                    sed -e "s/RESOURCENAME/$resourceName/g" \
                        -e "s/DRIVER/$driver/g" \
                        -e "s/PFNAMES/$pfname/g" \
                        -e "s/VENDOR/$vendor/g" \
                        -e "s/DEVICEID/$deviceid/g" \
                        -e "s/PCIADDRESS/$pciaddress/g" \
                        -e "s/NUMVF/$(echo $i | jq -r '.numVFsToCreate')/g" /var/sriov-networkpolicy-template.yaml &gt; /var/lib/rancher/rke2/server/manifests/$resourceName.yaml
                  done
              mode: 0755
              user:
                name: root
              group:
                name: root
        kernel_arguments:
          should_exist:
            - intel_iommu=on
            - intel_pstate=passive
            - processor.max_cstate=1
            - intel_idle.max_cstate=0
            - iommu=pt
            - mce=off
            - hugepagesz=1G hugepages=40
            - hugepagesz=2M hugepages=0
            - default_hugepagesz=1G
            - kthread_cpus=${NON-ISOLATED_CPU_CORES}
            - irqaffinity=${NON-ISOLATED_CPU_CORES}
            - isolcpus=${ISOLATED_CPU_CORES}
            - nohz_full=${ISOLATED_CPU_CORES}
            - rcu_nocbs=${ISOLATED_CPU_CORES}
            - rcu_nocb_poll
            - nosoftlockup
            - nohz=on
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
            - name: cpu-partitioning.service
              enabled: true
              contents: |
                [Unit]
                Description=cpu-partitioning
                Wants=network-online.target
                After=network.target network-online.target
                [Service]
                Type=oneshot
                User=root
                ExecStart=/bin/sh -c "echo isolated_cores=${ISOLATED_CPU_CORES} &gt; /etc/tuned/cpu-partitioning-variables.conf"
                ExecStartPost=/bin/sh -c "tuned-adm profile cpu-partitioning"
                ExecStartPost=/bin/sh -c "systemctl enable tuned.service"
                [Install]
                WantedBy=multi-user.target
            - name: sriov-custom-auto-vfs.service
              enabled: true
              contents: |
                [Unit]
                Description=SRIOV Custom Auto VF Creation
                Wants=network-online.target  rke2-server.target
                After=network.target network-online.target rke2-server.target
                [Service]
                User=root
                Type=forking
                TimeoutStartSec=900
                ExecStart=/bin/sh -c "while ! /var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml wait --for condition=ready nodes --all ; do sleep 2 ; done"
                ExecStartPost=/bin/sh -c "while [ $(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found --no-headers -A | wc -l) -eq 0 ]; do sleep 1; done"
                ExecStartPost=/bin/sh -c "/var/sriov-auto-filler.sh"
                RemainAfterExit=yes
                KillMode=process
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_VERSION}
    nodeName: "localhost.localdomain"</screen>
</section>
</section>
</chapter>
<chapter xml:id="atip-lifecycle">
<title>ライフサイクルのアクション</title>
<para>このセクションでは、デプロイしたATIPクラスタのライフサイクル管理アクションについて説明します。</para>
<section xml:id="id-management-cluster-upgrades">
<title>管理クラスタのアップグレード</title>
<para>管理クラスタのアップグレードには複数のコンポーネントが関係します。アップグレードする必要がある一般的なコンポーネントのリストについては、<literal>Day
2</literal>管理クラスタ(<xref linkend="day2-mgmt-cluster"/>)のドキュメントを参照してください。</para>
<para>このセットアップに固有のコンポーネントをアップグレードする手順を以下に示します。</para>
<para><emphasis role="strong">Metal<superscript>3</superscript>のアップグレード</emphasis></para>
<para><literal>Metal<superscript>3</superscript></literal>をアップグレードするには、次のコマンドを使用してHelmリポジトリキャッシュを更新し、最新のチャートをフェッチしてHelmチャートリポジトリから<literal>Metal<superscript>3</superscript></literal>をインストールします。</para>
<screen language="shell" linenumbering="unnumbered">helm repo update
helm fetch suse-edge/metal3</screen>
<para>その後、現在の設定をファイルにエクスポートしてから、その前のファイルを使用して<literal>Metal<superscript>3</superscript></literal>のバージョンをアップグレードすると、簡単にアップグレードできます。新しいバージョンで何らかの変更が必要な場合、アップグレードの前にそのファイルを編集できます。</para>
<screen language="shell" linenumbering="unnumbered">helm get values metal3 -n metal3-system -o yaml &gt; metal3-values.yaml
helm upgrade metal3 suse-edge/metal3 \
  --namespace metal3-system \
  -f metal3-values.yaml \
  --version=0.7.1</screen>
</section>
<section xml:id="id-downstream-cluster-upgrades">
<title>ダウンストリームクラスタのアップグレード</title>
<para>ダウンストリームクラスタをアップグレードするには、複数のコンポーネントを更新する必要があります。次の各セクションでは、各コンポーネントのアップグレードプロセスについて説明します。</para>
<para><emphasis role="strong">オペレーティングシステムのアップグレード</emphasis></para>
<para>このプロセスでは、次の<link
xl:href="atip-automated-provisioning.xml#eib-edge-image-connected">参照資料</link>を確認して、新しいオペレーティングシステムバージョンで新しいイメージを構築します。<literal>EIB</literal>で生成されたこの新しいイメージにより、次のプロビジョニングフェーズでは、指定した新しいオペレーティングシステムバージョンが使用されます。次の手順では、この新しいイメージを使用してノードをアップグレードします。</para>
<para><emphasis role="strong">RKE2クラスタのアップグレード</emphasis></para>
<para>自動化されたワークフローを使用して<literal>RKE2</literal>クラスタをアップグレードするために必要な変更は次のとおりです。</para>
<itemizedlist>
<listitem>
<para>次の<link
xl:href="atip-automated-provisioning.xml#single-node-provision">セクション</link>に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>RKE2ControlPlane</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>仕様ファイルにロールアウト戦略を追加します。</para>
</listitem>
<listitem>
<para><literal>${RKE2_NEW_VERSION}</literal>を置き換えて、<literal>RKE2</literal>クラスタのバージョンを新しいバージョンに変更します。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: controlplane.cluster.x-k8s.io/v1alpha1
kind: RKE2ControlPlane
metadata:
  name: single-node-cluster
  namespace: default
spec:
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: Metal3MachineTemplate
    name: single-node-cluster-controlplane
  replicas: 1
  serverConfig:
    cni: cilium
  rolloutStrategy:
    rollingUpdate:
      maxSurge: 0
  agentConfig:
    format: ignition
    additionalUserData:
      config: |
        variant: fcos
        version: 1.4.0
        systemd:
          units:
            - name: rke2-preinstall.service
              enabled: true
              contents: |
                [Unit]
                Description=rke2-preinstall
                Wants=network-online.target
                Before=rke2-install.service
                ConditionPathExists=!/run/cluster-api/bootstrap-success.complete
                [Service]
                Type=oneshot
                User=root
                ExecStartPre=/bin/sh -c "mount -L config-2 /mnt"
                ExecStart=/bin/sh -c "sed -i \"s/BAREMETALHOST_UUID/$(jq -r .uuid /mnt/openstack/latest/meta_data.json)/\" /etc/rancher/rke2/config.yaml"
                ExecStart=/bin/sh -c "echo \"node-name: $(jq -r .name /mnt/openstack/latest/meta_data.json)\" &gt;&gt; /etc/rancher/rke2/config.yaml"
                ExecStartPost=/bin/sh -c "umount /mnt"
                [Install]
                WantedBy=multi-user.target
    kubelet:
      extraArgs:
        - provider-id=metal3://BAREMETALHOST_UUID
    version: ${RKE2_NEW_VERSION}
    nodeName: "localhost.localdomain"</screen>
<itemizedlist>
<listitem>
<para>次の<link
xl:href="atip-automated-provisioning.xml#single-node-provision">セクション</link>に示す<literal>capi-provisioning-example.yaml</literal>のブロック<literal>Metal3MachineTemplate</literal>を次のように変更します。</para>
<itemizedlist>
<listitem>
<para>イメージ名およびチェックサムを、前の手順で生成した新しいバージョンに変更します。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>nodeReuse</literal>を追加して<literal>true</literal>に設定し、新しいノードが作成されないようにします。</para>
</listitem>
<listitem>
<para>ディレクティブ<literal>automatedCleaningMode</literal>を追加して<literal>metadata</literal>に設定し、ノードの自動クリーニングを有効にします。</para>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<screen language="yaml" linenumbering="unnumbered">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: Metal3MachineTemplate
metadata:
  name: single-node-cluster-controlplane
  namespace: default
spec:
  nodeReuse: True
  template:
    spec:
      automatedCleaningMode: metadata
      dataTemplate:
        name: single-node-cluster-controlplane-template
      hostSelector:
        matchLabels:
          cluster-role: control-plane
      image:
        checksum: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256
        checksumType: sha256
        format: raw
        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw</screen>
<para>これらの変更を行った後、次にコマンドを使用して<literal>capi-provisioning-example.yaml</literal>ファイルをクラスタに適用できます。</para>
<screen language="shell" linenumbering="unnumbered">kubectl apply -f capi-provisioning-example.yaml</screen>
</section>
</chapter>
</part>
<part xml:id="id-appendix">
<title>付録</title>
<chapter xml:id="id-release-notes">
<title>リリースノート</title>
<section xml:id="release-notes">
<title>要約</title>
<para>SUSE Edge
3.0は、インフラストラクチャとクラウドネイティブなアプリケーションをエッジにデプロイするという他に例を見ない課題に対処することを目的とした、緊密に統合されて包括的に検証されたエンドツーエンドのソリューションです。SUSE
Edgeが重点を置いているのは、独創的でありながら高い柔軟性とスケーラビリティを持つセキュアなプラットフォームを提供し、初期デプロイメントイメージの構築からノードのプロビジョニングとオンボーディング、アプリケーションのデプロイメント、可観測性、ライフサイクル管理にまで対応することです。</para>
<para>このソリューションは、顧客の要件や期待はさまざまであるため「万能」なエッジプラットフォームは存在しないという考え方に基づいて設計されています。エッジデプロイメントにより、実に困難な問題を解決し、継続的に進化させることが要求されます。たとえば、大規模なスケーラビリティ、ネットワークの可用性の制限、物理的なスペースの制約、新たなセキュリティの脅威と攻撃ベクトル、ハードウェアアーキテクチャとシステムリソースのバリエーション、レガシインフラストラクチャやレガシアプリケーションのデプロイとインタフェースの要件、耐用年数を延長している顧客ソリューションといった課題があります。</para>
<para>SUSE
Edgeは、最良のオープンソースソフトウェアに基づいてゼロから構築されており、SUSEが持つ、30年にわたってセキュアで安定した定評あるSUSE
Linuxプラットフォームを提供してきた歴史と、Rancherポートフォリオによって拡張性に優れ機能豊富なKubernetes管理を提供してきた経験の両方に合致するものです。SUSE
Edgeは、これらの機能の上に構築されており、小売、医療、輸送、物流、通信、スマート製造、産業用IoTなど、さまざまな市場セグメントに対応できる機能を提供します。</para>
<note>
<para>SUSE Adaptive Telco Infrastructure Platform (ATIP)はSUSE
Edgeの派生製品(ダウンストリーム製品)にあたり、このプラットフォームを通信事業者の要件に対処可能にするための最適化とコンポーネントが追加されています。明記されていない限り、すべてのリリースノートはSUSE
Edge 3.0とSUSE ATIP 3.0の両方に適用されます。</para>
</note>
</section>
<section xml:id="id-about">
<title>概要</title>
<para>これらのリリースノートは、明示的に指定および説明されていない限り、すべてのアーキテクチャで同一です。また、最新バージョンは、その他すべてのSUSE製品のリリースノートとともに、常に<link
xl:href="https://www.suse.com/releasenotes">https://www.suse.com/releasenotes</link>でオンラインで参照できます。</para>
<para>エントリが記載されるのは1回だけですが、そのエントリが重要で複数のセクションに属している場合は複数の場所で参照できます。リリースノートには通常、連続する2つのリリース間の変更のみが記載されます。特定の重要なエントリは、以前の製品バージョンのリリースノートから繰り返し記載される場合があります。このようなエントリを特定しやすくするために、該当するエントリにはその旨を示すメモが含まれています。</para>
<para>ただし、繰り返し記載されているエントリは厚意としてのみ提供されています。したがって、リリースを1つ以上スキップする場合は、スキップしたリリースのリリースノートも確認してください。現行リリースのリリースノートしか確認しないと、システムの動作に影響する可能性がある重要な変更を見逃す可能性があります。</para>
</section>
<section xml:id="id-release-3-0-0">
<title>リリース3.0.0</title>
<para>公開日: 2024年4月26日</para>
<para>概要: SUSE Edge 3.0.0は、SUSE Edge 3.0ポートフォリオの最初のリリースです。</para>
<section xml:id="id-new-features">
<title>新機能</title>
<itemizedlist>
<listitem>
<para>該当なし - これは3.0.zの最初のリリースです。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes">
<title>バグおよびセキュリティの修正</title>
<itemizedlist>
<listitem>
<para>該当なし - これは3.0.zの最初のリリースです。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions">
<title>コンポーネントバージョン</title>
<para>次の表に、3.0.0リリースを構成する個々のコンポーネントを示します。ここには、バージョン、Helmチャートバージョン(該当する場合)、およびリリースされたアーティファクトをバイナリ形式でプル可能な場所も記載されています。使用法とデプロイメントの例については、関連するドキュメントに従ってください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名前</para></entry>
<entry align="left" valign="top"><para>バージョン</para></entry>
<entry align="left" valign="top"><para>Helmチャートバージョン</para></entry>
<entry align="left" valign="top"><para>アーティファクトの場所(URL/イメージ)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>5.5 (最新)</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE
Microダウンロードページ</link><?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256
4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256
527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256
13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256
6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>4.3.11</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE
Managerダウンロードページ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>K3s</para></entry>
<entry align="left" valign="top"><para>1.28.8</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.28.8%2Bk3s1">アップストリームのK3sリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>RKE2</para></entry>
<entry align="left" valign="top"><para>1.28.8</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.28.8%2Brke2r1">アップストリームのRKE2リリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Rancher Prime</para></entry>
<entry align="left" valign="top"><para>2.8.3</para></entry>
<entry align="left" valign="top"><para>2.8.3</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.8.3/rancher-images.txt">Rancher
2.8.3 Images</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime
Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.6.1</para></entry>
<entry align="left" valign="top"><para>103.3.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt">Longhorn
1.6.1イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NM Configurator</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.2.3">NMConfiguratorアップストリームリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NeuVector</para></entry>
<entry align="left" valign="top"><para>5.3.0</para></entry>
<entry align="left" valign="top"><para>103.0.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0<?asciidoc-br?>
registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.1-s1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Cluster API (CAPI)</para></entry>
<entry align="left" valign="top"><para>1.6.2</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cluster-api-controller:1.6.2<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-metal3:1.6.0<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Metal<superscript>3</superscript></para></entry>
<entry align="left" valign="top"><para>1.16.0</para></entry>
<entry align="left" valign="top"><para>0.6.5</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metal3-chart:0.6.5<?asciidoc-br?>
registry.suse.com/edge/baremetal-operator:0.5.1<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6<?asciidoc-br?>
registry.suse.com/edge/ip-address-manager:1.6.0<?asciidoc-br?>
registry.suse.com/edge/ironic:23.0.1.2<?asciidoc-br?>
registry.suse.com/edge/ironic-ipa-downloader:1.3.1<?asciidoc-br?>
registry.suse.com/edge/kube-rbac-proxy:v0.14.2<?asciidoc-br?>
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metallb-chart:0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-controller:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-speaker:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/frr-k8s:v0.0.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Elemental</para></entry>
<entry align="left" valign="top"><para>1.4.3</para></entry>
<entry align="left" valign="top"><para>103.1.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.4.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.4.3<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.4.3</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Edge Image Builder</para></entry>
<entry align="left" valign="top"><para>1.0.1</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/edge-image-builder:1.0.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.1.1</para></entry>
<entry align="left" valign="top"><para>0.2.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-chart:0.2.4<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-operator:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-api:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-controller:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-handler:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirtダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.58.0</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cdi-chart:0.2.3<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/endpoint-copier-operator:v0.2.0<?asciidoc-br?>
registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (技術プレビュー)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0<?asciidoc-br?>
registry.suse.com/edge/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-webhook-configuration:v0.12.20</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note>
<para>SUSE Edge
zストリームリリースは、バージョン管理されたスタックとして緊密に統合されていて、綿密にテストされています。個々のコンポーネントを上記のバージョンとは異なるバージョンにアップグレードすると、システムのダウンタイムが発生する可能性が高くなります。テストされていない設定でEdgeクラスタを実行することは可能ですが、推奨されません。また、サポートチャンネルを通じて解決策を提供するのに時間がかかる場合があります。</para>
</note>
</section>
</section>
<section xml:id="id-release-3-0-1">
<title>リリース3.0.1</title>
<para>公開日: 2024年6月14日</para>
<para>概要: SUSE Edge 3.0.1は、SUSE Edge 3.0ポートフォリオの最初のzストリームリリースです。</para>
<section xml:id="id-new-features-2">
<title>新機能</title>
<itemizedlist>
<listitem>
<para>ElementalとEIBで、管理対象外のホストのノードリセットがサポートされました</para>
</listitem>
<listitem>
<para>SR-IOV Network Operatorチャートを組み込みました</para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>チャートで、信頼できるCA証明書を追加で提供できるようになりました</para>
</listitem>
<listitem>
<para>NM Configuratorで、MACを指定せずに統合設定を適用できるようになりました</para>
</listitem>
<listitem>
<para>EIBに<literal>version</literal>サブコマンドが追加され、EIBによって構築された各イメージにバージョンも自動的に含まれるようになりました</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-bug-security-fixes-2">
<title>バグおよびセキュリティの修正</title>
<itemizedlist>
<listitem>
<para>EIBで、カスタムスクリプトに実行ビットが自動的に設定されるようになりました: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/429">SUSE
Edge issue #429</link></para>
</listitem>
<listitem>
<para>EIBで、512バイトを超えるセクタサイズのディスクがサポートされるようになりました: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/447">SUSE
Edge issue #447</link></para>
</listitem>
<listitem>
<para>Helmチャート内のコンテナイメージを検出するEIBの機能を強化しました: <link
xl:href="https://github.com/suse-edge/edge-image-builder/issues/442">SUSE
Edge issue #442</link></para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-components-versions-2">
<title>コンポーネントバージョン</title>
<para>次の表に、3.0.1リリースを構成する個々のコンポーネントを示します。ここには、バージョン、Helmチャートバージョン(該当する場合)、およびリリースされたアーティファクトをバイナリ形式でプル可能な場所も記載されています。使用法とデプロイメントの例については、関連するマニュアルに従ってください。</para>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<tbody>
<row>
<entry align="left" valign="top"><para>名前</para></entry>
<entry align="left" valign="top"><para>バージョン</para></entry>
<entry align="left" valign="top"><para>Helmチャートバージョン</para></entry>
<entry align="left" valign="top"><para>アーティファクトの場所(URL/イメージ)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SLE Micro</para></entry>
<entry align="left" valign="top"><para>5.5 (最新)</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/sle-micro/">SLE
Microダウンロードページ</link><?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256
4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256
527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256
13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b)<?asciidoc-br?>
SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256
6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>SUSE Manager</para></entry>
<entry align="left" valign="top"><para>4.3.11</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link xl:href="https://www.suse.com/download/suse-manager/">SUSE
Managerダウンロードページ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">K3s</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.28.9</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/k3s-io/k3s/releases/tag/v1.28.9%2Bk3s1">アップストリームのK3sリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">RKE2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.28.9</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rke2/releases/tag/v1.28.9%2Brke2r1">アップストリームのRKE2リリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Rancher Prime</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.8.4</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">2.8.4</emphasis></para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/rancher/rancher/releases/download/v2.8.4/rancher-images.txt">Rancher
2.8.4イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.rancher.com/server-charts/prime">Rancher Prime
Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Longhorn</para></entry>
<entry align="left" valign="top"><para>1.6.1</para></entry>
<entry align="left" valign="top"><para>103.3.0</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt">Longhorn
1.6.1イメージ</link><?asciidoc-br?> <link
xl:href="https://charts.longhorn.io">Longhorn Helmリポジトリ</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">NM Configurator</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.3.0</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para><link
xl:href="https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.0">NMConfiguratorアップストリームリリース</link></para></entry>
</row>
<row>
<entry align="left" valign="top"><para>NeuVector</para></entry>
<entry align="left" valign="top"><para>5.3.0</para></entry>
<entry align="left" valign="top"><para>103.0.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0<?asciidoc-br?>
registry.suse.com/rancher
mirrored-neuvector-registry-adapter:0.1.1-s1<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-scanner:latest<?asciidoc-br?>
registry.suse.com/rancher/mirrored-neuvector-updater:latest</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Cluster API (CAPI)</para></entry>
<entry align="left" valign="top"><para>1.6.2</para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cluster-api-controller:1.6.2<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-metal3:1.6.0<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Metal<superscript>3</superscript></emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.16.0</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">0.7.1</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metal3-chart:0.7.1<?asciidoc-br?>
registry.suse.com/edge/baremetal-operator:0.5.1<?asciidoc-br?>
registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6<?asciidoc-br?>
registry.suse.com/edge/ip-address-manager:1.6.0<?asciidoc-br?>
registry.suse.com/edge/ironic:23.0.2.1<?asciidoc-br?>
registry.suse.com/edge/ironic-ipa-downloader:1.3.2<?asciidoc-br?>
registry.suse.com/edge/kube-rbac-proxy:v0.14.2 +.1
registry.suse.com/edge/mariadb:10.6.15.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>MetalLB</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>0.14.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/metallb-chart:0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-controller:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/metallb-speaker:v0.14.3<?asciidoc-br?>
registry.suse.com/edge/frr:8.4<?asciidoc-br?>
registry.suse.com/edge/frr-k8s:v0.0.8</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Elemental</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.4.4</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">103.1.0</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/rancher/elemental-operator-chart:1.4.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator-crds-chart:1.4.4<?asciidoc-br?>
registry.suse.com/rancher/elemental-operator:1.4.4</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">Edge Image Builder</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.0.2</emphasis></para></entry>
<entry align="left" valign="top"><para>該当なし</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/edge-image-builder:1.0.2</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirt</para></entry>
<entry align="left" valign="top"><para>1.1.1</para></entry>
<entry align="left" valign="top"><para>0.2.4</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-chart:0.2.4<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-operator:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-api:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-controller:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-handler:1.1.1<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>KubeVirtダッシュボード拡張機能</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>1.0.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Containerized Data Importer</para></entry>
<entry align="left" valign="top"><para>1.58.0</para></entry>
<entry align="left" valign="top"><para>0.2.3</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/cdi-chart:0.2.3<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0<?asciidoc-br?>
registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Endpoint Copier Operator</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>0.2.0</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/endpoint-copier-operator:v0.2.0<?asciidoc-br?>
registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0</para></entry>
</row>
<row>
<entry align="left" valign="top"><para>Akri (技術プレビュー)</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>0.12.20</para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/akri-chart:0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0<?asciidoc-br?>
registry.suse.com/edge/akri-agent:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-controller:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20<?asciidoc-br?>
registry.suse.com/edge/akri-webhook-configuration:v0.12.20</para></entry>
</row>
<row>
<entry align="left" valign="top"><para><emphasis role="strong">SR-IOV Network Operator</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.2.2</emphasis></para></entry>
<entry align="left" valign="top"><para><emphasis role="strong">1.2.2+up0.1.0</emphasis></para></entry>
<entry align="left" valign="top"><para>registry.suse.com/edge/sriov-network-operator-chart:1.2.2<?asciidoc-br?>
registry.suse.com/edge/sriov-crd-chart:1.2.2</para></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="id-components-verification">
<title>コンポーネントの検証</title>
<para>上記のコンポーネントはSoftware Bill Of Materials
(SBOM)のデータを使用して検証できます。たとえば、以下に説明するように<literal>cosign</literal>を使用します。</para>
<para><link
xl:href="https://www.suse.com/support/security/keys/">SUSE署名キーのソース</link>からSUSE
Edge Containerの公開鍵をダウンロードします。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cat key.pem
-----BEGIN PUBLIC KEY-----
MIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA7N0S2d8LFKW4WU43bq7Z
IZT537xlKe17OQEpYjNrdtqnSwA0/jLtK83m7bTzfYRK4wty/so0g3BGo+x6yDFt
SVXTPBqnYvabU/j7UKaybJtX3jc4SjaezeBqdi96h6yEslvg4VTZDpy6TFP5ZHxZ
A0fX6m5kU2/RYhGXItoeUmL5hZ+APYgYG4/455NBaZT2yOywJ6+1zRgpR0cRAekI
OZXl51k0ebsGV6ui/NGECO6MB5e3arAhszf8eHDE02FeNJw5cimXkgDh/1Lg3KpO
dvUNm0EPWvnkNYeMCKR+687QG0bXqSVyCbY6+HG/HLkeBWkv6Hn41oeTSLrjYVGa
T3zxPVQM726sami6pgZ5vULyOleQuKBZrlFhFLbFyXqv1/DokUqEppm2Y3xZQv77
fMNogapp0qYz+nE3wSK4UHPd9z+2bq5WEkQSalYxadyuqOzxqZgSoCNoX5iIuWte
Zf1RmHjiEndg/2UgxKUysVnyCpiWoGbalM4dnWE24102050Gj6M4B5fe73hbaRlf
NBqP+97uznnRlSl8FizhXzdzJiVPcRav1tDdRUyDE2XkNRXmGfD3aCmILhB27SOA
Lppkouw849PWBt9kDMvzelUYLpINYpHRi2+/eyhHNlufeyJ7e7d6N9VcvjR/6qWG
64iSkcF2DTW61CN5TrCe0k0CAwEAAQ==
-----END PUBLIC KEY-----</screen>
<para>コンテナイメージのハッシュを検証します。たとえば、<literal>crane</literal>を使用します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; crane digest registry.suse.com/edge/baremetal-operator:0.5.1
sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782</screen>
<para><literal>cosign</literal>を使用して検証します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 &gt; /dev/null
#
Verification for registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 --
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - The claims were present in the transparency log
  - The signatures were integrated into the transparency log when the certificate was valid
  - The signatures were verified against the specified public key</screen>
<para><link
xl:href="https://www.suse.com/support/security/sbom/">アップストリームドキュメント</link>の説明に従ってSBOMデータを抽出します。</para>
<screen language="bash" linenumbering="unnumbered">&gt; cosign verify-attestation --type spdxjson --key key.pem registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 | jq '.payload | @base64d | fromjson | .predicate'</screen>
</section>
<section xml:id="id-upgrade-steps">
<title>アップグレード手順</title>
<para>新しいzストリームリリースへのアップグレード方法の詳細については、Day 2のドキュメントを参照してください。</para>
</section>
<section xml:id="id-known-limitations">
<title>既知の制限事項</title>
<para>別途記載されていない限り、これらは3.0.0リリースと後続のすべてのzストリームバージョンに適用されます。</para>
<itemizedlist>
<listitem>
<para>Akriは初めて技術プレビュー製品としてリリースされるため、標準のサポート範囲に含まれません。</para>
</listitem>
<listitem>
<para>SUSE Edgeで使用されるRancher UI拡張機能は現在、Rancher
Marketplace経由ではデプロイできません。手動でデプロイする必要があります。<link
xl:href="https://github.com/rancher/rancher/issues/29105">Rancher issue
#29105</link></para>
</listitem>
<listitem>
<para>NVIDIA
GPUを使用している場合、SELinuxポリシーがないため、SELinuxをcontainerd階層で有効にすることはできません。<link
xl:href="https://bugzilla.suse.com/show_bug.cgi?id=1222725">Bugzilla
#1222725</link></para>
</listitem>
<listitem>
<para>Metal<superscript>3</superscript>とCluster API
(CAPI)を使用してデプロイする場合、クラスタはインストール後に自動的にRancherにインポートされません。この問題は今後のリリースで対処予定です。</para>
</listitem>
</itemizedlist>
</section>
<section xml:id="id-product-support-lifecycle">
<title>製品サポートライフサイクル</title>
<para>SUSE
Edgeは、SUSEが提供する定評あるサポートに支えられています。SUSEは、エンタープライズ品質のサポートサービスの提供において確固たる実績を誇るテクノロジリーダーです。詳細については、<link
xl:href="https://www.suse.com/lifecycle">https://www.suse.com/lifecycle</link>、およびサポートポリシーのページ(<link
xl:href="https://www.suse.com/support/policy.html">https://www.suse.com/support/policy.html</link>)を参照してください。サポートケースの作成、SUSEが重大度レベルを分類する方法、またはサポートの範囲について質問がある場合は、テクニカルサポートハンドブック(<link
xl:href="https://www.suse.com/support/handbook/">https://www.suse.com/support/handbook/</link>)を参照してください。</para>
<para>このマニュアルの発行時点では、SUSE
Edgeの各マイナーバージョン(「3.0」など)は12か月間の運用サポートでサポートされ、最初の6か月間は「完全」サポート、その後の6か月間は「保守サポート」が提供されます。「完全サポート」の対象期間中に、SUSEは新機能(既存の機能を損なわないもの)の導入や、バグ修正の投入、セキュリティパッチの提供を行う場合があります。「保守サポート」の期間中には、重大なセキュリティ修正とバグ修正のみが提供され、その他の修正はSUSEの裁量で提供されます。</para>
<para>明記されていない限り、記載されているコンポーネントはすべて一般提供(GA)とみなされ、SUSEの標準のサポート範囲の対象となります。一部のコンポーネントは「技術プレビュー」として記載されている場合があります。この場合、SUSEは評価のためにGA前の機能への早期アクセスをお客様に提供しますが、これらの機能には標準のサポートポリシーが適用されず、運用ユースケースには推奨されません。SUSEでは、技術プレビューのコンポーネントに関するフィードバックや、当該コンポーネントの改良についてのご提案を心からお待ちしております。しかし、機能がお客様のニーズを満たさない場合やSUSEが求める成熟度に達しない場合、一般提供になる前に技術プレビューの機能を廃止する権利を留保します。</para>
<para>SUSEは場合により、機能の廃止やAPIの仕様変更を行わなければならないことがあることに注意してください。機能の廃止やAPIの変更の理由としては、機能が新しい実装によって更新または置き換えられた、新しい機能セットが導入された、アップストリームの技術が利用できなくなった、アップストリームコミュニティによって互換性のない変更が導入された、などが考えられます。これは特定のマイナーリリース(x.z)内で発生することは意図されていないため、すべてのzストリームリリースではAPIの互換性と機能が維持されます。SUSEは、廃止に関する警告をリリースノート内で十分に余裕をもって提供し、併せて回避策、推奨事項、サービスの中断を最小限に抑える軽減策も提供するよう努めます。</para>
<para>SUSE Edgeチームはコミュニティからのフィードバックも歓迎しており、<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>の各コードリポジトリ内で問題を報告できます。</para>
</section>
<section xml:id="id-obtaining-source-code">
<title>ソースコードの取得</title>
<para>このSUSE製品には、GNU General Public License
(GPL)やその他のさまざまなオープンソースライセンスの下でSUSEにライセンスされた素材が含まれます。SUSEはGPLに従ってGPLでライセンスされた素材に対応するソースコードを提供する必要があるほか、その他すべてのオープンソースライセンスの要件にも準拠します。よって、SUSEはすべてのソースコードを利用可能にしており、一般的にSUSE
Edge GitHubリポジトリ(<link
xl:href="https://www.github.com/suse-edge">https://www.github.com/suse-edge</link>)にあります。また、依存コンポーネントについてはSUSE
Rancher GitHubリポジトリ(<link
xl:href="https://www.github.com/rancher">https://www.github.com/rancher</link>)にあり、特にSLE
Microについては<link
xl:href="https://www.suse.com/download/sle-micro/">https://www.suse.com/download/sle-micro</link>の「Medium
2」でソースコードをダウンロードできます。</para>
</section>
<section xml:id="id-legal-notices">
<title>法的通知</title>
<para>SUSEは、この文書の内容や使用に関していかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEは本書を改訂し、その内容に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の改訂または変更を通知する義務を負いません。</para>
<para>SUSEは、いかなるソフトウェアに関しても、いかなる表明や保証も行いません。特に、商品性または特定目的への適合性に関する明示的または暗黙的な保証は一切行いません。さらに、SUSEはSUSEソフトウェアのあらゆる部分に随時変更を加える権利を留保しますが、いかなる個人または団体に対しても当該の変更を通知する義務を負いません。</para>
<para>本契約の下で提供されるいかなる製品または技術情報も、米国の輸出管理法規および他国の貿易法の対象となる場合があります。お客様はすべての輸出管理規制を遵守し、成果物の輸出、再輸出、または輸入のために必要なライセンスまたは分類を取得することに同意します。お客様は、現行の米国輸出禁止リストに記載されている団体や米国輸出法に規定された禁輸国やテロ支援国への輸出や再輸出を行わないことに同意します。また、成果物を禁止されている核、ミサイル、または化学/生物兵器の最終用途に使用しないことにも同意します。SUSEソフトウェアの輸出に関する詳細情報については、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>を参照してください。SUSEは、必要な輸出許可の取得を怠ったことに対する責任を一切負いません。</para>
<para><emphasis role="strong">Copyright © 2024 SUSE LLC.</emphasis></para>
<para>このリリースノート文書は、Creative Commons Attribution-NoDerivatives 4.0 International
License
(CC-BY-ND-4.0)の下でライセンスされています。お客様は、この文書と併せてライセンスのコピーを受け取っている必要があります。受け取っていない場合は、<link
xl:href="https://creativecommons.org/licenses/by-nd/4.0/">https://creativecommons.org/licenses/by-nd/4.0/</link>を参照してください。</para>
<para>SUSEは、本書で説明されている製品に組み込まれた技術に関連する知的財産権を有しています。これらの知的財産権には、特に、<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>に記載されている1つまたは複数の米国特許、ならびに米国およびその他の国における1つまたは複数のその他の特許または出願中の特許申請が含まれていることがありますが、これらに限定されません。</para>
<para>SUSEの商標については、SUSEの商標とサービスマークのリスト(<link
xl:href="https://www.suse.com/company/legal/">https://www.suse.com/company/legal/</link>)を参照してください。第三者のすべての商標は各所有者の財産です。SUSEのブランド情報と使用要件については、<link
xl:href="https://brand.suse.com/">https://brand.suse.com/</link>で公開されているガイドラインを参照してください。</para>
</section>
</chapter>
</part>
</book>
