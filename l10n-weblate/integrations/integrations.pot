# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"POT-Creation-Date: 2024-07-23 11:17+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. type: Title =
#: asciidoc/integrations/create-package-obs.adoc:1
#, no-wrap
msgid "Create a package (RPM or Container image) using OBS (openSUSE Build Service)"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/create-package-obs.adoc:2
#, no-wrap
msgid "openSUSE Build Service"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:3
msgid ""
"The https://build.opensuse.org[openSUSE Build Service] is the public "
"instance of the https://openbuildservice.org/[Open Build Service] used for "
"development of the openSUSE distribution and to offer packages from same "
"source for Fedora, Debian, Ubuntu, SUSE Linux Enterprise and other "
"distributions."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:4
msgid ""
"This service is also able to build container images, using either a "
"`Dockerfile` or a KIWI configuration."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:5
msgid "Everyone can create a SUSE IdP account to be able to use this service."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:6
msgid ""
"A published container image will be available on "
"https://registry.opensuse.org[registry.opensuse.org] and a published package "
"would be available at https://download.opensuse.org[download.opensuse.org]"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/create-package-obs.adoc:7
#: asciidoc/integrations/nvidia-slemicro.adoc:6
#, no-wrap
msgid "Prerequisites"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:8
msgid ""
"In order to use OBS you first need an IdP account "
"(https://idp-portal.suse.com/univention/self-service/#page=createaccount[sign "
"up here]) and you need to log into "
"https://build.opensuse.org[build.opensuse.org]"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:9
msgid ""
"You will also need the `osc` (for openSUSE Commander) command as this "
"quickstart will do it the CLI way, but most things can be done in the WebUI "
"if you prefer."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:10
msgid "To install `osc`:"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:11
#, no-wrap
msgid "SUSE"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:12
#, no-wrap
msgid "zypper install osc\n"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:13
#, no-wrap
msgid "MacOS"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:14
#, no-wrap
msgid "brew install osc\n"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:15
#, no-wrap
msgid "PIP"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:16
#, no-wrap
msgid "pip install osc\n"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/create-package-obs.adoc:17
#, no-wrap
msgid "Create and configure a project"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:18
msgid ""
"We are going to create a project under your home namespace, this will bring "
"up your editor to configure it right away."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:19
#, no-wrap
msgid "osc meta prj -e \"home:$USERNAME:$PROJECTNAME\"\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/create-package-obs.adoc:20
msgid ""
"If you want to use your home project root just specify `home:$USERNAME` here "
"and in following steps."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:21
msgid "In the editor you can now fill the metadata to look similar to this:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:22
#, no-wrap
msgid ""
"<project name=\"home:$USERNAME\">\n"
"  <title/>\n"
"  <description/>\n"
"  <person userid=\"$USERNAME\" role=\"maintainer\"/>\n"
"  <!--\n"
"    If you want to build RPM packages you need a block like this one, here "
"for\n"
"    SLE-15 SP5 replace accordingly to the distribution you want to target\n"
"  -->\n"
"  <repository name=\"sp5\">\n"
"    <path project=\"SUSE:SLE-15-SP5:Update\" repository=\"standard\"/>\n"
"    <path project=\"SUSE:SLE-15-SP5:GA\" repository=\"standard\"/>\n"
"    <arch>x86_64</arch>\n"
"    <arch>aarch64</arch>\n"
"  </repository>\n"
"  <!-- If you want to build container images you need a block akin to this "
"one -->\n"
"  <repository name=\"containers\">\n"
"    <!--\n"
"        This defines the available source images for the build (here any "
"from\n"
"        registry.suse.com)\n"
"    -->\n"
"    <path project=\"SUSE:Registry\" repository=\"standard\"/>\n"
"    <!--\n"
"        This defines package repositories available during build, I am\n"
"        refering to the one above here so I can use the RPM packages "
"published\n"
"        in this project for the container images of the project\n"
"    -->\n"
"    <path project=\"home:$USERNAME:$PROJECTNAME\" repository=\"sp4\"/>\n"
"    <!-- This is the list of architecture you want to build for -->\n"
"    <arch>x86_64</arch>\n"
"    <arch>aarch64</arch>\n"
"  </repository>\n"
"</project>\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:23
msgid ""
"If you want to build containers you need to tweak the configuration of the "
"project as well:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:24
#, no-wrap
msgid "osc meta prjconf -e \"home:$USERNAME:$PROJECTNAME\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:25
msgid ""
"The configuration is different whether you want to use KIWI or Dockerfile "
"build system:"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:26
#: asciidoc/integrations/create-package-obs.adoc:55
#, no-wrap
msgid "Dockerfile"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:27
#, no-wrap
msgid ""
"%if \"%_repository\" == \"containers\"\n"
"Type: docker\n"
"Repotype: none\n"
"Patterntype: none\n"
"BuildEngine: podman\n"
"%endif\n"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:28
#, no-wrap
msgid "KIWI"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:29
#, no-wrap
msgid ""
"%if \"%_repository\" == \"containers\"\n"
"Type: kiwi\n"
"Repotype: none\n"
"Patterntype: none\n"
"%endif\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/create-package-obs.adoc:30
msgid ""
"If you want to build containers using both KIWI and `Dockerfiles` in the "
"same project, you need two repositories in your project's metadata (with "
"different names) and both snippets in project's configuration (one for each "
"repository)."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/create-package-obs.adoc:31
#, no-wrap
msgid "Create a package"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:32
msgid "To create a package in your project use the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:33
#, no-wrap
msgid "osc meta pkg -e home:$USERNAME:$PROJECTNAME $PACKAGENAME\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:34
msgid ""
"There you'll get another XML file to edit, you only have to set a title and "
"description here."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:35
msgid "Now you can checkout the directory to start adding your files:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:36
#, no-wrap
msgid "osc co home:$USERNAME:$PROJECTNAME/$PACKAGENAME\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:37
msgid ""
"Now go into the directory and when all is ready you can add your files and "
"commit using:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:38
#, no-wrap
msgid ""
"osc add <files>...\n"
"osc ci\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:39
msgid "Now let's see the specificities of RPM and Container packages"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/create-package-obs.adoc:40
#, no-wrap
msgid "RPM package"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:41
msgid ""
"An RPM package is defined by the presence of a spec file, I will not go into "
"the details of that file as this is way beyond the scope of that quickstart, "
"please refer to https://en.opensuse.org/Portal:Packaging for more details "
"about packaging."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:42
msgid ""
"I will however get into more details about the `_service` and `_constraints` "
"special files that may change the behavior of OBS."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:43
msgid ""
"The `_service` file allows one to define automation to happen on said time, "
"for RPM packages these are usually manually triggered.  It is then possible "
"to automate fetching a git repository into a tarball, updating the specfile "
"version from git info, vendoring go or rust dependencies, etc...You can get "
"more insight into what is possible here "
"https://en.opensuse.org/openSUSE:Build_Service_Concept_SourceService ."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:44
msgid "Here is one for a rust project for example:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:45
#, no-wrap
msgid ""
"<services>\n"
"  <service name=\"tar_scm\" mode=\"manual\">\n"
"    <param name=\"scm\">git</param>\n"
"    <param name=\"url\">https://github.com/project-akri/akri</param>\n"
"    <param name=\"filename\">akri</param>\n"
"    <param name=\"versionformat\">@PARENT_TAG@</param>\n"
"    <param name=\"versionrewrite-pattern\">v(.*)</param>\n"
"    <param name=\"revision\">v0.10.4</param>\n"
"  </service>\n"
"  <service name=\"recompress\" mode=\"manual\">\n"
"    <param name=\"file\">*.tar</param>\n"
"    <param name=\"compression\">xz</param>\n"
"  </service>\n"
"  <service name=\"set_version\" mode=\"manual\" />\n"
"  <service name=\"cargo_vendor\" mode=\"manual\">\n"
"    <param name=\"srcdir\">akri</param>\n"
"    <param name=\"compression\">xz</param>\n"
"  </service>\n"
"</services>\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:46
msgid ""
"The `_constraints` file allow to define \"restrictions\" about the builder "
"selected by OBS, like for example the disk size, if your build complains "
"about having not enough space, this is the file you should edit/create.  See "
"here for the complete guide: "
"https://openbuildservice.org/help/manuals/obs-user-guide/cha.obs.build_job_constraints.html"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:47
msgid "### Container image"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:48
msgid ""
"You can build a container image in two different ways, you can either use a "
"`Dockerfile` or a KIWI configuration."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:49
msgid ""
"Each method has their own benefits and drawbacks.  Kiwi supports using the "
"package manager from the host/build system, so it can build base images and "
"derive images which don't contain a package manager, like "
"`opensuse/busybox`. With Dockerfile, it's practically required to use a full "
"base image like `opensuse/tumbleweed`."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:50
msgid ""
"I won't go into details how a Dockerfile or a kiwi build works, I'll just "
"tell about the interaction with OBS."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:51
msgid ""
"First the `kiwi_metainfo_helper` service that you can add as a buildtime "
"source service allows to substitute buildtime placeholders to use in you "
"Dockerfile or kiwi configuration. You can find a list of the placeholders "
"here: "
"https://build.opensuse.org/package/view_file/openSUSE:Factory/obs-service-kiwi_metainfo_helper/README"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:52
msgid ""
"Another useful service is the `replace_using_package_version` service, that "
"allows to replace a placeholder with the version of a RPM package.  For "
"example if I have `foobar` package in version `1.2.3` in an available RPM "
"repository, I can use this service to automatically tag an image that has "
"this package installed. Here it would replace `%PKG_VERSION%` to `1.2`."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:53
#, no-wrap
msgid ""
"<services>\n"
"  <service mode=\"buildtime\" name=\"kiwi_metainfo_helper\"/>\n"
"  <service mode=\"buildtime\" name=\"replace_using_package_version\">\n"
"    <param name=\"file\">Dockerfile</param>\n"
"    <param name=\"regex\">%PKG_VERSION%</param>\n"
"    <param name=\"parse-version\">minor</param>\n"
"    <param name=\"package\">foobar</param>\n"
"  </service>\n"
"</services>\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:54
msgid "You now have to tell OBS about the name and tag of your image:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:56
msgid ""
"You can use one or multiple `BuildTag` as comments in your `Dockerfile` like "
"this:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:57
#, no-wrap
msgid ""
"#!BuildTag: foo/bar:latest foo/bar:%PKG_VERSION%.%RELEASE%\n"
"#!BuildTag: foo/bar:tag foo/bar:anothertag\n"
msgstr ""

#. type: Labeled list
#: asciidoc/integrations/create-package-obs.adoc:58
#, no-wrap
msgid "Kiwi"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/create-package-obs.adoc:59
msgid ""
"In the kiwi configuration to specify the tags of your image you use the "
"`containerconfig` element, like this:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/create-package-obs.adoc:60
#, no-wrap
msgid ""
"<preferences>\n"
"  <type image=\"docker\">\n"
"   <containerconfig\n"
"     name=\"foo/bar\"\n"
"     tag=\"latest\"\n"
"     additionaltags=\"atag,anothertag,%PKG_VERSION%.%RELEASE\">\n"
"     ...\n"
msgstr ""

#. type: Title =
#: asciidoc/integrations/linkerd.adoc:1
#, no-wrap
msgid "Linkerd Service Mesh"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/linkerd.adoc:2
msgid ""
"link:linkerd.io[Linkerd] is a lightweight service mesh for Kubernetes that "
"\"adds security, observability, and reliability\" without requiring extra "
"complexity."
msgstr ""

#. type: Title =
#: asciidoc/integrations/nats.adoc:1
#, no-wrap
msgid "NATS"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:2
msgid ""
"https://nats.io/[NATS] is a connective technology built for the "
"ever-increasingly hyper-connected world. It is a single technology that "
"enables applications to securely communicate across any combination of cloud "
"vendors, on-premises, edge, Web and mobile devices. NATS consists of a "
"family of open-source products that are tightly integrated but can be "
"deployed easily and independently. NATS is being used globally by thousands "
"of companies, spanning use cases including microservices, edge computing, "
"mobile and IoT, and can be used to augment or replace traditional messaging."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nats.adoc:3
#, no-wrap
msgid "Architecture"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:4
msgid ""
"NATS is an infrastructure that allows data exchange between applications in "
"the form of messages."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:5
#, no-wrap
msgid "NATS client applications"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:6
msgid ""
"NATS client libraries can be used to allow the applications to publish, "
"subscribe, request and reply between different instances.  These "
"applications are generally referred to as `client applications`."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:7
#, no-wrap
msgid "NATS service infrastructure"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:8
msgid ""
"The NATS services are provided by one or more NATS server processes that are "
"configured to interconnect with each other and provide a NATS service "
"infrastructure. The NATS service infrastructure can scale from a single NATS "
"server process running on an end device to a public global super-cluster of "
"many clusters spanning all major cloud providers and all regions of the "
"world."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:9
#, no-wrap
msgid "Simple messaging design"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:10
msgid ""
"NATS makes it easy for applications to communicate by sending and receiving "
"messages. These messages are addressed and identified by subject strings and "
"do not depend on network location.  Data is encoded and framed as a message "
"and sent by a publisher. The message is received, decoded and processed by "
"one or more subscribers."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:11
#, no-wrap
msgid "NATS JetStream"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:12
msgid ""
"NATS has a built-in distributed persistence system called JetStream.  "
"JetStream was created to solve the problems identified with streaming in "
"technology today — complexity, fragility and a lack of "
"scalability. JetStream also solves the problem with the coupling of the "
"publisher and the subscriber (the subscribers need to be up and running to "
"receive the message when it is published).  More information about NATS "
"JetStream can be found https://docs.nats.io/nats-concepts/jetstream[here]."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nats.adoc:13
#, no-wrap
msgid "Installation"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:14
#, no-wrap
msgid "Installing NATS on top of K3s"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:15
msgid ""
"NATS is built for multiple architectures so it can easily be installed on "
"<<components-k3s,K3s.>>"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:16
msgid "Let us create a values file to overwrite the default values of NATS."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:17
#, no-wrap
msgid ""
"cat > values.yaml <<EOF\n"
"cluster:\n"
"  # Enable the HA setup of the NATS\n"
"  enabled: true\n"
"  replicas: 3\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:18
#, no-wrap
msgid ""
"nats:\n"
"  jetstream:\n"
"    # Enable JetStream\n"
"    enabled: true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:19
#, no-wrap
msgid ""
"    memStorage:\n"
"      enabled: true\n"
"      size: 2Gi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:20
#, no-wrap
msgid ""
"    fileStorage:\n"
"      enabled: true\n"
"      size: 1Gi\n"
"      storageDirectory: /data/\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:21
msgid "Now let us install NATS via Helm:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:22
#, no-wrap
msgid ""
"helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n"
"helm install nats nats/nats --namespace nats --values values.yaml \\\n"
" --create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:23
msgid ""
"With the `values.yaml` file above, the following components will be in the "
"`nats` namespace:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:24
msgid ""
"HA version of NATS Statefulset containing three containers: NATS server + "
"Config reloader and Metrics sidecars."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:25
msgid ""
"NATS box container, which comes with a set of `NATS` utilities that can be "
"used to verify the setup."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:26
msgid ""
"JetStream also leverages its Key-Value back-end that comes with `PVCs` "
"bounded to the pods."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:27
#, no-wrap
msgid "Testing the setup"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:28
#, no-wrap
msgid "kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:29
msgid "Create a subscription for the test subject:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:30
#, no-wrap
msgid "nats sub test &\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:31
msgid "Send a message to the test subject:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:32
#, no-wrap
msgid "nats pub test hi\n"
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:33
#, no-wrap
msgid "Cleaning up"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:34
#, no-wrap
msgid ""
"helm -n nats uninstall nats\n"
"rm values.yaml\n"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:35
#, no-wrap
msgid "NATS as a back-end for K3s"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:36
msgid ""
"One component K3s leverages is https://github.com/k3s-io/kine[KINE], which "
"is a shim enabling the replacement of etcd with alternate storage back-ends "
"originally targeting relational databases.  As JetStream provides a Key "
"Value API, this makes it possible to have NATS as a back-end for the K3s "
"cluster."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:37
msgid ""
"There is an already merged PR which makes the built-in NATS in K3s "
"straightforward, but the change is still "
"https://github.com/k3s-io/k3s/issues/7410#issue-1692989394[not included] in "
"the K3s releases."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:38
msgid "For this reason, the K3s binary should be built manually."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:39
msgid ""
"In this tutorial, "
"https://suse-edge.github.io/docs/quickstart/slemicro-utm-aarch64[SLE Micro "
"on OSX on Apple Silicon (UTM)] VM is used."
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:40
msgid "Run the commands below on the OSX PC."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:41
#, no-wrap
msgid "Building K3s"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:42
#, no-wrap
msgid "git clone --depth 1 https://github.com/k3s-io/k3s.git && cd k3s\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:43
msgid ""
"The following command adds `nats` in the build tags to enable the NATS "
"built-in feature in K3s:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:44
#, no-wrap
msgid ""
"sed -i '' 's/TAGS=\"ctrd/TAGS=\"nats ctrd/g' scripts/build\n"
"make local\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:45
msgid ""
"Replace <node-ip> with the actual IP of the node where the K3s will be "
"started:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:46
#, no-wrap
msgid ""
"export NODE_IP=<node-ip>\n"
"sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:47
msgid ""
"Locally building K3s requires the buildx Docker CLI plugin.  It can be "
"https://github.com/docker/buildx#manual-download[manually installed] if `$ "
"make local` fails."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:48
#, no-wrap
msgid "Installing NATS CLI"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:49
#, no-wrap
msgid ""
"TMPDIR=$(mktemp -d)\n"
"nats_version=\"nats-0.0.35-linux-arm64\"\n"
"curl -o \"${TMPDIR}/nats.zip\" -sfL "
"https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip\n"
"unzip \"${TMPDIR}/nats.zip\" -d \"${TMPDIR}\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:50
#, no-wrap
msgid ""
"sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats\n"
"rm -rf ${TMPDIR}\n"
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:51
#, no-wrap
msgid "Running NATS as K3s back-end"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:52
msgid ""
"Let us `ssh` on the node and run the K3s with the `--datastore-endpoint` "
"flag pointing to `nats`."
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:53
msgid ""
"The command below starts K3s as a foreground process, so the logs can be "
"easily followed to see if there are any issues.  To not block the current "
"terminal, a `&` flag could be added before the command to start it as a "
"background process."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:54
#, no-wrap
msgid "k3s server  --datastore-endpoint=nats://\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:55
msgid ""
"For making the K3s server with the NATS back-end permanent on your "
"`slemicro` VM, the script below can be run, which creates a `systemd` "
"service with the needed configurations."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:56
#, no-wrap
msgid ""
"export INSTALL_K3S_SKIP_START=false\n"
"export INSTALL_K3S_SKIP_DOWNLOAD=true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:57
#, no-wrap
msgid ""
"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server \\\n"
" --datastore-endpoint=nats://\"  sh -\n"
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:58
#, no-wrap
msgid "Troubleshooting"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:59
msgid ""
"The following commands can be run on the node to verify that everything with "
"the stream works properly:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:60
#, no-wrap
msgid ""
"nats str report -a\n"
"nats str view -a\n"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:1
#, no-wrap
msgid "NVIDIA GPUs on SLE Micro"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:2
#, no-wrap
msgid "Intro"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:3
msgid ""
"This guide demonstrates how to implement host-level NVIDIA GPU support via "
"the pre-built https://github.com/NVIDIA/open-gpu-kernel-modules[open-source "
"drivers] on SLE Micro 5.5. These are drivers that are baked into the "
"operating system rather than dynamically loaded by NVIDIA's "
"https://github.com/NVIDIA/gpu-operator[GPU Operator]. This configuration is "
"highly desirable for customers that want to pre-bake all artifacts required "
"for deployment into the image, and where the dynamic selection of the driver "
"version, that is, the user selecting the version of the driver via "
"Kubernetes, is not a requirement. This guide initially explains how to "
"deploy the additional components onto a system that has already been "
"pre-deployed, but follows with a section that describes how to embed this "
"configuration into the initial deployment via Edge Image Builder. If you do "
"not want to run through the basics and set things up manually, skip right "
"ahead to that section."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:4
msgid ""
"It is important to call out that the support for these drivers is provided "
"by both SUSE and NVIDIA in tight collaboration, where the driver is built "
"and shipped by SUSE as part of the package repositories. However, if you "
"have any concerns or questions about the combination in which you use the "
"drivers, ask your SUSE or NVIDIA account managers for further assistance. If "
"you plan to use "
"https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/[NVIDIA AI "
"Enterprise] (NVAIE), ensure that you are using an "
"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems[NVAIE "
"certified GPU], which _may_ require the use of proprietary NVIDIA "
"drivers. If you are unsure, speak with your NVIDIA representative."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:5
msgid ""
"Further information about NVIDIA GPU operator integration is _not_ covered "
"in this guide. While integrating the NVIDIA GPU Operator for Kubernetes is "
"not covered here, you can still follow most of the steps in this guide to "
"set up the underlying operating system and simply enable the GPU operator to "
"use the _pre-installed_ drivers via the `driver.enabled=false` flag in the "
"NVIDIA GPU Operator Helm chart, where it will simply pick up the installed "
"drivers on the host. More comprehensive instructions are available from "
"NVIDIA "
"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options[here]. "
"SUSE recently also made a "
"https://documentation.suse.com/trd/kubernetes/single-html/gs_rke2-slebci_nvidia-gpu-operator/[Technical "
"Reference Document] (TRD) available that discusses how to use the GPU "
"operator and the NVIDIA proprietary drivers, should this be a requirement "
"for your use case."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:7
msgid ""
"If you are following this guide, it assumes that you have the following "
"already available:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:8
msgid ""
"At least one host with SLE Micro 5.5 installed; this can be physical or "
"virtual."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:9
msgid ""
"Your hosts are attached to a subscription as this is required for package "
"access — an evaluation is available "
"https://www.suse.com/download/sle-micro/[here]."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:10
msgid ""
"A "
"https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[compatible "
"NVIDIA GPU] installed (or _fully_ passed through to the virtual machine in "
"which SLE Micro is running)."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:11
msgid ""
"Access to the root user — these instructions assume you are the root user, "
"and _not_ escalating your privileges via `sudo`."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:12
#, no-wrap
msgid "Manual installation"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:13
msgid ""
"In this section, you are going to install the NVIDIA drivers directly onto "
"the SLE Micro operating system as the NVIDIA open-driver is now part of the "
"core SLE Micro package repositories, which makes it as easy as installing "
"the required RPM packages. There is no compilation or downloading of "
"executable packages required. Below we walk through deploying the \"G06\" "
"generation of driver, which supports the latest GPUs (see "
"https://en.opensuse.org/SDB:NVIDIA_drivers#Install[here] for further "
"information), so select an appropriate driver generation for the NVIDIA GPU "
"that your system has. For modern GPUs, the \"G06\" driver is the most common "
"choice."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:14
msgid ""
"Before we begin, it is important to recognize that besides the NVIDIA "
"open-driver that SUSE ships as part of SLE Micro, you might also need "
"additional NVIDIA components for your setup. These could include OpenGL "
"libraries, CUDA toolkits, command-line utilities such as `nvidia-smi`, and "
"container-integration components such as `nvidia-container-toolkit`. Many of "
"these components are not shipped by SUSE as they are proprietary NVIDIA "
"software, or it makes no sense for us to ship them instead of "
"NVIDIA. Therefore, as part of the instructions, we are going to configure "
"additional repositories that give us access to said components and walk "
"through certain examples of how to use these tools, resulting in a fully "
"functional system. It is important to distinguish between SUSE repositories "
"and NVIDIA repositories, as occasionally there can be a mismatch between the "
"package versions that NVIDIA makes available versus what SUSE has "
"built. This usually arises when SUSE makes a new version of the open-driver "
"available, and it takes a couple of days before the equivalent packages are "
"made available in NVIDIA repositories to match."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:15
msgid ""
"We recommend that you ensure that the driver version that you are selecting "
"is compatible with your GPU and meets any CUDA requirements that you may "
"have by checking:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:16
msgid ""
"The https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/[CUDA release "
"notes]"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:17
msgid ""
"The driver version that you plan on deploying has a matching version in the "
"http://download.nvidia.com/suse/sle15sp5/x86_64/[NVIDIA SLE15-SP5 "
"repository] and ensuring that you have equivalent package versions for the "
"supporting components available"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:18
#, no-wrap
msgid "[TIP] "
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:19
msgid ""
"To find the NVIDIA open-driver versions, either run `zypper se -s "
"nvidia-open-driver` on the target machine _or_ search the SUSE Customer "
"Center for the \"nvidia-open-driver\" in "
"https://scc.suse.com/packages?name=SUSE%20Linux%20Enterprise%20Micro&version=5.5&arch=x86_64[SLE "
"Micro 5.5 for x86_64]."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:20
msgid ""
"Here, you will see _four_ versions available, with _545.29.06_ being the "
"newest:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/integrations/nvidia-slemicro.adoc:21
#, no-wrap
msgid "SUSE Customer Centre"
msgstr ""

#. type: Target for macro image
#: asciidoc/integrations/nvidia-slemicro.adoc:22
#, no-wrap
msgid "scc-packages-nvidia.png"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nvidia-slemicro.adoc:23
msgid ""
"When you have confirmed that an equivalent version is available in the "
"NVIDIA repos, you are ready to install the packages on the host operating "
"system. For this, we need to open up a `transactional-update` session, which "
"creates a new read/write snapshot of the underlying operating system so we "
"can make changes to the immutable platform (for further instructions on "
"`transactional-update`, see "
"https://documentation.suse.com/sle-micro/5.4/html/SLE-Micro-all/sec-transactional-udate.html[here]):"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:24
#, no-wrap
msgid "transactional-update shell\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:25
msgid ""
"When you are in your `transactional-update` shell, add an additional package "
"repository from NVIDIA. This allows us to pull in additional utilities, for "
"example, `nvidia-smi`:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:26
#, no-wrap
msgid ""
"zypper ar https://download.nvidia.com/suse/sle15sp5/ nvidia-sle15sp5-main\n"
"zypper --gpg-auto-import-keys refresh\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:27
msgid ""
"You can then install the driver and `nvidia-compute-utils` for additional "
"utilities. If you do not need the utilities, you can omit it, but for "
"testing purposes, it is worth installing at this stage:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:28
#, no-wrap
msgid ""
"zypper install -y --auto-agree-with-licenses "
"nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:29
msgid ""
"If the installation fails, this might indicate a dependency mismatch between "
"the selected driver version and what NVIDIA ships in their "
"repositories. Refer to the previous section to verify that your versions "
"match. Attempt to install a different driver version. For example, if the "
"NVIDIA repositories have an earlier version, you can try specifying "
"`nvidia-open-driver-G06-signed-kmp=545.29.06` on your install command to "
"specify a version that aligns."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:30
msgid ""
"Next, if you are _not_ using a supported GPU (remembering that the list can "
"be found "
"https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[here]), "
"you can see if the driver works by enabling support at the module level, but "
"your mileage may vary — skip this step if you are using a _supported_ GPU:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:31
#: asciidoc/integrations/nvidia-slemicro.adoc:128
#, no-wrap
msgid ""
"sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' "
"/etc/modprobe.d/50-nvidia-default.conf\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:32
msgid ""
"Now that you have installed these packages, it is time to exit the "
"`transactional-update` session:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:33
#: asciidoc/integrations/nvidia-slemicro.adoc:73
#, no-wrap
msgid "exit\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:34
msgid ""
"Make sure that you have exited the `transactional-update` session before "
"proceeding."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:35
msgid ""
"Now that you have installed the drivers, it is time to reboot. As SLE Micro "
"is an immutable operating system, it needs to reboot into the new snapshot "
"that you created in a previous step. The drivers are only installed into "
"this new snapshot, hence it is not possible to load the drivers without "
"rebooting into this new snapshot, which happens automatically. Issue the "
"reboot command when you are ready:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:36
#: asciidoc/integrations/nvidia-slemicro.adoc:55
#, no-wrap
msgid "reboot\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:37
msgid ""
"Once the system has rebooted successfully, log back in and use the "
"`nvidia-smi` tool to verify that the driver is loaded successfully and that "
"it can both access and enumerate your GPUs:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:38
#, no-wrap
msgid "nvidia-smi\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:39
msgid ""
"The output of this command should show you something similar to the "
"following output, noting that in the example below, we have two GPUs:"
msgstr ""

#. type: Table
#: asciidoc/integrations/nvidia-slemicro.adoc:40
#, no-wrap
msgid ""
"Wed Feb 28 12:31:06 2024\n"
"+---------------------------------------------------------------------------------------+\n"
"| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA "
"Version: 12.3     |\n"
"|-----------------------------------------+----------------------+----------------------+\n"
"| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile "
"Uncorr. ECC |\n"
"| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  "
"Compute M. |\n"
"|                                         |                      |               "
"MIG M. |\n"
"|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    "
"0 |\n"
"| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      "
"Default |\n"
"|                                         |                      |             "
"Disabled |\n"
"+-----------------------------------------+----------------------+----------------------+\n"
"|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    "
"0 |\n"
"| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      "
"Default |\n"
"|                                         |                      |             "
"Disabled |\n"
"+-----------------------------------------+----------------------+----------------------+\n"
"\n"
"+---------------------------------------------------------------------------------------+\n"
"| Processes:                                                                            "
"|\n"
"|  GPU   GI   CI        PID   Type   Process name                            "
"GPU Memory |\n"
"|        ID   ID                                                             "
"Usage      |\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:41
#, no-wrap
msgid ""
"|  No running processes found                                                           "
"|\n"
"+---------------------------------------------------------------------------------------+\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:42
#, no-wrap
msgid ""
"This concludes the installation and verification process for the NVIDIA "
"drivers on your SLE Micro system.\n"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:43
#, no-wrap
msgid "Further validation of the manual installation"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:44
msgid ""
"At this stage, all we have been able to verify is that, at the host level, "
"the NVIDIA device can be accessed and that the drivers are loading "
"successfully. However, if we want to be sure that it is functioning, a "
"simple test would be to validate that the GPU can take instructions from a "
"user-space application, ideally via a container, and through the CUDA "
"library, as that is typically what a real workload would use. For this, we "
"can make a further modification to the host OS by installing the "
"`nvidia-container-toolkit` "
"(https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper[NVIDIA "
"Container Toolkit]). First, open another `transactional-update` shell, "
"noting that we could have done this in a single transaction in the previous "
"step, and see how to do this fully automated in a later section:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:45
msgid "[,shell]"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:46
msgid "transactional-update shell"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:47
#, no-wrap
msgid ""
"Next, install the `nvidia-container-toolkit` package from the NVIDIA "
"Container Toolkit repo:\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:48
#, no-wrap
msgid ""
"* The `nvidia-container-toolkit.repo` below contains a stable "
"(`nvidia-container-toolkit`) and an experimental "
"(`nvidia-container-toolkit-experimental`) repository. The stable repository "
"is recommended for production use. The experimental repository is disabled "
"by default.\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:49
#: asciidoc/integrations/nvidia-slemicro.adoc:52
#, no-wrap
msgid "[,shell]\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:50
msgid ""
"zypper ar "
"https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo "
"zypper --gpg-auto-import-keys install -y nvidia-container-toolkit"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:51
#, no-wrap
msgid "When you are ready, you can exit the `transactional-update` shell:\n"
msgstr ""

#. type: Title -
#: asciidoc/integrations/nvidia-slemicro.adoc:53
#, no-wrap
msgid "exit"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:54
msgid "...and reboot the machine into the new snapshot:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:56
msgid ""
"As before, you need to ensure that you have exited the `transactional-shell` "
"and rebooted the machine for your changes to be enacted."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:57
msgid ""
"With the machine rebooted, you can verify that the system can successfully "
"enumerate the devices using the NVIDIA Container Toolkit. The output should "
"be verbose, with INFO and WARN messages, but no ERROR messages:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:58
#, no-wrap
msgid "nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:59
msgid ""
"This ensures that any container started on the machine can employ NVIDIA GPU "
"devices that have been discovered. When ready, you can then run a "
"podman-based container. Doing this via `podman` gives us a good way of "
"validating access to the NVIDIA device from within a container, which should "
"give confidence for doing the same with Kubernetes at a later stage. Give "
"`podman` access to the labeled NVIDIA devices that were taken care of by the "
"previous command, based on "
"https://registry.suse.com/bci/bci-base-15sp5/index.html[SLE BCI], and simply "
"run the Bash command:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:60
#, no-wrap
msgid ""
"podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it "
"registry.suse.com/bci/bci-base:latest bash\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:61
msgid ""
"You will now execute commands from within a temporary podman container. It "
"does not have access to your underlying system and is ephemeral, so whatever "
"we do here will not persist, and you should not be able to break anything on "
"the underlying host. As we are now in a container, we can install the "
"required CUDA libraries, again checking the correct CUDA version for your "
"driver https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/[here], "
"although the previous output of `nvidia-smi` should show the required CUDA "
"version. In the example below, we are installing _CUDA 12.3_ and pulling "
"many examples, demos and development kits so you can fully validate the GPU:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:62
#, no-wrap
msgid ""
"zypper ar "
"http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ "
"cuda-sle15-sp5\n"
"zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 "
"cuda-demo-suite-12-3\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:63
msgid ""
"Once this has been installed successfully, do not exit the container. We "
"will run the `deviceQuery` CUDA example, which comprehensively validates GPU "
"access via CUDA, and from within the container itself:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:64
#, no-wrap
msgid "/usr/local/cuda-12/extras/demo_suite/deviceQuery\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:65
msgid ""
"If successful, you should see output that shows similar to the following, "
"noting the `Result = PASS` message at the end of the command, and noting "
"that in the output below, the system correctly identifies two GPUs, whereas "
"your environment may only have one:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:66
#, no-wrap
msgid "/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:67
#, no-wrap
msgid " CUDA Device Query (Runtime API) version (CUDART static linking)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:68
#, no-wrap
msgid "Detected 2 CUDA Capable device(s)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:69
#, no-wrap
msgid ""
"Device 0: \"NVIDIA A100-PCIE-40GB\"\n"
"  CUDA Driver Version / Runtime Version          12.2 / 12.1\n"
"  CUDA Capability Major/Minor version number:    8.0\n"
"  Total amount of global memory:                 40339 MBytes (42298834944 "
"bytes)\n"
"  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores\n"
"  GPU Max Clock rate:                            1410 MHz (1.41 GHz)\n"
"  Memory Clock rate:                             1215 Mhz\n"
"  Memory Bus Width:                              5120-bit\n"
"  L2 Cache Size:                                 41943040 bytes\n"
"  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, "
"65536), 3D=(16384, 16384, 16384)\n"
"  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n"
"  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 "
"layers\n"
"  Total amount of constant memory:               65536 bytes\n"
"  Total amount of shared memory per block:       49152 bytes\n"
"  Total number of registers available per block: 65536\n"
"  Warp size:                                     32\n"
"  Maximum number of threads per multiprocessor:  2048\n"
"  Maximum number of threads per block:           1024\n"
"  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n"
"  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n"
"  Maximum memory pitch:                          2147483647 bytes\n"
"  Texture alignment:                             512 bytes\n"
"  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n"
"  Run time limit on kernels:                     No\n"
"  Integrated GPU sharing Host Memory:            No\n"
"  Support host page-locked memory mapping:       Yes\n"
"  Alignment requirement for Surfaces:            Yes\n"
"  Device has ECC support:                        Enabled\n"
"  Device supports Unified Addressing (UVA):      Yes\n"
"  Device supports Compute Preemption:            Yes\n"
"  Supports Cooperative Kernel Launch:            Yes\n"
"  Supports MultiDevice Co-op Kernel Launch:      Yes\n"
"  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0\n"
"  Compute Mode:\n"
"     < Default (multiple host threads can use ::cudaSetDevice() with device "
"simultaneously) >\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:70
#, no-wrap
msgid ""
"Device 1: <snip to reduce output for multiple devices>\n"
"     < Default (multiple host threads can use ::cudaSetDevice() with device "
"simultaneously) >\n"
"> Peer access from NVIDIA A100-PCIE-40GB (GPU0) -> NVIDIA A100-PCIE-40GB "
"(GPU1) : Yes\n"
"> Peer access from NVIDIA A100-PCIE-40GB (GPU1) -> NVIDIA A100-PCIE-40GB "
"(GPU0) : Yes\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:71
#, no-wrap
msgid ""
"deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime "
"Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = "
"NVIDIA A100-PCIE-40GB\n"
"Result = PASS\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:72
msgid ""
"From here, you can continue to run any other CUDA workload — use compilers "
"and any other aspect of the CUDA ecosystem to run further tests. When done, "
"you can exit from the container, noting that whatever you have installed in "
"there is ephemeral (so will be lost!), and has not impacted the underlying "
"operating system:"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:74
#, no-wrap
msgid "Implementation with Kubernetes"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:75
msgid ""
"Now that we have proven the installation and use of the NVIDIA open-driver "
"on SLE Micro, let us explore configuring Kubernetes on the same "
"machine. This guide does not walk you through deploying Kubernetes, but it "
"assumes that you have installed https://k3s.io/[K3s] or "
"https://docs.rke2.io/install/quickstart[RKE2] and that your kubeconfig is "
"configured accordingly, so that standard `kubectl` commands can be executed "
"as the superuser. We assume that your node forms a single-node cluster, "
"although the core steps should be similar for multi-node clusters. First, "
"ensure that your `kubectl` access is working:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:76
#, no-wrap
msgid "kubectl get nodes\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:77
msgid "This should show something similar to the following:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:78
#, no-wrap
msgid ""
"NAME       STATUS   ROLES                       AGE   VERSION\n"
"node0001   Ready    control-plane,etcd,master   13d   v1.28.9+rke2r1\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:79
msgid ""
"What you should find is that your k3s/rke2 installation has detected the "
"NVIDIA Container Toolkit on the host and auto-configured the NVIDIA runtime "
"integration into `containerd` (the Container Runtime Interface that k3s/rke2 "
"use). Confirm this by checking the containerd `config.toml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:80
#, no-wrap
msgid "tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:81
msgid ""
"This must show something akin to the following. The equivalent K3s location "
"is `/var/lib/rancher/k3s/agent/etc/containerd/config.toml`:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:82
#, no-wrap
msgid ""
"[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\"]\n"
"  runtime_type = \"io.containerd.runc.v2\"\n"
"[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\".options]\n"
"  BinaryName = \"/usr/bin/nvidia-container-runtime\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:83
msgid ""
"If these entries are not present, the detection might have failed. This "
"could be due to the machine or the Kubernetes services not being "
"restarted. Add these manually as above, if required."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:84
msgid ""
"Next, we need to configure the NVIDIA `RuntimeClass` as an additional "
"Kubernetes runtime to the default, ensuring that any user requests for pods "
"that need access to the GPU can use the NVIDIA Container Toolkit to do so, "
"via the `nvidia-container-runtime`, as configured in the `containerd` "
"configuration:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:85
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: node.k8s.io/v1\n"
"kind: RuntimeClass\n"
"metadata:\n"
"  name: nvidia\n"
"handler: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:86
msgid ""
"The next step is to configure the "
"https://github.com/NVIDIA/k8s-device-plugin[NVIDIA Device Plugin], which "
"configures Kubernetes to leverage the NVIDIA GPUs as resources within the "
"cluster that can be used, working in combination with the NVIDIA Container "
"Toolkit. This tool initially detects all capabilities on the underlying "
"host, including GPUs, drivers and other capabilities (such as GL) and then "
"allows you to request GPU resources and consume them as part of your "
"applications."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:87
msgid ""
"First, you need to add and update the Helm repository for the NVIDIA Device "
"Plugin:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:88
#, no-wrap
msgid ""
"helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n"
"helm repo update\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:89
msgid "Now you can install the NVIDIA Device Plugin:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:90
#, no-wrap
msgid ""
"helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace "
"nvidia-device-plugin --create-namespace --version 0.14.5 --set "
"runtimeClassName=nvidia\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:91
msgid ""
"After a few minutes, you see a new pod running that will complete the "
"detection on your available nodes and tag them with the number of GPUs that "
"have been detected:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:92
#, no-wrap
msgid ""
"kubectl get pods -n nvidia-device-plugin\n"
"NAME                              READY   STATUS    RESTARTS      AGE\n"
"nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:93
#, no-wrap
msgid ""
"kubectl get node node0001 -o json | jq .status.capacity\n"
"{\n"
"  \"cpu\": \"128\",\n"
"  \"ephemeral-storage\": \"466889732Ki\",\n"
"  \"hugepages-1Gi\": \"0\",\n"
"  \"hugepages-2Mi\": \"0\",\n"
"  \"memory\": \"32545636Ki\",\n"
"  \"nvidia.com/gpu\": \"1\",                      <----\n"
"  \"pods\": \"110\"\n"
"}\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:94
msgid ""
"Now you are ready to create an NVIDIA pod that attempts to use this GPU. Let "
"us try with the CUDA Benchmark container:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:95
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: nbody-gpu-benchmark\n"
"  namespace: default\n"
"spec:\n"
"  restartPolicy: OnFailure\n"
"  runtimeClassName: nvidia\n"
"  containers:\n"
"  - name: cuda-container\n"
"    image: nvcr.io/nvidia/k8s/cuda-sample:nbody\n"
"    args: [\"nbody\", \"-gpu\", \"-benchmark\"]\n"
"    resources:\n"
"      limits:\n"
"        nvidia.com/gpu: 1\n"
"    env:\n"
"    - name: NVIDIA_VISIBLE_DEVICES\n"
"      value: all\n"
"    - name: NVIDIA_DRIVER_CAPABILITIES\n"
"      value: all\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:96
msgid ""
"If all went well, you can look at the logs and see the benchmark "
"information:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:97
#, no-wrap
msgid ""
"kubectl logs nbody-gpu-benchmark\n"
"Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance.\n"
"\t-fullscreen       (run n-body simulation in fullscreen mode)\n"
"\t-fp64             (use double precision floating point values for "
"simulation)\n"
"\t-hostmem          (stores simulation data in host memory)\n"
"\t-benchmark        (run benchmark to measure performance)\n"
"\t-numbodies=<N>    (number of bodies (>= 1) to run in simulation)\n"
"\t-device=<d>       (where d=0,1,2.... for the CUDA device to use)\n"
"\t-numdevices=<i>   (where i=(number of CUDA devices > 0) to use for "
"simulation)\n"
"\t-compare          (compares simulation results running once on the default "
"GPU and once on the CPU)\n"
"\t-cpu              (run n-body simulation on the CPU)\n"
"\t-tipsy=<file.bin> (load a tipsy model file for simulation)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:98
#, no-wrap
msgid ""
"NOTE: The CUDA Samples are not meant for performance measurements. Results "
"may vary when GPU Boost is enabled.\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:99
#, no-wrap
msgid ""
"> Windowed mode\n"
"> Simulation data stored in video memory\n"
"> Single precision floating point simulation\n"
"> 1 Devices used for simulation\n"
"GPU Device 0: \"Turing\" with compute capability 7.5\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:100
#, no-wrap
msgid ""
"> Compute 7.5 CUDA device: [Tesla T4]\n"
"40960 bodies, total time for 10 iterations: 101.677 ms\n"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:101
#, no-wrap
msgid "165.005 billion interactions per second"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:102
#, no-wrap
msgid "3300.103 single-precision GFLOP/s at 20 flops per interaction"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:103
msgid ""
"Finally, if your applications require OpenGL, you can install the required "
"NVIDIA OpenGL libraries at the host level, and the NVIDIA Device Plugin and "
"NVIDIA Container Toolkit can make them available to containers. To do this, "
"install the package as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:104
#, no-wrap
msgid "transactional-update pkg install nvidia-gl-G06\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:105
msgid ""
"You need to reboot to make this package available to your applications. The "
"NVIDIA Device Plugin should automatically redetect this via the NVIDIA "
"Container Toolkit."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:106
#, no-wrap
msgid "Bringing it together via Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:107
msgid ""
"Okay, so you have demonstrated full functionality of your applications and "
"GPUs on SLE Micro and you now want to use <<components-eib>> to provide it "
"all together via a deployable/consumable ISO or RAW disk image. This guide "
"does not explain how to use Edge Image Builder, but it provides the "
"necessary configurations to build such image. Below you can find an example "
"of an image definition, along with the necessary Kubernetes configuration "
"files, to ensure that all the required components are deployed out of the "
"box. Here is the directory structure of the Edge Image Builder directory for "
"the example shown below:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:108
#, no-wrap
msgid ""
".\n"
"├── base-images\n"
"│   └── SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"├── eib-config-iso.yaml\n"
"├── kubernetes\n"
"│   ├── config\n"
"│   │   └── server.yaml\n"
"│   ├── helm\n"
"│   │   └── values\n"
"│   │       └── nvidia-device-plugin.yaml\n"
"│   └── manifests\n"
"│       └── nvidia-runtime-class.yaml\n"
"└── rpms\n"
"    └── gpg-keys\n"
"        └── nvidia-container-toolkit.key\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:109
msgid ""
"Let us explore those files. First, here is a sample image definition for a "
"single-node cluster running K3s that deploys the utilities and OpenGL "
"packages, too (`eib-config-iso.yaml`):"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:110
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  arch: x86_64\n"
"  imageType: iso\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  outputImageName: deployimage.iso\n"
"operatingSystem:\n"
"  time:\n"
"    timezone: Europe/London\n"
"    ntp:\n"
"      pools:\n"
"        - 2.suse.pool.ntp.org\n"
"  isoConfiguration:\n"
"    installDevice: /dev/sda\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1\n"
"  packages:\n"
"    packageList:\n"
"      - nvidia-open-driver-G06-signed-kmp-default\n"
"      - nvidia-compute-utils-G06\n"
"      - nvidia-gl-G06\n"
"      - nvidia-container-toolkit\n"
"    additionalRepos:\n"
"      - url: https://download.nvidia.com/suse/sle15sp5/\n"
"      - url: "
"https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64\n"
"    sccRegistrationCode: <snip>\n"
"kubernetes:\n"
"  version: v1.28.9+k3s1\n"
"  helm:\n"
"    charts:\n"
"      - name: nvidia-device-plugin\n"
"        version: v0.14.5\n"
"        installationNamespace: kube-system\n"
"        targetNamespace: nvidia-device-plugin\n"
"        createNamespace: true\n"
"        valuesFile: nvidia-device-plugin.yaml\n"
"        repositoryName: nvidia\n"
"    repositories:\n"
"      - name: nvidia\n"
"        url: https://nvidia.github.io/k8s-device-plugin\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:111
msgid ""
"This is just an example. You may need to customize it to fit your "
"requirements and expectations. Additionally, if using SLE Micro, you need to "
"provide your own `sccRegistrationCode` to resolve package dependencies and "
"pull the NVIDIA drivers."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:112
msgid ""
"Besides this, we need to add additional components, so they get loaded by "
"Kubernetes at boot time. The EIB directory needs a `kubernetes` directory "
"first, with subdirectories for the configuration, Helm chart values and any "
"additional manifests required:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:113
#, no-wrap
msgid "mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:114
msgid ""
"Let us now set up the (optional) Kubernetes configuration by choosing a CNI "
"(which defaults to Cilium if unselected) and enabling SELinux:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:115
#, no-wrap
msgid ""
"cat << EOF > kubernetes/config/server.yaml\n"
"cni: cilium\n"
"selinux: true\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:116
msgid ""
"Now ensure that the NVIDIA RuntimeClass is created on the Kubernetes "
"cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:117
#, no-wrap
msgid ""
"cat << EOF > kubernetes/manifests/nvidia-runtime-class.yaml\n"
"apiVersion: node.k8s.io/v1\n"
"kind: RuntimeClass\n"
"metadata:\n"
"  name: nvidia\n"
"handler: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:118
msgid ""
"We use the built-in Helm Controller to deploy the NVIDIA Device Plugin "
"through Kubernetes itself.  Let's provide the runtime class in the values "
"file for the chart:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:119
#, no-wrap
msgid ""
"cat << EOF > kubernetes/helm/values/nvidia-device-plugin.yaml\n"
"runtimeClassName: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:120
msgid ""
"We need to grab the NVIDIA Container Toolkit RPM public key before "
"proceeding:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:121
#, no-wrap
msgid ""
"mkdir -p rpms/gpg-keys\n"
"curl -o rpms/gpg-keys/nvidia-container-toolkit.key "
"https://nvidia.github.io/libnvidia-container/gpgkey\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:122
msgid ""
"All the required artifacts, including Kubernetes binary, container images, "
"Helm charts (and any referenced images), will be automatically air-gapped, "
"meaning that the systems at deploy time should require no Internet "
"connectivity by default. Now you need only to grab the SLE Micro ISO from "
"the https://www.suse.com/download/sle-micro/[SUSE Downloads Page] (and place "
"it in the `base-images` directory), and you can call the Edge Image Builder "
"tool to generate the ISO for you. To complete the example, here is the "
"command that was used to build the image:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:123
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v /path/to/eib-files/:/eib \\\n"
"registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
"build --definition-file eib-config-iso.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:124
msgid ""
"For further instructions, please see the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md[documentation] "
"for Edge Image Builder."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:125
#, no-wrap
msgid "Resolving issues"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nvidia-slemicro.adoc:126
#, no-wrap
msgid "nvidia-smi does not find the GPU"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:127
msgid ""
"Check the kernel messages using `dmesg`. If this indicates that it cannot "
"allocate `NvKMSKapDevice`, apply the unsupported GPU workaround:"
msgstr ""

#. type: delimited block _
#: asciidoc/integrations/nvidia-slemicro.adoc:129
msgid ""
"_NOTE_: You will need to reload the kernel module, or reboot, if you change "
"the kernel module configuration in the above step for it to take effect."
msgstr ""
