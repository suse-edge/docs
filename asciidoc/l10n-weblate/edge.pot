# SOME DESCRIPTIVE TITLE
# Copyright (C) YEAR Free Software Foundation, Inc.
# This file is distributed under the same license as the PACKAGE package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PACKAGE VERSION\n"
"POT-Creation-Date: 2024-07-26 13:42+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: \n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#. type: Title =
#: asciidoc/edge-book/welcome.adoc:1
#, no-wrap
msgid "SUSE Edge Documentation"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:2
msgid ""
"Welcome to the SUSE Edge documentation. You will find quick start guides, "
"validated designs, guidance on using components, third-party integrations, "
"and best practices for managing your edge computing infrastructure and "
"workloads."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/welcome.adoc:3
#, no-wrap
msgid "What is SUSE Edge?"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:4
msgid ""
"SUSE Edge is a purpose-built, tightly integrated, and comprehensively "
"validated end-to-end solution for addressing the unique challenges of the "
"deployment of infrastructure and cloud-native applications at the edge. Its "
"driving focus is to provide an opinionated, yet highly flexible, highly "
"scalable, and secure platform that spans initial deployment image building, "
"node provisioning and onboarding, application deployment, observability, and "
"complete lifecycle operations. The platform is built on best-of-breed open "
"source software from the ground up, consistent with both our 30-year history "
"in delivering secure, stable, and certified SUSE Linux platforms and our "
"experience in providing highly scalable and feature-rich Kubernetes "
"management with our Rancher portfolio. SUSE Edge builds on-top of these "
"capabilities to deliver functionality that can address a wide number of "
"market segments, including retail, medical, transportation, logistics, "
"telecommunications, smart manufacturing, and Industrial IoT."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/welcome.adoc:5
#, no-wrap
msgid "Design Philosophy"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:6
msgid ""
"The solution is designed with the notion that there is no "
"\"one-size-fits-all\" edge platform due to customers’ widely varying "
"requirements and expectations. Edge deployments push us to solve, and "
"continually evolve, some of the most challenging problems, including massive "
"scalability, restricted network availability, physical space constraints, "
"new security threats and attack vectors, variations in hardware architecture "
"and system resources, the requirement to deploy and interface with legacy "
"infrastructure and applications, and customer solutions that have extended "
"lifespans. Since many of these challenges are different from traditional "
"ways of thinking, e.g. deployment of infrastructure and applications within "
"data centers or in the public cloud, we have to look into the design in much "
"more granular detail, and rethinking many common assumptions."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:7
msgid ""
"For example, we find value in minimalism, modularity, and ease of "
"operations. Minimalism is important for edge environments since the more "
"complex a system is, the more likely it is to break. When looking at "
"hundreds of locations, up to hundreds of thousands, complex systems will "
"break in complex ways.  Modularity in our solution allows for more user "
"choice while removing unneeded complexity in the deployed platform. We also "
"need to balance these with the ease of operations. Humans may make mistakes "
"when repeating a process thousands of times, so the platform should make "
"sure any potential mistakes are recoverable, eliminating the need for "
"on-site technician visits, but also strive for consistency and "
"standardization."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/welcome.adoc:8
#, no-wrap
msgid "Which Quick Start should you use?"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:9
msgid ""
"Due to the varying set of operating environments and lifecycle requirements, "
"we've implemented support for a number of distinct deployment patterns that "
"loosely align to market segments and use-cases that SUSE Edge operates "
"in. We have documented a quickstart guide for each of these deployment "
"patterns to help you get familiar with the SUSE Edge platform based around "
"your needs. The three deployment patterns that we support today are "
"described below, with a link to the respective quickstart page."
msgstr ""

#. type: Title ===
#: asciidoc/edge-book/welcome.adoc:10
#, no-wrap
msgid "Direct network provisioning"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:11
msgid ""
"Direct network provisioning is where you know the details of the hardware "
"you wish to deploy to and have direct access to the out-of-band management "
"interface to orchestrate and automate the entire provisioning process. In "
"this scenario, our customers expect a solution to be able to provision edge "
"sites fully automated from a centralized location, going much further than "
"the creation of a boot image by minimizing the manual operations at the edge "
"location; simply rack, power, and attach the required networks to the "
"physical hardware, and the automation process powers up the machine via the "
"out-of-band management (e.g. via the Redfish API) and handles the "
"provisioning, onboarding, and deployment of infrastructure without user "
"intervention. The key for this to work is that the systems are known to the "
"administrators; they know which hardware is in which location, and that "
"deployment is expected to be handled centrally."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:12
msgid ""
"This solution is the most robust since you are directly interacting with the "
"hardware's management interface, are dealing with known hardware, and have "
"fewer constraints on network availability. Functionality wise, this solution "
"extensively uses Cluster API and Metal^3^ for automated provisioning from "
"baremetal, through operating system, Kubernetes, and layered applications, "
"and provides the ability to link into the rest of the common lifecycle "
"management capabilities of SUSE Edge post-deployment. The quickstart for "
"this solution can be found in <<quickstart-metal3>>."
msgstr ""

#. type: Title ===
#: asciidoc/edge-book/welcome.adoc:13
#, no-wrap
msgid "\"Phone home\" network provisioning"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:14
msgid ""
"Sometimes you are operating in an environment where the central management "
"cluster cannot manage the hardware directly (for example, your remote "
"network is behind a firewall or there is no out-of-band management "
"interface; common in \"PC\" type hardware often found at the edge). In this "
"scenario, we provide tooling to remotely provision clusters and their "
"workloads with no need to know where hardware is being shipped when it is "
"bootstrapped. This is what most people think of when they think about edge "
"computing; it’s the thousands or tens of thousands of somewhat unknown "
"systems booting up at edge locations and securely phoning home, validating "
"who they are, and receiving their instructions on what they’re supposed to "
"do. Our requirements here expect provisioning and lifecycle management with "
"very little user-intervention other than either pre-imaging the machine at "
"the factory, or simply attaching a boot image, e.g. via USB, and switching "
"the system on. The primary challenges in this space are addressing scale, "
"consistency, security, and lifecycle of these devices in the wild."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:15
msgid ""
"This solution provides a great deal of flexibility and consistency in the "
"way that systems are provisioned and on-boarded, regardless of their "
"location, system type or specification, or when they're powered on for the "
"first time. SUSE Edge enables full flexibility and customization of the "
"system via Edge Image Builder, and leverages the registration capabilities "
"Rancher's Elemental offering for node on-boarding and Kubernetes "
"provisioning, along with SUSE Manager for operating system patching. The "
"quick start for this solution can be found in <<quickstart-elemental>>."
msgstr ""

#. type: Title ===
#: asciidoc/edge-book/welcome.adoc:16
#, no-wrap
msgid "Image-based provisioning"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:17
msgid ""
"For customers that need to operate in standalone, air-gapped, or network "
"limited environments, SUSE Edge provides a solution that enables customers "
"to generate fully customized installation media that contains all of the "
"required deployment artifacts to enable both single-node and multi-node "
"highly-available Kubernetes clusters at the edge, including any workloads or "
"additional layered components required, all without any network connectivity "
"to the outside world, and without the intervention of a centralized "
"management platform. The user-experience follows closely to the \"phone "
"home\" solution in that installation media is provided to the target "
"systems, but the solution will \"bootstrap in-place\". In this scenario, "
"it's possible to attach the resulting clusters into Rancher for ongoing "
"management (i.e. going from a \"disconnected\" to \"connected\" mode of "
"operation without major reconfiguration or redeployment), or can continue to "
"operate in isolation. Note that in both cases the same consistent mechanism "
"for automating lifecycle operations can be applied."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:18
msgid ""
"Furthermore, this solution can be used to quickly create management clusters "
"that may host the centralized infrastructure that supports both the "
"\"directed network provisioning\" and \"phone home network provisioning\" "
"models as it can be the quickest and most simple way to provision all types "
"of Edge infrastructure. This solution heavily utilizes the capabilities of "
"SUSE Edge Image Builder to create fully customized and unattended "
"installation media; the quickstart can be found in <<quickstart-eib>>."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/welcome.adoc:19
#, no-wrap
msgid "Components used in SUSE Edge"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:20
msgid ""
"SUSE Edge is comprised of both existing SUSE components, including those "
"from the Linux and Rancher teams, along with additional features and "
"components built by the Edge team to enable SUSE to address both the "
"infrastructure requirements and intricacies. The list of components, along "
"with a link to a high-level description of each and how it's used in SUSE "
"Edge can be found below:"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:21
msgid "<<components-rancher,Rancher>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:22
msgid "<<components-rancher-dashboard-extensions,Rancher Dashboard Extensions>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:23
msgid "<<components-fleet,Fleet>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:24
msgid "<<components-slmicro,SLE Micro>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:25
msgid "<<components-metal3,Metal³>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:26
msgid "<<components-eib,Edge Image Builder>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:27
msgid "<<components-nmc, NetworkManager Configurator>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:28
msgid "<<components-elemental,Elemental>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:29
msgid "<<components-akri,Akri>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:30
msgid "<<components-k3s,K3s>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:31
msgid "<<components-rke2,RKE2>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:32
msgid "<<components-longhorn,Longhorn>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:33
msgid "<<components-neuvector,NeuVector>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:34
msgid "<<components-metallb,MetalLB>>"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/welcome.adoc:35
msgid "<<components-kubevirt,KubeVirt>>"
msgstr ""

#. type: Title =
#: asciidoc/quickstart/metal3.adoc:1
#, no-wrap
msgid "BMC automated deployments with Metal^3^"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:2
msgid ""
"Metal^3^ is a https://metal3.io/[CNCF project] which provides bare-metal "
"infrastructure management capabilities for Kubernetes."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:3 asciidoc/components/metal3.adoc:3
msgid ""
"Metal^3^ provides Kubernetes-native resources to manage the lifecycle of "
"bare-metal servers which support management via out-of-band protocols such "
"as https://www.dmtf.org/standards/redfish[Redfish]."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:4 asciidoc/components/metal3.adoc:4
msgid ""
"It also has mature support for https://cluster-api.sigs.k8s.io/[Cluster API "
"(CAPI)] which enables management of infrastructure resources across multiple "
"infrastructure providers via broadly adopted vendor-neutral APIs."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:5 asciidoc/guides/metallb-k3s.adoc:4
#, no-wrap
msgid "Why use this method"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:6 asciidoc/components/metal3.adoc:6
msgid ""
"This method is useful for scenarios where the target hardware supports "
"out-of-band management, and a fully automated infrastructure management flow "
"is desired."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:7
msgid ""
"A management cluster is configured to provide declarative APIs that enable "
"inventory and state management of downstream cluster bare-metal servers, "
"including automated inspection, cleaning and provisioning/deprovisioning."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:8 asciidoc/quickstart/elemental.adoc:4
#, no-wrap
msgid "High-level architecture"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/metal3.adoc:9
#, no-wrap
msgid "quickstart-metal3-architecture.png"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:10 asciidoc/quickstart/eib.adoc:4
#: asciidoc/components/rancher-dashboard-extensions.adoc:4
#: asciidoc/components/networking.adoc:19 asciidoc/components/longhorn.adoc:3
#: asciidoc/components/virtualization.adoc:11
#: asciidoc/guides/metallb-k3s.adoc:16 asciidoc/guides/metallb-kube-api.adoc:3
#: asciidoc/guides/air-gapped-eib-deployments.adoc:10
#: asciidoc/integrations/nvidia-slemicro.adoc:6
#, no-wrap
msgid "Prerequisites"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:11
msgid ""
"There are some specific constraints related to the downstream cluster server "
"hardware and networking:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:12
msgid "Management cluster"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:13
msgid "Must have network connectivity to the target server management/BMC API"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:14
msgid "Must have network connectivity to the target server control plane network"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:15
msgid ""
"For multi-node management clusters, an additional reserved IP address is "
"required"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:16
msgid "Hosts to be controlled"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:17
msgid "Must support out-of-band management via Redfish, iDRAC or iLO interfaces"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:18
msgid "Must support deployment via virtual media (PXE is not currently supported)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:19
msgid ""
"Must have network connectivity to the management cluster for access to the "
"Metal^3^ provisioning APIs"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:20
msgid ""
"Some tools are required, these can be installed either on the management "
"cluster, or on a host which can access it."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:21
msgid ""
"https://kubernetes.io/docs/reference/kubectl/kubectl/[Kubectl], "
"https://helm.sh[Helm] and "
"https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl[Clusterctl]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:22
msgid ""
"A container runtime such as https://podman.io[Podman] or "
"https://rancherdesktop.io[Rancher Desktop]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:23
msgid ""
"The `SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz` OS image file must be "
"downloaded from the https://scc.suse.com/[SUSE Customer Center] or the "
"https://www.suse.com/download/sle-micro/[SUSE Download page]."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:24
#, no-wrap
msgid "Setup Management Cluster"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:25
msgid "The basic steps to install a management cluster and use Metal^3^ are:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:26
msgid "Install an RKE2 management cluster"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:27
msgid "Install Rancher"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:28
msgid "Install a storage provider"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:29
msgid "Install the Metal^3^ dependencies"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:30
msgid "Install CAPI dependencies"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:31
msgid "Build a SLEMicro OS image for downstream cluster hosts"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:32
msgid "Register BareMetalHost CRs to define the bare-metal inventory"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:33
msgid "Create a downstream cluster by defining CAPI resources"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:34
msgid ""
"This guide assumes an existing RKE2 cluster and Rancher (including "
"cert-manager) has been installed, for example by using <<components-eib, "
"Edge Image Builder>>."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:35
msgid ""
"The steps here can also be fully automated as described in the "
"<<atip-management-cluster, ATIP management cluster documentation>>."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:36
#, no-wrap
msgid "Installing Metal^3^ dependencies"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:37
msgid ""
"If not already installed as part of the Rancher installation, cert-manager "
"must be installed and running."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:38
msgid ""
"A persistent storage provider must be installed. Longhorn is recommended but "
"local-path can also be used for dev/PoC environments. The instructions below "
"assume a StorageClass has been "
"https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/[marked "
"as default], otherwise additional configuration for the Metal^3^ chart is "
"required."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:39
msgid ""
"An additional IP is required, which is managed by "
"https://metallb.universe.tf/[MetalLB] to provide a consistent endpoint for "
"the Metal^3^ management services.  This IP must be part of the control plane "
"subnet and reserved for static configuration (not part of any DHCP pool)."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:40
msgid ""
"If the management cluster is a single node, the requirement for an "
"additional floating IP managed via MetalLB can be avoided, see <<Single-node "
"configuration, Single-node configuration>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:41
msgid "First, we install MetalLB:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:42 asciidoc/guides/metallb-k3s.adoc:23
#, no-wrap
msgid ""
"helm install \\\n"
"  metallb oci://registry.suse.com/edge/metallb-chart \\\n"
"  --namespace metallb-system \\\n"
"  --create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:43
msgid ""
"Then we define an `IPAddressPool` and `L2Advertisment` using the reserved "
"IP, defined as `STATIC_IRONIC_IP` below:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:44
#, no-wrap
msgid "export STATIC_IRONIC_IP=<STATIC_IRONIC_IP>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:45
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: IPAddressPool\n"
"metadata:\n"
"  name: ironic-ip-pool\n"
"  namespace: metallb-system\n"
"spec:\n"
"  addresses:\n"
"  - ${STATIC_IRONIC_IP}/32\n"
"  serviceAllocation:\n"
"    priority: 100\n"
"    serviceSelectors:\n"
"    - matchExpressions:\n"
"      - {key: app.kubernetes.io/name, operator: In, values: "
"[metal3-ironic]}\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:46
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: L2Advertisement\n"
"metadata:\n"
"  name: ironic-ip-pool-l2-adv\n"
"  namespace: metallb-system\n"
"spec:\n"
"  ipAddressPools:\n"
"  - ironic-ip-pool\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:47
msgid "Now Metal^3^ can be installed:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:48
#, no-wrap
msgid ""
"helm install \\\n"
"  metal3 oci://registry.suse.com/edge/metal3-chart \\\n"
"  --namespace metal3-system \\\n"
"  --create-namespace \\\n"
"  --set global.ironicIP=\"${STATIC_IRONIC_IP}\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:49
msgid ""
"It can take around two minutes for the initContainer to run on this "
"deployment, so ensure the pods are all running before proceeding:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:50
#, no-wrap
msgid ""
"kubectl get pods -n metal3-system\n"
"NAME                                                    READY   STATUS    "
"RESTARTS   AGE\n"
"baremetal-operator-controller-manager-85756794b-fz98d   2/2     Running   0          "
"15m\n"
"metal3-metal3-ironic-677bc5c8cc-55shd                   4/4     Running   0          "
"15m\n"
"metal3-metal3-mariadb-7c7d6fdbd8-64c7l                  1/1     Running   0          "
"15m\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:51
msgid ""
"Do not proceed to the following steps until all pods in the `metal3-system` "
"namespace are running"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:52
#, no-wrap
msgid "Installing cluster API dependencies"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:53
msgid "First, we need to disable the Rancher-embedded CAPI controller:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:54
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: management.cattle.io/v3\n"
"kind: Feature\n"
"metadata:\n"
"  name: embedded-cluster-api\n"
"spec:\n"
"  value: false\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:55
#, no-wrap
msgid ""
"kubectl delete mutatingwebhookconfiguration.admissionregistration.k8s.io "
"mutating-webhook-configuration\n"
"kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io "
"validating-webhook-configuration\n"
"kubectl wait --for=delete namespace/cattle-provisioning-capi-system "
"--timeout=300s\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:56
msgid "Then, to use the SUSE images, a configuration file is needed:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:57
#, no-wrap
msgid ""
"mkdir ~/.cluster-api\n"
"cat >  ~/.cluster-api/clusterctl.yaml <<EOF\n"
"images:\n"
"  all:\n"
"    repository: registry.suse.com/edge\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:58
msgid ""
"Install "
"https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl[clusterctl] "
"1.6.x, after which we will install the core, infrastructure, bootstrap and "
"control plane providers as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:59
#, no-wrap
msgid ""
"clusterctl init --core \"cluster-api:v1.6.2\" --infrastructure "
"\"metal3:v1.6.0\" --bootstrap \"rke2:v0.2.6\" --control-plane "
"\"rke2:v0.2.6\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:60
msgid ""
"After some time, the controller pods should be running in the `capi-system`, "
"`capm3-system`, `rke2-bootstrap-system` and `rke2-control-plane-system` "
"namespaces."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:61
#, no-wrap
msgid "Prepare downstream cluster image"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:62
#: asciidoc/product/atip-automated-provision.adoc:21
#: asciidoc/product/atip-automated-provision.adoc:69
msgid ""
"<<components-eib, Edge Image Builder>> is used to prepare a modified "
"SLEMicro base image which is provisioned on downstream cluster hosts."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:63
#: asciidoc/product/atip-automated-provision.adoc:22
msgid ""
"Much of the configuration via Edge Image Builder is possible, but in this "
"guide, we cover the minimal configurations necessary to set up the "
"downstream cluster."
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/metal3.adoc:64
#, no-wrap
msgid "Image configuration"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:65
msgid ""
"When running Edge Image Builder, a directory is mounted from the host, so it "
"is necessary to create a directory structure to to store the configuration "
"files used to define the target image."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:66
#: asciidoc/product/atip-automated-provision.adoc:28
msgid ""
"`downstream-cluster-config.yaml` is the image definition file, see "
"<<quickstart-eib>> for more details."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:67
#: asciidoc/product/atip-automated-provision.adoc:29
#: asciidoc/product/atip-automated-provision.adoc:78
msgid ""
"The base image when downloaded is `xz` compressed, which must be "
"uncompressed with `unxz` and copied/moved under the `base-images` folder."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:68
msgid ""
"The `network` folder is optional, see <<metal3-add-network-eib>> for more "
"details."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:69
msgid ""
"The custom/scripts directory contains scripts to be run on first-boot; "
"currently a `growfs.sh` script is required to resize the OS root partition "
"on deployment"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:70
#, no-wrap
msgid ""
"├── downstream-cluster-config.yaml\n"
"├── base-images/\n"
"│   └ SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
"├── network/\n"
"|   └ configure-network.sh\n"
"└── custom/\n"
"    └ scripts/\n"
"        └ growfs.sh\n"
msgstr ""

#. type: Title =====
#: asciidoc/quickstart/metal3.adoc:71
#: asciidoc/product/atip-automated-provision.adoc:33
#: asciidoc/product/atip-automated-provision.adoc:83
#, no-wrap
msgid "Downstream cluster image definition file"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:72
#: asciidoc/product/atip-automated-provision.adoc:34
msgid ""
"The `downstream-cluster-config.yaml` file is the main configuration file for "
"the downstream cluster image. The following is a minimal example for "
"deployment via Metal^3^:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:73
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: RAW\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
"  outputImageName: SLE-Micro-eib-output.raw\n"
"operatingSystem:\n"
"  kernelArgs:\n"
"    - ignition.platform.id=openstack\n"
"    - net.ifnames=1\n"
"  systemd:\n"
"    disable:\n"
"      - rebootmgr\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: ${ROOT_PASSWORD}\n"
"      sshKeys:\n"
"      - ${USERKEY1}\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:74
#: asciidoc/product/atip-automated-provision.adoc:36
msgid ""
"`$\\{ROOT_PASSWORD\\}` is the encrypted password for the root user, which "
"can be useful for test/debugging.  It can be generated with the `openssl "
"passwd -6 PASSWORD` command"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:75
#: asciidoc/product/atip-automated-provision.adoc:37
msgid ""
"For the production environments, it is recommended to use the SSH keys that "
"can be added to the users block replacing the `$\\{USERKEY1\\}` with the "
"real SSH keys."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/metal3.adoc:76
#: asciidoc/product/atip-automated-provision.adoc:38
msgid ""
"`net.ifnames=1` enables "
"https://documentation.suse.com/smart/network/html/network-interface-predictable-naming/index.html[Predictable "
"Network Interface Naming]"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/metal3.adoc:77
#: asciidoc/product/atip-automated-provision.adoc:39
msgid ""
"This matches the default configuration for the metal3 chart, but the setting "
"must match the configured chart `predictableNicNames` value."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/metal3.adoc:78
#: asciidoc/product/atip-automated-provision.adoc:40
msgid ""
"Also note `ignition.platform.id=openstack` is mandatory, without this "
"argument SLEMicro configuration via ignition will fail in the Metal^3^ "
"automated flow."
msgstr ""

#. type: Title =====
#: asciidoc/quickstart/metal3.adoc:79
#: asciidoc/product/atip-automated-provision.adoc:41
#: asciidoc/product/atip-automated-provision.adoc:85
#, no-wrap
msgid "Growfs script"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:80
msgid ""
"Currently is a custom script (`custom/scripts/growfs.sh`) which is required "
"to grow the file system to the match the disk size on first-boot after "
"provisioning. The `growfs.sh` script contains the following information:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:81
#: asciidoc/product/atip-automated-provision.adoc:43
#: asciidoc/product/atip-automated-provision.adoc:87
#, no-wrap
msgid ""
"#!/bin/bash\n"
"growfs() {\n"
"  mnt=\"$1\"\n"
"  dev=\"$(findmnt --fstab --target ${mnt} --evaluate --real --output SOURCE "
"--noheadings)\"\n"
"  # /dev/sda3 -> /dev/sda, /dev/nvme0n1p3 -> /dev/nvme0n1\n"
"  parent_dev=\"/dev/$(lsblk --nodeps -rno PKNAME \"${dev}\")\"\n"
"  # Last number in the device name: /dev/nvme0n1p42 -> 42\n"
"  partnum=\"$(echo \"${dev}\" | sed 's/^.*[^0-9]\\([0-9]\\+\\)$/\\1/')\"\n"
"  ret=0\n"
"  growpart \"$parent_dev\" \"$partnum\" || ret=$?\n"
"  [ $ret -eq 0 ] || [ $ret -eq 1 ] || exit 1\n"
"  /usr/lib/systemd/systemd-growfs \"$mnt\"\n"
"}\n"
"growfs /\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/metal3.adoc:82
#: asciidoc/product/atip-automated-provision.adoc:44
msgid ""
"Add your own custom scripts to be executed during the provisioning process "
"using the same approach.  For more information, see <<quickstart-eib>>."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/metal3.adoc:83
#: asciidoc/product/atip-automated-provision.adoc:45
msgid ""
"The bug related to this workaround is "
"https://bugzilla.suse.com/show_bug.cgi?id=1217430"
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/metal3.adoc:84
#: asciidoc/product/atip-management-cluster.adoc:247
#: asciidoc/product/atip-automated-provision.adoc:63
#, no-wrap
msgid "Image creation"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:85
#: asciidoc/product/atip-automated-provision.adoc:64
#: asciidoc/product/atip-automated-provision.adoc:118
msgid ""
"Once the directory structure is prepared following the previous sections, "
"run the following command to build the image:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:86
#: asciidoc/product/atip-automated-provision.adoc:65
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v $PWD:/eib \\\n"
" registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
" build --definition-file downstream-cluster-config.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:87
msgid ""
"This creates the output image file named `SLE-Micro-eib-output.raw`, based "
"on the definition described above."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:88
msgid ""
"The output image must then be made available via a webserver, either the "
"media-server container enabled via the <<metal3-media-server,Metal^3^ "
"chart>> or some other locally accessible server.  In the examples below, we "
"refer to this server as `imagecache.local:8080`"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:89
#, no-wrap
msgid "Adding BareMetalHost inventory"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:90
msgid ""
"Registering bare-metal servers for automated deployment requires creating "
"two resources: a Secret storing BMC access credentials and a Metal^3^ "
"BareMetalHost resource defining the BMC connection and other details:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:91
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: controlplane-0-credentials\n"
"type: Opaque\n"
"data:\n"
"  username: YWRtaW4=\n"
"  password: cGFzc3dvcmQ=\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:92
#, no-wrap
msgid ""
"apiVersion: metal3.io/v1alpha1\n"
"kind: BareMetalHost\n"
"metadata:\n"
"  name: controlplane-0\n"
"  labels:\n"
"    cluster-role: control-plane\n"
"spec:\n"
"  online: true\n"
"  bootMACAddress: \"00:f3:65:8a:a3:b0\"\n"
"  bmc:\n"
"    address: "
"redfish-virtualmedia://192.168.125.1:8000/redfish/v1/Systems/68bd0fb6-d124-4d17-a904-cdf33efe83ab\n"
"    disableCertificateVerification: true\n"
"    credentialsName: controlplane-0-credentials\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:93
msgid "Note the following:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:94
msgid ""
"The Secret username/password must be base64 encoded. Note this should not "
"include any trailing newlines (for example, use `echo -n`, not just `echo`!)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:95
msgid ""
"The `cluster-role` label may be set now or later on cluster creation. In the "
"example below, we expect `control-plane` or `worker`"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:96
msgid ""
"`bootMACAddress` must be a valid MAC that matches the control plane NIC of "
"the host"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:97
msgid ""
"The `bmc` address is the connection to the BMC management API, the following "
"are supported:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:98
msgid ""
"`redfish-virtualmedia://<IP ADDRESS>/redfish/v1/Systems/<SYSTEM ID>`: "
"Redfish virtual media, for example, SuperMicro"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:99
msgid ""
"`idrac-virtualmedia://<IP ADDRESS>/redfish/v1/Systems/System.Embedded.1`: "
"Dell iDRAC"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:100
msgid ""
"See the "
"https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md[Upstream "
"API docs] for more details on the BareMetalHost API"
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/metal3.adoc:101
#, no-wrap
msgid "Configuring Static IPs"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:102
msgid ""
"The BareMetalHost example above assumes DHCP provides the controlplane "
"network configuration, but for scenarios where manual configuration is "
"needed such as static IPs it is possible to provide additional "
"configuration, as described below."
msgstr ""

#. type: Title =====
#: asciidoc/quickstart/metal3.adoc:103
#, no-wrap
msgid "Additional script for static network configuration"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:104
msgid ""
"When creating the base image with Edge Image Builder, in the `network` "
"folder, create the following `configure-network.sh` file."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:105
msgid ""
"This consumes configuration drive data on first-boot, and configures the "
"host networking using the https://github.com/suse-edge/nm-configurator[NM "
"Configurator tool]."
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:106
#: asciidoc/product/atip-management-cluster.adoc:136
#: asciidoc/product/atip-management-cluster.adoc:209
#: asciidoc/product/atip-automated-provision.adoc:53
#: asciidoc/product/atip-automated-provision.adoc:90
#, no-wrap
msgid "#!/bin/bash\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:107
#: asciidoc/product/atip-automated-provision.adoc:54
#, no-wrap
msgid "set -eux\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:108
#: asciidoc/product/atip-automated-provision.adoc:55
#, no-wrap
msgid ""
"# Attempt to statically configure a NIC in the case where we find a "
"network_data.json\n"
"# In a configuration drive\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:109
#: asciidoc/product/atip-automated-provision.adoc:56
#, no-wrap
msgid ""
"CONFIG_DRIVE=$(blkid --label config-2 || true)\n"
"if [ -z \"${CONFIG_DRIVE}\" ]; then\n"
"  echo \"No config-2 device found, skipping network configuration\"\n"
"  exit 0\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:110
#: asciidoc/product/atip-automated-provision.adoc:57
#, no-wrap
msgid "mount -o ro $CONFIG_DRIVE /mnt\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:111
#: asciidoc/product/atip-automated-provision.adoc:58
#, no-wrap
msgid "NETWORK_DATA_FILE=\"/mnt/openstack/latest/network_data.json\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:112
#: asciidoc/product/atip-automated-provision.adoc:59
#, no-wrap
msgid ""
"if [ ! -f \"${NETWORK_DATA_FILE}\" ]; then\n"
"  umount /mnt\n"
"  echo \"No network_data.json found, skipping network configuration\"\n"
"  exit 0\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:113
#: asciidoc/product/atip-automated-provision.adoc:60
#, no-wrap
msgid ""
"DESIRED_HOSTNAME=$(cat /mnt/openstack/latest/meta_data.json | tr ',{}' '\\n' "
"| grep '\\\"metal3-name\\\"' | sed 's/.*\\\"metal3-name\\\": "
"\\\"\\(.*\\)\\\"/\\1/')\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:114
#: asciidoc/product/atip-automated-provision.adoc:61
#, no-wrap
msgid ""
"mkdir -p /tmp/nmc/{desired,generated}\n"
"cp ${NETWORK_DATA_FILE} /tmp/nmc/desired/${DESIRED_HOSTNAME}.yaml\n"
"umount /mnt\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:115
#: asciidoc/product/atip-automated-provision.adoc:62
#, no-wrap
msgid ""
"./nmc generate --config-dir /tmp/nmc/desired --output-dir "
"/tmp/nmc/generated\n"
"./nmc apply --config-dir /tmp/nmc/generated\n"
msgstr ""

#. type: Title =====
#: asciidoc/quickstart/metal3.adoc:116
#, no-wrap
msgid "Additional secret with host network configuration"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:117
msgid ""
"An additional secret containing data in the https://nmstate.io/[nmstate] "
"format supported by <<components-nmc,NM Configurator>> can be defined for "
"each host."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:118
msgid ""
"The secret is then referenced in the `BareMetalHost` resource via the "
"`preprovisioningNetworkDataName` spec field."
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:119
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: controlplane-0-networkdata\n"
"type: Opaque\n"
"stringData:\n"
"  networkData: |\n"
"    interfaces:\n"
"    - name: enp1s0\n"
"      type: ethernet\n"
"      state: up\n"
"      mac-address: \"00:f3:65:8a:a3:b0\"\n"
"      ipv4:\n"
"        address:\n"
"        - ip:  192.168.125.200\n"
"          prefix-length: 24\n"
"        enabled: true\n"
"        dhcp: false\n"
"    dns-resolver:\n"
"      config:\n"
"        server:\n"
"        - 192.168.125.1\n"
"    routes:\n"
"      config:\n"
"      - destination: 0.0.0.0/0\n"
"        next-hop-address: 192.168.125.1\n"
"        next-hop-interface: enp1s0\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:120
#, no-wrap
msgid ""
"apiVersion: metal3.io/v1alpha1\n"
"kind: BareMetalHost\n"
"metadata:\n"
"  name: controlplane-0\n"
"  labels:\n"
"    cluster-role: control-plane\n"
"spec:\n"
"  preprovisioningNetworkDataName: controlplane-0-networkdata\n"
"# Remaining content as in previous example\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:121
msgid ""
"although optional in the nmstate API, the mac-address is mandatory for "
"configuration via NM Configurator and must be provided."
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/metal3.adoc:122
#, no-wrap
msgid "BareMetalHost preparation"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:123
msgid ""
"After creating the BareMetalHost resource and associated secrets as "
"described above, a host preparation workflow is triggered:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:124
msgid "A ramdisk image is booted by virtualmedia attachment to the target host BMC"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:125
msgid ""
"The ramdisk inspects hardware details, and prepares the host for "
"provisioning (for example by cleaning disks of previous data)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:126
msgid ""
"On completion of this process, hardware details in the BareMetalHost "
"`status.hardware` field are updated and can be verified"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:127
msgid ""
"This process can take several minutes, but when completed you should see the "
"BareMetalHost state become `available`:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:128
#, no-wrap
msgid ""
"% kubectl get baremetalhost\n"
"NAME             STATE       CONSUMER   ONLINE   ERROR   AGE\n"
"controlplane-0   available              true             9m44s\n"
"worker-0         available              true             9m44s\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:129
#, no-wrap
msgid "Creating downstream clusters"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:130
msgid ""
"We now create Cluster API resources which define the downstream cluster, and "
"Machine resources which will cause the BareMetalHost resources to be "
"provisioned, then bootstrapped to form an RKE2 cluster."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:131
#, no-wrap
msgid "Control plane deployment"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:132
msgid ""
"To deploy the controlplane we define a yaml manifest similar to the one "
"below, which contains the following resources:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:133
msgid ""
"Cluster resource defines the cluster name, networks, and type of "
"controlplane/infrastructure provider (in this case RKE2/Metal3)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:134
msgid ""
"Metal3Cluster defines the controlplane endpoint (host IP for single-node, "
"LoadBalancer endpoint for multi-node, this example assumes single-node)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:135
msgid ""
"RKE2ControlPlane defines the RKE2 version and any additional configuration "
"needed during cluster bootstrapping"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:136 asciidoc/quickstart/metal3.adoc:151
msgid ""
"Metal3MachineTemplate defines the OS Image to be applied to the "
"BareMetalHost resources, and the hostSelector defines which BareMetalHosts "
"to consume"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:137 asciidoc/quickstart/metal3.adoc:152
msgid ""
"Metal3DataTemplate defines additional metaData to be passed to the "
"BareMetalHost (note networkData is not currently supported in the Edge "
"solution)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:138
msgid ""
"Note for simplicity this example assumes a single-node controlplane, where "
"the BareMetalHost is configured with an IP of `192.168.125.200` - for more "
"advanced multi-node examples please see the <<atip-automated-provisioning, "
"ATIP documentation>>"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:139
#, no-wrap
msgid ""
"apiVersion: cluster.x-k8s.io/v1beta1\n"
"kind: Cluster\n"
"metadata:\n"
"  name: sample-cluster\n"
"  namespace: default\n"
"spec:\n"
"  clusterNetwork:\n"
"    pods:\n"
"      cidrBlocks:\n"
"        - 192.168.0.0/18\n"
"    services:\n"
"      cidrBlocks:\n"
"        - 10.96.0.0/12\n"
"  controlPlaneRef:\n"
"    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"    kind: RKE2ControlPlane\n"
"    name: sample-cluster\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3Cluster\n"
"    name: sample-cluster\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:140
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3Cluster\n"
"metadata:\n"
"  name: sample-cluster\n"
"  namespace: default\n"
"spec:\n"
"  controlPlaneEndpoint:\n"
"    host: 192.168.125.200\n"
"    port: 6443\n"
"  noCloudProvider: true\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:141
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: sample-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: sample-cluster-controlplane\n"
"  replicas: 1\n"
"  agentConfig:\n"
"    format: ignition\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    version: v1.28.9+rke2r1\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:142
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3MachineTemplate\n"
"metadata:\n"
"  name: sample-cluster-controlplane\n"
"  namespace: default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      dataTemplate:\n"
"        name: sample-cluster-controlplane-template\n"
"      hostSelector:\n"
"        matchLabels:\n"
"          cluster-role: control-plane\n"
"      image:\n"
"        checksum: "
"http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256\n"
"        checksumType: sha256\n"
"        format: raw\n"
"        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:143
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3DataTemplate\n"
"metadata:\n"
"  name: sample-cluster-controlplane-template\n"
"  namespace: default\n"
"spec:\n"
"  clusterName: sample-cluster\n"
"  metaData:\n"
"    objectNames:\n"
"      - key: name\n"
"        object: machine\n"
"      - key: local-hostname\n"
"        object: machine\n"
"      - key: local_hostname\n"
"        object: machine\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:144 asciidoc/quickstart/metal3.adoc:157
msgid ""
"When the example above has been copied and adapted to suit your environment, "
"it can be applied via `kubectl` then the cluster status can be monitored "
"with `clusterctl`"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:145
#, no-wrap
msgid "% kubectl apply -f rke2-control-plane.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:146
#, no-wrap
msgid ""
"# Wait for the cluster to be provisioned - status can be checked via "
"clusterctl\n"
"% clusterctl describe cluster sample-cluster\n"
"NAME                                                    READY  SEVERITY  "
"REASON  SINCE  MESSAGE\n"
"Cluster/sample-cluster                                  True                     "
"22m\n"
"├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     "
"27m\n"
"├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     "
"22m\n"
"│ └─Machine/sample-cluster-chflc                        True                     "
"23m\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:147
#, no-wrap
msgid "Worker/Compute deployment"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:148
msgid ""
"Similar to the controlplane we define a yaml manifest, which contains the "
"following resources:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:149
msgid ""
"MachineDeployment defines the number of replicas (hosts) and the "
"bootstrap/infrastructure provider (in this case RKE2/Metal3)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:150
msgid ""
"RKE2ConfigTemplate describes the RKE2 version and first-boot configuration "
"for agent host bootstrapping"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:153
#, no-wrap
msgid ""
"apiVersion: cluster.x-k8s.io/v1beta1\n"
"kind: MachineDeployment\n"
"metadata:\n"
"  labels:\n"
"    cluster.x-k8s.io/cluster-name: sample-cluster\n"
"  name: sample-cluster\n"
"  namespace: default\n"
"spec:\n"
"  clusterName: sample-cluster\n"
"  replicas: 1\n"
"  selector:\n"
"    matchLabels:\n"
"      cluster.x-k8s.io/cluster-name: sample-cluster\n"
"  template:\n"
"    metadata:\n"
"      labels:\n"
"        cluster.x-k8s.io/cluster-name: sample-cluster\n"
"    spec:\n"
"      bootstrap:\n"
"        configRef:\n"
"          apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1\n"
"          kind: RKE2ConfigTemplate\n"
"          name: sample-cluster-workers\n"
"      clusterName: sample-cluster\n"
"      infrastructureRef:\n"
"        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"        kind: Metal3MachineTemplate\n"
"        name: sample-cluster-workers\n"
"      nodeDrainTimeout: 0s\n"
"      version: v1.28.9+rke2r1\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:154
#, no-wrap
msgid ""
"apiVersion: bootstrap.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ConfigTemplate\n"
"metadata:\n"
"  name: sample-cluster-workers\n"
"  namespace: default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      agentConfig:\n"
"        format: ignition\n"
"        version: v1.28.9+rke2r1\n"
"        kubelet:\n"
"          extraArgs:\n"
"            - provider-id=metal3://BAREMETALHOST_UUID\n"
"        additionalUserData:\n"
"          config: |\n"
"            variant: fcos\n"
"            version: 1.4.0\n"
"            systemd:\n"
"              units:\n"
"                - name: rke2-preinstall.service\n"
"                  enabled: true\n"
"                  contents: |\n"
"                    [Unit]\n"
"                    Description=rke2-preinstall\n"
"                    Wants=network-online.target\n"
"                    Before=rke2-install.service\n"
"                    "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                    [Service]\n"
"                    Type=oneshot\n"
"                    User=root\n"
"                    ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                    ExecStart=/bin/sh -c \"sed -i "
"\\\"s/BAREMETALHOST_UUID/$(jq -r .uuid "
"/mnt/openstack/latest/meta_data.json)/\\\" /etc/rancher/rke2/config.yaml\"\n"
"                    ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                    ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                    [Install]\n"
"                    WantedBy=multi-user.target\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:155
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3MachineTemplate\n"
"metadata:\n"
"  name: sample-cluster-workers\n"
"  namespace: default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      dataTemplate:\n"
"        name: sample-cluster-workers-template\n"
"      hostSelector:\n"
"        matchLabels:\n"
"          cluster-role: worker\n"
"      image:\n"
"        checksum: "
"http://imagecache.local:8080/SLE-Micro-eib-output.raw.sha256\n"
"        checksumType: sha256\n"
"        format: raw\n"
"        url: http://imagecache.local:8080/SLE-Micro-eib-output.raw\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:156
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3DataTemplate\n"
"metadata:\n"
"  name: sample-cluster-workers-template\n"
"  namespace: default\n"
"spec:\n"
"  clusterName: sample-cluster\n"
"  metaData:\n"
"    objectNames:\n"
"      - key: name\n"
"        object: machine\n"
"      - key: local-hostname\n"
"        object: machine\n"
"      - key: local_hostname\n"
"        object: machine\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:158
#, no-wrap
msgid "% kubectl apply -f rke2-agent.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:159
#, no-wrap
msgid ""
"# Wait some time for the compute/agent hosts to be provisioned\n"
"% clusterctl describe cluster sample-cluster\n"
"NAME                                                    READY  SEVERITY  "
"REASON  SINCE  MESSAGE\n"
"Cluster/sample-cluster                                  True                     "
"25m\n"
"├─ClusterInfrastructure - Metal3Cluster/sample-cluster  True                     "
"30m\n"
"├─ControlPlane - RKE2ControlPlane/sample-cluster        True                     "
"25m\n"
"│ └─Machine/sample-cluster-chflc                        True                     "
"27m\n"
"└─Workers\n"
"  └─MachineDeployment/sample-cluster                    True                     "
"22m\n"
"    └─Machine/sample-cluster-56df5b4499-zfljj           True                     "
"23m\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:160
#, no-wrap
msgid "Cluster deprovisioning"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:161
msgid ""
"The downstream cluster may be deprovisioned by deleting the resources "
"applied in the creation steps above:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:162
#, no-wrap
msgid ""
"% kubectl delete -f rke2-agent.yaml\n"
"% kubectl delete -f rke2-control-plane.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:163
msgid ""
"This triggers deprovisioning of the BareMetalHost resources, which may take "
"several minutes, after which they should be in available state again:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:164
#, no-wrap
msgid ""
"% kubectl get bmh\n"
"NAME             STATE            CONSUMER                            ONLINE   "
"ERROR   AGE\n"
"controlplane-0   deprovisioning   sample-cluster-controlplane-vlrt6   false            "
"10m\n"
"worker-0         deprovisioning   sample-cluster-workers-785x5        false            "
"10m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:165
#, no-wrap
msgid "...\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:166
#, no-wrap
msgid ""
"% kubectl get bmh\n"
"NAME             STATE       CONSUMER   ONLINE   ERROR   AGE\n"
"controlplane-0   available              false            15m\n"
"worker-0         available              false            15m\n"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:167 asciidoc/components/sle-micro.adoc:13
#: asciidoc/components/metal3.adoc:8
#: asciidoc/components/edge-image-builder.adoc:22
#: asciidoc/components/elemental.adoc:16 asciidoc/components/metallb.adoc:15
#, no-wrap
msgid "Known issues"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:168
msgid ""
"The upstream https://github.com/metal3-io/ip-address-manager[IP Address "
"Management controller] is currently not supported, because it's not yet "
"compatible with our choice of network configuration tooling and first-boot "
"toolchain in SLEMicro."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:169
msgid ""
"Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are "
"not currently supported."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:170 asciidoc/components/metal3.adoc:11
msgid "Only deployment via redfish-virtualmedia is currently supported."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:171
msgid "Deployed clusters are not currently imported into Rancher"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:172
msgid ""
"Due to disabling the Rancher embedded CAPI controller, a management cluster "
"configured for Metal^3^ as described above cannot also be used for other "
"cluster provisioning methods such as <<components-elemental, Elemental>>"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:173
#, no-wrap
msgid "Planned changes"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:174
msgid ""
"Deployed clusters imported into Rancher, this is planned via "
"https://turtles.docs.rancher.com/[Rancher Turtles] in future"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:175
msgid ""
"Aligning with Rancher Turtles is also expected to remove the requirement to "
"disable the Rancher embedded CAPI, so other cluster methods should be "
"possible via the management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:176
msgid ""
"Enable support of the IPAM resources and configuration via networkData "
"fields"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/metal3.adoc:177
#, no-wrap
msgid "Additional resources"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:178
msgid ""
"The <<atip, ATIP Documentation>> has examples of more advanced usage of "
"Metal^3^ for telco use-cases."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:179
#, no-wrap
msgid "Single-node configuration"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:180
msgid ""
"For test/PoC environments where the management cluster is a single node, it "
"is possible to avoid the requirement for an additional floating IP managed "
"via MetalLB."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:181
msgid ""
"In this mode, the endpoint for the management cluster APIs is the IP of the "
"management cluster, therefore it should be reserved when using DHCP or "
"statically configured to ensure the management cluster IP does not change - "
"referred to as `<MANAGEMENT_CLUSTER_IP>` below."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:182
msgid "To enable this scenario the metal3 chart values required are as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:183
#, no-wrap
msgid ""
"global:\n"
"  ironicIP: <MANAGEMENT_CLUSTER_IP>\n"
"metal3-ironic:\n"
"  service:\n"
"    type: NodePort\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/metal3.adoc:184
#, no-wrap
msgid "Disabling TLS for virtualmedia ISO attachment"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:185
msgid ""
"Some server vendors verify the SSL connection when attaching virtual-media "
"ISO images to the BMC, which can cause a problem because the generated "
"certificates for the Metal3 deployment are self-signed, to work around this "
"issue it's possible to disable TLS only for the virtualmedia disk attachment "
"with metal3 chart values as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:186
#, no-wrap
msgid ""
"global:\n"
"  enable_vmedia_tls: false\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:187
msgid ""
"An alternative solution is to configure the BMCs with the CA cert - in this "
"case you can read the certificates from the cluster using `kubectl`:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/metal3.adoc:188
#, no-wrap
msgid "kubectl get secret -n metal3-system ironic-vmedia-cert -o yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/metal3.adoc:189
msgid ""
"The certificate can then be configured on the server BMC console, although "
"the process for that is vendor specific (and not possible for all vendors, "
"in which case the `enable_vmedia_tls` flag may be required)."
msgstr ""

#. type: Title =
#: asciidoc/quickstart/elemental.adoc:1
#, no-wrap
msgid "Remote host onboarding with Elemental"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:2
msgid ""
"This section documents the \"phone home network provisioning\" solution as "
"part of SUSE Edge, where we use Elemental to assist with node "
"onboarding. Elemental is a software stack enabling remote host registration "
"and centralized full cloud-native OS management with Kubernetes. In the SUSE "
"Edge stack we use the registration feature of Elemental to enable remote "
"host onboarding into Rancher so that hosts can be integrated into a "
"centralized management platform and from there, deploy and manage Kubernetes "
"clusters along with layered components, applications, and their lifecycle, "
"all from a common place."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:3
msgid ""
"This approach can be useful in scenarios where the devices that you want to "
"control are not on the same network as the upstream cluster or do not have a "
"out-of-band management controller onboard to allow more direct control, and "
"where you're booting many different \"unknown\" systems at the edge, and "
"need to securely onboard and manage them at scale. This is a common scenario "
"for use cases in retail, industrial IoT, or other spaces where you have "
"little control over the network your devices are being installed in."
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:5
#, no-wrap
msgid "quickstart-elemental-architecture.png"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/elemental.adoc:6
#, no-wrap
msgid "Resources needed"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:7
msgid ""
"The following describes the minimum system and environmental requirements to "
"run through this quickstart:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:8
msgid ""
"A host for the centralized management cluster (the one hosting Rancher and "
"Elemental):"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:9
msgid ""
"Minimum 8 GB RAM and 20 GB disk space for development or testing (see "
"https://ranchermanager.docs.rancher.com/pages-for-subheaders/installation-requirements#hardware-requirements[here] "
"for production use)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:10
msgid ""
"A target node to be provisioned, i.e. the edge device (a virtual machine can "
"be used for demoing or testing purposes)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:11
msgid "Minimum 4GB RAM, 2 CPU cores, and 20 GB disk"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:12
msgid ""
"A resolvable host name for the management cluster or a static IP address to "
"use with a service like sslip.io"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:13
msgid "A host to build the installation media via Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:14
msgid ""
"Running SLES 15 SP5, openSUSE Leap 15.5, or another compatible operating "
"system that supports Podman."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:15
msgid ""
"With https://kubernetes.io/docs/reference/kubectl/kubectl/[Kubectl], "
"https://podman.io[Podman], and https://helm.sh[Helm] installed"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:16
msgid "A USB flash drive to boot from (if using physical hardware)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:17
msgid ""
"Existing data found on target machines will be overwritten as part of the "
"process, please make sure you backup any data on any USB storage devices and "
"disks attached to target deployment nodes."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:18
msgid ""
"This guide is created using a Digital Ocean droplet to host the upstream "
"cluster and an Intel NUC as the downstream device. For building the "
"installation media, SUSE Linux Enterprise Server is used."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/elemental.adoc:19
#, no-wrap
msgid "How to use Elemental"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:20
msgid "The basic steps to install and use Elemental are:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:21
msgid "<<build-bootstrap-cluster>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:22
msgid "<<install-rancher>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:23
msgid "<<install-elemental>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:24
msgid "<<build-installation-media>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:25
msgid "<<boot-downstream-nodes>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:26
msgid "<<create-downstream-clusters>>"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:27
#, no-wrap
msgid "Build bootstrap cluster [[build-bootstrap-cluster]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:28
msgid ""
"Start by creating a cluster capable of hosting Rancher and Elemental. This "
"cluster needs to be routable from the network that the downstream nodes are "
"connected to."
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/elemental.adoc:29
#, no-wrap
msgid "Create Kubernetes cluster"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:30
msgid ""
"If you are using a hyperscaler (such as Azure, AWS or Google Cloud), the "
"easiest way to set up a cluster is using their built-in tools. For the sake "
"of conciseness in this guide, we do not detail the process of each of these "
"options."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:31
msgid ""
"If you are installing onto bare metal or another hosting service where you "
"need to also provide the Kubernetes distribution itself, we recommend using "
"https://docs.rke2.io/install/quickstart[RKE2]."
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/elemental.adoc:32
#, no-wrap
msgid "Set up DNS"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:33
msgid ""
"Before continuing, you need to set up access to your cluster. As with the "
"setup of the cluster itself, how you configure DNS will be different "
"depending on where it is being hosted."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:34
msgid ""
"If you do not want to handle setting up DNS records (for example, this is "
"just an ephemeral test server), you can use a service like "
"https://sslip.io[sslip.io] instead. With this service, you can resolve any "
"IP address with `<address>.sslip.io`."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:35
#, no-wrap
msgid "Install Rancher [[install-rancher]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:36
msgid ""
"To install Rancher, you need to get access to the Kubernetes API of the "
"cluster you just created. This looks differently depending on what "
"distribution of Kubernetes is being used."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:37
msgid ""
"For RKE2, the kubeconfig file will have been written to "
"`/etc/rancher/rke2/rke2.yaml`.  Save this file as `~/.kube/config` on your "
"local system.  You may need to edit the file to include the correct "
"externally routable IP address or host name."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:38
msgid ""
"Install Rancher easily with the commands from the "
"https://ranchermanager.docs.rancher.com/pages-for-subheaders/install-upgrade-on-a-kubernetes-cluster[Rancher "
"Documentation]:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:39
msgid "Install https://cert-manager.io[cert-manager]:"
msgstr ""

#. type: Labeled list
#: asciidoc/quickstart/elemental.adoc:40 asciidoc/quickstart/elemental.adoc:53
#: asciidoc/quickstart/elemental.adoc:129
#, no-wrap
msgid "Linux"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:41 asciidoc/quickstart/elemental.adoc:47
#: asciidoc/day2/mgmt-cluster.adoc:75
#, no-wrap
msgid "helm repo add rancher-prime https://charts.rancher.com/server-charts/prime\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:42 asciidoc/quickstart/elemental.adoc:48
#, no-wrap
msgid "kubectl create namespace cattle-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:43
#, no-wrap
msgid ""
"kubectl apply -f "
"https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml\n"
"helm repo add jetstack https://charts.jetstack.io\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:44 asciidoc/components/longhorn.adoc:21
#, no-wrap
msgid "helm repo update\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:45
#, no-wrap
msgid ""
"helm install cert-manager jetstack/cert-manager \\\n"
" --namespace cert-manager \\\n"
" --create-namespace\n"
msgstr ""

#. type: Labeled list
#: asciidoc/quickstart/elemental.adoc:46 asciidoc/quickstart/elemental.adoc:55
#, no-wrap
msgid "Windows"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:49
#, no-wrap
msgid ""
"kubectl apply -f "
"https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.crds.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:50
#, no-wrap
msgid "helm repo add jetstack https://charts.jetstack.io\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:51
#, no-wrap
msgid ""
"helm repo update\n"
"helm install cert-manager jetstack/cert-manager `\n"
"  --namespace cert-manager `\n"
"  --create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:52
msgid "Then install Rancher itself:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:54
#, no-wrap
msgid ""
"helm install rancher rancher-prime/rancher \\\n"
"  --namespace cattle-system \\\n"
"  --set hostname=<DNS or sslip from above> \\\n"
"  --set replicas=1 \\\n"
"  --set bootstrapPassword=<PASSWORD_FOR_RANCHER_ADMIN>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:56
#, no-wrap
msgid ""
"helm install rancher rancher-prime/rancher `\n"
"  --namespace cattle-system `\n"
"  --set hostname=<DNS or sslip from above> `\n"
"  --set replicas=1 `\n"
"  --set bootstrapPassword=<PASSWORD_FOR_RANCHER_ADMIN>\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:57
msgid ""
"If this is intended to be a production system, please use cert-manager to "
"configure a real certificate (such as one from Let's Encrypt)."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:58
msgid ""
"Browse to the host name you set up and log in to Rancher with the "
"`bootstrapPassword` you used. You will be guided through a short setup "
"process."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:59
#, no-wrap
msgid "Install Elemental [[install-elemental]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:60
msgid ""
"With Rancher installed, you can now install the Elemental operator and "
"required CRD's. The Helm chart for Elemental is published as an OCI artifact "
"so the installation is a little simpler than other charts.  It can be "
"installed from either the same shell you used to install Rancher or in the "
"browser from within Rancher's shell."
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:61
#, no-wrap
msgid ""
"helm install --create-namespace -n cattle-elemental-system \\\n"
" elemental-operator-crds \\\n"
" oci://registry.suse.com/rancher/elemental-operator-crds-chart \\\n"
" --version 1.4.4\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:62
#, no-wrap
msgid ""
"helm install --create-namespace -n cattle-elemental-system \\\n"
" elemental-operator \\\n"
" oci://registry.suse.com/rancher/elemental-operator-chart \\\n"
" --version 1.4.4\n"
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/elemental.adoc:63
#, no-wrap
msgid "(Optionally) Install the Elemental UI extension"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:64
msgid ""
"To use the Elemental UI, log in to your Rancher instance, click the "
"three-dot menu in the upper left:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:65
#, no-wrap
msgid "Installing Elemental extension1"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:66
#, no-wrap
msgid "installing-elemental-extension-1.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:67
msgid ""
"From the \"Available\" tab on this page, click \"Install\" on the Elemental "
"card:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:68
#, no-wrap
msgid "Installing Elemental extension 2"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:69
#, no-wrap
msgid "installing-elemental-extension-2.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:70
msgid "Confirm that you want to install the extension:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:71
#, no-wrap
msgid "Installing Elemental extension 3"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:72
#, no-wrap
msgid "installing-elemental-extension-3.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:73
msgid "After it installs, you will be prompted to reload the page."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:74
#, no-wrap
msgid "Installing Elemental extension 4"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:75
#, no-wrap
msgid "installing-elemental-extension-4.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:76
msgid ""
"Once you reload, you can access the Elemental extension through the \"OS "
"Management\" global app."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:77
#, no-wrap
msgid "Accessing Elemental extension"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:78
#, no-wrap
msgid "accessing-elemental-extension.png"
msgstr ""

#. type: Title ====
#: asciidoc/quickstart/elemental.adoc:79
#, no-wrap
msgid "Configure Elemental"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:80
msgid ""
"For simplicity, we recommend setting the variable `$ELEM` to the full path "
"of where you want the configuration directory:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:81
#, no-wrap
msgid ""
"export ELEM=$HOME/elemental\n"
"mkdir -p $ELEM\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:82
msgid ""
"To allow machines to register to Elemental, we need to create a "
"`MachineRegistration` object in the `fleet-default` namespace."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:83
msgid "Let us create a basic version of this object:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:84
#, no-wrap
msgid ""
"cat << EOF > $ELEM/registration.yaml\n"
"apiVersion: elemental.cattle.io/v1beta1\n"
"kind: MachineRegistration\n"
"metadata:\n"
"  name: ele-quickstart-nodes\n"
"  namespace: fleet-default\n"
"spec:\n"
"  machineName: \"\\${System Information/Manufacturer}-\\${System "
"Information/UUID}\"\n"
"  machineInventoryLabels:\n"
"    manufacturer: \"\\${System Information/Manufacturer}\"\n"
"    productName: \"\\${System Information/Product Name}\"\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:85
#, no-wrap
msgid "kubectl apply -f $ELEM/registration.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:86 asciidoc/quickstart/elemental.adoc:118
msgid ""
"The `cat` command escapes each `$` with a backslash (`\\`) so that Bash does "
"not template them. Remove the backslashes if copying manually."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:87
msgid "Once the object is created, find and note the endpoint that gets assigned:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:88
#, no-wrap
msgid ""
"REGISURL=$(kubectl get machineregistration ele-quickstart-nodes -n "
"fleet-default -o jsonpath='{.status.registrationURL}')\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:89
msgid "Alternatively, this can also be done from the UI."
msgstr ""

#. type: Labeled list
#: asciidoc/quickstart/elemental.adoc:90 asciidoc/quickstart/elemental.adoc:104
#: asciidoc/quickstart/elemental.adoc:141
#, no-wrap
msgid "UI Extension"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:91
msgid "From the OS Management extension, click \"Create Registration Endpoint\":"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:92
#, no-wrap
msgid "Click Create Registration"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:93
#, no-wrap
msgid "click-create-registration.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:94
msgid "Give this configuration a name."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:95
#, no-wrap
msgid "Add Name"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:96
#, no-wrap
msgid "create-registration-name.png"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:97
msgid ""
"You can ignore the Cloud Configuration field as the data here is overridden "
"by the following steps with Edge Image Builder."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:98
msgid ""
"Next, scroll down and click \"Add Label\" for each label you want to be on "
"the resource that gets created when a machine registers. This is useful for "
"distinguishing machines."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:99
#, no-wrap
msgid "Add Labels"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:100
#, no-wrap
msgid "create-registration-labels.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:101
msgid "Lastly, click \"Create\" to save the configuration."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:102
#, no-wrap
msgid "Click Create"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:103
#, no-wrap
msgid "create-registration-create.png"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:105
msgid ""
"If you just created the configuration, you should see the Registration URL "
"listed and can click \"Copy\" to copy the address:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/quickstart/elemental.adoc:106
#, no-wrap
msgid "Copy URL"
msgstr ""

#. type: Target for macro image
#: asciidoc/quickstart/elemental.adoc:107
#, no-wrap
msgid "get-registration-url.png"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:108
msgid ""
"If you clicked away from that screen, you can click \"Registration "
"Endpoints\" in the left menu, then click the name of the endpoint you just "
"created."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:109
msgid "This URL is used in the next step."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:110
#, no-wrap
msgid "Build the installation media [[build-installation-media]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:111
msgid ""
"While the current version of Elemental has a way to build its own "
"installation media, in SUSE Edge 3.0 we do this with the Edge Image Builder "
"instead, so the resulting system is built with "
"https://www.suse.com/products/micro/[SLE Micro] as the base Operating "
"System."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:112
msgid ""
"For more details on the Edge Image Builder, check out the "
"<<quickstart-eib,Getting Started Guide for it>> and also the "
"<<components-eib,Component Documentation>>."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:113
msgid "From a Linux system with Podman installed, run:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:114
#, no-wrap
msgid ""
"mkdir -p $ELEM/eib_quickstart/base-images\n"
"mkdir -p $ELEM/eib_quickstart/elemental\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:115
#, no-wrap
msgid "curl $REGISURL -o $ELEM/eib_quickstart/elemental/elemental_config.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:116
#, no-wrap
msgid ""
"cat << EOF > $ELEM/eib_quickstart/eib-config.yaml\n"
"apiVersion: 1.0\n"
"image:\n"
"    imageType: iso\n"
"    arch: x86_64\n"
"    baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso\n"
"    outputImageName: elemental-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"\\$6\\$jHugJNNd3HElGsUZ\\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"EOF\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:117
msgid "The unencoded password is `eib`."
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:119
#, no-wrap
msgid ""
"podman run --privileged --rm -it -v $ELEM/eib_quickstart/:/eib \\\n"
" registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
" build --definition-file eib-config.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:120
msgid ""
"If you are booting a physical device, we need to burn the image to a USB "
"flash drive. This can be done with:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:121
#, no-wrap
msgid ""
"sudo dd if=/eib_quickstart/elemental-image.iso of=/dev/<PATH_TO_DISK_DEVICE> "
"status=progress\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:122
#, no-wrap
msgid "Boot the downstream nodes [[boot-downstream-nodes]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:123
msgid ""
"Now that we have created the installation media, we can boot our downstream "
"nodes with it."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:124
msgid ""
"For each of the systems that you want to control with Elemental, add the "
"installation media and boot the device. After installation, it will reboot "
"and register itself."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:125
msgid ""
"If you are using the UI extension, you should see your node appear in the "
"\"Inventory of Machines.\""
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:126
msgid ""
"Do not remove the installation medium until you've seen the login prompt; "
"during first-boot files are still accessed on the USB stick."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/elemental.adoc:127
#, no-wrap
msgid "Create downstream clusters [[create-downstream-clusters]]"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:128
msgid ""
"There are two objects we need to create when provisioning a new cluster "
"using Elemental."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:130
msgid ""
"The first is the `MachineInventorySelectorTemplate`. This object allows us "
"to specify a mapping between clusters and the machines in the inventory."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:131
msgid ""
"Create a selector which will match any machine in the inventory with a "
"label:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:132
#, no-wrap
msgid ""
"cat << EOF > $ELEM/selector.yaml\n"
"apiVersion: elemental.cattle.io/v1beta1\n"
"kind: MachineInventorySelectorTemplate\n"
"metadata:\n"
"  name: location-123-selector\n"
"  namespace: fleet-default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      selector:\n"
"        matchLabels:\n"
"          locationID: '123'\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:133
msgid "Apply the resource to the cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:134
#, no-wrap
msgid "kubectl apply -f $ELEM/selector.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:135
msgid "Obtain the name of the machine and add the matching label:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:136
#, no-wrap
msgid ""
"MACHINENAME=$(kubectl get MachineInventory -n fleet-default | awk 'NR>1 "
"{print $1}')\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:137
#, no-wrap
msgid ""
"kubectl label MachineInventory -n fleet-default \\\n"
" $MACHINENAME locationID=123\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:138
msgid ""
"Create a simple single-node K3s cluster resource and apply it to the "
"cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:139
#, no-wrap
msgid ""
"cat << EOF > $ELEM/cluster.yaml\n"
"apiVersion: provisioning.cattle.io/v1\n"
"kind: Cluster\n"
"metadata:\n"
"  name: location-123\n"
"  namespace: fleet-default\n"
"spec:\n"
"  kubernetesVersion: v1.28.9+k3s1\n"
"  rkeConfig:\n"
"    machinePools:\n"
"      - name: pool1\n"
"        quantity: 1\n"
"        etcdRole: true\n"
"        controlPlaneRole: true\n"
"        workerRole: true\n"
"        machineConfigRef:\n"
"          kind: MachineInventorySelectorTemplate\n"
"          name: location-123-selector\n"
"          apiVersion: elemental.cattle.io/v1beta1\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:140
#, no-wrap
msgid "kubectl apply -f $ELEM/cluster.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:142
msgid ""
"The UI extension allows for a few shortcuts to be taken. Note that managing "
"multiple locations may involve too much manual work."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:143
msgid ""
"As before, open the left three-dot menu and select \"OS Management.\" This "
"brings you back to the main screen for managing your Elemental systems."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:144
msgid ""
"On the left sidebar, click \"Inventory of Machines.\" This opens the "
"inventory of machines that have registered."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:145
msgid ""
"To create a cluster from these machines, select the systems you want, click "
"the \"Actions\" drop-down list, then \"Create Elemental Cluster.\" This "
"opens the Cluster Creation dialog while also creating a "
"MachineSelectorTemplate to use in the background."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:146
msgid ""
"On this screen, configure the cluster you want to be built. For this quick "
"start, K3s v1.28.9+k3s1 is selected and the rest of the options are left as "
"is."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:147
msgid "You may need to scroll down to see more options."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:148
msgid ""
"After creating these objects, you should see a new Kubernetes cluster spin "
"up using the new node you just installed with."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:149
msgid ""
"To allow for easier grouping of systems, you could add a startup script that "
"finds something in the environment that is known to be unique to that "
"location."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:150
msgid ""
"For example, if you know that each location will have a unique subnet, you "
"can write a script that finds the network prefix and adds a label to the "
"corresponding MachineInventory."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:151
msgid "This would typically be custom to your system's design but could look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:152
#, no-wrap
msgid ""
"INET=`ip addr show dev eth0 | grep \"inet\\ \"`\n"
"elemental-register --label \"network=$INET\" \\\n"
" --label \"network=$INET\" /oem/registration\n"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/elemental.adoc:153
#, no-wrap
msgid "Node Reset"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:154
msgid ""
"SUSE Rancher Elemental supports the ability to perform a \"node reset\" "
"which can optionally trigger when either a whole cluster is deleted from "
"Rancher, a single node is deleted from a cluster, or a node is manually "
"deleted from the machine inventory. This is useful when you want to reset "
"and clean-up any orphaned resources and want to automatically bring the "
"cleaned node back into the machine inventory so it can be reused. This is "
"not enabled by default, and thus any system that is removed, will not be "
"cleaned up (i.e. data will not be removed, and any Kubernetes cluster "
"resources will continue to operate on the downstream clusters) and it will "
"require manual intervention to wipe data and re-register the machine to "
"Rancher via Elemental."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:155
msgid ""
"If you wish for this functionality to be enabled by default, you need to "
"make sure that your `MachineRegistration` explicitly enables this by adding "
"`config.elemental.reset.enabled: true`, for example:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:156
#, no-wrap
msgid ""
"config:\n"
"  elemental:\n"
"    registration:\n"
"      auth: tpm\n"
"    reset:\n"
"      enabled: true\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:157
msgid ""
"Then, all systems registered with this `MachineRegistration` will "
"automatically receive the `elemental.cattle.io/resettable: 'true'` "
"annotation in their configuration. If you wish to do this manually on "
"individual nodes, e.g. because you've got an existing `MachineInventory` "
"that doesn't have this annotation, or you have already deployed nodes, you "
"can modify the `MachineInventory` and add the `resettable` configuration, "
"for example:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/elemental.adoc:158
#, no-wrap
msgid ""
"apiVersion: elemental.cattle.io/v1beta1\n"
"kind: MachineInventory\n"
"metadata:\n"
"  annotations:\n"
"    elemental.cattle.io/os.unmanaged: 'true'\n"
"    elemental.cattle.io/resettable: 'true'\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:159
msgid ""
"In SUSE Edge 3.0, the Elemental Operator puts down a marker on the operating "
"system that will trigger the cleanup process automatically; it will stop all "
"Kubernetes services, remove all persistent data, uninstall all Kubernetes "
"services, cleanup any remaining Kubernetes/Rancher directories, and force a "
"re-registration to Rancher via the original Elemental `MachineRegistration` "
"configuration. This happens automaticaly, there is no need for any manual "
"intervention. The script that gets called can be found in "
"`/opt/edge/elemental_node_cleanup.sh` and is triggered via `systemd.path` "
"upon the placement of the marker, so its execution is immediate."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/elemental.adoc:160
msgid ""
"Using the `resettable` functionality assumes that the desired behavior when "
"removing a node/cluster from Rancher is to wipe data and force a "
"re-registration. Data loss is guaranteed in this situation, so only use this "
"if you're sure that you want automatic reset to be performed."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/elemental.adoc:161
#, no-wrap
msgid "Next steps"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:162
msgid "Here are some recommended resources to research after using this guide:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:163
msgid "End-to-end automation in <<components-fleet>>"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/elemental.adoc:164
msgid "Additional network configuration options in <<components-nmc>>"
msgstr ""

#. type: Title =
#: asciidoc/quickstart/eib.adoc:1
#, no-wrap
msgid "Standalone clusters with Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:2
msgid ""
"Edge Image Builder (EIB) is a tool that streamlines the process of "
"generating customized, ready-to-boot (CRB) disk images for bootstrapping "
"machines, even in fully air-gapped scenarios. EIB is used to create "
"deployment images for use in all three of the SUSE Edge deployment "
"footprints, as it's flexible enough to offer the smallest customizations, "
"e.g. adding a user or setting the timezone, through offering a "
"comprehensively configured image that sets up, for example, complex "
"networking configurations, deploys multi-node Kubernetes clusters, deploys "
"customer workloads, and registers to the centralized management platform via "
"Rancher/Elemental and SUSE Manager. EIB runs as in a container image, making "
"it incredibly portable across platforms and ensuring that all of the "
"required dependencies are self-contained, having a very minimal impact on "
"the installed packages of the system that's being used to operate the tool."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:3
msgid ""
"For more information, read the <<components-eib,Edge Image Builder "
"Introduction>>."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:5
msgid ""
"An x86_64 physical host (or virtual machine) running SLES 15 SP5, openSUSE "
"Leap 15.5, or openSUSE Tumbleweed."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:6 asciidoc/components/networking.adoc:22
msgid "An available container runtime (e.g. Podman)"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:7
msgid ""
"A downloaded copy of the latest SLE Micro 5.5 SelfInstall \"GM2\" ISO image "
"found https://www.suse.com/download/sle-micro/[here]."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:8
msgid ""
"Other operating systems may function so long as a compatible container "
"runtime is available, but testing on other platforms has not been "
"extensive. The documentation focuses on Podman, but the same functionality "
"should be able to be achieved with Docker."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:9
#, no-wrap
msgid "Getting the EIB Image"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:10
msgid ""
"The EIB container image is publicly available and can be downloaded from the "
"SUSE Edge registry by running the following command on your image build "
"host:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:11 asciidoc/components/networking.adoc:26
#, no-wrap
msgid "podman pull registry.suse.com/edge/edge-image-builder:1.0.2\n"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/eib.adoc:12
#, no-wrap
msgid "Creating the image configuration directory"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:13
msgid ""
"As EIB runs within a container, we need to mount a configuration directory "
"from the host, enabling you to specify your desired configuration, and "
"during the build process EIB has access to any required input files and "
"supporting artifacts. This directory must follow a specific structure. Let's "
"create it, assuming that this directory will exist in your home directory, "
"and called \"eib\":"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:14 asciidoc/components/networking.adoc:29
#, no-wrap
msgid ""
"export CONFIG_DIR=$HOME/eib\n"
"mkdir -p $CONFIG_DIR/base-images\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:15
msgid ""
"In the previous step we created a \"base-images\" directory that will host "
"the SLE Micro 5.5 input image, let's ensure that the downloaded image is "
"copied over to the configuration directory:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:16
#, no-wrap
msgid ""
"cp "
"/path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso "
"$CONFIG_DIR/base-images/slemicro.iso\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:17
msgid ""
"During the EIB run, the original base image is *not* modified; a new and "
"customized version is created with the desired configuration in the root of "
"the EIB config directory."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:18 asciidoc/components/networking.adoc:33
#: asciidoc/components/networking.adoc:41
#: asciidoc/components/networking.adoc:62
#: asciidoc/components/networking.adoc:175
msgid "The configuration directory at this point should look like the following:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:19
#, no-wrap
msgid ""
"└── base-images/\n"
"    └── slemicro.iso\n"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:20 asciidoc/components/networking.adoc:35
#, no-wrap
msgid "Creating the image definition file"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:21
msgid ""
"The definition file describes the majority of configurable options that the "
"Edge Image Builder supports, a full example of options can be found "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/pkg/image/testdata/full-valid-example.yaml[here], "
"and we would recommend that you take a look at the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md[upstream "
"building images guide] for more comprehensive examples than the one we're "
"going to run through below. Let's start with a very basic definition file "
"for our OS image:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:22
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/iso-definition.yaml\n"
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:23
msgid ""
"This definition specifies that we're generating an output image for an "
"`x86_64` based system. The image that will be used as the base for further "
"modification is an `iso` image named `slemicro.iso`, expected to be located "
"at `$CONFIG_DIR/base-images/slemicro.iso`. It also outlines that after EIB "
"finishes modifying the image, the output image will be named "
"`eib-image.iso`, and by default will reside in `$CONFIG_DIR`."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:24
msgid "Now our directory structure should look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:25
#, no-wrap
msgid ""
"├── iso-definition.yaml\n"
"└── base-images/\n"
"    └── slemicro.iso\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:26
msgid ""
"In the following sections we'll walk through a few examples of common "
"operations:"
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:27
#, no-wrap
msgid "Configuring OS Users"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:28
msgid ""
"EIB allows you to preconfigure users with login information, such as "
"passwords or SSH keys, including setting a fixed root password. As part of "
"this example we're going to fix the root password, and the first step is to "
"use `OpenSSL` to create a one-way encrypted password:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:29
#, no-wrap
msgid "openssl passwd -6 SecurePassword\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:30
msgid "This will output something similar to:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:31
#, no-wrap
msgid "$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:32
msgid ""
"We can then add a section in the definition file called `operatingSystem` "
"with a `users` array inside it. The resulting file should look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:33
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:34
msgid ""
"It's also possible to add additional users, create the home directories, set "
"user-id's, add ssh-key authentication, and modify group information. Please "
"refer to the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md[upstream "
"building images guide] for further examples."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:35
#, no-wrap
msgid "Configuring RPM packages"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:36
msgid ""
"One of the major features of EIB is to provide a mechanism to add additional "
"software packages to the image, so when the installation completes the "
"system is able to leverage the installed packages right away. EIB permits "
"users to specify the following:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:37
msgid "Packages by their name within a list in the image definition"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:38
msgid "Network repositories to search for these packages in"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:39
msgid ""
"SUSE Customer Center (SCC) credentials to search official SUSE repositories "
"for the listed packages"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:40
msgid ""
"Via an `$CONFIG_DIR/rpms` directory, side-load custom RPM's that don't exist "
"in network repositories"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:41
msgid ""
"Via the same directory (`$CONFIG_DIR/rpms/gpg-keys`), GPG-keys to enable "
"validation of third party packages"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:42
msgid ""
"EIB will then run through a package resolution process at image build time, "
"taking the base image as the input, and attempts to pull and install all "
"supplied packages, either specified via the list or provided locally. EIB "
"downloads all of the packages, including any dependencies into a repository "
"that exists within the output image and instructs the system to install "
"these during the first boot process. Doing this process during the image "
"build guarantees that the packages will successfully install during "
"first-boot on the desired platform, e.g. the node at the edge. This is also "
"advantageous in environments where you want to bake the additional packages "
"into the image rather than pull them over the network when in operation, "
"e.g. for air-gapped or restricted network environments."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:43
msgid ""
"As a simple example to demonstrate this, we are going to install the "
"`nvidia-container-toolkit` RPM package found in the third party "
"vendor-supported NVIDIA repository:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:44
#, no-wrap
msgid ""
"  packages:\n"
"    packageList:\n"
"      - nvidia-container-toolkit\n"
"    additionalRepos:\n"
"      - url: "
"https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:45
msgid "The resulting definition file looks like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:46
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1\n"
"  packages:\n"
"    packageList:\n"
"      - nvidia-container-toolkit\n"
"    additionalRepos:\n"
"      - url: "
"https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:47
msgid ""
"The above is a simple example, but for completeness, download the NVIDIA "
"package signing key before running the image generation:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:48
#, no-wrap
msgid ""
"$ mkdir -p $CONFIG_DIR/rpms/gpg-keys\n"
"$ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey > "
"rpms/gpg-keys/nvidia.gpg\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:49
msgid ""
"Adding in additional RPM's via this method is meant for the addition of "
"supported third party components or user-supplied (and maintained) packages; "
"this mechanism should not be used to add packages that would not usually be "
"supported on SLE Micro. If this mechanism is used to add components from "
"openSUSE repositories (which are not supported), including from newer "
"releases or service packs, you may end up with an unsupported configuration, "
"especially when dependency resolution results in core parts of the operating "
"system being replaced, even though the resulting system may appear to "
"function as expected. If you're unsure, contact your SUSE representative for "
"assistance in determining the supportability of your desired configuration."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:50
msgid ""
"A more comprehensive guide with additional examples can be found in the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/installing-packages.md[upstream "
"installing packages guide]."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:51
#, no-wrap
msgid "Configuring Kubernetes cluster and user workloads"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:52
msgid ""
"Another feature of EIB is the ability to use it to automate the deployment "
"of both single-node and multi-node highly-available Kubernetes clusters that "
"\"bootstrap in place\", i.e. don't require any form of centralized "
"management infrastructure to coordinate. The primary driver behind this "
"approach is for air-gapped deployments, or network restricted environments, "
"but it also serves as a way of quickly bootstrapping standalone clusters, "
"even if full and unrestricted network access is available."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:53
msgid ""
"This method enables not only the deployment of the customized operating "
"system, but also the ability to specify Kubernetes configuration, any "
"additional layered components via Helm charts, and any user workloads via "
"supplied Kubernetes manifests. However, the design principle behind using "
"this method is that we default to assuming that the user is wanting to "
"air-gap and therefore any items specified in the image definition will be "
"pulled into the image, which includes user-supplied workloads, where EIB "
"will make sure that any discovered images that are required by definitions "
"supplied are copied locally, and are served by the embedded image registry "
"in the resulting deployed system."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:54
msgid ""
"In this next example, we're going to take our existing image definition and "
"will specify a Kubernetes configuration (in this example it doesn't list the "
"systems and their roles, so we default to assuming single-node), which will "
"instruct EIB to provision a single-node RKE2 Kubernetes cluster. To show the "
"automation of both the deployment of both user-supplied workloads (via "
"manifest) and layered components (via Helm), we are going to install "
"KubeVirt via the SUSE Edge Helm chart, as well as NGINX via a Kubernetes "
"manifest. The additional configuration we need to append to the existing "
"image definition is as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:55
#, no-wrap
msgid ""
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  manifests:\n"
"    urls:\n"
"      - https://k8s.io/examples/application/nginx-app.yaml\n"
"  helm:\n"
"    charts:\n"
"      - name: kubevirt-chart\n"
"        version: 0.2.4\n"
"        repositoryName: suse-edge\n"
"    repositories:\n"
"      - name: suse-edge\n"
"        url: oci://registry.suse.com/edge\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:56
msgid "The resulting full definition file should now look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:57
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$G392FCbxVgnLaFw1$Ujt00mdpJ3tDHxEg1snBU3GjujQf6f8kvopu7jiCBIhRbRvMmKUqwcmXAKggaSSKeUUOEtCP3ZUoZQY7zTXnC1\n"
"  packages:\n"
"    packageList:\n"
"      - nvidia-container-toolkit\n"
"    additionalRepos:\n"
"      - url: "
"https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  manifests:\n"
"    urls:\n"
"      - https://k8s.io/examples/application/nginx-app.yaml\n"
"  helm:\n"
"    charts:\n"
"      - name: kubevirt-chart\n"
"        version: 0.2.4\n"
"        repositoryName: suse-edge\n"
"    repositories:\n"
"      - name: suse-edge\n"
"        url: oci://registry.suse.com/edge\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:58
msgid ""
"Further examples of options such as multi-node deployments, custom "
"networking, and Helm chart options/values can be found in the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes[upstream "
"documentation]."
msgstr ""

#. type: Title ===
#: asciidoc/quickstart/eib.adoc:59
#, no-wrap
msgid "Configuring the network"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:60
msgid ""
"In the last example in this quickstart, let's configure the network that "
"will be brought up when a system is provisioned with the image generated by "
"EIB. It's important to understand that unless a network configuration is "
"supplied, the default model is that DHCP will be used on all interfaces "
"discovered at boot time. However, this is not always a desirable "
"configuration, especially if DHCP is not available and you need to provide "
"static configurations, or you need to set up more complex networking "
"constructs, e.g. bonds, LACP, and VLAN's, or need to override certain "
"parameters, e.g. hostnames, DNS servers, and routes."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:61
msgid ""
"EIB provides the ability to provide either per-node configurations (where "
"the system in question is uniquely identified by its MAC address), or an "
"override for supplying an identical configuration to each machine, which is "
"more useful when the system MAC addresses aren't known. An additional tool "
"is used by EIB called Network Manager Configurator, or `nmc` for short, "
"which is a tool built by the SUSE Edge team to allow custom networking "
"configurations to be applied based on the https://nmstate.io/[nmstate.io] "
"declarative network schema, and at boot time will identify the node it's "
"booting on and will apply the desired network configuration prior to any "
"services coming up."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:62
msgid ""
"We'll now apply a static network configuration for a system with a single "
"interface by describing the desired network state in a node-specific file "
"(based on the desired hostname) in the required `network` directory:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:63
#, no-wrap
msgid "mkdir $CONFIG_DIR/network\n"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:64
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/host1.local.yaml\n"
"routes:\n"
"  config:\n"
"  - destination: 0.0.0.0/0\n"
"    metric: 100\n"
"    next-hop-address: 192.168.122.1\n"
"    next-hop-interface: eth0\n"
"    table-id: 254\n"
"  - destination: 192.168.122.0/24\n"
"    metric: 100\n"
"    next-hop-address:\n"
"    next-hop-interface: eth0\n"
"    table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"    - 192.168.122.1\n"
"    - 8.8.8.8\n"
"interfaces:\n"
"- name: eth0\n"
"  type: ethernet\n"
"  state: up\n"
"  mac-address: 34:8A:B1:4B:16:E7\n"
"  ipv4:\n"
"    address:\n"
"    - ip: 192.168.122.50\n"
"      prefix-length: 24\n"
"    dhcp: false\n"
"    enabled: true\n"
"  ipv6:\n"
"    enabled: false\n"
"EOF\n"
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:65
msgid ""
"The above example is set up for the default `192.168.122.0/24` subnet "
"assuming that testing is being executed on a virtual machine, please adapt "
"to suit your environment, not forgetting the MAC address. As the same image "
"can be used to provision multiple nodes, networking configured by EIB (via "
"`nmc`) is dependent on it being able to uniquely identify the node by its "
"MAC address, and hence during boot `nmc` will apply the correct networking "
"configuration to each machine. This means that you'll need to know the MAC "
"addresses of the systems you want to install onto. Alternatively, the "
"default behavior is to rely on DHCP, but you can utilize the "
"`configure-network.sh` hook to apply a common configuration to all nodes - "
"see the <<components-nmc,networking guide>> for further details."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:66
msgid "The resulting file structure should look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:67
#, no-wrap
msgid ""
"├── iso-definition.yaml\n"
"├── base-images/\n"
"│   └── slemicro.iso\n"
"└── network/  \n"
"    └── host1.local.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:68
msgid ""
"The network configuration we just created will be parsed and the necessary "
"NetworkManager connection files will be automatically generated and inserted "
"into the new installation image that EIB will create. These files will be "
"applied during the provisioning of the host, resulting in a complete network "
"configuration."
msgstr ""

#. type: delimited block =
#: asciidoc/quickstart/eib.adoc:69
msgid ""
"Please refer to the <<components-nmc, Edge Networking component>> for a more "
"comprehensive explanation of the above configuration and examples of this "
"feature."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/eib.adoc:70
#, no-wrap
msgid "Building the image"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:71
msgid ""
"Now that we've got a base image and an image definition for EIB to consume, "
"let's go ahead and build the image. For this, we simply use `podman` to call "
"the EIB container with the \"build\" command, specifying the definition "
"file:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:72
#, no-wrap
msgid ""
"podman run --rm -it --privileged -v $CONFIG_DIR:/eib \\\n"
"registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
"build --definition-file iso-definition.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:73
msgid "The output of the command should be similar to:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:74
#, no-wrap
msgid ""
"Setting up Podman API listener...\n"
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Resolving package dependencies...\n"
"Rpm .......................... [SUCCESS]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 5.9 MB/s)\n"
"Populating Embedded Artifact Registry... 100%  (3/3, 11 it/min)\n"
"Embedded Artifact Registry ... [SUCCESS]\n"
"Keymap ....................... [SUCCESS]\n"
"Configuring Kubernetes component...\n"
"The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.\n"
"Downloading file: rke2_installer.sh\n"
"Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 98 "
"MB/s)\n"
"Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, "
"100 MB/s)\n"
"Downloading file: rke2.linux-amd64.tar.gz 100%  (34/34 MB, 101 MB/s)\n"
"Downloading file: sha256sum-amd64.txt 100%  (3.9/3.9 kB, 1.5 MB/s)\n"
"Downloading file: dl-manifest-1.yaml 100%  (498/498 B, 7.2 MB/s)\n"
"Kubernetes ................... [SUCCESS]\n"
"Certificates ................. [SKIPPED]\n"
"Building ISO image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:75
msgid "The built ISO image is stored at `$CONFIG_DIR/eib-image.iso`:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:76
#, no-wrap
msgid ""
"├── iso-definition.yaml\n"
"├── eib-image.iso\n"
"├── _build\n"
"│   └── cache/\n"
"│       └── ...\n"
"│   └── build-<timestamp>/\n"
"│       └── ...\n"
"├── base-images/\n"
"│   └── slemicro.iso\n"
"└── network/\n"
"    └── host1.local.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:77
msgid ""
"Each build creates a time-stamped folder in `$CONFIG_DIR/_build/` that "
"includes the logs of the build, the artifacts used during the build, and the "
"`combustion` and `artefacts` directories which contain all the scripts and "
"artifacts that are added to the CRB image."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:78
msgid "The contents of this directory should look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/quickstart/eib.adoc:79
#, no-wrap
msgid ""
"├── build-<timestamp>/\n"
"│   │── combustion/\n"
"│   │   ├── 05-configure-network.sh\n"
"│   │   ├── 10-rpm-install.sh\n"
"│   │   ├── 12-keymap-setup.sh\n"
"│   │   ├── 13b-add-users.sh\n"
"│   │   ├── 20-k8s-install.sh\n"
"│   │   ├── 26-embedded-registry.sh\n"
"│   │   ├── 48-message.sh\n"
"│   │   ├── network/\n"
"│   │   │   ├── host1.local/\n"
"│   │   │   │   └── eth0.nmconnection\n"
"│   │   │   └── host_config.yaml\n"
"│   │   ├── nmc\n"
"│   │   └── script\n"
"│   │── artefacts/\n"
"│   │   │── registry/\n"
"│   │   │   ├── hauler\n"
"│   │   │   ├── nginx:1.14.2-registry.tar.zst\n"
"│   │   │   ├── rancher_kubectl:v1.28.7-registry.tar.zst\n"
"│   │   │   └── "
"registry.suse.com_suse_sles_15.5_virt-operator:1.1.1-150500.8.12.1-registry.tar.zst\n"
"│   │   │── rpms/\n"
"│   │   │   └── rpm-repo\n"
"│   │   │       ├── addrepo0\n"
"│   │   │       │   └── x86_64\n"
"│   │   │       │       ├── nvidia-container-toolkit-1.15.0-1.x86_64.rpm\n"
"│   │   │       │       ├── ...\n"
"│   │   │       ├── repodata\n"
"│   │   │       │   ├── ...\n"
"│   │   │       └── zypper-success\n"
"│   │   └── kubernetes/\n"
"│   │       ├── rke2_installer.sh\n"
"│   │       ├── registries.yaml\n"
"│   │       ├── server.yaml\n"
"│   │       ├── images/\n"
"│   │       │   ├── rke2-images-cilium.linux-amd64.tar.zst\n"
"│   │       │   └── rke2-images-core.linux-amd64.tar.zst\n"
"│   │       ├── install/\n"
"│   │       │   ├── rke2.linux-amd64.tar.gz\n"
"│   │       │   └── sha256sum-amd64.txt\n"
"│   │       └── manifests/\n"
"│   │           ├── dl-manifest-1.yaml\n"
"│   │           └── kubevirt-chart.yaml\n"
"│   ├── createrepo.log\n"
"│   ├── eib-build.log\n"
"│   ├── embedded-registry.log\n"
"│   ├── helm\n"
"│   │   └── kubevirt-chart\n"
"│   │       └── kubevirt-0.2.4.tgz\n"
"│   ├── helm-pull.log\n"
"│   ├── helm-template.log\n"
"│   ├── iso-build.log\n"
"│   ├── iso-build.sh\n"
"│   ├── iso-extract\n"
"│   │   └── ...\n"
"│   ├── iso-extract.log\n"
"│   ├── iso-extract.sh\n"
"│   ├── modify-raw-image.sh\n"
"│   ├── network-config.log\n"
"│   ├── podman-image-build.log\n"
"│   ├── podman-system-service.log\n"
"│   ├── prepare-resolver-base-tarball-image.log\n"
"│   ├── prepare-resolver-base-tarball-image.sh\n"
"│   ├── raw-build.log\n"
"│   ├── raw-extract\n"
"│   │   └── ...\n"
"│   └── resolver-image-build\n"
"│       └──...\n"
"└── cache\n"
"    └── ...\n"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:80
msgid ""
"If the build fails, `eib-build.log` is the first log that contains "
"information. From there, it will direct you to the component that failed for "
"debugging."
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:81
msgid "At this point, you should have a ready-to-use image that will:"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:82
msgid "Deploy SLE Micro 5.5"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:83
msgid "Configure the root password"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:84
msgid "Install the `nvidia-container-toolkit` package"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:85
msgid "Configure an embedded container registry to serve content locally"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:86
msgid "Install single-node RKE2"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:87
msgid "Configure static networking"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:88
msgid "Install KubeVirt"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:89
msgid "Deploy a user-supplied manifest"
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/eib.adoc:90
#, no-wrap
msgid "Debugging the image build process"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:91
msgid ""
"If the image build process fails, refer to the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/debugging.md[upstream "
"debugging guide]."
msgstr ""

#. type: Title ==
#: asciidoc/quickstart/eib.adoc:92
#, no-wrap
msgid "Testing your newly built image"
msgstr ""

#. type: Plain text
#: asciidoc/quickstart/eib.adoc:93
msgid ""
"For instructions on how to test the newly built CRB image, refer to the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/testing-guide.md[upstream "
"image testing guide]."
msgstr ""

#. type: Title =====
#: asciidoc/components/rancher.adoc:1 asciidoc/day2/mgmt-cluster.adoc:137
#, no-wrap
msgid "Rancher"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:2
msgid ""
"See Rancher upstream documentation at "
"https://ranchermanager.docs.rancher.com."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:3
msgid ""
"Rancher is a powerful open-source Kubernetes management platform that "
"streamlines the deployment, operations and monitoring of Kubernetes clusters "
"across multiple environments. Whether you manage clusters on premises, in "
"the cloud, or at the edge, Rancher provides a unified and centralized "
"platform for all your Kubernetes needs."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher.adoc:4
#, no-wrap
msgid "Key Features of Rancher"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:5
msgid ""
"**Multi-cluster management:** Rancher's intuitive interface lets you manage "
"Kubernetes clusters from anywhere—public clouds, private data centers and "
"edge locations."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:6
msgid ""
"**Security and compliance:** Rancher enforces security policies, role-based "
"access control (RBAC), and compliance standards across your Kubernetes "
"landscape."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:7
msgid ""
"**Simplified cluster operations:** Rancher automates cluster provisioning, "
"upgrades and troubleshooting, simplifying Kubernetes operations for teams of "
"all sizes."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:8
msgid ""
"**Centralized application catalog:** The Rancher application catalog offers "
"a diverse range of Helm charts and Kubernetes Operators, making it easy to "
"deploy and manage containerized applications."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:9
msgid ""
"**Continuous delivery:** Rancher supports GitOps and CI/CD pipelines, "
"enabling automated and streamlined application delivery processes."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher.adoc:10
#, no-wrap
msgid "Rancher's use in SUSE Edge"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:11
msgid "Rancher provides several core functionalities to the SUSE Edge stack:"
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:12
#, no-wrap
msgid "Centralized Kubernetes management"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:13
msgid ""
"In typical edge deployments with numerous distributed clusters, Rancher acts "
"as a central control plane for managing these Kubernetes clusters. It offers "
"a unified interface for provisioning, upgrading, monitoring, and "
"troubleshooting, simplifying operations, and ensuring consistency."
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:14
#, no-wrap
msgid "Simplified cluster deployment"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:15
msgid ""
"Rancher streamlines Kubernetes cluster creation on the lightweight SLE Micro "
"(SUSE Linux Enterprise Micro) operating system, easing the rollout of edge "
"infrastructure with robust Kubernetes capabilities."
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:16
#, no-wrap
msgid "Application deployment and management"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:17
msgid ""
"The integrated Rancher application catalog can simplify deploying and "
"managing containerized applications across SUSE Edge clusters, enabling "
"seamless edge workload deployment."
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:18
#, no-wrap
msgid "Security and policy enforcement"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:19
msgid ""
"Rancher provides policy-based governance tools, role-based access control "
"(RBAC), and integration with external authentication providers. This helps "
"SUSE Edge deployments maintain security and compliance, critical in "
"distributed environments."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher.adoc:20 asciidoc/components/sle-micro.adoc:7
#: asciidoc/components/elemental.adoc:11 asciidoc/components/k3s.adoc:7
#: asciidoc/components/rke2.adoc:16 asciidoc/components/metallb.adoc:12
#, no-wrap
msgid "Best practices"
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:21
#, no-wrap
msgid "GitOps"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:22
msgid ""
"Rancher includes Fleet as a built-in component to allow manage cluster "
"configurations and application deployments with code stored in git."
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher.adoc:23
#, no-wrap
msgid "Observability"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:24
msgid ""
"Rancher includes built-in monitoring and logging tools like Prometheus and "
"Grafana for comprehensive insights into your cluster health and performance."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher.adoc:25 asciidoc/components/longhorn.adoc:46
#: asciidoc/components/neuvector.adoc:16
#: asciidoc/components/virtualization.adoc:166
#, no-wrap
msgid "Installing with Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:26
msgid ""
"SUSE Edge is using <<components-eib>> in order to customize base SLE Micro "
"OS images.  Follow <<rancher-install>> for an air-gapped installation of "
"Rancher on top of Kubernetes clusters provisioned by EIB."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher.adoc:27
#, no-wrap
msgid "Additional Resources"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:28
msgid "https://rancher.com/docs/[Rancher Documentation]"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:29
msgid "https://www.rancher.academy/[Rancher Academy]"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:30
msgid "https://rancher.com/community/[Rancher Community]"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:31
msgid "https://helm.sh/[Helm Charts]"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher.adoc:32
msgid "https://operatorhub.io/[Kubernetes Operators]"
msgstr ""

#. type: Title =
#: asciidoc/components/rancher-dashboard-extensions.adoc:1
#, no-wrap
msgid "Rancher Dashboard Extensions"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:2
msgid ""
"Extensions allow users, developers, partners, and customers to extend and "
"enhance the Rancher UI. SUSE Edge 3.0 provides KubeVirt and Akri dashboard "
"extensions."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:3
msgid ""
"See "
"`https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions[Rancher "
"documentation]` for general information about Rancher Dashboard Extensions."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:5
msgid ""
"To enable extensions Rancher requires ui-plugin operator to be "
"installed. When using the Rancher Dashboard UI, navigate to *Extensions* in "
"the left navigation *Configuration* section. If the ui-plugin operator is "
"not installed you'll get a prompt asking to enable the extensions support as "
"described "
"`https://ranchermanager.docs.rancher.com/integrations-in-rancher/rancher-extensions#installing-extensions[here]`."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:6
msgid "The operator can be also installed using Helm:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:7
#, no-wrap
msgid ""
"helm repo add rancher-charts https://charts.rancher.io/\n"
"helm upgrade --create-namespace -n cattle-ui-plugin-system \\\n"
"  --install ui-plugin-operator rancher-charts/ui-plugin-operator\n"
"helm upgrade --create-namespace -n cattle-ui-plugin-system \\\n"
"  --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:8
msgid ""
"Or with Fleet by creating a dedicated GitRepo resource. For more information "
"see xref:fleet.adoc[Fleet] section and "
"`https://github.com/suse-edge/fleet-examples/blob/main/gitrepos/rancher-ui-plugin-operator-gitrepo.yaml[fleet-examples]` "
"repository."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher-dashboard-extensions.adoc:9
#: asciidoc/components/k3s.adoc:8 asciidoc/components/rke2.adoc:17
#: asciidoc/components/virtualization.adoc:149
#: asciidoc/integrations/nats.adoc:13
#, no-wrap
msgid "Installation"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:10
msgid ""
"All SUSE Edge 3.0 components including dashboard extensions are distributed "
"as OCI artifacts. Rancher Dashboard Apps/Marketplace does not support OCI "
"based Helm repositories "
"`https://github.com/rancher/dashboard/issues/9815[yet]`. Therefore, to "
"install SUSE Edge Extensions you can use Helm or Fleet:"
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher-dashboard-extensions.adoc:11
#, no-wrap
msgid "Installing with Helm"
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:12
#, no-wrap
msgid ""
"# KubeVirt extension\n"
"helm install kubevirt-dashboard-extension "
"oci://registry.suse.com/edge/kubevirt-dashboard-extension-chart --version "
"1.0.0 --namespace cattle-ui-plugin-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:13
#, no-wrap
msgid ""
"# Akri extension\n"
"helm install akri-dashboard-extension "
"oci://registry.suse.com/edge/akri-dashboard-extension-chart --version 1.0.0 "
"--namespace cattle-ui-plugin-system\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/rancher-dashboard-extensions.adoc:14
msgid "The extensions need to be installed in `cattle-ui-plugin-system` namespace."
msgstr ""

#. type: delimited block =
#: asciidoc/components/rancher-dashboard-extensions.adoc:15
msgid "After an extension is installed, Rancher Dashboard UI needs to be reloaded."
msgstr ""

#. type: Title ===
#: asciidoc/components/rancher-dashboard-extensions.adoc:16
#, no-wrap
msgid "Installing with Fleet"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:17
msgid ""
"Installing Dashboard Extensions with Fleet requires defining a `gitRepo` "
"resource which points to a Git repository with custom `fleet.yaml` bundle "
"configuration file(s)."
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:18
#, no-wrap
msgid ""
"# KubeVirt extension fleet.yaml\n"
"defaultNamespace: cattle-ui-plugin-system\n"
"helm:\n"
"  releaseName: kubevirt-dashboard-extension\n"
"  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart\n"
"  version: \"1.0.0\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:19
#, no-wrap
msgid ""
"# Akri extension fleet.yaml\n"
"defaultNamespace: cattle-ui-plugin-system\n"
"helm:\n"
"  releaseName: akri-dashboard-extension\n"
"  chart: oci://registry.suse.com/edge/akri-dashboard-extension-chart\n"
"  version: \"1.0.0\"\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/rancher-dashboard-extensions.adoc:20
msgid ""
"The `releaseName` property is required and needs to match the extension name "
"to get the extension correctly installed by ui-plugin-operator."
msgstr ""

#. type: delimited block -
#: asciidoc/components/rancher-dashboard-extensions.adoc:21
#, no-wrap
msgid ""
"cat <<- EOF | kubectl apply -f -\n"
"apiVersion: fleet.cattle.io/v1alpha1\n"
"metadata:\n"
"  name: edge-dashboard-extensions\n"
"  namespace: fleet-local\n"
"spec:\n"
"  repo: https://github.com/suse-edge/fleet-examples.git\n"
"  branch: main\n"
"  paths:\n"
"  - fleets/kubevirt-dashboard-extension/\n"
"  - fleets/akri-dashboard-extension/\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:22
msgid ""
"For more information see xref:fleet.adoc[Fleet] section and "
"`https://github.com/suse-edge/fleet-examples[fleet-examples]` repository."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:23
msgid ""
"Once the Extensions are installed they are listed in *Extensions* section "
"under *Installed* tabs. Since they are not installed via Apps/Marketplace, "
"they are marked with `Third-Party` label."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/rancher-dashboard-extensions.adoc:24
#, no-wrap
msgid "installed-dashboard-extensions.png"
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher-dashboard-extensions.adoc:25
#, no-wrap
msgid "KubeVirt Dashboard Extension"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:26
msgid ""
"KubeVirt Extension provides basic virtual machine management for Rancher "
"dashboard UI. It's capabilities are described in "
"xref:virtualization.adoc#kubevirt-dashboard-extension[Edge Virtualization]."
msgstr ""

#. type: Title ==
#: asciidoc/components/rancher-dashboard-extensions.adoc:27
#, no-wrap
msgid "Akri Dashboard Extension"
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:28
msgid ""
"Akri is a Kubernetes Resource Interface that lets you easily expose "
"heterogeneous leaf devices (such as IP cameras and USB devices) as resources "
"in a Kubernetes cluster, while also supporting the exposure of embedded "
"hardware resources such as GPUs and FPGAs. Akri continually detects nodes "
"that have access to these devices and schedules workloads based on them."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:29
#: asciidoc/components/akri.adoc:26
msgid ""
"Akri Dashboard Extension allows you to use Rancher Dashboard user interface "
"to manage and monitor leaf devices and run workloads once these devices are "
"discovered."
msgstr ""

#. type: Plain text
#: asciidoc/components/rancher-dashboard-extensions.adoc:30
msgid ""
"Extension capabilities are further described in "
"xref:akri.adoc#akri-dashboard-extension[Akri section]."
msgstr ""

#. type: Title =
#: asciidoc/components/fleet.adoc:1
#, no-wrap
msgid "Fleet"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:2
msgid ""
"https://fleet.rancher.io[Fleet] is a container management and deployment "
"engine designed to offer users more control on the local cluster and "
"constant monitoring through GitOps. Fleet focuses not only on the ability to "
"scale, but it also gives users a high degree of control and visibility to "
"monitor exactly what is installed on the cluster."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:3
msgid ""
"Fleet can manage deployments from Git of raw Kubernetes YAML, Helm charts, "
"Kustomize, or any combination of the three. Regardless of the source, all "
"resources are dynamically turned into Helm charts, and Helm is used as the "
"engine to deploy all resources in the cluster. As a result, users can enjoy "
"a high degree of control, consistency and auditability of their clusters."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:4
msgid ""
"For information about how Fleet works, see "
"https://ranchermanager.docs.rancher.com/integrations-in-rancher/fleet/architecture[this "
"page]."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:5
#, no-wrap
msgid "Installing Fleet with Helm"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:6
msgid ""
"Fleet comes built-in to Rancher, but it can be also "
"https://fleet.rancher.io/installation[installed] as a standalone application "
"on any Kubernetes cluster using Helm."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:7
#, no-wrap
msgid "Using Fleet with Rancher"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:8
msgid ""
"Rancher uses Fleet to deploy applications across managed "
"clusters. Continuous delivery with Fleet introduces GitOps at scale, "
"designed to manage applications running on large numbers of clusters."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:9
msgid ""
"Fleet shines as an integrated part of Rancher. Clusters managed with Rancher "
"automatically get the Fleet agent deployed as part of the "
"installation/import process and the cluster is immediately available to be "
"managed by Fleet."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:10
#, no-wrap
msgid "Accessing Fleet in the Rancher UI"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:11
msgid ""
"Fleet comes preinstalled in Rancher and is managed by the *Continuous "
"Delivery* option in the Rancher UI. For additional information on Continuous "
"Delivery and other Fleet troubleshooting tips, refer "
"https://fleet.rancher.io/troubleshooting[here]."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/fleet.adoc:12
#, no-wrap
msgid "fleet-dashboard.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:13
msgid "Continuous Delivery section consists of following items:"
msgstr ""

#. type: Title ===
#: asciidoc/components/fleet.adoc:14
#, no-wrap
msgid "Dashboard"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:15
msgid ""
"An overview page of all GitOps repositories across all workspaces. Only the "
"workspaces with repositories are displayed."
msgstr ""

#. type: Title ===
#: asciidoc/components/fleet.adoc:16
#, no-wrap
msgid "Git repos"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:17
msgid ""
"A list of GitOps repositories in the selected workspace. Select the active "
"workspace using the drop-down list at the top of the page."
msgstr ""

#. type: Title ===
#: asciidoc/components/fleet.adoc:18
#, no-wrap
msgid "Clusters"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:19
msgid ""
"A list of managed clusters. By default, all Rancher-managed clusters are "
"added to the `fleet-default` workspace. `fleet-local` workspace includes the "
"local (management) cluster. From here, it is possible to `Pause` or `Force "
"update` the clusters or move the cluster into another workspace. Editing the "
"cluster allows to update labels and annotations used for grouping the "
"clusters."
msgstr ""

#. type: Title ===
#: asciidoc/components/fleet.adoc:20
#, no-wrap
msgid "Cluster groups"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:21
msgid ""
"This section allows custom grouping of the clusters within the workspace "
"using selectors."
msgstr ""

#. type: Title ===
#: asciidoc/components/fleet.adoc:22
#, no-wrap
msgid "Advanced"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:23
msgid ""
"The \"Advanced\" section allows to manage workspaces and other related Fleet "
"resources."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:24
#, no-wrap
msgid ""
"Example of installing KubeVirt with Rancher and Fleet using Rancher "
"dashboard"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:25
msgid "Create a Git repository containing the `fleet.yaml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/fleet.adoc:26
#, no-wrap
msgid ""
"defaultNamespace: kubevirt\n"
"helm:\n"
"  chart: \"oci://registry.suse.com/edge/kubevirt-chart\"\n"
"  version: \"0.2.4\"\n"
"  # kubevirt namespace is created by kubevirt as well, we need to take "
"ownership of it\n"
"  takeOwnership: true\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:27
msgid ""
"In the Rancher dashboard, navigate to *☰ > Continuous Delivery > Git Repos* "
"and click `Add Repository`."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:28
msgid ""
"The Repository creation wizard guides through creation of the Git "
"repo. Provide *Name*, *Repository URL* (referencing the Git repository "
"created in the previous step) and select the appropriate branch or "
"revision. In the case of a more complex repository, specify *Paths* to use "
"multiple directories in a single repository."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/fleet.adoc:29
#, no-wrap
msgid "fleet-create-repo1.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:30
msgid "Click `Next`."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:31
msgid ""
"In the next step, you can define where the workloads will get "
"deployed. Cluster selection offers several basic options: you can select no "
"clusters, all clusters, or directly choose a specific managed cluster or "
"cluster group (if defined). The \"Advanced\" option allows to directly edit "
"the selectors via YAML."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/fleet.adoc:32
#, no-wrap
msgid "fleet-create-repo2.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:33
msgid ""
"Click `Create`. The repository gets created. From now on, the workloads are "
"installed and kept in sync on the clusters matching the repository "
"definition."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:34
#, no-wrap
msgid "Debugging and troubleshooting"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:35
msgid ""
"The \"Advanced\" navigation section provides overviews of lower-level Fleet "
"resources. https://fleet.rancher.io/ref-bundle-stages[A bundle] is an "
"internal resource used for the orchestration of resources from Git. When a "
"Git repo is scanned, it produces one or more bundles."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:36
msgid ""
"To find bundles relevant to a specific repository, go to the Git repo detail "
"page and click the `Bundles` tab."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/fleet.adoc:37
#, no-wrap
msgid "fleet-repo-bundles.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:38
msgid ""
"For each cluster, the bundle is applied to a BundleDeployment resource that "
"is created. To view BundleDeployment details, click the `Graph` button in "
"the upper right of the Git repo detail page.  A graph of *Repo > Bundles > "
"BundleDeployments* is loaded. Click the BundleDeployment in the graph to see "
"its details and click the `Id` to view the BundleDeployment YAML."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/fleet.adoc:39
#, no-wrap
msgid "fleet-repo-graph.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:40
msgid ""
"For additional information on Fleet troubleshooting tips, refer "
"https://fleet.rancher.io/troubleshooting[here]."
msgstr ""

#. type: Title ==
#: asciidoc/components/fleet.adoc:41
#, no-wrap
msgid "Fleet examples"
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:42
msgid ""
"The Edge team maintains a "
"https://github.com/suse-edge/fleet-examples[repository] with examples of "
"installing Edge projects with Fleet."
msgstr ""

#. type: Plain text
#: asciidoc/components/fleet.adoc:43
msgid ""
"The Fleet project includes a "
"https://github.com/rancher/fleet-examples[fleet-examples] repository that "
"covers all use cases for https://fleet.rancher.io/gitrepo-content[Git "
"repository structure]."
msgstr ""

#. type: Title =
#: asciidoc/components/sle-micro.adoc:1
#, no-wrap
msgid "SLE Micro"
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:2
msgid ""
"See https://documentation.suse.com/sle-micro/5.5/[SLE Micro official "
"documentation]"
msgstr ""

#. type: delimited block _
#: asciidoc/components/sle-micro.adoc:3
msgid ""
"SUSE Linux Enterprise Micro is a lightweight and secure operating system for "
"the edge. It merges the enterprise-hardened components of SUSE Linux "
"Enterprise with the features that developers want in a modern, immutable "
"operating system. As a result, you get a reliable infrastructure platform "
"with best-in-class compliance that is also simple to use."
msgstr ""

#. type: Title ==
#: asciidoc/components/sle-micro.adoc:4
#, no-wrap
msgid "How does SUSE Edge use SLE Micro?"
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:5
msgid ""
"We use SLE Micro as the base operating system for our platform stack. This "
"provides us with a secure, stable and minimal base for building upon."
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:6
msgid ""
"SLE Micro is unique in its use of file system (Btrfs) snapshots to allow for "
"easy rollbacks in case something goes wrong with an upgrade. This allows for "
"secure remote upgrades for the entire platform even without physical access "
"in case of issues."
msgstr ""

#. type: Title ===
#: asciidoc/components/sle-micro.adoc:8 asciidoc/components/elemental.adoc:12
#, no-wrap
msgid "Installation media"
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:9
msgid ""
"SUSE Edge uses the <<components-eib,Edge Image Builder>> to preconfigure the "
"SLE Micro self-install installation image."
msgstr ""

#. type: Title ===
#: asciidoc/components/sle-micro.adoc:10
#, no-wrap
msgid "Local administration"
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:11
msgid ""
"SLE Micro comes with Cockpit to allow the local management of the host "
"through a Web application."
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:12
msgid ""
"This service is disabled by default but can be started by enabling the "
"systemd service `cockpit.socket`."
msgstr ""

#. type: Plain text
#: asciidoc/components/sle-micro.adoc:14
msgid ""
"There is no desktop environment available in SLE Micro at the moment but a "
"containerized solution is in development."
msgstr ""

#. type: Title =====
#: asciidoc/components/metal3.adoc:1
#: asciidoc/components/edge-image-builder.adoc:10
#: asciidoc/day2/mgmt-cluster.adoc:148
#, no-wrap
msgid "Metal^3^"
msgstr ""

#. type: Plain text
#: asciidoc/components/metal3.adoc:2
msgid ""
"https://metal3.io/[Metal^3^] is a CNCF project which provides bare-metal "
"infrastructure management capabilities for Kubernetes."
msgstr ""

#. type: Title ==
#: asciidoc/components/metal3.adoc:5
#, no-wrap
msgid "How does SUSE Edge use Metal3?"
msgstr ""

#. type: Plain text
#: asciidoc/components/metal3.adoc:7
msgid ""
"This method provides declarative APIs that enable inventory and state "
"management of bare-metal servers, including automated inspection, cleaning "
"and provisioning/deprovisioning."
msgstr ""

#. type: Plain text
#: asciidoc/components/metal3.adoc:9
msgid ""
"The upstream https://github.com/metal3-io/ip-address-manager[IP Address "
"Management controller] is currently not supported, because it is not yet "
"compatible with our choice of network configuration tooling."
msgstr ""

#. type: Plain text
#: asciidoc/components/metal3.adoc:10
msgid ""
"Relatedly, the IPAM resources and Metal3DataTemplate networkData fields are "
"not supported."
msgstr ""

#. type: Title =
#: asciidoc/components/edge-image-builder.adoc:1
#, no-wrap
msgid "Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:2
msgid ""
"See the https://github.com/suse-edge/edge-image-builder[Official "
"Repository]."
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:3
msgid ""
"Edge Image Builder (EIB) is a tool that streamlines the generation of "
"customized, ready-to-boot (CRB) disk images for bootstrapping "
"machines. These images enable the end-to-end deployment of the entire SUSE "
"software stack with a single image."
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:4
msgid ""
"Whilst EIB can create CRB images for all provisioning scenarios, EIB "
"demonstrates a tremendous value in air-gapped deployments with limited or "
"completely isolated networks."
msgstr ""

#. type: Title ==
#: asciidoc/components/edge-image-builder.adoc:5
#, no-wrap
msgid "How does SUSE Edge use Edge Image Builder?"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:6
msgid ""
"SUSE Edge uses EIB for the simplified and quick configuration of customized "
"SLE Micro images for a variety of scenarios. These scenarios include the "
"bootstrapping of virtual and bare-metal machines with:"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:7
msgid "Fully air-gapped deployments of K3s/RKE2 Kubernetes (single & multi-node)"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:8
msgid "Fully air-gapped Helm chart and Kubernetes manifest deployments"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:9
msgid "Registration to Rancher via Elemental API"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:11
msgid ""
"Customized networking (for example, static IP, host name, VLAN's, bonding, "
"etc.)"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:12
msgid ""
"Customized operating system configurations (for example, users, groups, "
"passwords, SSH keys, proxies, NTP, custom SSL certificates, etc.)"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:13
msgid ""
"Air-gapped installation of host-level and side-loaded RPM packages "
"(including dependency resolution)"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:14
msgid "Registration to SUSE Manager for OS management"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:15
msgid "Embedded container images"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:16
msgid "Kernel command-line arguments"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:17
msgid "Systemd units to be enabled/disabled at boot time"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:18
msgid "Custom scripts and files for any manual tasks"
msgstr ""

#. type: Title ==
#: asciidoc/components/edge-image-builder.adoc:19
#, no-wrap
msgid "Getting started"
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:20
msgid ""
"Comprehensive documentation for the usage and testing of Edge Image Builder "
"can be found "
"https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs[here]."
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:21
msgid ""
"Additionally, here is a <<quickstart-eib,quick start guide>> for Edge Image "
"Builder covering a basic deployment scenario."
msgstr ""

#. type: Plain text
#: asciidoc/components/edge-image-builder.adoc:23
msgid ""
"EIB air-gaps Helm charts through templating the Helm charts and parsing all "
"the images within the template. If a Helm chart does not keep all of its "
"images within the template and instead side-loads the images, EIB will not "
"be able to air-gap those images automatically. The solution to this is to "
"manually add any undetected images to the `embeddedArtifactRegistry` section "
"of the definition file."
msgstr ""

#. type: Title =
#: asciidoc/components/networking.adoc:1
#, no-wrap
msgid "Edge Networking"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:2
msgid ""
"This section describes the approach to network configuration in the SUSE "
"Edge solution.  We will show how to configure NetworkManager on SLE Micro in "
"a declarative manner, and explain how the related tools are integrated."
msgstr ""

#. type: Title ==
#: asciidoc/components/networking.adoc:3
#, no-wrap
msgid "Overview of NetworkManager"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:4
msgid ""
"NetworkManager is a tool that manages the primary network connection and "
"other connection interfaces."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:5
msgid ""
"NetworkManager stores network configurations as connection files that "
"contain the desired state.  These connections are stored as files in the "
"`/etc/NetworkManager/system-connections/` directory."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:6
msgid ""
"Details about NetworkManager can be found in the "
"https://documentation.suse.com/sle-micro/5.5/html/SLE-Micro-all/cha-nm-configuration.html[upstream "
"SLE Micro documentation]."
msgstr ""

#. type: Title ==
#: asciidoc/components/networking.adoc:7
#, no-wrap
msgid "Overview of nmstate"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:8
msgid ""
"nmstate is a widely adopted library (with an accompanying CLI tool) which "
"offers a declarative API for network configurations via a predefined schema."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:9
msgid ""
"Details about nmstate can be found in the https://nmstate.io/[upstream "
"documentation]."
msgstr ""

#. type: Title ==
#: asciidoc/components/networking.adoc:10
#, no-wrap
msgid "Enter: NetworkManager Configurator (nmc)"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:11
msgid ""
"The network customization options available in SUSE Edge are achieved via a "
"CLI tool called NetworkManager Configurator or _nmc_ for short.  It is "
"leveraging the functionality provided by the nmstate library and, as such, "
"it is fully capable of configuring static IP addresses, DNS servers, VLANs, "
"bonding, bridges, etc.  This tool allows us to generate network "
"configurations from predefined desired states and to apply those across many "
"different nodes in an automated fashion."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:12
msgid ""
"Details about the NetworkManager Configurator (nmc) can be found in the "
"https://github.com/suse-edge/nm-configurator[upstream repository]."
msgstr ""

#. type: Title ==
#: asciidoc/components/networking.adoc:13
#, no-wrap
msgid "How does SUSE Edge use NetworkManager Configurator?"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:14
msgid ""
"SUSE Edge utilizes _nmc_ for the network customizations in the various "
"different provisioning models:"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:15
msgid ""
"Custom network configurations in the Direct Network Provisioning scenarios "
"(<<quickstart-metal3>>)"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:16
msgid ""
"Declarative static configurations in the Image Based Provisioning scenarios "
"(<<quickstart-eib>>)"
msgstr ""

#. type: Title ==
#: asciidoc/components/networking.adoc:17
#, no-wrap
msgid "Configuring with Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:18
msgid ""
"Edge Image Builder (EIB) is a tool which enables configuring multiple hosts "
"with a single OS image.  In this section we'll show how you can use a "
"declarative approach to describe the desired network states, how those are "
"converted to the respective NetworkManager connections, and are then applied "
"during the provisioning process."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:20
msgid ""
"If you're following this guide, it's assumed that you've got the following "
"already available:"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:21
msgid ""
"An x86_64 physical host (or virtual machine) running SLES 15 SP5 or openSUSE "
"Leap 15.5"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:23
msgid ""
"A copy of the SLE Micro 5.5 RAW image found "
"https://www.suse.com/download/sle-micro/[here]"
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:24
#, no-wrap
msgid "Getting the Edge Image Builder container image"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:25
msgid ""
"The EIB container image is publicly available and can be downloaded from the "
"SUSE Edge registry by running:"
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:27
#, no-wrap
msgid "Creating the image configuration directory [[image-config-dir-creation]]"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:28
msgid "Let's start with creating the configuration directory:"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:30
msgid ""
"We will now ensure that the downloaded base image copy is moved over to the "
"configuration directory:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:31
#, no-wrap
msgid ""
"mv /path/to/downloads/SLE-Micro.x86_64-5.5.0-Default-GM.raw "
"$CONFIG_DIR/base-images/\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:32
msgid "NOTE: EIB is never going to modify the base image input."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:34
#, no-wrap
msgid ""
"└── base-images/\n"
"    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:36
msgid ""
"The definition file describes the majority of configurable options that the "
"Edge Image Builder supports."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:37
msgid "Let's start with a very basic definition file for our OS image:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:38
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/definition.yaml\n"
"apiVersion: 1.0\n"
"image:\n"
"  arch: x86_64\n"
"  imageType: raw\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
"  outputImageName: modified-image.raw\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:39
msgid ""
"The `image` section is required, and it specifies the input image, its "
"architecture and type, as well as what the output image will be called.  The "
"`operatingSystem` section is optional, and contains configuration to enable "
"login on the provisioned systems with the `root/eib` username/password."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:40
msgid ""
"NOTE: Feel free to use your own encrypted password by running `openssl "
"passwd -6 <password>`."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:42
#, no-wrap
msgid ""
"├── definition.yaml\n"
"└── base-images/\n"
"    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:43
#, no-wrap
msgid "Defining the network configurations [[default-network-definition]]"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:44
msgid ""
"The desired network configurations are not part of the image definition file "
"that we just created.  We'll now populate those under the special `network/` "
"directory. Let's create it:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:45
#: asciidoc/components/networking.adoc:137
#: asciidoc/components/networking.adoc:168
#, no-wrap
msgid "mkdir -p $CONFIG_DIR/network\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:46
msgid ""
"As previously mentioned, the NetworkManager Configurator (_nmc_) tool "
"expects an input in the form of predefined schema.  You can find how to set "
"up a wide variety of different networking options in the "
"https://nmstate.io/examples.html[upstream NMState examples documentation]."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:47
msgid ""
"This guide will explain how to configure the networking on three different "
"nodes:"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:48
msgid "A node which uses two Ethernet interfaces"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:49
msgid "A node which uses network bonding"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:50
msgid "A node which uses a network bridge"
msgstr ""

#. type: delimited block =
#: asciidoc/components/networking.adoc:51
msgid ""
"Using completely different network setups is not recommended in production "
"builds, especially if configuring Kubernetes clusters.  Networking "
"configurations should generally be homogeneous amongst nodes or at least "
"amongst roles within a given cluster.  This guide is including various "
"different options only to serve as an example reference."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:52
msgid ""
"NOTE: The following assumes a default `libvirt` network with an IP address "
"range `192.168.122.1/24`.  Adjust accordingly if this differs in your "
"environment."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:53
msgid ""
"Let's create the desired states for the first node which we will call "
"`node1.suse.com`:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:54
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/node1.suse.com.yaml\n"
"routes:\n"
"  config:\n"
"    - destination: 0.0.0.0/0\n"
"      metric: 100\n"
"      next-hop-address: 192.168.122.1\n"
"      next-hop-interface: eth0\n"
"      table-id: 254\n"
"    - destination: 192.168.122.0/24\n"
"      metric: 100\n"
"      next-hop-address:\n"
"      next-hop-interface: eth0\n"
"      table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"      - 192.168.122.1\n"
"      - 8.8.8.8\n"
"interfaces:\n"
"  - name: eth0\n"
"    type: ethernet\n"
"    state: up\n"
"    mac-address: 34:8A:B1:4B:16:E1\n"
"    ipv4:\n"
"      address:\n"
"        - ip: 192.168.122.50\n"
"          prefix-length: 24\n"
"      dhcp: false\n"
"      enabled: true\n"
"    ipv6:\n"
"      enabled: false\n"
"  - name: eth3\n"
"    type: ethernet\n"
"    state: down\n"
"    mac-address: 34:8A:B1:4B:16:E2\n"
"    ipv4:\n"
"      address:\n"
"        - ip: 192.168.122.55\n"
"          prefix-length: 24\n"
"      dhcp: false\n"
"      enabled: true\n"
"    ipv6:\n"
"      enabled: false\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:55
msgid ""
"In this example we define a desired state of two Ethernet interfaces (eth0 "
"and eth3), their requested IP addresses, routing, and DNS resolution."
msgstr ""

#. type: delimited block =
#: asciidoc/components/networking.adoc:56
msgid ""
"You must ensure that the MAC addresses of all Ethernet interfaces are "
"listed.  Those are used during the provisioning process as the identifiers "
"of the nodes and serve to determine which configurations should be applied.  "
"This is how we are able to configure multiple nodes using a single ISO or "
"RAW image."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:57
msgid ""
"Next up is the second node which we will call `node2.suse.com` and which "
"will use network bonding:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:58
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/node2.suse.com.yaml\n"
"routes:\n"
"  config:\n"
"    - destination: 0.0.0.0/0\n"
"      metric: 100\n"
"      next-hop-address: 192.168.122.1\n"
"      next-hop-interface: bond99\n"
"      table-id: 254\n"
"    - destination: 192.168.122.0/24\n"
"      metric: 100\n"
"      next-hop-address:\n"
"      next-hop-interface: bond99\n"
"      table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"      - 192.168.122.1\n"
"      - 8.8.8.8\n"
"interfaces:\n"
"  - name: bond99\n"
"    type: bond\n"
"    state: up\n"
"    ipv4:\n"
"      address:\n"
"        - ip: 192.168.122.60\n"
"          prefix-length: 24\n"
"      enabled: true\n"
"    link-aggregation:\n"
"      mode: balance-rr\n"
"      options:\n"
"        miimon: '140'\n"
"      port:\n"
"        - eth0\n"
"        - eth1\n"
"  - name: eth0\n"
"    type: ethernet\n"
"    state: up\n"
"    mac-address: 34:8A:B1:4B:16:E3\n"
"    ipv4:\n"
"      enabled: false\n"
"    ipv6:\n"
"      enabled: false\n"
"  - name: eth1\n"
"    type: ethernet\n"
"    state: up\n"
"    mac-address: 34:8A:B1:4B:16:E4\n"
"    ipv4:\n"
"      enabled: false\n"
"    ipv6:\n"
"      enabled: false\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:59
msgid ""
"In this example we define a desired state of two Ethernet interfaces (eth0 "
"and eth1) which are not enabling IP addressing, as well as a bond with a "
"round-robin policy and its respective address which is going to be used to "
"forward the network traffic."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:60
msgid ""
"Lastly, we'll create the third and final desired state file which will be "
"utilizing a network bridge and which we'll call `node3.suse.com`:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:61
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/node3.suse.com.yaml\n"
"routes:\n"
"  config:\n"
"    - destination: 0.0.0.0/0\n"
"      metric: 100\n"
"      next-hop-address: 192.168.122.1\n"
"      next-hop-interface: linux-br0\n"
"      table-id: 254\n"
"    - destination: 192.168.122.0/24\n"
"      metric: 100\n"
"      next-hop-address:\n"
"      next-hop-interface: linux-br0\n"
"      table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"      - 192.168.122.1\n"
"      - 8.8.8.8\n"
"interfaces:\n"
"  - name: eth0\n"
"    type: ethernet\n"
"    state: up\n"
"    mac-address: 34:8A:B1:4B:16:E5\n"
"    ipv4:\n"
"      enabled: false\n"
"    ipv6:\n"
"      enabled: false\n"
"  - name: linux-br0\n"
"    type: linux-bridge\n"
"    state: up\n"
"    ipv4:\n"
"      address:\n"
"        - ip: 192.168.122.70\n"
"          prefix-length: 24\n"
"      dhcp: false\n"
"      enabled: true\n"
"    bridge:\n"
"      options:\n"
"        group-forward-mask: 0\n"
"        mac-ageing-time: 300\n"
"        multicast-snooping: true\n"
"        stp:\n"
"          enabled: true\n"
"          forward-delay: 15\n"
"          hello-time: 2\n"
"          max-age: 20\n"
"          priority: 32768\n"
"      port:\n"
"        - name: eth0\n"
"          stp-hairpin-mode: false\n"
"          stp-path-cost: 100\n"
"          stp-priority: 32\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:63
#, no-wrap
msgid ""
"├── definition.yaml\n"
"├── network/\n"
"│   │── node1.suse.com.yaml\n"
"│   │── node2.suse.com.yaml\n"
"│   └── node3.suse.com.yaml\n"
"└── base-images/\n"
"    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:64
msgid ""
"NOTE: The names of the files under the `network/` directory are "
"intentional.  They correspond to the hostnames which will be set during the "
"provisioning process."
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:65
#, no-wrap
msgid "Building the OS image"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:66
msgid ""
"Now that all the necessary configurations are in place, we can build the "
"image by simply running:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:67
#: asciidoc/components/networking.adoc:140
#: asciidoc/components/networking.adoc:178
#, no-wrap
msgid ""
"podman run --rm -it -v $CONFIG_DIR:/eib "
"registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file "
"definition.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:68
#: asciidoc/guides/air-gapped-eib-deployments.adoc:58
#: asciidoc/guides/air-gapped-eib-deployments.adoc:79
#: asciidoc/guides/air-gapped-eib-deployments.adoc:95
#: asciidoc/guides/air-gapped-eib-deployments.adoc:110
msgid "The output should be similar to the following:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:69
#, no-wrap
msgid ""
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Rpm .......................... [SKIPPED]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Embedded Artifact Registry ... [SKIPPED]\n"
"Keymap ....................... [SUCCESS]\n"
"Kubernetes ................... [SKIPPED]\n"
"Certificates ................. [SKIPPED]\n"
"Building RAW image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:70
msgid ""
"The snippet above tells us that the `Network` component has successfully "
"been configured, and we can proceed with provisioning our edge nodes."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:71
msgid ""
"NOTE: A log file (`network-config.log`) and the respective NetworkManager "
"connection files can be inspected in the resulting `_build` directory under "
"a timestamped directory for the image run."
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:72
#, no-wrap
msgid "Provisioning the edge nodes"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:73
msgid "Let's copy the resulting RAW image:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:74
#, no-wrap
msgid ""
"mkdir edge-nodes && cd edge-nodes\n"
"for i in {1..4}; do cp $CONFIG_DIR/modified-image.raw node$i.raw; done\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:75
msgid ""
"You will notice that we copied the built image four times but only specified "
"the network configurations for three nodes.  This is because we also want to "
"showcase what will happen if we provision a node which does not match any of "
"the desired configurations."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:76
msgid ""
"NOTE: This guide will use virtualization for the node provisioning "
"examples. Ensure the necessary extensions are enabled in the BIOS (see "
"https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware[here] "
"for details)."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:77
msgid ""
"We will be using `virt-install` to create virtual machines using the copied "
"raw disks.  Each virtual machine will be using 10 GB of RAM and 6 vCPUs."
msgstr ""

#. type: Title ====
#: asciidoc/components/networking.adoc:78
#, no-wrap
msgid "Provisioning the first node"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:79
#: asciidoc/components/networking.adoc:105
#: asciidoc/components/networking.adoc:114
#: asciidoc/components/networking.adoc:124
msgid "Let's create the virtual machine:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:80
#, no-wrap
msgid ""
"virt-install --name node1 --ram 10000 --vcpus 6 --disk "
"path=node1.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics "
"none --console pty,target_type=serial --network "
"default,mac=34:8A:B1:4B:16:E1 --network default,mac=34:8A:B1:4B:16:E2 "
"--virt-type kvm --import\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:81
msgid ""
"NOTE: It is important that we create the network interfaces with the same "
"MAC addresses as the ones in the desired state we described above."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:82
msgid ""
"Once the operation is complete, we will see something similar to the "
"following:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:83
#, no-wrap
msgid ""
"Starting install...\n"
"Creating domain...\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:84
#, no-wrap
msgid ""
"Running text console command: virsh --connect qemu:///system console node1\n"
"Connected to domain 'node1'\n"
"Escape character is ^] (Ctrl + ])\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:85
#, no-wrap
msgid ""
"Welcome to SUSE Linux Enterprise Micro 5.5  (x86_64) - Kernel "
"5.14.21-150500.55.19-default (ttyS0).\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:86
#, no-wrap
msgid ""
"SSH host key: SHA256:XN/R5Tw43reG+QsOw480LxCnhkc/1uqMdwlI6KUBY70 (RSA)\n"
"SSH host key: SHA256:/96yGrPGKlhn04f1rb9cXv/2WJt4TtrIN5yEcN66r3s (DSA)\n"
"SSH host key: SHA256:Dy/YjBQ7LwjZGaaVcMhTWZNSOstxXBsPsvgJTJq5t00 (ECDSA)\n"
"SSH host key: SHA256:TNGqY1LRddpxD/jn/8dkT/9YmVl9hiwulqmayP+wOWQ (ED25519)\n"
"eth0: 192.168.122.50\n"
"eth1:\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:87
#, no-wrap
msgid ""
"Configured with the Edge Image Builder\n"
"Activate the web console with: systemctl enable --now cockpit.socket\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:88
#, no-wrap
msgid "node1 login:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:89
msgid ""
"We're now able to log in with the `root:eib` credentials pair.  We're also "
"able to SSH into the host if we prefer that over the `virsh console` we're "
"presented with here."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:90
msgid "Once logged in, let's confirm that all the settings are in place."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:91
msgid "Verify that the hostname is properly set:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:92
#, no-wrap
msgid ""
"node1:~ # hostnamectl\n"
" Static hostname: node1.suse.com\n"
" ...\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:93
#: asciidoc/components/networking.adoc:144
#: asciidoc/components/networking.adoc:182
msgid "Verify that the routing is properly configured:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:94
#, no-wrap
msgid ""
"node1:~ # ip r\n"
"default via 192.168.122.1 dev eth0 proto static metric 100\n"
"192.168.122.0/24 dev eth0 proto static scope link metric 100\n"
"192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.50 metric "
"100\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:95
#: asciidoc/components/networking.adoc:146
#: asciidoc/components/networking.adoc:184
msgid "Verify that Internet connection is available:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:96
#, no-wrap
msgid ""
"node1:~ # ping google.com\n"
"PING google.com (142.250.72.78) 56(84) bytes of data.\n"
"64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 "
"time=13.2 ms\n"
"64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 "
"time=13.4 ms\n"
"^C\n"
"--- google.com ping statistics ---\n"
"2 packets transmitted, 2 received, 0% packet loss, time 1002ms\n"
"rtt min/avg/max/mdev = 13.248/13.304/13.361/0.056 ms\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:97
msgid ""
"Verify that exactly two Ethernet interfaces are configured and only one of "
"those is active:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:98
#, no-wrap
msgid ""
"node1:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e1 brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s2\n"
"    altname ens2\n"
"    inet 192.168.122.50/24 brd 192.168.122.255 scope global noprefixroute "
"eth0\n"
"       valid_lft forever preferred_lft forever\n"
"3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e2 brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s3\n"
"    altname ens3\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:99
#, no-wrap
msgid ""
"node1:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME  UUID                                  TYPE      DEVICE  FILENAME\n"
"eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
"eth1  7e211aea-3d14-59cf-a4fa-be91dac5dbba  ethernet  --      "
"/etc/NetworkManager/system-connections/eth1.nmconnection\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:100
msgid ""
"You'll notice that the second interface is `eth1` instead of the predefined "
"`eth3` in our desired networking state.  This is the case because the "
"NetworkManager Configurator (_nmc_) is able to detect that the OS has given "
"a different name for the NIC with MAC address `34:8a:b1:4b:16:e2` and it "
"adjusts its settings accordingly."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:101
msgid ""
"Verify this has indeed happened by inspecting the Combustion phase of the "
"provisioning:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:102
#, no-wrap
msgid ""
"node1:~ # journalctl -u combustion | grep nmc\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc::apply_conf] Identified host: "
"node1.suse.com\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc::apply_conf] Set hostname: node1.suse.com\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface "
"'eth0'...\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc::apply_conf] Processing interface "
"'eth3'...\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc::apply_conf] Using interface name 'eth1' "
"instead of the preconfigured 'eth3'\n"
"Apr 23 09:20:19 localhost.localdomain combustion[1360]: "
"[2024-04-23T09:20:19Z INFO  nmc] Successfully applied config\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:103
msgid ""
"We will now provision the rest of the nodes, but we will only show the "
"differences in the final configuration.  Feel free to apply any or all of "
"the above checks for all nodes you are about to provision."
msgstr ""

#. type: Title ====
#: asciidoc/components/networking.adoc:104
#, no-wrap
msgid "Provisioning the second node"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:106
#, no-wrap
msgid ""
"virt-install --name node2 --ram 10000 --vcpus 6 --disk "
"path=node2.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics "
"none --console pty,target_type=serial --network "
"default,mac=34:8A:B1:4B:16:E3 --network default,mac=34:8A:B1:4B:16:E4 "
"--virt-type kvm --import\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:107
msgid ""
"Once the virtual machine is up and running, we can confirm that this node is "
"using bonded interfaces:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:108
#, no-wrap
msgid ""
"node2:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast "
"master bond99 state UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s2\n"
"    altname ens2\n"
"3: eth1: <BROADCAST,MULTICAST,SLAVE,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast "
"master bond99 state UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff permaddr "
"34:8a:b1:4b:16:e4\n"
"    altname enp0s3\n"
"    altname ens3\n"
"4: bond99: <BROADCAST,MULTICAST,MASTER,UP,LOWER_UP> mtu 1500 qdisc noqueue "
"state UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e3 brd ff:ff:ff:ff:ff:ff\n"
"    inet 192.168.122.60/24 brd 192.168.122.255 scope global noprefixroute "
"bond99\n"
"       valid_lft forever preferred_lft forever\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:109
msgid "Confirm that the routing is using the bond:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:110
#, no-wrap
msgid ""
"node2:~ # ip r\n"
"default via 192.168.122.1 dev bond99 proto static metric 100\n"
"192.168.122.0/24 dev bond99 proto static scope link metric 100\n"
"192.168.122.0/24 dev bond99 proto kernel scope link src 192.168.122.60 "
"metric 300\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:111
#: asciidoc/components/networking.adoc:120
msgid "Ensure that the static connection files are properly utilized:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:112
#, no-wrap
msgid ""
"node2:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME    UUID                                  TYPE      DEVICE  FILENAME\n"
"bond99  4a920503-4862-5505-80fd-4738d07f44c6  bond      bond99  "
"/etc/NetworkManager/system-connections/bond99.nmconnection\n"
"eth0    dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
"eth1    0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    "
"/etc/NetworkManager/system-connections/eth1.nmconnection\n"
msgstr ""

#. type: Title ====
#: asciidoc/components/networking.adoc:113
#, no-wrap
msgid "Provisioning the third node"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:115
#, no-wrap
msgid ""
"virt-install --name node3 --ram 10000 --vcpus 6 --disk "
"path=node3.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics "
"none --console pty,target_type=serial --network "
"default,mac=34:8A:B1:4B:16:E5 --virt-type kvm --import\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:116
msgid ""
"Once the virtual machine is up and running, we can confirm that this node is "
"using a network bridge:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:117
#, no-wrap
msgid ""
"node3:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast master "
"linux-br0 state UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s2\n"
"    altname ens2\n"
"3: linux-br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state "
"UP group default qlen 1000\n"
"    link/ether 34:8a:b1:4b:16:e5 brd ff:ff:ff:ff:ff:ff\n"
"    inet 192.168.122.70/24 brd 192.168.122.255 scope global noprefixroute "
"linux-br0\n"
"       valid_lft forever preferred_lft forever\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:118
msgid "Confirm that the routing is using the bridge:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:119
#, no-wrap
msgid ""
"node3:~ # ip r\n"
"default via 192.168.122.1 dev linux-br0 proto static metric 100\n"
"192.168.122.0/24 dev linux-br0 proto static scope link metric 100\n"
"192.168.122.0/24 dev linux-br0 proto kernel scope link src 192.168.122.70 "
"metric 425\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:121
#, no-wrap
msgid ""
"node3:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME       UUID                                  TYPE      DEVICE     "
"FILENAME\n"
"linux-br0  1f8f1469-ed20-5f2c-bacb-a6767bee9bc0  bridge    linux-br0  "
"/etc/NetworkManager/system-connections/linux-br0.nmconnection\n"
"eth0       dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0       "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
msgstr ""

#. type: Title ====
#: asciidoc/components/networking.adoc:122
#, no-wrap
msgid "Provisioning the fourth node"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:123
msgid ""
"Lastly, we will provision a node which will not match any of the predefined "
"configurations by a MAC address.  In these cases, we will default to DHCP to "
"configure the network interfaces."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:125
#, no-wrap
msgid ""
"virt-install --name node4 --ram 10000 --vcpus 6 --disk "
"path=node4.raw,format=raw --osinfo detect=on,name=sle-unknown --graphics "
"none --console pty,target_type=serial --network default --virt-type kvm "
"--import\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:126
msgid ""
"Once the virtual machine is up and running, we can confirm that this node is "
"using a random IP address for its network interface:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:127
#, no-wrap
msgid ""
"localhost:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 52:54:00:56:63:71 brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s2\n"
"    altname ens2\n"
"    inet 192.168.122.86/24 brd 192.168.122.255 scope global dynamic "
"noprefixroute eth0\n"
"       valid_lft 3542sec preferred_lft 3542sec\n"
"    inet6 fe80::5054:ff:fe56:6371/64 scope link noprefixroute\n"
"       valid_lft forever preferred_lft forever\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:128
msgid "Verify that nmc failed to apply static configurations for this node:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:129
#, no-wrap
msgid ""
"localhost:~ # journalctl -u combustion | grep nmc\n"
"Apr 23 12:15:45 localhost.localdomain combustion[1357]: "
"[2024-04-23T12:15:45Z ERROR nmc] Applying config failed: None of the "
"preconfigured hosts match local NICs\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:130
msgid "Verify that the Ethernet interface was configured via DHCP:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:131
#, no-wrap
msgid ""
"localhost:~ # journalctl | grep eth0\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7801] manager: (eth0): new Ethernet device "
"(/org/freedesktop/NetworkManager/Devices/2)\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7802] device (eth0): state change: unmanaged -> unavailable "
"(reason 'managed', sys-iface-state: 'external')\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7929] device (eth0): carrier: link connected\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7931] device (eth0): state change: unavailable -> disconnected "
"(reason 'carrier-changed', sys-iface-state: 'managed')\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7944] device (eth0): Activation: starting connection 'Wired "
"Connection' (300ed658-08d4-4281-9f8c-d1b8882d29b9)\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7945] device (eth0): state change: disconnected -> prepare "
"(reason 'none', sys-iface-state: 'managed')\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7947] device (eth0): state change: prepare -> config (reason "
"'none', sys-iface-state: 'managed')\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7953] device (eth0): state change: config -> ip-config (reason "
"'none', sys-iface-state: 'managed')\n"
"Apr 23 12:15:29 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874529.7964] dhcp4 (eth0): activation: beginning transaction (timeout "
"in 90 seconds)\n"
"Apr 23 12:15:33 localhost.localdomain NetworkManager[704]: <info>  "
"[1713874533.1272] dhcp4 (eth0): state changed new lease, "
"address=192.168.122.86\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:132
#, no-wrap
msgid ""
"localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME              UUID                                  TYPE      DEVICE  "
"FILENAME\n"
"Wired Connection  300ed658-08d4-4281-9f8c-d1b8882d29b9  ethernet  eth0    "
"/var/run/NetworkManager/system-connections/default_connection.nmconnection\n"
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:133
#, no-wrap
msgid "Unified node configurations"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:134
msgid ""
"There are occasions where relying on known MAC addresses is not an "
"option. In these cases we can opt for the so-called _unified configuration_ "
"which allows us to specify settings in an `_all.yaml` file which will then "
"be applied across all provisioned nodes."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:135
#: asciidoc/components/networking.adoc:160
msgid ""
"We will build and provision an edge node using different configuration "
"structure. Follow all steps starting from <<image-config-dir-creation>> up "
"until <<default-network-definition>>."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:136
msgid ""
"In this example we define a desired state of two Ethernet interfaces (eth0 "
"and eth1) - one using DHCP, and one assigned a static IP address."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:138
#, no-wrap
msgid ""
"cat <<- EOF > $CONFIG_DIR/network/_all.yaml\n"
"interfaces:\n"
"- name: eth0\n"
"  type: ethernet\n"
"  state: up\n"
"  ipv4:\n"
"    dhcp: true\n"
"    enabled: true\n"
"  ipv6:\n"
"    enabled: false\n"
"- name: eth1\n"
"  type: ethernet\n"
"  state: up\n"
"  ipv4:\n"
"    address:\n"
"    - ip: 10.0.0.1\n"
"      prefix-length: 24\n"
"    enabled: true\n"
"    dhcp: false\n"
"  ipv6:\n"
"    enabled: false\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:139
#: asciidoc/components/networking.adoc:177 asciidoc/components/longhorn.adoc:52
#: asciidoc/guides/air-gapped-eib-deployments.adoc:56
#: asciidoc/guides/air-gapped-eib-deployments.adoc:77
#: asciidoc/guides/air-gapped-eib-deployments.adoc:93
#: asciidoc/guides/air-gapped-eib-deployments.adoc:108
msgid "Let's build the image:"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:141
#: asciidoc/components/networking.adoc:179
msgid ""
"Once the image is successfully built, let's create a virtual machine using "
"it:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:142
#, no-wrap
msgid ""
"virt-install --name node1 --ram 10000 --vcpus 6 --disk "
"path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo "
"detect=on,name=sle-unknown --graphics none --console pty,target_type=serial "
"--network default --network default --virt-type kvm --import\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:143
#: asciidoc/components/networking.adoc:181
msgid ""
"The provisioning process might take a few minutes. Once it's finished, log "
"in to the system with the provided credentials."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:145
#, no-wrap
msgid ""
"localhost:~ # ip r\n"
"default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.100 metric "
"100\n"
"10.0.0.0/24 dev eth1 proto kernel scope link src 10.0.0.1 metric 101\n"
"192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.100 metric "
"100\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:147
#, no-wrap
msgid ""
"localhost:~ # ping google.com\n"
"PING google.com (142.250.72.46) 56(84) bytes of data.\n"
"64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=1 ttl=56 "
"time=14.3 ms\n"
"64 bytes from den16s08-in-f14.1e100.net (142.250.72.46): icmp_seq=2 ttl=56 "
"time=14.2 ms\n"
"^C\n"
"--- google.com ping statistics ---\n"
"2 packets transmitted, 2 received, 0% packet loss, time 1001ms\n"
"rtt min/avg/max/mdev = 14.196/14.260/14.324/0.064 ms\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:148
msgid "Verify that the Ethernet interfaces are configured and active:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:149
#, no-wrap
msgid ""
"localhost:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 52:54:00:26:44:7a brd ff:ff:ff:ff:ff:ff\n"
"    altname enp1s0\n"
"    inet 192.168.122.100/24 brd 192.168.122.255 scope global dynamic "
"noprefixroute eth0\n"
"       valid_lft 3505sec preferred_lft 3505sec\n"
"3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 52:54:00:ec:57:9e brd ff:ff:ff:ff:ff:ff\n"
"    altname enp7s0\n"
"    inet 10.0.0.1/24 brd 10.0.0.255 scope global noprefixroute eth1\n"
"       valid_lft forever preferred_lft forever\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:150
#, no-wrap
msgid ""
"localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME  UUID                                  TYPE      DEVICE  FILENAME\n"
"eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
"eth1  0523c0a1-5f5e-5603-bcf2-68155d5d322e  ethernet  eth1    "
"/etc/NetworkManager/system-connections/eth1.nmconnection\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:151
#, no-wrap
msgid ""
"localhost:~ # cat /etc/NetworkManager/system-connections/eth0.nmconnection\n"
"[connection]\n"
"autoconnect=true\n"
"autoconnect-slaves=-1\n"
"id=eth0\n"
"interface-name=eth0\n"
"type=802-3-ethernet\n"
"uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:152
#, no-wrap
msgid ""
"[ipv4]\n"
"dhcp-client-id=mac\n"
"dhcp-send-hostname=true\n"
"dhcp-timeout=2147483647\n"
"ignore-auto-dns=false\n"
"ignore-auto-routes=false\n"
"method=auto\n"
"never-default=false\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:153
#: asciidoc/components/networking.adoc:156
#, no-wrap
msgid ""
"[ipv6]\n"
"addr-gen-mode=0\n"
"dhcp-timeout=2147483647\n"
"method=disabled\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:154
#, no-wrap
msgid ""
"localhost:~ # cat /etc/NetworkManager/system-connections/eth1.nmconnection\n"
"[connection]\n"
"autoconnect=true\n"
"autoconnect-slaves=-1\n"
"id=eth1\n"
"interface-name=eth1\n"
"type=802-3-ethernet\n"
"uuid=0523c0a1-5f5e-5603-bcf2-68155d5d322e\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:155
#, no-wrap
msgid ""
"[ipv4]\n"
"address0=10.0.0.1/24\n"
"dhcp-timeout=2147483647\n"
"method=manual\n"
msgstr ""

#. type: Title ===
#: asciidoc/components/networking.adoc:157
#, no-wrap
msgid "Custom network configurations"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:158
msgid ""
"We have already covered the default network configuration for Edge Image "
"Builder which relies on the NetworkManager Configurator.  However, there is "
"also the option to modify it via a custom script. Whilst this option is very "
"flexible and is also not MAC address dependant, its limitation stems from "
"the fact that using it is much less convenient when bootstrapping multiple "
"nodes with a single image."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:159
msgid ""
"NOTE: It is recommended to use the default network configuration via files "
"describing the desired network states under the `/network` directory.  Only "
"opt for custom scripting when that behaviour is not applicable to your use "
"case."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:161
msgid ""
"In this example, we will create a custom script which applies static "
"configuration for the `eth0` interface on all provisioned nodes, as well as "
"removing and disabling the automatically created wired connections by "
"NetworkManager. This is beneficial in situations where you want to make sure "
"that every node in your cluster has an identical networking configuration, "
"and as such you do not need to be concerned with the MAC address of each "
"node prior to image creation."
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:162
msgid "Let's start by storing the connection file in the `/custom/files` directory:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:163
#, no-wrap
msgid "mkdir -p $CONFIG_DIR/custom/files\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:164
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/custom/files/eth0.nmconnection\n"
"[connection]\n"
"autoconnect=true\n"
"autoconnect-slaves=-1\n"
"autoconnect-retries=1\n"
"id=eth0\n"
"interface-name=eth0\n"
"type=802-3-ethernet\n"
"uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70\n"
"wait-device-timeout=60000\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:165
#: asciidoc/components/networking.adoc:190
#, no-wrap
msgid ""
"[ipv4]\n"
"dhcp-timeout=2147483647\n"
"method=auto\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:166
#, no-wrap
msgid ""
"[ipv6]\n"
"addr-gen-mode=eui64\n"
"dhcp-timeout=2147483647\n"
"method=disabled\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:167
msgid ""
"Now that the static configuration is created, we will also create our custom "
"network script:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:169
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/configure-network.sh\n"
"#!/bin/bash\n"
"set -eux\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:170
#, no-wrap
msgid ""
"# Remove and disable wired connections\n"
"mkdir -p /etc/NetworkManager/conf.d/\n"
"printf \"[main]\\nno-auto-default=*\\n\" > "
"/etc/NetworkManager/conf.d/no-auto-default.conf\n"
"rm -f /var/run/NetworkManager/system-connections/* || true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:171
#, no-wrap
msgid ""
"# Copy pre-configured network configuration files into NetworkManager\n"
"mkdir -p /etc/NetworkManager/system-connections/\n"
"cp eth0.nmconnection /etc/NetworkManager/system-connections/\n"
"chmod 600 /etc/NetworkManager/system-connections/*.nmconnection\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:172
#, no-wrap
msgid "chmod a+x $CONFIG_DIR/network/configure-network.sh\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:173
msgid ""
"NOTE: The nmc binary will still be included by default, so it can also be "
"used in the `configure-network.sh` script if necessary."
msgstr ""

#. type: delimited block =
#: asciidoc/components/networking.adoc:174
msgid ""
"The custom script must always be provided under "
"`/network/configure-network.sh` in the configuration directory. If present, "
"all other files will be ignored.  It is NOT possible to configure a network "
"by working with both static configurations in YAML format and a custom "
"script simultaneously."
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:176
#, no-wrap
msgid ""
"├── definition.yaml\n"
"├── custom/\n"
"│   └── files/\n"
"│       └── eth0.nmconnection\n"
"├── network/\n"
"│   └── configure-network.sh\n"
"└── base-images/\n"
"    └── SLE-Micro.x86_64-5.5.0-Default-GM.raw\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:180
#, no-wrap
msgid ""
"virt-install --name node1 --ram 10000 --vcpus 6 --disk "
"path=$CONFIG_DIR/modified-image.raw,format=raw --osinfo "
"detect=on,name=sle-unknown --graphics none --console pty,target_type=serial "
"--network default --virt-type kvm --import\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:183
#, no-wrap
msgid ""
"localhost:~ # ip r\n"
"default via 192.168.122.1 dev eth0 proto dhcp src 192.168.122.185 metric "
"100\n"
"192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.185 metric "
"100\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:185
#, no-wrap
msgid ""
"localhost:~ # ping google.com\n"
"PING google.com (142.250.72.78) 56(84) bytes of data.\n"
"64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=1 ttl=56 "
"time=13.6 ms\n"
"64 bytes from den16s09-in-f14.1e100.net (142.250.72.78): icmp_seq=2 ttl=56 "
"time=13.6 ms\n"
"^C\n"
"--- google.com ping statistics ---\n"
"2 packets transmitted, 2 received, 0% packet loss, time 1001ms\n"
"rtt min/avg/max/mdev = 13.592/13.599/13.606/0.007 ms\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/networking.adoc:186
msgid ""
"Verify that an Ethernet interface is statically configured using our "
"connection file and is active:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:187
#, no-wrap
msgid ""
"localhost:~ # ip a\n"
"1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group "
"default qlen 1000\n"
"    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n"
"    inet 127.0.0.1/8 scope host lo\n"
"       valid_lft forever preferred_lft forever\n"
"    inet6 ::1/128 scope host\n"
"       valid_lft forever preferred_lft forever\n"
"2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state "
"UP group default qlen 1000\n"
"    link/ether 52:54:00:31:d0:1b brd ff:ff:ff:ff:ff:ff\n"
"    altname enp0s2\n"
"    altname ens2\n"
"    inet 192.168.122.185/24 brd 192.168.122.255 scope global dynamic "
"noprefixroute eth0\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:188
#, no-wrap
msgid ""
"localhost:~ # nmcli -f NAME,UUID,TYPE,DEVICE,FILENAME con show\n"
"NAME  UUID                                  TYPE      DEVICE  FILENAME\n"
"eth0  dfd202f5-562f-5f07-8f2a-a7717756fb70  ethernet  eth0    "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:189
#, no-wrap
msgid ""
"localhost:~ # cat  "
"/etc/NetworkManager/system-connections/eth0.nmconnection\n"
"[connection]\n"
"autoconnect=true\n"
"autoconnect-slaves=-1\n"
"autoconnect-retries=1\n"
"id=eth0\n"
"interface-name=eth0\n"
"type=802-3-ethernet\n"
"uuid=dfd202f5-562f-5f07-8f2a-a7717756fb70\n"
"wait-device-timeout=60000\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/networking.adoc:191
#, no-wrap
msgid ""
"[ipv6]\n"
"addr-gen-mode=eui64\n"
"dhcp-timeout=2147483647\n"
"method=disabled\n"
msgstr ""

#. type: Title =
#: asciidoc/components/elemental.adoc:1
#, no-wrap
msgid "Elemental"
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:2
msgid ""
"Elemental is a software stack enabling centralized and full cloud-native OS "
"management with Kubernetes. The Elemental stack consists of a number of "
"components that either reside on Rancher itself, or on the edge nodes. The "
"core components are:"
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:3
msgid ""
"*elemental-operator* - The core operator that resides on Rancher and handles "
"registration requests from clients."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:4
msgid ""
"*elemental-register* - The client that runs on the edge nodes allowing "
"registration via the `elemental-operator`."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:5
msgid ""
"*elemental-system-agent* - An agent that resides on the edge nodes; its "
"configuration is fed from `elemental-register` and it receives a `plan` for "
"configuring the `rancher-system-agent`"
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:6
msgid ""
"*rancher-system-agent* - Once the edge node has fully registered, this takes "
"over from `elemental-system-agent` and waits for further `plans` from "
"Rancher Manager (e.g. for Kubernetes installation)."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:7
msgid ""
"See https://elemental.docs.rancher.com/[Elemental upstream documentation] "
"for full information about Elemental and its relationship to Rancher."
msgstr ""

#. type: Title ==
#: asciidoc/components/elemental.adoc:8
#, no-wrap
msgid "How does SUSE Edge use Elemental?"
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:9
msgid ""
"We use portions of Elemental for managing remote devices where Metal^3^ is "
"not an option (for example, there is no BMC, or the device is behind a NAT "
"gateway). This tooling allows for an operator to bootstrap their devices in "
"a lab before knowing when or where they will be shipped to. Namely, we "
"leverage the `elemental-register` and `elemental-system-agent` components to "
"enable the onboarding of SLE Micro hosts to Rancher for \"phone home\" "
"network provisioning use-cases. When using Edge Image Builder (EIB) to "
"create deployment images, the automatic registration through Rancher via "
"Elemental can be achieved by specifying the registration configuration in "
"the configuration directory for EIB."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:10
msgid ""
"In SUSE Edge 3.0 we do *not* leverage the operating system management "
"aspects of Elemental, and therefore it's not possible to manage your "
"operating system patching via Rancher. Instead of using the Elemental tools "
"to build deployment images, SUSE Edge uses the Edge Image Builder tooling, "
"which consumes the registration configuration."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:13
msgid ""
"The SUSE Edge recommended way of building deployments image that can "
"leverage Elemental for registration to Rancher in the \"phone home network "
"provisioning\" deployment footprint is to follow the instructions detailed "
"in the <<quickstart-elemental,remote host onboarding with Elemental>> "
"quickstart."
msgstr ""

#. type: Title ===
#: asciidoc/components/elemental.adoc:14
#, no-wrap
msgid "Labels"
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:15
msgid ""
"Elemental tracks its inventory with the `MachineInventory` CRD and provides "
"a way to select inventory, e.g. for selecting machines to deploy Kubernetes "
"clusters to, based on labels. This provides a way for users to predefine "
"most (if not all) of their infrastructure needs prior to hardware even being "
"purchased. Also, since nodes can add/remove labels on their respective "
"inventory object (by re-running `elemental-register` with the additional "
"flag `--label \"FOO=BAR\"`), we can write scripts that will discover and let "
"Rancher know where a node is booted."
msgstr ""

#. type: Plain text
#: asciidoc/components/elemental.adoc:17
msgid ""
"The Elemental UI does not currently know how to build installation media or "
"update non-\"Elemental Teal\" operating systems. This should be addressed in "
"future releases."
msgstr ""

#. type: Title =
#: asciidoc/components/akri.adoc:1
#, no-wrap
msgid "Akri"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:2
msgid ""
"Akri is a CNCF-Sandbox project that aims to discover leaf devices to present "
"those as Kubernetes native resource.  It also allows scheduling a pod or a "
"job for each discovered device.  Devices can be node-local or networked, and "
"can use a wide variety of protocols."
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:3
msgid "Akri's upstream documentation is available at: https://docs.akri.sh"
msgstr ""

#. type: Title ==
#: asciidoc/components/akri.adoc:4
#, no-wrap
msgid "How does SUSE Edge use Akri?"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:5
msgid "Akri is currently tech-preview in the SUSE Edge stack."
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:6
msgid ""
"Akri is available as part of the Edge Stack whenever there is a need to "
"discover and schedule workload against leaf devices."
msgstr ""

#. type: Title ===
#: asciidoc/components/akri.adoc:7
#, no-wrap
msgid "Installing Akri"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:8
msgid ""
"Akri is available as a Helm chart within the Edge Helm repository.  The "
"recommended way of configuring Akri is by using the given Helm chart to "
"deploy the different components (agent, controller, discovery-handlers), and "
"then use your preferred deployment mechanism to deploy Akri's Configuration "
"CRDs."
msgstr ""

#. type: Title ===
#: asciidoc/components/akri.adoc:9
#, no-wrap
msgid "Configuring Akri"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:10
msgid ""
"Akri is configured using a `akri.sh/Configuration` object, this object takes "
"in all information about how to discover the devices, as well as what to do "
"when a matching one is discovered."
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:11
msgid "Here is an example configuration breakdown with all fields explained:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/akri.adoc:12
#, no-wrap
msgid ""
"apiVersion: akri.sh/v0\n"
"kind: Configuration\n"
"metadata:\n"
"  name: sample-configuration\n"
"spec:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:13
msgid ""
"This part describes the configuration of the discovery handler, you have to "
"specify its name (the handlers available as part of Akri's chart are `udev`, "
"`opcua`, `onvif`).  The `discoveryDetails` is handler specific, refer to the "
"handler's documentation on how to configure it."
msgstr ""

#. type: delimited block -
#: asciidoc/components/akri.adoc:14
#, no-wrap
msgid ""
"  discoveryHandler:\n"
"    name: debugEcho\n"
"    discoveryDetails: |+\n"
"      descriptions:\n"
"        - \"foo\"\n"
"        - \"bar\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:15
msgid ""
"This section defines the workload to be deployed for every discovered "
"device.  The example shows a minimal version of a `Pod` configuration in "
"`brokerPodSpec`, all usual fields of a Pod's spec can be used here.  It also "
"shows the Akri specific syntax to request the device in the `resources` "
"section."
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:16
msgid ""
"You can alternatively use a Job instead of a Pod, using the `brokerJobSpec` "
"key instead, and providing the spec part of a Job to it."
msgstr ""

#. type: delimited block -
#: asciidoc/components/akri.adoc:17
#, no-wrap
msgid ""
"  brokerSpec:\n"
"    brokerPodSpec:\n"
"      containers:\n"
"      - name: broker-container\n"
"        image: rancher/hello-world\n"
"        resources:\n"
"          requests:\n"
"            \"{{PLACEHOLDER}}\" : \"1\"\n"
"          limits:\n"
"            \"{{PLACEHOLDER}}\" : \"1\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:18
msgid ""
"These two sections show how to configure Akri to deploy a service per broker "
"(`instanceService`), or pointing to all brokers (`configurationService`).  "
"These are containing all elements pertaining to a usual Service."
msgstr ""

#. type: delimited block -
#: asciidoc/components/akri.adoc:19
#, no-wrap
msgid ""
"  instanceServiceSpec:\n"
"    type: ClusterIp\n"
"    ports:\n"
"    - name: http\n"
"      port: 80\n"
"      protocol: tcp\n"
"      targetPort: 80\n"
"  configurationServiceSpec:\n"
"    type: ClusterIp\n"
"    ports:\n"
"    - name: https\n"
"      port: 443\n"
"      protocol: tcp\n"
"      targetPort: 443\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:20
msgid ""
"The `brokerProperties` field is a key/value store that will be exposed as "
"additional environment variables to any pod requesting a discovered device."
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:21
msgid ""
"The capacity is the allowed number of concurrent users of a discovered "
"device."
msgstr ""

#. type: delimited block -
#: asciidoc/components/akri.adoc:22
#, no-wrap
msgid ""
"  brokerProperties:\n"
"    key: value\n"
"  capacity: 1\n"
msgstr ""

#. type: Title ===
#: asciidoc/components/akri.adoc:23
#, no-wrap
msgid "Writing and deploying additional Discovery Handlers"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:24
msgid ""
"In case the protocol used by your device isn't covered by an existing "
"discovery handler, you can write your own using "
"https://docs.akri.sh/development/handler-development[this guide]"
msgstr ""

#. type: Title ===
#: asciidoc/components/akri.adoc:25
#, no-wrap
msgid "Akri Rancher Dashboard Extension"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:27
msgid ""
"Once the extension is installed you can navigate to any Akri-enabled managed "
"cluster using cluster explorer. Under *Akri* navigation group you can see "
"Configurations and Instances sections."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/akri.adoc:28
#, no-wrap
msgid "akri-extension-configurations.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:29
msgid ""
"The configurations list provides information about Configuration Discovery "
"Handler and number of instances. Clicking the name opens a configuration "
"detail page."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/akri.adoc:30
#, no-wrap
msgid "akri-extension-configuration-detail.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:31
msgid ""
"You can also edit or create a new Configuration. Extension allows you to "
"select discovery handler, set up Broker Pod or Job, configure Configuration "
"and Instance services and set the Configuration capacity."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/akri.adoc:32
#, no-wrap
msgid "akri-extension-configuration-edit.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:33
msgid "Discovered devices are listed in the *Instances* list."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/akri.adoc:34
#, no-wrap
msgid "akri-extension-instances-list.png"
msgstr ""

#. type: Plain text
#: asciidoc/components/akri.adoc:35
msgid ""
"Clicking the Instance name opens a detail page allowing to view the "
"workloads and instance service."
msgstr ""

#. type: Target for macro image
#: asciidoc/components/akri.adoc:36
#, no-wrap
msgid "akri-extension-instance-detail.png"
msgstr ""

#. type: Title =
#: asciidoc/components/k3s.adoc:1
#, no-wrap
msgid "K3s"
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:2
msgid ""
"https://k3s.io/[K3s] is a highly available, certified Kubernetes "
"distribution designed for production workloads in unattended, "
"resource-constrained, remote locations or inside IoT appliances."
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:3
msgid ""
"It is packaged as a single and small binary, so installations and updates "
"are fast and easy."
msgstr ""

#. type: Title ==
#: asciidoc/components/k3s.adoc:4
#, no-wrap
msgid "How does SUSE Edge use K3s"
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:5
msgid ""
"K3s can be used as the Kubernetes distribution backing the SUSE Edge stack.  "
"It is meant to be installed on a SLE Micro operating system."
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:6
msgid ""
"Using K3s as the SUSE Edge stack Kubernetes distribution is only recommended "
"when etcd as a backend does not fit your constraints. If etcd as a backend "
"is possible, it is better to use <<components-rke2,RKE2>>."
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:9
msgid ""
"The recommended way of installing K3s as part of the SUSE Edge stack is by "
"using Edge Image Builder (EIB). See <<components-eib,its documentation>> for "
"more details on how to configure it to deploy K3s."
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:10
msgid "It automatically supports HA setup, as well as Elemental setup."
msgstr ""

#. type: Title ===
#: asciidoc/components/k3s.adoc:11
#, no-wrap
msgid "Fleet for GitOps workflow"
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:12
msgid ""
"The SUSE Edge stack uses Fleet as its preferred GitOps tool.  For more "
"information around its installation and use, refer to <<components-fleet,the "
"Fleet section>> in this documentation."
msgstr ""

#. type: Title ===
#: asciidoc/components/k3s.adoc:13
#, no-wrap
msgid "Storage management"
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:14
msgid ""
"K3s comes with local-path storage preconfigured, which is suitable for "
"single-node clusters.  For clusters spanning over multiple nodes, we "
"recommend using <<components-longhorn,Longhorn>>."
msgstr ""

#. type: Title ===
#: asciidoc/components/k3s.adoc:15
#, no-wrap
msgid "Load balancing and HA"
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:16
msgid ""
"If you installed K3s using EIB, this part is already covered by the EIB "
"documentation in the HA section."
msgstr ""

#. type: Plain text
#: asciidoc/components/k3s.adoc:17
msgid ""
"Otherwise, you need to install and configure MetalLB as per our "
"<<guides-metallb-k3s,MetalLB documentation>>."
msgstr ""

#. type: Title =
#: asciidoc/components/rke2.adoc:1
#, no-wrap
msgid "RKE2"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:2
msgid "See https://docs.rke2.io/[RKE2 official documentation]."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:3
msgid ""
"RKE2 is a fully conformant Kubernetes distribution that focuses on security "
"and compliance by:"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:4
msgid ""
"Providing defaults and configuration options that allow clusters to pass the "
"CIS Kubernetes Benchmark v1.6 or v1.23 with minimal operator intervention"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:5
msgid "Enabling FIPS 140-2 compliance"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:6
msgid ""
"Regularly scanning components for CVEs using https://trivy.dev[trivy] in the "
"RKE2 build pipeline"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:7
msgid ""
"RKE2 launches control plane components as static pods, managed by the "
"kubelet. The embedded container runtime is containerd."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:8
msgid ""
"Note: RKE2 is also known as RKE Government in order to convey another use "
"case and sector it currently targets."
msgstr ""

#. type: Title ==
#: asciidoc/components/rke2.adoc:9
#, no-wrap
msgid "RKE2 vs K3s"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:10
msgid ""
"K3s is a fully compliant and lightweight Kubernetes distribution focused on "
"Edge, IoT, ARM or just for situations where a PhD in K8s clusterology is "
"infeasible."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:11
msgid ""
"RKE2 combines the best of both worlds from the 1.x version of RKE (hereafter "
"referred to as RKE1) and K3s."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:12
msgid "From K3s, it inherits the usability, ease of operation and deployment model."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:13
msgid ""
"From RKE1, it inherits close alignment with upstream Kubernetes. In places, "
"K3s has diverged from upstream Kubernetes in order to optimize for edge "
"deployments, but RKE1 and RKE2 can stay closely aligned with upstream."
msgstr ""

#. type: Title ==
#: asciidoc/components/rke2.adoc:14
#, no-wrap
msgid "How does SUSE Edge use RKE2?"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:15
msgid ""
"RKE2 is a fundamental piece of the SUSE Edge stack. It sits on top of "
"<<components-slmicro,SUSE Linux Micro>>, providing a standard Kubernetes "
"interface required to deploy Edge workloads with minimal footprint."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:18
msgid ""
"The recommended way of installing RKE2 as part of the SUSE Edge stack is by "
"using Edge Image Builder (EIB). See the <<components-eib,EIB documentation>> "
"for more details on how to configure it to deploy RKE2."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:19
msgid ""
"EIB is flexible enough to support any parameter required by RKE2, such as "
"specifying the RKE2 version, the "
"https://docs.rke2.io/reference/server_config[servers] or the "
"https://docs.rke2.io/reference/linux_agent_config[agents] configuration, "
"covering all the Edge use cases."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:20
msgid ""
"For other use cases involving Metal^3^, RKE2 is also being used and "
"installed. In those particular cases, the "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2[Cluster API "
"Provider RKE2] automatically deploys RKE2 on clusters being provisioned with "
"Metal^3^ using the Edge Stack."
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:21
msgid ""
"In those cases, the RKE2 configuration must be applied on the different CRDs "
"involved. An example of how to provide a different CNI using the "
"`RKE2ControlPlane` CRD looks like:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/rke2.adoc:22
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  serverConfig:\n"
"    cni: calico\n"
"    cniMultusEnable: true\n"
"...\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:23
msgid ""
"For more information about the Metal^3^ use cases, see "
"<<components-metal3>>."
msgstr ""

#. type: Title ===
#: asciidoc/components/rke2.adoc:24
#, no-wrap
msgid "High availability"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:25
msgid ""
"For HA deployments, EIB automatically deploys and configures "
"<<components-metallb,MetalLB>> and the "
"link:https://github.com/suse-edge/endpoint-copier-operator[Endpoint Copier "
"Operator] to expose the RKE2 API endpoint externally."
msgstr ""

#. type: Title ===
#: asciidoc/components/rke2.adoc:26
#, no-wrap
msgid "Networking"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:27
msgid ""
"The supported CNI for the Edge Stack is "
"https://docs.cilium.io/en/stable/[Cilium] with optionally adding the "
"meta-plugin https://github.com/k8snetworkplumbingwg/multus-cni[Multus], but "
"RKE2 supports https://docs.rke2.io/install/network_options[a few others] as "
"well."
msgstr ""

#. type: Title ===
#: asciidoc/components/rke2.adoc:28
#, no-wrap
msgid "Storage"
msgstr ""

#. type: Plain text
#: asciidoc/components/rke2.adoc:29
msgid ""
"RKE2 does not provide any kind of persistent storage class or operators. For "
"clusters spanning over multiple nodes, it is recommended to use "
"<<components-longhorn,Longhorn>>."
msgstr ""

#. type: Title =
#: asciidoc/components/longhorn.adoc:1
#, no-wrap
msgid "Longhorn"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:2
msgid ""
"Longhorn is a lightweight, reliable and user-friendly distributed block "
"storage system designed for Kubernetes.  As an open source project, Longhorn "
"was initially developed by Rancher Labs and is currently incubated under the "
"CNCF."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:4
#: asciidoc/integrations/nvidia-slemicro.adoc:7
msgid ""
"If you are following this guide, it assumes that you have the following "
"already available:"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:5
msgid ""
"At least one host with SLE Micro 5.5 installed; this can be physical or "
"virtual"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:6
msgid "A Kubernetes cluster installed; either K3s or RKE2"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:7 asciidoc/guides/metallb-k3s.adoc:19
#: asciidoc/guides/metallb-kube-api.adoc:8
msgid "Helm"
msgstr ""

#. type: Title ==
#: asciidoc/components/longhorn.adoc:8
#, no-wrap
msgid "Manual installation of Longhorn"
msgstr ""

#. type: Title ===
#: asciidoc/components/longhorn.adoc:9
#, no-wrap
msgid "Installing Open-iSCSI"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:10
msgid ""
"A core requirement of deploying and using Longhorn is the installation of "
"the `open-iscsi` package and the `iscsid` daemon running on all Kubernetes "
"nodes.  This is necessary, since Longhorn relies on `iscsiadm` on the host "
"to provide persistent volumes to Kubernetes."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:11
msgid "Let's install it:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:12
#, no-wrap
msgid "transactional-update pkg install open-iscsi\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:13
msgid ""
"It is important to note that once the operation is completed, the package is "
"only installed into a new snapshot as SLE Micro is an immutable operating "
"system.  In order to load it and for the `iscsid` daemon to start running, "
"we must reboot into that new snapshot that we just created.  Issue the "
"reboot command when you are ready:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:14
#: asciidoc/integrations/nvidia-slemicro.adoc:36
#: asciidoc/integrations/nvidia-slemicro.adoc:55
#, no-wrap
msgid "reboot\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/longhorn.adoc:15
msgid ""
"For additional help installing open-iscsi, refer to the "
"https://longhorn.io/docs/1.6.1/deploy/install/#installing-open-iscsi[official "
"Longhorn documentation]."
msgstr ""

#. type: Title ===
#: asciidoc/components/longhorn.adoc:16
#, no-wrap
msgid "Installing Longhorn"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:17
msgid ""
"There are several ways to install Longhorn on your Kubernetes clusters.  "
"This guide will follow through the Helm installation, however feel free to "
"follow the https://longhorn.io/docs/1.6.1/deploy/install/[official "
"documentation] if another approach is desired."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:18
msgid "Add the Longhorn Helm repository:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:19
#, no-wrap
msgid "helm repo add longhorn https://charts.longhorn.io\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:20
msgid "Fetch the latest charts from the repository:"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:22
msgid "Install Longhorn in the longhorn-system namespace:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:23
#, no-wrap
msgid ""
"helm install longhorn longhorn/longhorn --namespace longhorn-system "
"--create-namespace --version 1.6.1\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:24
msgid "Confirm that the deployment succeeded:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:25
#, no-wrap
msgid "kubectl -n longhorn-system get pods\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:26
#, no-wrap
msgid ""
"localhost:~ # kubectl -n longhorn-system get pod\n"
"NAMESPACE         NAME                                                READY   "
"STATUS      RESTARTS        AGE\n"
"longhorn-system   longhorn-ui-5fc9fb76db-z5dc9                        1/1     "
"Running     0               90s\n"
"longhorn-system   longhorn-ui-5fc9fb76db-dcb65                        1/1     "
"Running     0               90s\n"
"longhorn-system   longhorn-manager-wts2v                              1/1     "
"Running     1 (77s ago)     90s\n"
"longhorn-system   longhorn-driver-deployer-5d4f79ddd-fxgcs            1/1     "
"Running     0               90s\n"
"longhorn-system   instance-manager-a9bf65a7808a1acd6616bcd4c03d925b   1/1     "
"Running     0               70s\n"
"longhorn-system   engine-image-ei-acb7590c-htqmp                      1/1     "
"Running     0               70s\n"
"longhorn-system   csi-attacher-5c4bfdcf59-j8xww                       1/1     "
"Running     0               50s\n"
"longhorn-system   csi-provisioner-667796df57-l69vh                    1/1     "
"Running     0               50s\n"
"longhorn-system   csi-attacher-5c4bfdcf59-xgd5z                       1/1     "
"Running     0               50s\n"
"longhorn-system   csi-provisioner-667796df57-dqkfr                    1/1     "
"Running     0               50s\n"
"longhorn-system   csi-attacher-5c4bfdcf59-wckt8                       1/1     "
"Running     0               50s\n"
"longhorn-system   csi-resizer-694f8f5f64-7n2kq                        1/1     "
"Running     0               50s\n"
"longhorn-system   csi-snapshotter-959b69d4b-rp4gk                     1/1     "
"Running     0               50s\n"
"longhorn-system   csi-resizer-694f8f5f64-r6ljc                        1/1     "
"Running     0               50s\n"
"longhorn-system   csi-resizer-694f8f5f64-k7429                        1/1     "
"Running     0               50s\n"
"longhorn-system   csi-snapshotter-959b69d4b-5k8pg                     1/1     "
"Running     0               50s\n"
"longhorn-system   csi-provisioner-667796df57-n5w9s                    1/1     "
"Running     0               50s\n"
"longhorn-system   csi-snapshotter-959b69d4b-x7b7t                     1/1     "
"Running     0               50s\n"
"longhorn-system   longhorn-csi-plugin-bsc8c                           3/3     "
"Running     0               50s\n"
msgstr ""

#. type: Title ==
#: asciidoc/components/longhorn.adoc:27
#, no-wrap
msgid "Creating Longhorn volumes"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:28
msgid ""
"Longhorn utilizes Kubernetes resources called `StorageClass` in order to "
"automatically provision `PersistentVolume` objects for pods.  Think of "
"`StorageClass` as a way for administrators to describe the _classes_ or "
"_profiles_ of storage they offer."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:29
msgid "Let's create a `StorageClass` with some default options:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:30
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"kind: StorageClass\n"
"apiVersion: storage.k8s.io/v1\n"
"metadata:\n"
"  name: longhorn-example\n"
"provisioner: driver.longhorn.io\n"
"allowVolumeExpansion: true\n"
"parameters:\n"
"  numberOfReplicas: \"3\"\n"
"  staleReplicaTimeout: \"2880\" # 48 hours in minutes\n"
"  fromBackup: \"\"\n"
"  fsType: \"ext4\"\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:31
msgid ""
"Now that we have our `StorageClass` in place, we need a "
"`PersistentVolumeClaim` referencing it.  A `PersistentVolumeClaim` (PVC) is "
"a request for storage by a user. PVCs consume `PersistentVolume` resources.  "
"Claims can request specific sizes and access modes (e.g., they can be "
"mounted once read/write or many times read-only)."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:32
msgid "Let's create a `PersistentVolumeClaim`:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:33
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: PersistentVolumeClaim\n"
"metadata:\n"
"  name: longhorn-volv-pvc\n"
"  namespace: longhorn-system\n"
"spec:\n"
"  accessModes:\n"
"    - ReadWriteOnce\n"
"  storageClassName: longhorn-example\n"
"  resources:\n"
"    requests:\n"
"      storage: 2Gi\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:34
msgid ""
"That's it! Once we have the `PersistentVolumeClaim` created, we can proceed "
"with attaching it to a `Pod`.  When the `Pod` is deployed, Kubernetes "
"creates the Longhorn volume and binds it to the `Pod` if storage is "
"available."
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:35
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: volume-test\n"
"  namespace: longhorn-system\n"
"spec:\n"
"  containers:\n"
"  - name: volume-test\n"
"    image: nginx:stable-alpine\n"
"    imagePullPolicy: IfNotPresent\n"
"    volumeMounts:\n"
"    - name: volv\n"
"      mountPath: /data\n"
"    ports:\n"
"    - containerPort: 80\n"
"  volumes:\n"
"  - name: volv\n"
"    persistentVolumeClaim:\n"
"      claimName: longhorn-volv-pvc\n"
"EOF\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/longhorn.adoc:36
msgid ""
"The concept of storage in Kubernetes is a complex, but important topic. We "
"briefly mentioned some of the most common Kubernetes resources, however, we "
"suggest to familiarize yourself with the "
"https://longhorn.io/docs/1.6.1/terminology/[terminology documentation] that "
"Longhorn offers."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:37
msgid "In this example, the result should look something like this:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:38
#, no-wrap
msgid ""
"localhost:~ # kubectl get storageclass\n"
"NAME                 PROVISIONER          RECLAIMPOLICY   VOLUMEBINDINGMODE   "
"ALLOWVOLUMEEXPANSION   AGE\n"
"longhorn (default)   driver.longhorn.io   Delete          Immediate           "
"true                   12m\n"
"longhorn-example     driver.longhorn.io   Delete          Immediate           "
"true                   24s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:39
#, no-wrap
msgid ""
"localhost:~ # kubectl get pvc -n longhorn-system\n"
"NAME                STATUS   VOLUME                                     "
"CAPACITY   ACCESS MODES   STORAGECLASS       AGE\n"
"longhorn-volv-pvc   Bound    pvc-f663a92e-ac32-49ae-b8e5-8a6cc29a7d1e   2Gi        "
"RWO            longhorn-example   54s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:40
#, no-wrap
msgid ""
"localhost:~ # kubectl get pods -n longhorn-system\n"
"NAME                                                READY   STATUS    "
"RESTARTS      AGE\n"
"csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0             "
"14m\n"
"csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0             "
"14m\n"
"csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0             "
"14m\n"
"csi-provisioner-667796df57-fmz2d                    1/1     Running   0             "
"14m\n"
"csi-provisioner-667796df57-p7rjr                    1/1     Running   0             "
"14m\n"
"csi-provisioner-667796df57-w9fdq                    1/1     Running   0             "
"14m\n"
"csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0             "
"14m\n"
"csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0             "
"14m\n"
"csi-resizer-694f8f5f64-zlncz                        1/1     Running   0             "
"14m\n"
"csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0             "
"14m\n"
"csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0             "
"14m\n"
"csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0             "
"14m\n"
"engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0             "
"14m\n"
"instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0             "
"14m\n"
"longhorn-csi-plugin-gd2jx                           3/3     Running   0             "
"14m\n"
"longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0             "
"15m\n"
"longhorn-manager-z4lnl                              1/1     Running   0             "
"15m\n"
"longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 (14m "
"ago)   15m\n"
"longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 (14m "
"ago)   15m\n"
"volume-test                                         1/1     Running   0             "
"26s\n"
msgstr ""

#. type: Title ==
#: asciidoc/components/longhorn.adoc:41
#, no-wrap
msgid "Accessing the UI"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:42
msgid ""
"If you installed Longhorn with kubectl or Helm, you need to set up an "
"Ingress controller to allow external traffic into the "
"cluster. Authentication is not enabled by default. If the Rancher catalog "
"app was used, Rancher automatically created an Ingress controller with "
"access control (the rancher-proxy)."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:43
msgid "Get the Longhorn’s external service IP address:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:44
#, no-wrap
msgid "kubectl -n longhorn-system get svc\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:45
msgid ""
"Once you have retrieved the `longhorn-frontend` IP address, you can start "
"using the UI by navigating to it in your browser."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:47
msgid ""
"SUSE Edge is using <<components-eib>> in order to customize base SLE Micro "
"OS images.  We are going to demonstrate how to do so for provisioning an "
"RKE2 cluster with Longhorn on top of it."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:48
msgid "Let's create the definition file:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:49
#, no-wrap
msgid ""
"export CONFIG_DIR=$HOME/eib\n"
"mkdir -p $CONFIG_DIR\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:50
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/iso-definition.yaml\n"
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  arch: x86_64\n"
"  outputImageName: eib-image.iso\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  helm:\n"
"    charts:\n"
"      - name: longhorn\n"
"        version: 1.6.1\n"
"        repositoryName: longhorn\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"    repositories:\n"
"      - name: longhorn\n"
"        url: https://charts.longhorn.io\n"
"operatingSystem:\n"
"  packages:\n"
"    sccRegistrationCode: <reg-code>\n"
"    packageList:\n"
"      - open-iscsi\n"
"  users:\n"
"  - username: root\n"
"    encryptedPassword: "
"\\$6\\$jHugJNNd3HElGsUZ\\$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"EOF\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/longhorn.adoc:51
msgid ""
"Customizing any of the Helm chart values is possible via a separate file "
"provided under `helm.charts[].valuesFile`.  Refer to the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#kubernetes[upstream "
"documentation] for details."
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:53
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v $CONFIG_DIR:/eib "
"registry.suse.com/edge/edge-image-builder:1.0.2 build --definition-file "
"$CONFIG_DIR/iso-definition.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:54
msgid ""
"After the image is built, you can use it to install your OS on a physical or "
"virtual host.  Once the provisioning is complete, you are able to log in to "
"the system using the `root:eib` credentials pair."
msgstr ""

#. type: Plain text
#: asciidoc/components/longhorn.adoc:55
msgid "Ensure that Longhorn has been successfully deployed:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/longhorn.adoc:56
#, no-wrap
msgid ""
"localhost:~ # /var/lib/rancher/rke2/bin/kubectl --kubeconfig "
"/etc/rancher/rke2/rke2.yaml -n longhorn-system get pods\n"
"NAME                                                READY   STATUS    "
"RESTARTS        AGE\n"
"csi-attacher-5c4bfdcf59-qmjtz                       1/1     Running   0               "
"103s\n"
"csi-attacher-5c4bfdcf59-s7n65                       1/1     Running   0               "
"103s\n"
"csi-attacher-5c4bfdcf59-w9xgs                       1/1     Running   0               "
"103s\n"
"csi-provisioner-667796df57-fmz2d                    1/1     Running   0               "
"103s\n"
"csi-provisioner-667796df57-p7rjr                    1/1     Running   0               "
"103s\n"
"csi-provisioner-667796df57-w9fdq                    1/1     Running   0               "
"103s\n"
"csi-resizer-694f8f5f64-2rb8v                        1/1     Running   0               "
"103s\n"
"csi-resizer-694f8f5f64-z9v9x                        1/1     Running   0               "
"103s\n"
"csi-resizer-694f8f5f64-zlncz                        1/1     Running   0               "
"103s\n"
"csi-snapshotter-959b69d4b-5dpvj                     1/1     Running   0               "
"103s\n"
"csi-snapshotter-959b69d4b-lwwkv                     1/1     Running   0               "
"103s\n"
"csi-snapshotter-959b69d4b-tzhwc                     1/1     Running   0               "
"103s\n"
"engine-image-ei-5cefaf2b-hvdv5                      1/1     Running   0               "
"109s\n"
"instance-manager-0ee452a2e9583753e35ad00602250c5b   1/1     Running   0               "
"109s\n"
"longhorn-csi-plugin-gd2jx                           3/3     Running   0               "
"103s\n"
"longhorn-driver-deployer-9f4fc86-j6h2b              1/1     Running   0               "
"2m28s\n"
"longhorn-manager-z4lnl                              1/1     Running   0               "
"2m28s\n"
"longhorn-ui-5f4b7bbf69-bln7h                        1/1     Running   3 "
"(2m7s ago)    2m28s\n"
"longhorn-ui-5f4b7bbf69-lh97n                        1/1     Running   3 "
"(2m10s ago)   2m28s\n"
msgstr ""

#. type: delimited block =
#: asciidoc/components/longhorn.adoc:57
msgid ""
"This installation will not work for completely air-gapped environments.  In "
"those cases, please refer to <<longhorn-install>>."
msgstr ""

#. type: Title =
#: asciidoc/components/neuvector.adoc:1
#, no-wrap
msgid "NeuVector"
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:2
msgid ""
"NeuVector is a security solution for Kubernetes that provides L7 network "
"security, runtime security, supply chain security, and compliance checks in "
"a cohesive package."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:3
msgid ""
"NeuVector is deployed as a platform of several containers that communicate "
"with each other on various ports and interfaces. The following are the "
"different containers deployed:"
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:4
msgid ""
"Manager. A stateless container which presents the Web-based "
"console. Typically, only one is needed and this can run anywhere. Failure of "
"the Manager does not affect any of the operations of the controller or "
"enforcer. However, certain notifications (events) and recent connection data "
"are cached in memory by the Manager so viewing of these would be affected."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:5
msgid ""
"Controller. The ‘control plane’ for NeuVector must be deployed in an HA "
"configuration, so configuration is not lost in a node failure. These can run "
"anywhere, although customers often choose to place these on ‘management’, "
"master or infra nodes because of their criticality."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:6
msgid ""
"Enforcer. This container is deployed as a DaemonSet so one Enforcer is on "
"every node to be protected. Typically deploys to every worker node but "
"scheduling can be enabled for master and infra nodes to deploy there as "
"well. Note: If the Enforcer is not on a cluster node and connections come "
"from a pod on that node, NeuVector labels them as ‘unmanaged’ workloads."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:7
msgid ""
"Scanner. Performs the vulnerability scanning using the built-in CVE "
"database, as directed by the Controller. Multiple scanners can be deployed "
"to increase scanning capacity. Scanners can run anywhere but are often run "
"on the nodes where the controllers run. See below for sizing considerations "
"of scanner nodes. A scanner can also be invoked independently when used for "
"build-phase scanning, for example, within a pipeline that triggers a scan, "
"retrieves the results, and stops the scanner. The scanner contains the "
"latest CVE database so should be updated daily."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:8
msgid ""
"Updater. The updater triggers an update of the scanner through a Kubernetes "
"cron job when an update of the CVE database is desired. Please be sure to "
"configure this for your environment."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:9
msgid ""
"A more in-depth NeuVector onboarding and best practices documentation can be "
"found "
"https://open-docs.neuvector.com/deploying/production/NV_Onboarding_5.0.pdf[here]."
msgstr ""

#. type: Title ==
#: asciidoc/components/neuvector.adoc:10
#, no-wrap
msgid "How does SUSE Edge use NeuVector?"
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:11
msgid ""
"SUSE Edge provides a leaner configuration of NeuVector as a starting point "
"for edge deployments."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:12
msgid ""
"Find the NeuVector configuration changes "
"https://github.com/suse-edge/charts/blob/main/packages/neuvector-core/generated-changes/patch/values.yaml.patch[here]."
msgstr ""

#. type: Title ==
#: asciidoc/components/neuvector.adoc:13
#, no-wrap
msgid "Important notes"
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:14
msgid ""
"The `Scanner` container must have enough memory to pull the image to be "
"scanned into memory and expand it. To scan images exceeding 1 GB, increase "
"the scanner's memory to slightly above the largest expected image size."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:15
msgid ""
"High network connections expected in Protect mode. The `Enforcer` requires "
"CPU and memory when in Protect (inline firewall blocking) mode to hold and "
"inspect connections and possible payload (DLP). Increasing memory and "
"dedicating a CPU core to the `Enforcer` can ensure adequate packet filtering "
"capacity."
msgstr ""

#. type: Plain text
#: asciidoc/components/neuvector.adoc:17
msgid ""
"SUSE Edge is using <<components-eib>> in order to customize base SLE Micro "
"OS images.  Follow <<neuvector-install>> for an air-gapped installation of "
"NeuVector on top of Kubernetes clusters provisioned by EIB."
msgstr ""

#. type: Title =
#: asciidoc/components/metallb.adoc:1
#, no-wrap
msgid "MetalLB"
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:2
msgid "See https://metallb.universe.tf/[MetalLB official documentation]."
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:3 asciidoc/guides/metallb-k3s.adoc:2
msgid ""
"MetalLB is a load-balancer implementation for bare-metal Kubernetes "
"clusters, using standard routing protocols."
msgstr ""

#. type: delimited block _
#: asciidoc/components/metallb.adoc:4
msgid ""
"In bare-metal environments, setting up network load balancers is notably "
"more complex than in cloud environments. Unlike the straightforward API "
"calls in cloud setups, bare metal requires either dedicated network "
"appliances or a combination of load balancers and Virtual IP (VIP) "
"configurations to manage High Availability (HA) or address the potential "
"Single Point of Failure (SPOF) inherent in a single node load "
"balancer. These configurations are not easily automated, posing challenges "
"in Kubernetes deployments where components dynamically scale up and down."
msgstr ""

#. type: delimited block _
#: asciidoc/components/metallb.adoc:5
msgid ""
"MetalLB addresses these challenges by harnessing the Kubernetes model to "
"create LoadBalancer type services as if they were operating in a cloud "
"environment, even on bare-metal setups."
msgstr ""

#. type: delimited block _
#: asciidoc/components/metallb.adoc:6
msgid ""
"There are two different approaches, via "
"https://metallb.universe.tf/concepts/layer2/[L2 mode] (using ARP _tricks_) "
"or via https://metallb.universe.tf/concepts/bgp/[BGP]. Mainly L2 does not "
"need any special network gear but BGP is in general better. It depends on "
"the use cases."
msgstr ""

#. type: Title ==
#: asciidoc/components/metallb.adoc:7
#, no-wrap
msgid "How does SUSE Edge use MetalLB?"
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:8
msgid "SUSE Edge uses MetalLB in two key ways:"
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:9
msgid ""
"As a Load Balancer Solution: MetalLB serves as the Load Balancer solution "
"for bare-metal machines."
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:10
msgid ""
"For an HA K3s/RKE2 Setup: MetalLB allows for load balancing the Kubernetes "
"API using a Virtual IP address."
msgstr ""

#. type: delimited block =
#: asciidoc/components/metallb.adoc:11
msgid ""
"In order to be able to expose the API, the `endpoint-copier-operator` is "
"used to keep in sync the K8s API endpoints from the 'kubernetes' service to "
"a 'kubernetes-vip' LoadBalancer service."
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:13
msgid ""
"Installation of MetalLB in L2 mode is detailed in <<guides-metallb-k3s,the "
"MetalLB guide>>."
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:14
msgid ""
"A guide on installing MetalLB in front of the kube-api-server to achieve HA "
"setups can be found in the <<guides-metallb-kubernetes,MetalLB in front of "
"the Kubernetes API server>> tutorial."
msgstr ""

#. type: Plain text
#: asciidoc/components/metallb.adoc:16
msgid ""
"K3S LoadBalancer Solution: K3S comes with its Load Balancer solution, "
"`Klipper`. To use MetalLB, Klipper must be disabled. This can be done by "
"starting the K3s server with the `--disable servicelb` option, as described "
"in the https://docs.k3s.io/networking[K3s documentation]."
msgstr ""

#. type: Title =
#: asciidoc/components/virtualization.adoc:1
#, no-wrap
msgid "Edge Virtualization"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:2
msgid ""
"This section describes how you can use Edge Virtualization to run virtual "
"machines on your edge nodes. It is important to point out that Edge "
"Virtualization is not a comprehensive solution and has limited features; it "
"attempts to solve requirements for lightweight virtualization where basic "
"virtual machine capabilities are required. SUSE provides a more "
"comprehensive virtualization (and hyperconverged infrastructure) solution "
"with https://harvesterhci.io/[Harvester]."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:3
msgid "SUSE Edge Virtualization supports two methods of running virtual machines:"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:4
msgid ""
"Deploying the virtual machines manually via libvirt+qemu-kvm at the host "
"level"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:5
msgid ""
"Deploying the KubeVirt operator for Kubernetes-based management of virtual "
"machines"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:6
msgid ""
"Both options are valid, but only the second one is covered below. If you "
"want to use the standard out-of-the box virtualization mechanisms provided "
"by SLE Micro, a comprehensive guide can be found "
"https://documentation.suse.com/sles/15-SP5/html/SLES-all/chap-virtualization-introduction.html[here], "
"and whilst it was primarily written for SUSE Linux Enterprise Server, the "
"concepts are almost identical."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:7
msgid ""
"This guide initially explains how to deploy the additional virtualization "
"components onto a system that has already been pre-deployed, but follows "
"with a section that describes how to embed this configuration in the initial "
"deployment via Edge Image Builder. If you do not want to run through the "
"basics and set things up manually, skip right ahead to that section."
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:8
#, no-wrap
msgid "KubeVirt overview"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:9
msgid ""
"KubeVirt allows for managing Virtual Machines with Kubernetes alongside the "
"rest of your containerized workloads. It does this by running the user space "
"portion of the Linux virtualization stack in a container. This minimizes the "
"requirements on the host system, allowing for easier setup and management."
msgstr ""

#. type: delimited block =
#: asciidoc/components/virtualization.adoc:10
msgid ""
"Details about KubeVirt's architecture can be found in "
"link:https://kubevirt.io/user-guide/architecture/[the upstream "
"documentation.]"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:12
msgid ""
"If you are following this guide, we assume you have the following already "
"available:"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:13
msgid ""
"At least one physical host with SLE Micro 5.5+ installed, and with "
"virtualization extensions enabled in the BIOS (see "
"https://documentation.suse.com/sles/15-SP5/html/SLES-all/cha-virt-support.html#sec-kvm-requires-hardware[here] "
"for details)."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:14
msgid ""
"Across your nodes, a K3s/RKE2 Kubernetes cluster already deployed and with "
"an appropriate `kubeconfig` that enables superuser access to the cluster."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:15
#: asciidoc/integrations/nvidia-slemicro.adoc:11
msgid ""
"Access to the root user — these instructions assume you are the root user, "
"and _not_ escalating your privileges via `sudo`."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:16
msgid ""
"You have https://helm.sh/docs/intro/install/[Helm] available locally with an "
"adequate network connection to be able to push configurations to your "
"Kubernetes cluster and download the required images."
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:17
#, no-wrap
msgid "Manual installation of Edge Virtualization"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:18
msgid ""
"This guide will not walk you through the deployment of Kubernetes, but it "
"assumes that you have either installed the SUSE Edge-appropriate version of "
"https://k3s.io/[K3s] or https://docs.rke2.io/install/quickstart[RKE2] and "
"that you have your kubeconfig configured accordingly so that standard "
"`kubectl` commands can be executed as the superuser. We assume your node "
"forms a single-node cluster, although there are no significant differences "
"expected for multi-node deployments."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:19
msgid ""
"SUSE Edge Virtualization is deployed via three separate Helm charts, "
"specifically:"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:20
msgid ""
"*KubeVirt*: The core virtualization components, that is, Kubernetes CRDs, "
"operators and other components required for enabling Kubernetes to deploy "
"and manage virtual machines."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:21
msgid ""
"*KubeVirt Dashboard Extension*: An optional Rancher UI extension that allows "
"basic virtual machine management, for example, starting/stopping of virtual "
"machines as well as accessing the console."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:22
msgid ""
"*Containerized Data Importer (CDI)*: An additional component that enables "
"persistent-storage integration for KubeVirt, providing capabilities for "
"virtual machines to use existing Kubernetes storage back-ends for data, but "
"also allowing users to import or clone data volumes for virtual machines."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:23
msgid ""
"Each of these Helm charts is versioned according to the SUSE Edge release "
"you are currently using. For production/supported usage, employ the "
"artifacts that can be found in the SUSE Registry."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:24
msgid "First, ensure that your `kubectl` access is working:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:25
#, no-wrap
msgid "$ kubectl get nodes\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:26
#: asciidoc/components/virtualization.adoc:33
#: asciidoc/components/virtualization.adoc:42
#: asciidoc/integrations/nvidia-slemicro.adoc:77
msgid "This should show something similar to the following:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:27
#, no-wrap
msgid ""
"NAME                   STATUS   ROLES                       AGE     "
"VERSION\n"
"node1.edge.rdo.wales   Ready    control-plane,etcd,master   4h20m   "
"v1.28.9+rke2r1\n"
"node2.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   "
"v1.28.9+rke2r1\n"
"node3.edge.rdo.wales   Ready    control-plane,etcd,master   4h15m   "
"v1.28.9+rke2r1\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:28
msgid ""
"Now you can proceed to install the *KubeVirt* and *Containerized Data "
"Importer (CDI)* Helm charts:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:29
#, no-wrap
msgid ""
"$ helm install kubevirt oci://registry.suse.com/edge/kubevirt-chart "
"--namespace kubevirt-system --create-namespace\n"
"$ helm install cdi oci://registry.suse.com/edge/cdi-chart --namespace "
"cdi-system --create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:30
msgid ""
"In a few minutes, you should have all KubeVirt and CDI components "
"deployed. You can validate this by checking all the deployed resources in "
"the `kubevirt-system` and `cdi-system` namespace."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:31
msgid "Verify KubeVirt resources:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:32
#, no-wrap
msgid "$ kubectl get all -n kubevirt-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:34
#, no-wrap
msgid ""
"NAME                                   READY   STATUS    RESTARTS      AGE\n"
"pod/virt-operator-5fbcf48d58-p7xpm     1/1     Running   0             "
"2m24s\n"
"pod/virt-operator-5fbcf48d58-wnf6s     1/1     Running   0             "
"2m24s\n"
"pod/virt-handler-t594x                 1/1     Running   0             93s\n"
"pod/virt-controller-5f84c69884-cwjvd   1/1     Running   1 (64s ago)   93s\n"
"pod/virt-controller-5f84c69884-xxw6q   1/1     Running   1 (64s ago)   93s\n"
"pod/virt-api-7dfc54cf95-v8kcl          1/1     Running   1 (59s ago)   "
"118s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:35
#, no-wrap
msgid ""
"NAME                                  TYPE        CLUSTER-IP      "
"EXTERNAL-IP   PORT(S)   AGE\n"
"service/kubevirt-prometheus-metrics   ClusterIP   None            <none>        "
"443/TCP   2m1s\n"
"service/virt-api                      ClusterIP   10.43.56.140    <none>        "
"443/TCP   2m1s\n"
"service/kubevirt-operator-webhook     ClusterIP   10.43.201.121   <none>        "
"443/TCP   2m1s\n"
"service/virt-exportproxy              ClusterIP   10.43.83.23     <none>        "
"443/TCP   2m1s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:36
#, no-wrap
msgid ""
"NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   "
"AVAILABLE   NODE SELECTOR            AGE\n"
"daemonset.apps/virt-handler   1         1         1       1            1           "
"kubernetes.io/os=linux   93s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:37
#, no-wrap
msgid ""
"NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n"
"deployment.apps/virt-operator     2/2     2            2           2m24s\n"
"deployment.apps/virt-controller   2/2     2            2           93s\n"
"deployment.apps/virt-api          1/1     1            1           118s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:38
#, no-wrap
msgid ""
"NAME                                         DESIRED   CURRENT   READY   "
"AGE\n"
"replicaset.apps/virt-operator-5fbcf48d58     2         2         2       "
"2m24s\n"
"replicaset.apps/virt-controller-5f84c69884   2         2         2       "
"93s\n"
"replicaset.apps/virt-api-7dfc54cf95          1         1         1       "
"118s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:39
#, no-wrap
msgid ""
"NAME                            AGE     PHASE\n"
"kubevirt.kubevirt.io/kubevirt   2m24s   Deployed\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:40
msgid "Verify CDI resources:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:41
#, no-wrap
msgid "$ kubectl get all -n cdi-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:43
#, no-wrap
msgid ""
"NAME                                   READY   STATUS    RESTARTS   AGE\n"
"pod/cdi-operator-55c74f4b86-692xb      1/1     Running   0          2m24s\n"
"pod/cdi-apiserver-db465b888-62lvr      1/1     Running   0          2m21s\n"
"pod/cdi-deployment-56c7d74995-mgkfn    1/1     Running   0          2m21s\n"
"pod/cdi-uploadproxy-7d7b94b968-6kxc2   1/1     Running   0          2m22s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:44
#, no-wrap
msgid ""
"NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   "
"PORT(S)    AGE\n"
"service/cdi-uploadproxy          ClusterIP   10.43.117.7    <none>        "
"443/TCP    2m22s\n"
"service/cdi-api                  ClusterIP   10.43.20.101   <none>        "
"443/TCP    2m22s\n"
"service/cdi-prometheus-metrics   ClusterIP   10.43.39.153   <none>        "
"8080/TCP   2m21s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:45
#, no-wrap
msgid ""
"NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n"
"deployment.apps/cdi-operator      1/1     1            1           2m24s\n"
"deployment.apps/cdi-apiserver     1/1     1            1           2m22s\n"
"deployment.apps/cdi-deployment    1/1     1            1           2m21s\n"
"deployment.apps/cdi-uploadproxy   1/1     1            1           2m22s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:46
#, no-wrap
msgid ""
"NAME                                         DESIRED   CURRENT   READY   "
"AGE\n"
"replicaset.apps/cdi-operator-55c74f4b86      1         1         1       "
"2m24s\n"
"replicaset.apps/cdi-apiserver-db465b888      1         1         1       "
"2m21s\n"
"replicaset.apps/cdi-deployment-56c7d74995    1         1         1       "
"2m21s\n"
"replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       "
"2m22s\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:47
msgid ""
"To verify that the `VirtualMachine` custom resource definitions (CRDs) are "
"deployed, you can validate with:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:48
#, no-wrap
msgid "$ kubectl explain virtualmachine\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:49
msgid ""
"This should print out the definition of the `VirtualMachine` object, which "
"should print as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:50
#, no-wrap
msgid ""
"GROUP:      kubevirt.io\n"
"KIND:       VirtualMachine\n"
"VERSION:    v1\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:51
#, no-wrap
msgid ""
"DESCRIPTION:\n"
"    VirtualMachine handles the VirtualMachines that are not running or are "
"in a\n"
"    stopped state The VirtualMachine contains the template to create the\n"
"    VirtualMachineInstance. It also mirrors the running state of the "
"created\n"
"    VirtualMachineInstance in its status.\n"
"(snip)\n"
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:52
#, no-wrap
msgid "Deploying virtual machines"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:53
msgid ""
"Now that KubeVirt and CDI are deployed, let us define a simple virtual "
"machine based on https://get.opensuse.org/tumbleweed/[openSUSE "
"Tumbleweed]. This virtual machine has the most simple of configurations, "
"using standard \"pod networking\" for a networking configuration identical "
"to any other pod. It also employs non-persistent storage, ensuring the "
"storage is ephemeral, just like in any container that does not have a "
"https://kubernetes.io/docs/concepts/storage/persistent-volumes/[PVC]."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:54
#, no-wrap
msgid ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: kubevirt.io/v1\n"
"kind: VirtualMachine\n"
"metadata:\n"
"  name: tumbleweed\n"
"  namespace: default\n"
"spec:\n"
"  runStrategy: Always\n"
"  template:\n"
"    spec:\n"
"      domain:\n"
"        devices: {}\n"
"        machine:\n"
"          type: q35\n"
"        memory:\n"
"          guest: 2Gi\n"
"        resources: {}\n"
"      volumes:\n"
"      - containerDisk:\n"
"          image: "
"registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest\n"
"        name: tumbleweed-containerdisk-0\n"
"      - cloudInitNoCloud:\n"
"          userDataBase64: "
"I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK\n"
"        name: cloudinitdisk\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:55
msgid "This should print that a `VirtualMachine` was created:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:56
#, no-wrap
msgid "virtualmachine.kubevirt.io/tumbleweed created\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:57
msgid ""
"This `VirtualMachine` definition is minimal, specifying little about the "
"configuration. It simply outlines that it is a machine type "
"\"https://wiki.qemu.org/Features/Q35[q35]\" with 2 GB of memory that uses a "
"disk image based on an ephemeral "
"`https://kubevirt.io/user-guide/virtual_machines/disks_and_volumes/#containerdisk[containerDisk]` "
"(that is, a disk image that is stored in a container image from a remote "
"image repository), and specifies a base64 encoded cloudInit disk, which we "
"only use for user creation and password enforcement at boot time (use "
"`base64 -d` to decode it)."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:58
msgid ""
"NOTE: This virtual machine image is only for testing. The image is not "
"officially supported and is only meant as a documentation example."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:59
msgid ""
"This machine takes a few minutes to boot as it needs to download the "
"openSUSE Tumbleweed disk image, but once it has done so, you can view "
"further details about the virtual machine by checking the virtual machine "
"information:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:60
#, no-wrap
msgid "$ kubectl get vmi\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:61
msgid ""
"This should print the node that the virtual machine was started on, and the "
"IP address of the virtual machine. Remember, since it uses pod networking, "
"the reported IP address will be just like any other pod, and routable as "
"such:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:62
#, no-wrap
msgid ""
"NAME         AGE     PHASE     IP           NODENAME               READY\n"
"tumbleweed   4m24s   Running   10.42.2.98   node3.edge.rdo.wales   True\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:63
msgid ""
"When running these commands on the Kubernetes cluster nodes themselves, with "
"a CNI that routes traffic directly to pods (for example, Cilium), you should "
"be able to `ssh` directly to the machine itself. Substitute the following IP "
"address with the one that was assigned to your virtual machine:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:64
#, no-wrap
msgid ""
"$ ssh suse@10.42.2.98\n"
"(password is \"suse\")\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:65
msgid ""
"Once you are in this virtual machine, you can play around, but remember that "
"it is limited in terms of resources, and only has 1 GB disk space. When you "
"are finished, `Ctrl-D` or `exit` to disconnect from the SSH session."
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:66
msgid ""
"The virtual machine process is still wrapped in a standard Kubernetes "
"pod. The `VirtualMachine` CRD is a representation of the desired virtual "
"machine, but the process in which the virtual machine is actually started is "
"via the "
"`https://github.com/kubevirt/kubevirt/blob/main/docs/components.md#virt-launcher[virt-launcher]` "
"pod, a standard Kubernetes pod, just like any other application. For every "
"virtual machine started, you can see there is a `virt-launcher` pod:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:67
#, no-wrap
msgid "$ kubectl get pods\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:68
msgid ""
"This should then show the one `virt-launcher` pod for the Tumbleweed machine "
"that we have defined:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:69
#, no-wrap
msgid ""
"NAME                             READY   STATUS    RESTARTS   AGE\n"
"virt-launcher-tumbleweed-8gcn4   3/3     Running   0          10m\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:70
msgid ""
"If we take a look into this `virt-launcher` pod, you see it is executing "
"`libvirt` and `qemu-kvm` processes. We can enter the pod itself and have a "
"look under the covers, noting that you need to adapt the following command "
"for your pod name:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:71
#, no-wrap
msgid "$ kubectl exec -it virt-launcher-tumbleweed-8gcn4 -- bash\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:72
msgid ""
"Once you are in the pod, try running `virsh` commands along with looking at "
"the processes. You will see the `qemu-system-x86_64` binary running, along "
"with certain processes for monitoring the virtual machine. You will also see "
"the location of the disk image and how the networking is plugged (as a tap "
"device):"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:73
#, no-wrap
msgid ""
"qemu@tumbleweed:/> ps ax\n"
"  PID TTY      STAT   TIME COMMAND\n"
"    1 ?        Ssl    0:00 /usr/bin/virt-launcher-monitor --qemu-timeout "
"269s --name tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c "
"--namespace default --kubevirt-share-dir /var/run/kubevirt "
"--ephemeral-disk-dir /var/run/kubevirt-ephemeral-disks --container-disk-dir "
"/var/run/kube\n"
"   12 ?        Sl     0:01 /usr/bin/virt-launcher --qemu-timeout 269s --name "
"tumbleweed --uid b9655c11-38f7-4fa8-8f5d-bfe987dab42c --namespace default "
"--kubevirt-share-dir /var/run/kubevirt --ephemeral-disk-dir "
"/var/run/kubevirt-ephemeral-disks --container-disk-dir "
"/var/run/kubevirt/con\n"
"   24 ?        Sl     0:00 /usr/sbin/virtlogd -f "
"/etc/libvirt/virtlogd.conf\n"
"   25 ?        Sl     0:01 /usr/sbin/virtqemud -f "
"/var/run/libvirt/virtqemud.conf\n"
"   83 ?        Sl     0:31 /usr/bin/qemu-system-x86_64 -name "
"guest=default_tumbleweed,debug-threads=on -S -object "
"{\"qom-type\":\"secret\",\"id\":\"masterKey0\",\"format\":\"raw\",\"file\":\"/var/run/kubevirt-private/libvirt/qemu/lib/domain-1-default_tumbleweed/master-key.aes\"} "
"-machine pc-q35-7.1,usb\n"
"  286 pts/0    Ss     0:00 bash\n"
"  320 pts/0    R+     0:00 ps ax\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:74
#, no-wrap
msgid ""
"qemu@tumbleweed:/> virsh list --all\n"
" Id   Name                 State\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:75
#, no-wrap
msgid " 1    default_tumbleweed   running\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:76
#, no-wrap
msgid ""
"qemu@tumbleweed:/> virsh domblklist 1\n"
" Target   Source\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:77
#, no-wrap
msgid ""
" sda      "
"/var/run/kubevirt-ephemeral-disks/disk-data/tumbleweed-containerdisk-0/disk.qcow2\n"
" sdb      "
"/var/run/kubevirt-ephemeral-disks/cloud-init-data/default/tumbleweed/noCloud.iso\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:78
#, no-wrap
msgid ""
"qemu@tumbleweed:/> virsh domiflist 1\n"
" Interface   Type       Source   Model                     MAC\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:79
#, no-wrap
msgid ""
" tap0        ethernet   -        virtio-non-transitional   "
"e6:e9:1a:05:c0:92\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:80
msgid "qemu@tumbleweed:/> exit exit"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:81
#, no-wrap
msgid "Finally, let us delete this virtual machine to clean up:\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:82
#: asciidoc/components/virtualization.adoc:90
#: asciidoc/components/virtualization.adoc:93
#: asciidoc/components/virtualization.adoc:96
#: asciidoc/components/virtualization.adoc:99
#: asciidoc/components/virtualization.adoc:103
#: asciidoc/components/virtualization.adoc:106
#: asciidoc/components/virtualization.adoc:110
#: asciidoc/components/virtualization.adoc:113
#: asciidoc/components/virtualization.adoc:116
#: asciidoc/components/virtualization.adoc:119
#: asciidoc/components/virtualization.adoc:133
#: asciidoc/components/virtualization.adoc:136
#: asciidoc/components/virtualization.adoc:139
#: asciidoc/components/virtualization.adoc:142
#: asciidoc/components/virtualization.adoc:145
#: asciidoc/integrations/nvidia-slemicro.adoc:49
#: asciidoc/integrations/nvidia-slemicro.adoc:52
#, no-wrap
msgid "[,shell]\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:83
msgid ""
"$ kubectl delete vm/tumbleweed virtualmachine.kubevirt.io \"tumbleweed\" "
"deleted"
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:84
#, no-wrap
msgid "Using virtctl"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:85
msgid ""
"Along with the standard Kubernetes CLI tooling, that is, `kubectl`, KubeVirt "
"comes with an accompanying CLI utility that allows you to interface with "
"your cluster in a way that bridges some gaps between the virtualization "
"world and the world that Kubernetes was designed for. For example, the "
"`virtctl` tool provides the capability of managing the lifecycle of virtual "
"machines (starting, stopping, restarting, etc.), providing access to the "
"virtual consoles, uploading virtual machine images, as well as interfacing "
"with Kubernetes constructs such as services, without using the API or CRDs "
"directly."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:86
msgid "Let us download the latest stable version of the `virtctl` tool:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:87
#: asciidoc/components/virtualization.adoc:126
#: asciidoc/integrations/nvidia-slemicro.adoc:45
msgid "[,shell]"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:88
msgid ""
"$ export VERSION=v1.1.0 $ wget "
"https://github.com/kubevirt/kubevirt/releases/download/${VERSION}/virtctl-${VERSION}-linux-amd64"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:89
#, no-wrap
msgid ""
"If you are using a different architecture or a non-Linux machine, you can "
"find other releases https://github.com/kubevirt/kubevirt/releases[here]. You "
"need to make this executable before proceeding, and it may be useful to move "
"it to a location within your `$PATH`:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:91
msgid ""
"$ mv virtctl-${VERSION}-linux-amd64 /usr/local/bin/virtctl $ chmod a+x "
"/usr/local/bin/virtctl"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:92
#, no-wrap
msgid ""
"You can then use the `virtctl` command-line tool to create virtual "
"machines. Let us replicate our previous virtual machine, noting that we are "
"piping the output directly into `kubectl apply`:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:94
#, no-wrap
msgid ""
"$ virtctl create vm --name virtctl-example --memory=1Gi \\\n"
"    "
"--volume-containerdisk=src:registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest "
"\\\n"
"    --cloud-init-user-data "
"\"I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScK\" "
"| kubectl apply -f -\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:95
#, no-wrap
msgid ""
"This should then show the virtual machine running (it should start a lot "
"quicker this time given that the container image will be cached):\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:97
msgid ""
"$ kubectl get vmi NAME AGE PHASE IP NODENAME READY virtctl-example 52s "
"Running 10.42.2.29 node3.edge.rdo.wales True"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:98
#, no-wrap
msgid "Now we can use `virtctl` to connect directly to the virtual machine:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:100
msgid "$ virtctl ssh suse@virtctl-example (password is \"suse\" - Ctrl-D to exit)"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:101
#, no-wrap
msgid ""
"There are plenty of other commands that can be used by `virtctl`. For "
"example, `virtctl console` can give you access to the serial console if "
"networking is not working, and you can use `virtctl  guestosinfo` to get "
"comprehensive OS information, subject to the guest having the "
"`qemu-guest-agent` installed and running.\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:102
#, no-wrap
msgid "Finally, let us pause and resume the virtual machine:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:104
msgid ""
"$ virtctl pause vm virtctl-example VMI virtctl-example was scheduled to "
"pause"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:105
#, no-wrap
msgid ""
"You find that the `VirtualMachine` object shows as *Paused* and the "
"`VirtualMachineInstance` object shows as *Running* but *READY=False*:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:107
msgid "$ kubectl get vm NAME AGE STATUS READY virtctl-example 8m14s Paused False"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:108
msgid ""
"$ kubectl get vmi NAME AGE PHASE IP NODENAME READY virtctl-example 8m15s "
"Running 10.42.2.29 node3.edge.rdo.wales False"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:109
#, no-wrap
msgid "You also find that you can no longer connect to the virtual machine:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:111
msgid ""
"$ virtctl ssh suse@virtctl-example can't access VMI virtctl-example: "
"Operation cannot be fulfilled on virtualmachineinstance.kubevirt.io "
"\"virtctl-example\": VMI is paused"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:112
#, no-wrap
msgid "Let us resume the virtual machine and try again:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:114
msgid ""
"$ virtctl unpause vm virtctl-example VMI virtctl-example was scheduled to "
"unpause"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:115
#, no-wrap
msgid "Now we should be able to re-establish a connection:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:117
msgid ""
"$ virtctl ssh suse@virtctl-example suse@vmi/virtctl-example.default's "
"password: suse@virtctl-example:~> exit logout"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:118
#, no-wrap
msgid "Finally, let us remove the virtual machine:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:120
msgid ""
"$ kubectl delete vm/virtctl-example virtualmachine.kubevirt.io "
"\"virtctl-example\" deleted"
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:121
#, no-wrap
msgid "Simple ingress networking"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:122
msgid ""
"In this section, we show how you can expose virtual machines as standard "
"Kubernetes services and make them available via the Kubernetes ingress "
"service, for example, "
"https://docs.rke2.io/networking/networking_services#nginx-ingress-controller[NGINX "
"with RKE2] or "
"https://docs.k3s.io/networking/networking-services#traefik-ingress-controller[Traefik "
"with K3s]. This document assumes that these components are already "
"configured appropriately and that you have an appropriate DNS pointer, for "
"example, via a wild card, to point at your Kubernetes server nodes or your "
"ingress virtual IP for proper ingress resolution."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:123
msgid ""
"> NOTE: In SUSE Edge 3.0+, if you are using K3s in a multi-server node "
"configuration, you might have needed to configure a MetalLB-based VIP for "
"Ingress; this is not required for RKE2."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:124
msgid ""
"In the example environment, another openSUSE Tumbleweed virtual machine is "
"deployed, cloud-init is used to install NGINX as a simple Web server at boot "
"time, and a simple message is configured to be returned to verify that it "
"works as expected when a call is made. To see how this is done, simply "
"`base64 -d` the cloud-init section in the output below."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:125
msgid "Let us create this virtual machine now:"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:127
#, no-wrap
msgid ""
"$ kubectl apply -f - <<EOF\n"
"apiVersion: kubevirt.io/v1\n"
"kind: VirtualMachine\n"
"metadata:\n"
"  name: ingress-example\n"
"  namespace: default\n"
"spec:\n"
"  runStrategy: Always\n"
"  template:\n"
"    metadata:\n"
"      labels:\n"
"        app: nginx\n"
"    spec:\n"
"      domain:\n"
"        devices: {}\n"
"        machine:\n"
"          type: q35\n"
"        memory:\n"
"          guest: 2Gi\n"
"        resources: {}\n"
"      volumes:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:128
#, no-wrap
msgid "containerDisk:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:129
#, no-wrap
msgid ""
"image: "
"registry.opensuse.org/home/roxenham/tumbleweed-container-disk/containerfile/cloud-image:latest\n"
"name: tumbleweed-containerdisk-0\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:130
#, no-wrap
msgid "cloudInitNoCloud:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:131
#, no-wrap
msgid ""
"userDataBase64: "
"I2Nsb3VkLWNvbmZpZwpkaXNhYmxlX3Jvb3Q6IGZhbHNlCnNzaF9wd2F1dGg6IFRydWUKdXNlcnM6CiAgLSBkZWZhdWx0CiAgLSBuYW1lOiBzdXNlCiAgICBncm91cHM6IHN1ZG8KICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIHN1ZG86ICBBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMCiAgICBsb2NrX3Bhc3N3ZDogRmFsc2UKICAgIHBsYWluX3RleHRfcGFzc3dkOiAnc3VzZScKcnVuY21kOgogIC0genlwcGVyIGluIC15IG5naW54CiAgLSBzeXN0ZW1jdGwgZW5hYmxlIC0tbm93IG5naW54CiAgLSBlY2hvICJJdCB3b3JrcyEiID4gL3Nydi93d3cvaHRkb2NzL2luZGV4Lmh0bQo=\n"
"name: cloudinitdisk\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:132
#, no-wrap
msgid ""
"When this virtual machine has successfully started, we can use the `virtctl` "
"command to expose the `VirtualMachineInstance` with an external port of "
"`8080` and a target port of `80` (where NGINX listens by default). We use "
"the `virtctl` command here as it understands the mapping between the virtual "
"machine object and the pod. This creates a new service for us:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:134
msgid ""
"$ virtctl expose vmi ingress-example --port=8080 --target-port=80 "
"--name=ingress-example Service ingress-example successfully exposed for vmi "
"ingress-example"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:135
#, no-wrap
msgid "We will then have an appropriate service automatically created:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:137
msgid ""
"$ kubectl get svc/ingress-example NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S)  "
"AGE ingress-example ClusterIP 10.43.217.19 <none> 8080/TCP 9s"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:138
#, no-wrap
msgid ""
"Next, if you then use `kubectl create ingress`, we can create an ingress "
"object that points to this service. Adapt the URL (known as the \"host\" in "
"the "
"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/[ingress] "
"object) here to match your DNS configuration and ensure that you point it to "
"port `8080`:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:140
msgid ""
"$ kubectl create ingress ingress-example "
"--rule=ingress-example.suse.local/=ingress-example:8080"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:141
#, no-wrap
msgid ""
"With DNS being configured correctly, you should be able to curl the URL "
"immediately:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:143
msgid "$ curl ingress-example.suse.local It works!"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:144
#, no-wrap
msgid ""
"Let us clean up by removing this virtual machine and its service and ingress "
"resources:\n"
msgstr ""

#. type: Plain text
#: asciidoc/components/virtualization.adoc:146
msgid ""
"$ kubectl delete vm/ingress-example svc/ingress-example "
"ingress/ingress-example virtualmachine.kubevirt.io \"ingress-example\" "
"deleted service \"ingress-example\" deleted ingress.networking.k8s.io "
"\"ingress-example\" deleted"
msgstr ""

#. type: Title ==
#: asciidoc/components/virtualization.adoc:147
#, no-wrap
msgid "Using the Rancher UI extension"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:148
msgid ""
"SUSE Edge Virtualization provides a UI extension for Rancher Manager, "
"enabling basic virtual machine management using the Rancher dashboard UI."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:150
msgid ""
"See xref:rancher-dashboard-extensions.adoc[Rancher Dashboard Extensions "
"section] for installation guidance."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:151
msgid "[#kubevirt-dashboard-extension]"
msgstr ""

#. type: Title ===
#: asciidoc/components/virtualization.adoc:152
#, no-wrap
msgid "Using KubeVirt Rancher Dashboard Extension"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:153
msgid ""
"The extension introduces a new *KubeVirt* section to the Cluster "
"Explorer. This section is added to any managed cluster which has KubeVirt "
"installed."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:154
msgid "The extension allows you to directly interact with two KubeVirt resources:"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:155
msgid ""
"1. `Virtual Machine instances` — A resource representing a single running "
"virtual machine instance.  2. `Virtual Machines` — A resource used to manage "
"virtual machines lifecycle."
msgstr ""

#. type: Title ====
#: asciidoc/components/virtualization.adoc:156
#, no-wrap
msgid "Creating a virtual machine"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:157
msgid ""
"1. Navigate to *Cluster Explorer* clicking KubeVirt-enabled managed cluster "
"in the left navigation.  2. Navigate to *KubeVirt > Virtual Machines* page "
"and click `Create from YAML` in the upper right of the screen.  3. Fill in "
"or paste a virtual machine definition and press `Create`. Use virtual "
"machine definition from Deploying Virtual Machines section as an "
"inspiration."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:158
msgid "image::virtual-machines-page.png[]"
msgstr ""

#. type: Title ====
#: asciidoc/components/virtualization.adoc:159
#, no-wrap
msgid "Starting and stopping virtual machines"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:160
msgid ""
"Start and stop virtual machines using the action menu accessed from the *⋮* "
"drop-down list to the right of each virtual machine or use group actions at "
"the top of the list by selecting virtual machines to perform the action on."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:161
msgid ""
"It is possible to run start and stop actions only on the virtual machines "
"which have `spec.running` property defined. In case when `spec.runStrategy` "
"is used, it is not possible to directly start and stop such a machine. For "
"more information, see "
"https://kubevirt.io/user-guide/virtual_machines/run_strategies/#run-strategies[KubeVirt "
"documentation]."
msgstr ""

#. type: Title ====
#: asciidoc/components/virtualization.adoc:162
#, no-wrap
msgid "Accessing virtual machine console"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:163
msgid ""
"The \"Virtual machines\" list provides a `Console` drop-down list that "
"allows to connect to the machine using *VNC or Serial Console*. This action "
"is only available to running machines."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:164
msgid ""
"In some cases, it takes a short while before the console is accessible on a "
"freshly started virtual machine."
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:165
msgid "image::vnc-console-ui.png[]"
msgstr ""

#. type: delimited block -
#: asciidoc/components/virtualization.adoc:167
msgid ""
"SUSE Edge is using <<components-eib>> in order to customize base SLE Micro "
"OS images.  Follow <<kubevirt-install>> for an air-gapped installation of "
"both KubeVirt and CDI on top of Kubernetes clusters provisioned by EIB."
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-k3s.adoc:1 asciidoc/guides/metallb-k3s.adoc:13
#, no-wrap
msgid "MetalLB on K3s (using L2)"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:3
msgid "In this guide, we demonstrate how to deploy MetalLB in layer 2 mode."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:5
msgid ""
"MetalLB is a compelling choice for load balancing in bare-metal Kubernetes "
"clusters for several reasons:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:6
msgid ""
"Native Integration with Kubernetes: MetalLB seamlessly integrates with "
"Kubernetes, making it easy to deploy and manage using familiar Kubernetes "
"tools and practices."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:7
msgid ""
"Bare-Metal Compatibility: Unlike cloud-based load balancers, MetalLB is "
"designed specifically for on-premises deployments where traditional load "
"balancers might not be available or feasible."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:8
msgid ""
"Supports Multiple Protocols: MetalLB supports both Layer 2 and BGP (Border "
"Gateway Protocol) modes, providing flexibility for different network "
"architectures and requirements."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:9
msgid ""
"High Availability: By distributing load-balancing responsibilities across "
"multiple nodes, MetalLB ensures high availability and reliability for your "
"services."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:10
msgid ""
"Scalability: MetalLB can handle large-scale deployments, scaling alongside "
"your Kubernetes cluster to meet increasing demand."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:11
msgid ""
"In layer 2 mode, one node assumes the responsibility of advertising a "
"service to the local network. From the network’s perspective, it simply "
"looks like that machine has multiple IP addresses assigned to its network "
"interface."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:12
msgid ""
"The major advantage of the layer 2 mode is its universality: it works on any "
"Ethernet network, with no special hardware required, not even fancy routers."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:14
msgid ""
"In this quick start, L2 mode will be used, so it means we do not need any "
"special network gear but just a couple of free IPs in our network range, "
"ideally outside of the DHCP pool so they are not assigned."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:15
msgid ""
"In this example, our DHCP pool is `192.168.122.100-192.168.122.200` (yes, "
"three IPs, see <<traefik-and-metallb,Traefik and MetalLB>> for the reason of "
"the extra IP) for a `192.168.122.0/24` network, so anything outside this "
"range is OK (besides the gateway and other hosts that can be already "
"running!)"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:17
msgid "A K3s cluster where MetalLB is going to be deployed."
msgstr ""

#. type: delimited block =
#: asciidoc/guides/metallb-k3s.adoc:18
msgid ""
"K3S comes with its own service load balancer named Klipper. You "
"https://metallb.universe.tf/configuration/k3s/[need to disable it to run "
"MetalLB]. To disable Klipper, K3s needs to be installed using the "
"`--disable=servicelb` flag."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:20
msgid ""
"A couple of free IPs in our network range. In this case, "
"`192.168.122.10-192.168.122.12`"
msgstr ""

#. type: Title ===
#: asciidoc/guides/metallb-k3s.adoc:21
#, no-wrap
msgid "Deployment"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:22
msgid "MetalLB leverages Helm (and other methods as well), so:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:24
#, no-wrap
msgid ""
"while ! kubectl wait --for condition=ready -n metallb-system $(kubectl "
"get\\\n"
" pods -n metallb-system -l app.kubernetes.io/component=controller -o "
"name)\\\n"
" --timeout=10s; do\n"
" sleep 2\n"
"done\n"
msgstr ""

#. type: Title ===
#: asciidoc/guides/metallb-k3s.adoc:25
#, no-wrap
msgid "Configuration"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:26
msgid ""
"At this point, the installation is completed. Now it is time to "
"https://metallb.universe.tf/configuration/[configure] using our example "
"values:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:27
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: IPAddressPool\n"
"metadata:\n"
"  name: ip-pool\n"
"  namespace: metallb-system\n"
"spec:\n"
"  addresses:\n"
"  - 192.168.122.10/32\n"
"  - 192.168.122.11/32\n"
"  - 192.168.122.12/32\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:28
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: L2Advertisement\n"
"metadata:\n"
"  name: ip-pool-l2-adv\n"
"  namespace: metallb-system\n"
"spec:\n"
"  ipAddressPools:\n"
"  - ip-pool\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:29
msgid ""
"Now, it is ready to be used. You can customize many things for L2 mode, such "
"as:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:30
msgid ""
"https://metallb.universe.tf/usage/#ipv6-and-dual-stack-services[IPv6 And "
"Dual Stack Services]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:31
msgid ""
"https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#controlling-automatic-address-allocation[Control "
"automatic address allocation]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:32
msgid ""
"https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/#reduce-scope-of-address-allocation-to-specific-namespace-and-service[Reduce "
"the scope of address allocation to specific namespaces and services]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:33
msgid ""
"https://metallb.universe.tf/configuration/_advanced_l2_configuration/#limiting-the-set-of-nodes-where-the-service-can-be-announced-from[Limiting "
"the set of nodes where the service can be announced from]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:34
msgid ""
"https://metallb.universe.tf/configuration/_advanced_l2_configuration/#specify-network-interfaces-that-lb-ip-can-be-announced-from[Specify "
"network interfaces that LB IP can be announced from]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:35
msgid ""
"And a lot more for "
"https://metallb.universe.tf/configuration/_advanced_bgp_configuration/[BGP]."
msgstr ""

#. type: Title ===
#: asciidoc/guides/metallb-k3s.adoc:36
#, no-wrap
msgid "Traefik and MetalLB"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:37
msgid ""
"Traefik is deployed by default with K3s "
"(https://docs.k3s.io/networking#traefik-ingress-controller[it can be "
"disabled] with `--disable=traefik`) and it is by default exposed as "
"`LoadBalancer` (to be used with Klipper). However, as Klipper needs to be "
"disabled, Traefik service for ingress is still a `LoadBalancer` type. So at "
"the moment of deploying MetalLB, the first IP will be assigned automatically "
"to Traefik Ingress."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:38
#, no-wrap
msgid ""
"# Before deploying MetalLB\n"
"kubectl get svc -n kube-system traefik\n"
"NAME      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      "
"AGE\n"
"traefik   LoadBalancer   10.43.44.113   <pending>     "
"80:31093/TCP,443:32095/TCP   28s\n"
"# After deploying MetalLB\n"
"kubectl get svc -n kube-system traefik\n"
"NAME      TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)                      "
"AGE\n"
"traefik   LoadBalancer   10.43.44.113   192.168.122.10   "
"80:31093/TCP,443:32095/TCP   3m10s\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:39
msgid "This will be applied <<ingress-with-metallb,later>> in the process."
msgstr ""

#. type: Title ===
#: asciidoc/guides/metallb-k3s.adoc:40
#, no-wrap
msgid "Usage"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:41
msgid "Let us create an example deployment:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:42
#, no-wrap
msgid ""
"cat <<- EOF | kubectl apply -f -\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:43
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Namespace\n"
"metadata:\n"
"  name: hello-kubernetes\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:44
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: ServiceAccount\n"
"metadata:\n"
"  name: hello-kubernetes\n"
"  namespace: hello-kubernetes\n"
"  labels:\n"
"    app.kubernetes.io/name: hello-kubernetes\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:45
#, no-wrap
msgid ""
"apiVersion: apps/v1\n"
"kind: Deployment\n"
"metadata:\n"
"  name: hello-kubernetes\n"
"  namespace: hello-kubernetes\n"
"  labels:\n"
"    app.kubernetes.io/name: hello-kubernetes\n"
"spec:\n"
"  replicas: 2\n"
"  selector:\n"
"    matchLabels:\n"
"      app.kubernetes.io/name: hello-kubernetes\n"
"  template:\n"
"    metadata:\n"
"      labels:\n"
"        app.kubernetes.io/name: hello-kubernetes\n"
"    spec:\n"
"      serviceAccountName: hello-kubernetes\n"
"      containers:\n"
"        - name: hello-kubernetes\n"
"          image: \"paulbouwer/hello-kubernetes:1.10\"\n"
"          imagePullPolicy: IfNotPresent\n"
"          ports:\n"
"            - name: http\n"
"              containerPort: 8080\n"
"              protocol: TCP\n"
"          livenessProbe:\n"
"            httpGet:\n"
"              path: /\n"
"              port: http\n"
"          readinessProbe:\n"
"            httpGet:\n"
"              path: /\n"
"              port: http\n"
"          env:\n"
"          - name: HANDLER_PATH_PREFIX\n"
"            value: \"\"\n"
"          - name: RENDER_PATH_PREFIX\n"
"            value: \"\"\n"
"          - name: KUBERNETES_NAMESPACE\n"
"            valueFrom:\n"
"              fieldRef:\n"
"                fieldPath: metadata.namespace\n"
"          - name: KUBERNETES_POD_NAME\n"
"            valueFrom:\n"
"              fieldRef:\n"
"                fieldPath: metadata.name\n"
"          - name: KUBERNETES_NODE_NAME\n"
"            valueFrom:\n"
"              fieldRef:\n"
"                fieldPath: spec.nodeName\n"
"          - name: CONTAINER_IMAGE\n"
"            value: \"paulbouwer/hello-kubernetes:1.10\"\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:46
msgid "And finally, the service:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:47
#, no-wrap
msgid ""
"cat <<- EOF | kubectl apply -f -\n"
"apiVersion: v1\n"
"kind: Service\n"
"metadata:\n"
"  name: hello-kubernetes\n"
"  namespace: hello-kubernetes\n"
"  labels:\n"
"    app.kubernetes.io/name: hello-kubernetes\n"
"spec:\n"
"  type: LoadBalancer\n"
"  ports:\n"
"    - port: 80\n"
"      targetPort: http\n"
"      protocol: TCP\n"
"      name: http\n"
"  selector:\n"
"    app.kubernetes.io/name: hello-kubernetes\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:48
msgid "Let us see it in action:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:49
#, no-wrap
msgid ""
"kubectl get svc -n hello-kubernetes\n"
"NAME               TYPE           CLUSTER-IP     EXTERNAL-IP      PORT(S)        "
"AGE\n"
"hello-kubernetes   LoadBalancer   10.43.127.75   192.168.122.11   "
"80:31461/TCP   8s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:50
#, no-wrap
msgid ""
"curl http://192.168.122.11\n"
"<!DOCTYPE html>\n"
"<html>\n"
"<head>\n"
"    <title>Hello Kubernetes!</title>\n"
"    <link rel=\"stylesheet\" type=\"text/css\" href=\"/css/main.css\">\n"
"    <link rel=\"stylesheet\" "
"href=\"https://fonts.googleapis.com/css?family=Ubuntu:300\" >\n"
"</head>\n"
"<body>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:51
#, no-wrap
msgid ""
"  <div class=\"main\">\n"
"    <img src=\"/images/kubernetes.png\"/>\n"
"    <div class=\"content\">\n"
"      <div id=\"message\">\n"
"  Hello world!\n"
"</div>\n"
"<div id=\"info\">\n"
"  <table>\n"
"    <tr>\n"
"      <th>namespace:</th>\n"
"      <td>hello-kubernetes</td>\n"
"    </tr>\n"
"    <tr>\n"
"      <th>pod:</th>\n"
"      <td>hello-kubernetes-7c8575c848-2c6ps</td>\n"
"    </tr>\n"
"    <tr>\n"
"      <th>node:</th>\n"
"      <td>allinone (Linux 5.14.21-150400.24.46-default)</td>\n"
"    </tr>\n"
"  </table>\n"
"</div>\n"
"<div id=\"footer\">\n"
"  paulbouwer/hello-kubernetes:1.10 (linux/amd64)\n"
"</div>\n"
"    </div>\n"
"  </div>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:52 asciidoc/guides/metallb-k3s.adoc:59
#, no-wrap
msgid ""
"</body>\n"
"</html>\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-k3s.adoc:53
#, no-wrap
msgid "Ingress with MetalLB"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:54
msgid ""
"As Traefik is already serving as an ingress controller, we can expose any "
"HTTP/HTTPS traffic via an `Ingress` object such as:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:55
#, no-wrap
msgid ""
"IP=$(kubectl get svc -n kube-system traefik -o "
"jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n"
"cat <<- EOF | kubectl apply -f -\n"
"apiVersion: networking.k8s.io/v1\n"
"kind: Ingress\n"
"metadata:\n"
"  name: hello-kubernetes-ingress\n"
"  namespace: hello-kubernetes\n"
"spec:\n"
"  rules:\n"
"  - host: hellok3s.${IP}.sslip.io\n"
"    http:\n"
"      paths:\n"
"        - path: \"/\"\n"
"          pathType: Prefix\n"
"          backend:\n"
"            service:\n"
"              name: hello-kubernetes\n"
"              port:\n"
"                name: http\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:56
msgid "And then:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:57
#, no-wrap
msgid ""
"curl http://hellok3s.${IP}.sslip.io\n"
"<!DOCTYPE html>\n"
"<html>\n"
"<head>\n"
"    <title>Hello Kubernetes!</title>\n"
"    <link rel=\"stylesheet\" type=\"text/css\" href=\"/css/main.css\">\n"
"    <link rel=\"stylesheet\" "
"href=\"https://fonts.googleapis.com/css?family=Ubuntu:300\" >\n"
"</head>\n"
"<body>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:58
#, no-wrap
msgid ""
"  <div class=\"main\">\n"
"    <img src=\"/images/kubernetes.png\"/>\n"
"    <div class=\"content\">\n"
"      <div id=\"message\">\n"
"  Hello world!\n"
"</div>\n"
"<div id=\"info\">\n"
"  <table>\n"
"    <tr>\n"
"      <th>namespace:</th>\n"
"      <td>hello-kubernetes</td>\n"
"    </tr>\n"
"    <tr>\n"
"      <th>pod:</th>\n"
"      <td>hello-kubernetes-7c8575c848-fvqm2</td>\n"
"    </tr>\n"
"    <tr>\n"
"      <th>node:</th>\n"
"      <td>allinone (Linux 5.14.21-150400.24.46-default)</td>\n"
"    </tr>\n"
"  </table>\n"
"</div>\n"
"<div id=\"footer\">\n"
"  paulbouwer/hello-kubernetes:1.10 (linux/amd64)\n"
"</div>\n"
"    </div>\n"
"  </div>\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:60
msgid "Also, to verify that MetalLB works correctly, `arping` can be used as:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:61
msgid "`+arping hellok3s.${IP}.sslip.io+`"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:62
msgid "Expected result:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-k3s.adoc:63
#, no-wrap
msgid ""
"ARPING 192.168.64.210\n"
"60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=0 time=1.169 msec\n"
"60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=1 time=2.992 msec\n"
"60 bytes from 92:12:36:00:d3:58 (192.168.64.210): index=2 time=2.884 msec\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:64
msgid "In the example above, the traffic flows as follows:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:65
msgid "`+hellok3s.${IP}.sslip.io+` is resolved to the actual IP."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:66
msgid "Then the traffic is handled by the `metallb-speaker` pod."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:67
msgid "`metallb-speaker` redirects the traffic to the `traefik` controller."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-k3s.adoc:68
msgid "Finally, Traefik forwards the request to the `hello-kubernetes` service."
msgstr ""

#. type: Title =
#: asciidoc/guides/metallb-kube-api.adoc:1
#, no-wrap
msgid "MetalLB in front of the Kubernetes API server"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:2
msgid ""
"This guide demonstrates using a MetalLB service to expose the K3s API "
"externally on an HA K3s cluster with three control-plane nodes.  To achieve "
"this, a Kubernetes Service of type `LoadBalancer` and Endpoints will be "
"manually created. The Endpoints keep the IPs of all control plane nodes "
"available in the cluster.  For the Endpoint to be continuously synchronized "
"with the events occurring in the cluster (adding/removing a node or a node "
"goes offline), the "
"https://github.com/suse-edge/endpoint-copier-operator[Endpoint Copier "
"Operator] will be deployed. The operator monitors the events happening in "
"the default `kubernetes` Endpoint and updates the managed one automatically "
"to keep them in sync.  Since the managed Service is of type `LoadBalancer`, "
"`MetalLB` assigns it a static `ExternalIP`. This `ExternalIP` will be used "
"to communicate with the API Server."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:4
msgid "Three hosts to deploy K3s on top."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:5
msgid "Ensure the hosts have different host names."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:6
msgid "For testing, these could be virtual machines"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:7
msgid ""
"At least 2 available IPs in the network (one for the Traefik and one for the "
"managed service)."
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-kube-api.adoc:9
#, no-wrap
msgid "Installing K3s"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/metallb-kube-api.adoc:10
msgid ""
"If you do not want a fresh cluster but want to use an existing one, skip "
"this step and proceed to the next one."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:11
msgid ""
"First, a free IP in the network must be reserved that will be used later for "
"`ExternalIP` of the managed Service."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:12
msgid "SSH to the first host and install `K3s` in cluster mode as:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:13
#, no-wrap
msgid ""
"# Export the free IP mentioned above\n"
"export VIP_SERVICE_IP=<ip>\n"
"export INSTALL_K3S_SKIP_START=false\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:14
#, no-wrap
msgid ""
"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server --cluster-init \\\n"
" --disable=servicelb --write-kubeconfig-mode=644 --tls-san=${VIP_SERVICE_IP} "
"\\\n"
" --tls-san=https://${VIP_SERVICE_IP}.sslip.io\" K3S_TOKEN=foobar sh -\n"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/metallb-kube-api.adoc:15
msgid ""
"Make sure that `--disable=servicelb` flag is provided in the `k3s server` "
"command."
msgstr ""

#. type: delimited block =
#: asciidoc/guides/metallb-kube-api.adoc:16
msgid "From now on, the commands should be run on the local machine."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:17
msgid "To access the API server from outside, the IP of the K3s VM will be used."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:18
#, no-wrap
msgid ""
"# Replace <node-ip> with the actual IP of the machine\n"
"export NODE_IP=<node-ip>\n"
"scp ${NODE_IP}:/etc/rancher/k3s/k3s.yaml ~/.kube/config && sed \\\n"
" -i '' \"s/127.0.0.1/${NODE_IP}/g\" ~/.kube/config && chmod 600 "
"~/.kube/config\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-kube-api.adoc:19
#, no-wrap
msgid "Configuring an existing K3s cluster"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/metallb-kube-api.adoc:20
msgid "This step is valid only if you intend to use an existing K3s cluster."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:21
msgid ""
"To use an existing K3s cluster, the `servicelb` LB should be disabled and "
"also `tls-san` flags modified."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:22
msgid ""
"To change the K3s flags, `/etc/systemd/system/k3s.service` should be "
"modified on all the VMs in the cluster."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:23
msgid "The flags should be inserted in the `ExecStart`. For example:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:24
#, no-wrap
msgid ""
"# Replace the <vip-service-ip> with the actual ip\n"
"ExecStart=/usr/local/bin/k3s \\\n"
"    server \\\n"
"        '--cluster-init' \\\n"
"        '--write-kubeconfig-mode=644' \\\n"
"        '--disable=servicelb' \\\n"
"        '--tls-san=<vip-service-ip>' \\\n"
"        '--tls-san=https://<vip-service-ip>.sslip.io' \\\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:25
msgid ""
"Then the following commands should be executed for K3s to load the new "
"configurations:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:26
#, no-wrap
msgid ""
"systemctl daemon-reload\n"
"systemctl restart k3s\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-kube-api.adoc:27
#, no-wrap
msgid "Installing MetalLB"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:28
msgid ""
"To deploy `MetalLB`, the "
"https://suse-edge.github.io/docs/quickstart/metallb[MetalLB on K3s] guide "
"can be used."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:29
msgid ""
"*NOTE:* Ensure that the IP addresses of the `ip-pool` IPAddressPool do not "
"overlap with the IP addresses previously selected for the `LoadBalancer` "
"service."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:30
msgid ""
"Create a separate `IpAddressPool` that will be used only for the managed "
"Service."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:31
#, no-wrap
msgid ""
"# Export the VIP_SERVICE_IP on the local machine\n"
"# Replace with the actual IP\n"
"export VIP_SERVICE_IP=<ip>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:32
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: IPAddressPool\n"
"metadata:\n"
"  name: kubernetes-vip-ip-pool\n"
"  namespace: metallb-system\n"
"spec:\n"
"  addresses:\n"
"  - ${VIP_SERVICE_IP}/32\n"
"  serviceAllocation:\n"
"    priority: 100\n"
"    namespaces:\n"
"      - default\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:33
#, no-wrap
msgid ""
"cat <<-EOF | kubectl apply -f -\n"
"apiVersion: metallb.io/v1beta1\n"
"kind: L2Advertisement\n"
"metadata:\n"
"  name: ip-pool-l2-adv\n"
"  namespace: metallb-system\n"
"spec:\n"
"  ipAddressPools:\n"
"  - ip-pool\n"
"  - kubernetes-vip-ip-pool\n"
"EOF\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-kube-api.adoc:34
#, no-wrap
msgid "Installing the Endpoint Copier Operator"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:35
#, no-wrap
msgid ""
"helm install \\\n"
"endpoint-copier-operator "
"oci://registry.suse.com/edge/endpoint-copier-operator-chart \\\n"
"--namespace endpoint-copier-operator \\\n"
"--create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:36
msgid "The command above will deploy three different resources in the cluster:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:37
msgid ""
"The `endpoint-copier-operator` operator Deployment with two replicas. One "
"will be the leader and the other will take over the leader role if needed."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:38
msgid ""
"A Kubernetes service called `kubernetes-vip` in the `default` namespace that "
"will be a copy of the `kubernetes` Service but from type `LoadBalancer`."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:39
msgid ""
"An Endpoint resource called `kubernetes-vip` in the `default` namespace that "
"will be a copy of the `kubernetes` Endpoint."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:40
msgid "Verify that the `kubernetes-vip` Service has the correct IP address:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:41
#, no-wrap
msgid ""
"kubectl get service kubernetes-vip -n default \\\n"
" -o=jsonpath='{.status.loadBalancer.ingress[0].ip}'\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:42
msgid ""
"Ensure that the `kubernetes-vip` and `kubernetes` Endpoints resources in the "
"`default` namespace point to the same IPs."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:43
#, no-wrap
msgid "kubectl get endpoints kubernetes kubernetes-vip\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:44
msgid ""
"If everything is correct, the last thing left is to use the `VIP_SERVICE_IP` "
"in our `Kubeconfig`."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:45
#, no-wrap
msgid "sed -i '' \"s/${NODE_IP}/${VIP_SERVICE_IP}/g\" ~/.kube/config\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:46
msgid "From now on, all the `kubectl` will go through the `kubernetes-vip` service."
msgstr ""

#. type: Title ==
#: asciidoc/guides/metallb-kube-api.adoc:47
#, no-wrap
msgid "Adding control-plane nodes"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:48
msgid "To monitor the entire process, two more terminal tabs can be opened."
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:49
msgid "First terminal:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:50
#, no-wrap
msgid "watch kubectl get nodes\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:51
msgid "Second terminal:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:52
#, no-wrap
msgid "watch kubectl get endpoints\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/metallb-kube-api.adoc:53
msgid "Now execute the commands below on the second and third nodes."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:54
#, no-wrap
msgid ""
"# Export the VIP_SERVICE_IP in the VM\n"
"# Replace with the actual IP\n"
"export VIP_SERVICE_IP=<ip>\n"
"export INSTALL_K3S_SKIP_START=false\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/metallb-kube-api.adoc:55
#, no-wrap
msgid ""
"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server \\\n"
" --server https://${VIP_SERVICE_IP}:6443 --disable=servicelb \\\n"
" --write-kubeconfig-mode=644\" K3S_TOKEN=foobar sh -\n"
msgstr ""

#. type: Title =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:1
#, no-wrap
msgid "Air-gapped deployments with Edge Image Builder"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:2
#: asciidoc/integrations/nvidia-slemicro.adoc:2
#, no-wrap
msgid "Intro"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:3
msgid ""
"This guide will show how to deploy several of the SUSE Edge components "
"completely air-gapped on SLE Micro 5.5 utilizing <<components-eib,Edge Image "
"Builder(EIB)>>. With this, you'll be able to boot into a customized, ready "
"to boot (CRB) image created by EIB and have the specified components "
"deployed on either a RKE2 or K3s cluster without an Internet connection or "
"any manual steps. This configuration is highly desirable for customers that "
"want to pre-bake all artifacts required for deployment into their OS image, "
"so they are immediately available on boot."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:4
msgid "We will cover an air-gapped installation of:"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:5
msgid "<<components-rancher>>"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:6
msgid "<<components-neuvector>>"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:7
msgid "<<components-longhorn>>"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:8
msgid "<<components-kubevirt>>"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:9
msgid ""
"EIB will parse and pre-download all images referenced in the provided Helm "
"charts and Kubernetes manifests. However, some of those may be attempting to "
"pull container images and create Kubernetes resources based on those at "
"runtime. In these cases we have to manually specify the necessary images in "
"the definition file if we want to set up a completely air-gapped "
"environment."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:11
msgid ""
"If you're following this guide, it's assumed that you are already familiar "
"with <<components-eib,EIB>>. If not, please follow the "
"<<quickstart-eib,quick start guide>> to better understand the concepts shown "
"in practice below."
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:12
#, no-wrap
msgid "Libvirt Network Configuration"
msgstr ""

#. type: Title =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:13
#, no-wrap
msgid "[NOTE] "
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:14
msgid ""
"To demo the air-gapped deployment, this guide will be done using a simulated "
"air-gapped `libvirt` network and the following configuration will be "
"tailored to that. For your own deployments, you may have to modify the "
"`host1.local.yaml` configuration that will be introduced in the next step."
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:15
msgid ""
"If you would like to use the same `libvirt` network configuration, follow "
"along. If not, skip to <<config-dir-creation>>."
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:16
msgid ""
"Let's create an isolated network configuration with an IP address range "
"`192.168.100.2/24` for DHCP:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:17
#, no-wrap
msgid ""
"cat << EOF > isolatednetwork.xml\n"
"<network>\n"
"  <name>isolatednetwork</name>\n"
"  <bridge name='virbr1' stp='on' delay='0'/>\n"
"  <ip address='192.168.100.1' netmask='255.255.255.0'>\n"
"    <dhcp>\n"
"      <range start='192.168.100.2' end='192.168.100.254'/>\n"
"    </dhcp>\n"
"  </ip>\n"
"</network>\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:18
msgid "Now, the only thing left is to create the network and start it:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:19
#, no-wrap
msgid ""
"virsh net-define isolatednetwork.xml\n"
"virsh net-start isolatednetwork\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:20
#, no-wrap
msgid "Base Directory Configuration [[config-dir-creation]]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:21
msgid ""
"The base directory configuration is the same across all different "
"components, so we will set it up here."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:22
msgid "We will first create the necessary subdirectories:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:23
#, no-wrap
msgid ""
"export CONFIG_DIR=$HOME/config\n"
"mkdir -p $CONFIG_DIR/base-images\n"
"mkdir -p $CONFIG_DIR/network\n"
"mkdir -p $CONFIG_DIR/kubernetes/helm/values\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:24
msgid ""
"Make sure to add whichever base image you plan to use into the `base-images` "
"directory. This guide will focus on the Self Install ISO found "
"https://www.suse.com/download/sle-micro/[here]."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:25
msgid "Let's copy the downloaded image:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:26
#, no-wrap
msgid ""
"cp SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso "
"$CONFIG_DIR/base-images/slemicro.iso\n"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:27
msgid "EIB is never going to modify the base image input."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:28
msgid "Let's create a file containing the desired network configuration:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:29
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/network/host1.local.yaml\n"
"routes:\n"
"  config:\n"
"  - destination: 0.0.0.0/0\n"
"    metric: 100\n"
"    next-hop-address: 192.168.100.1\n"
"    next-hop-interface: eth0\n"
"    table-id: 254\n"
"  - destination: 192.168.100.0/24\n"
"    metric: 100\n"
"    next-hop-address:\n"
"    next-hop-interface: eth0\n"
"    table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"    - 192.168.100.1\n"
"    - 8.8.8.8\n"
"interfaces:\n"
"- name: eth0\n"
"  type: ethernet\n"
"  state: up\n"
"  mac-address: 34:8A:B1:4B:16:E7\n"
"  ipv4:\n"
"    address:\n"
"    - ip: 192.168.100.50\n"
"      prefix-length: 24\n"
"    dhcp: false\n"
"    enabled: true\n"
"  ipv6:\n"
"    enabled: false\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:30
msgid ""
"This configuration ensures the following are present on the provisioned "
"systems (using the specified MAC address):"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:31
msgid "an Ethernet interface with a static IP address"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:32
msgid "routing"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:33
msgid "DNS"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:34
msgid "hostname (`host1.local`)"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:35
msgid "The resulting file structure should now look like:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:36
#, no-wrap
msgid ""
"├── kubernetes/\n"
"│   └── helm/\n"
"│       └── values/\n"
"├── base-images/\n"
"│   └── slemicro.iso\n"
"└── network/  \n"
"    └── host1.local.yaml\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:37
#, no-wrap
msgid "Base Definition File"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:38
msgid ""
"Edge Image Builder is using _definition files_ to modify the SLE Micro "
"images. These files contain the majority of configurable options.  Many of "
"these options will be repeated across the different component sections, so "
"we will list and explain those here."
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:39
msgid ""
"Full list of customization options in the definition file can be found in "
"the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md#image-definition-file[upstream "
"documentation]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:40
msgid ""
"We will take a look at the following fields which will be present in all "
"definition files:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:41
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"embeddedArtifactRegistry:\n"
"  images:\n"
"    - ...\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:42
msgid ""
"The `image` section is required, and it specifies the input image, its "
"architecture and type, as well as what the output image will be called."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:43
msgid ""
"The `operatingSystem` section is optional, and contains configuration to "
"enable login on the provisioned systems with the `root/eib` "
"username/password."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:44
msgid ""
"The `kubernetes` section is optional, and it defines the Kubernetes type and "
"version. We are going to use Kubernetes 1.28.9 and RKE2 by default.  Use "
"`kubernetes.version: v1.28.9+k3s1` if K3s is desired instead. Unless "
"explicitly configured via the `kubernetes.nodes` field, all clusters we "
"bootstrap in this guide will be single-node ones."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:45
msgid ""
"The `embeddedArtifactRegistry` section will include all images which are "
"only referenced and pulled at runtime for the specific component."
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:46
#, no-wrap
msgid "Rancher Installation [[rancher-install]]"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:47
msgid ""
"The <<components-rancher,Rancher>> deployment that will be demonstrated will "
"be highly slimmed down for demonstration purposes. For your actual "
"deployments, additional artifacts may be necessary depending on your "
"configuration."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:48
msgid ""
"The https://github.com/rancher/rancher/releases/tag/v2.8.4[Rancher v2.8.4] "
"release assets contain a `rancher-images.txt` file which lists all the "
"images required for an air-gapped installation."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:49
msgid ""
"There are about 602 container images in total which means that the resulting "
"CRB image would be roughly 28GB+. For our Rancher installation, we will "
"strip down that list to the smallest working configuration. From there, you "
"can add back any images you may need for your deployments."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:50
msgid "We will create the definition file and include the stripped down image list:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:51
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  network:\n"
"    apiVIP: 192.168.100.151\n"
"  manifests:\n"
"    urls:\n"
"    - "
"https://github.com/cert-manager/cert-manager/releases/download/v1.14.2/cert-manager.crds.yaml\n"
"  helm:\n"
"    charts:\n"
"      - name: rancher\n"
"        version: 2.8.4\n"
"        repositoryName: rancher-prime\n"
"        valuesFile: rancher-values.yaml\n"
"        targetNamespace: cattle-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: cert-manager\n"
"        installationNamespace: kube-system\n"
"        createNamespace: true\n"
"        repositoryName: jetstack\n"
"        targetNamespace: cert-manager\n"
"        version: 1.14.2\n"
"    repositories:\n"
"      - name: jetstack\n"
"        url: https://charts.jetstack.io\n"
"      - name: rancher-prime\n"
"        url:  https://charts.rancher.com/server-charts/prime\n"
"embeddedArtifactRegistry:\n"
"  images:\n"
"    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2\n"
"    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1\n"
"    - name: registry.rancher.com/rancher/cis-operator:v1.0.13\n"
"    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7\n"
"    - name: "
"registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1\n"
"    - name: "
"registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1\n"
"    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9\n"
"    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4\n"
"    - name: registry.rancher.com/rancher/fleet:v0.9.4\n"
"    - name: registry.rancher.com/rancher/gitjob:v0.9.7\n"
"    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5\n"
"    - name: "
"registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410\n"
"    - name: "
"registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423\n"
"    - name: "
"registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124\n"
"    - name: "
"registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325\n"
"    - name: "
"registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305\n"
"    - name: "
"registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125\n"
"    - name: "
"registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418\n"
"    - name: "
"registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423\n"
"    - name: "
"registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401\n"
"    - name: "
"registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416\n"
"    - name: "
"registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208\n"
"    - name: "
"registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926\n"
"    - name: "
"registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208\n"
"    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1\n"
"    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10\n"
"    - name: "
"registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0\n"
"    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1\n"
"    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228\n"
"    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7\n"
"    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1\n"
"    - name: registry.rancher.com/rancher/kubectl:v1.28.7\n"
"    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine\n"
"    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26\n"
"    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4\n"
"    - name: "
"registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1\n"
"    - name: registry.rancher.com/rancher/pause:3.6\n"
"    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0\n"
"    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1\n"
"    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2\n"
"    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2\n"
"    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4\n"
"    - name: "
"registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client\n"
"    - name: "
"registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy\n"
"    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4\n"
"    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1\n"
"    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5\n"
"    - name: registry.rancher.com/rancher/rancher:v2.8.4\n"
"    - name: registry.rancher.com/rancher/rke-tools:v0.1.96\n"
"    - name: "
"registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412\n"
"    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/security-scan:v0.2.15\n"
"    - name: registry.rancher.com/rancher/shell:v0.1.24\n"
"    - name: "
"registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1\n"
"    - name: "
"registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc\n"
"    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1\n"
"    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0\n"
"    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1\n"
"    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5\n"
"    - name: registry.rancher.com/rancher/kubectl:v1.20.2\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:52
msgid ""
"As compared to the full list of 602 container images, this slimmed down "
"version only contains 62 which makes the new CRB image only about 7GB."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:53
msgid "We also need to create a Helm values file for Rancher:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:54
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/kubernetes/helm/values/rancher-values.yaml\n"
"hostname: 192.168.100.50.sslip.io\n"
"replicas: 1\n"
"bootstrapPassword: \"adminadminadmin\"\n"
"systemDefaultRegistry: registry.rancher.com\n"
"useBundledSystemChart: true\n"
"EOF\n"
msgstr ""

#. type: delimited block =
#: asciidoc/guides/air-gapped-eib-deployments.adoc:55
msgid ""
"Setting the `systemDefaultRegistry` to `registry.rancher.com` allows Rancher "
"to automatically look for images in the embedded artifact registry started "
"within the CRB image at boot. Omitting this field may result in failure to "
"find the container images on the node."
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:57
#: asciidoc/guides/air-gapped-eib-deployments.adoc:78
#: asciidoc/guides/air-gapped-eib-deployments.adoc:94
#: asciidoc/guides/air-gapped-eib-deployments.adoc:109
#, no-wrap
msgid ""
"podman run --rm -it --privileged -v $CONFIG_DIR:/eib \\\n"
"registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
"build --definition-file eib-iso-definition.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:59
#, no-wrap
msgid ""
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Rpm .......................... [SKIPPED]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 17 MB/s)\n"
"Populating Embedded Artifact Registry... 100% (69/69, 26 it/min)\n"
"Embedded Artifact Registry ... [SUCCESS]\n"
"Keymap ....................... [SUCCESS]\n"
"Configuring Kubernetes component...\n"
"The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.\n"
"Downloading file: rke2_installer.sh\n"
"Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (780/780 MB, 115 "
"MB/s)\n"
"Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, "
"108 MB/s)\n"
"Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 117 MB/s)\n"
"Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 34 MB/s)\n"
"Downloading file: dl-manifest-1.yaml 100% (437/437 kB, 106 MB/s)\n"
"Kubernetes ................... [SUCCESS]\n"
"Certificates ................. [SKIPPED]\n"
"Building ISO image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:60
msgid ""
"Once a node using the built image is provisioned, we can verify the Rancher "
"installation:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:61
#, no-wrap
msgid ""
"/var/lib/rancher/rke2/bin/kubectl get all -A --kubeconfig "
"/etc/rancher/rke2/rke2.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:62
#: asciidoc/guides/air-gapped-eib-deployments.adoc:83
#: asciidoc/guides/air-gapped-eib-deployments.adoc:99
#: asciidoc/guides/air-gapped-eib-deployments.adoc:115
#: asciidoc/guides/air-gapped-eib-deployments.adoc:123
msgid ""
"The output should be similar to the following, showing that everything has "
"been successfully deployed:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:63
#, no-wrap
msgid ""
"NAMESPACE                         NAME                                                        "
"READY   STATUS      RESTARTS   AGE\n"
"cattle-fleet-local-system         pod/fleet-agent-68f4d5d5f7-tdlk7                            "
"1/1     Running     0          34s\n"
"cattle-fleet-system               pod/fleet-controller-85564cc978-pbtvk                       "
"1/1     Running     0          5m51s\n"
"cattle-fleet-system               pod/gitjob-9dc58fb5b-7cwsw                                  "
"1/1     Running     0          5m51s\n"
"cattle-provisioning-capi-system   "
"pod/capi-controller-manager-5c57b4b8f7-wlp5k                1/1     Running     "
"0          4m52s\n"
"cattle-system                     pod/helm-operation-4fk5c                                    "
"0/2     Completed   0          37s\n"
"cattle-system                     pod/helm-operation-6zgbq                                    "
"0/2     Completed   0          4m54s\n"
"cattle-system                     pod/helm-operation-cjds5                                    "
"0/2     Completed   0          5m37s\n"
"cattle-system                     pod/helm-operation-kt5c2                                    "
"0/2     Completed   0          5m21s\n"
"cattle-system                     pod/helm-operation-ppgtw                                    "
"0/2     Completed   0          5m30s\n"
"cattle-system                     pod/helm-operation-tvcwk                                    "
"0/2     Completed   0          5m54s\n"
"cattle-system                     pod/helm-operation-wpxd4                                    "
"0/2     Completed   0          53s\n"
"cattle-system                     pod/rancher-58575f9575-svrg2                                "
"1/1     Running     0          6m34s\n"
"cattle-system                     pod/rancher-webhook-5c6556f7ff-vgmkt                        "
"1/1     Running     0          5m19s\n"
"cert-manager                      pod/cert-manager-6c69f9f796-fkm8f                           "
"1/1     Running     0          7m14s\n"
"cert-manager                      "
"pod/cert-manager-cainjector-584f44558c-wg7p6                1/1     Running     "
"0          7m14s\n"
"cert-manager                      pod/cert-manager-webhook-76f9945d6f-lv2nv                   "
"1/1     Running     0          7m14s\n"
"endpoint-copier-operator          "
"pod/endpoint-copier-operator-58964b659b-l64dk               1/1     Running     "
"0          7m16s\n"
"endpoint-copier-operator          "
"pod/endpoint-copier-operator-58964b659b-z9t9d               1/1     Running     "
"0          7m16s\n"
"kube-system                       pod/cilium-fht55                                            "
"1/1     Running     0          7m32s\n"
"kube-system                       pod/cilium-operator-558bbf6cfd-gwfwf                        "
"1/1     Running     0          7m32s\n"
"kube-system                       pod/cilium-operator-558bbf6cfd-qsxb5                        "
"0/1     Pending     0          7m32s\n"
"kube-system                       pod/cloud-controller-manager-host1.local                    "
"1/1     Running     0          7m21s\n"
"kube-system                       pod/etcd-host1.local                                        "
"1/1     Running     0          7m8s\n"
"kube-system                       pod/helm-install-cert-manager-fvbtt                         "
"0/1     Completed   0          8m12s\n"
"kube-system                       "
"pod/helm-install-endpoint-copier-operator-5kkgw             0/1     "
"Completed   0          8m12s\n"
"kube-system                       pod/helm-install-metallb-zfphb                              "
"0/1     Completed   0          8m12s\n"
"kube-system                       pod/helm-install-rancher-nc4nt                              "
"0/1     Completed   2          8m12s\n"
"kube-system                       pod/helm-install-rke2-cilium-7wq87                          "
"0/1     Completed   0          8m12s\n"
"kube-system                       pod/helm-install-rke2-coredns-nl4gc                         "
"0/1     Completed   0          8m12s\n"
"kube-system                       pod/helm-install-rke2-ingress-nginx-svjqd                   "
"0/1     Completed   0          8m12s\n"
"kube-system                       pod/helm-install-rke2-metrics-server-gqgqz                  "
"0/1     Completed   0          8m12s\n"
"kube-system                       "
"pod/helm-install-rke2-snapshot-controller-crd-r6b5p         0/1     "
"Completed   0          8m12s\n"
"kube-system                       "
"pod/helm-install-rke2-snapshot-controller-ss9v4             0/1     "
"Completed   1          8m12s\n"
"kube-system                       "
"pod/helm-install-rke2-snapshot-validation-webhook-vlkpn     0/1     "
"Completed   0          8m12s\n"
"kube-system                       pod/kube-apiserver-host1.local                              "
"1/1     Running     0          7m29s\n"
"kube-system                       pod/kube-controller-manager-host1.local                     "
"1/1     Running     0          7m30s\n"
"kube-system                       pod/kube-proxy-host1.local                                  "
"1/1     Running     0          7m30s\n"
"kube-system                       pod/kube-scheduler-host1.local                              "
"1/1     Running     0          7m42s\n"
"kube-system                       "
"pod/rke2-coredns-rke2-coredns-6c8d9bb6d-qlwc8               1/1     Running     "
"0          7m31s\n"
"kube-system                       "
"pod/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf-j5r2z   1/1     Running     "
"0          7m31s\n"
"kube-system                       pod/rke2-ingress-nginx-controller-4h2mm                     "
"1/1     Running     0          7m3s\n"
"kube-system                       pod/rke2-metrics-server-544c8c66fc-lsrc6                    "
"1/1     Running     0          7m15s\n"
"kube-system                       "
"pod/rke2-snapshot-controller-59cc9cd8f4-4wx75               1/1     Running     "
"0          7m14s\n"
"kube-system                       "
"pod/rke2-snapshot-validation-webhook-54c5989b65-5kp2x       1/1     Running     "
"0          7m15s\n"
"metallb-system                    pod/metallb-controller-5895d8446d-z54lm                     "
"1/1     Running     0          7m15s\n"
"metallb-system                    pod/metallb-speaker-fxwgk                                   "
"1/1     Running     0          7m15s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:64
#, no-wrap
msgid ""
"NAMESPACE                         NAME                                              "
"TYPE           CLUSTER-IP      EXTERNAL-IP       PORT(S)\n"
"         AGE\n"
"cattle-fleet-system               service/gitjob                                    "
"ClusterIP      10.43.30.8      <none>            80/TCP\n"
"         5m51s\n"
"cattle-provisioning-capi-system   service/capi-webhook-service                      "
"ClusterIP      10.43.7.100     <none>            443/TCP\n"
"         4m52s\n"
"cattle-system                     service/rancher                                   "
"ClusterIP      10.43.100.229   <none>            80/TCP,443/TCP\n"
"         6m34s\n"
"cattle-system                     service/rancher-webhook                           "
"ClusterIP      10.43.121.133   <none>            443/TCP\n"
"         5m19s\n"
"cert-manager                      service/cert-manager                              "
"ClusterIP      10.43.140.65    <none>            9402/TCP\n"
"         7m14s\n"
"cert-manager                      service/cert-manager-webhook                      "
"ClusterIP      10.43.108.158   <none>            443/TCP\n"
"         7m14s\n"
"default                           service/kubernetes                                "
"ClusterIP      10.43.0.1       <none>            443/TCP\n"
"         8m26s\n"
"default                           service/kubernetes-vip                            "
"LoadBalancer   10.43.138.138   192.168.100.151   "
"9345:31006/TCP,6443:31599/TCP   8m21s\n"
"kube-system                       service/cilium-agent                              "
"ClusterIP      None            <none>            9964/TCP\n"
"         7m32s\n"
"kube-system                       service/rke2-coredns-rke2-coredns                 "
"ClusterIP      10.43.0.10      <none>            53/UDP,53/TCP\n"
"         7m31s\n"
"kube-system                       "
"service/rke2-ingress-nginx-controller-admission   ClusterIP      "
"10.43.157.19    <none>            443/TCP\n"
"         7m3s\n"
"kube-system                       service/rke2-metrics-server                       "
"ClusterIP      10.43.4.123     <none>            443/TCP\n"
"         7m15s\n"
"kube-system                       service/rke2-snapshot-validation-webhook          "
"ClusterIP      10.43.91.161    <none>            443/TCP\n"
"         7m16s\n"
"metallb-system                    service/metallb-webhook-service                   "
"ClusterIP      10.43.71.192    <none>            443/TCP\n"
"         7m15s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:65
#, no-wrap
msgid ""
"NAMESPACE        NAME                                           DESIRED   "
"CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE\n"
"kube-system      daemonset.apps/cilium                          1         1         "
"1       1            1           kubernetes.io/os=linux   7m32s\n"
"kube-system      daemonset.apps/rke2-ingress-nginx-controller   1         1         "
"1       1            1           kubernetes.io/os=linux   7m3s\n"
"metallb-system   daemonset.apps/metallb-speaker                 1         1         "
"1       1            1           kubernetes.io/os=linux   7m15s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:66
#, no-wrap
msgid ""
"NAMESPACE                         NAME                                                   "
"READY   UP-TO-DATE   AVAILABLE   AGE\n"
"cattle-fleet-local-system         deployment.apps/fleet-agent                            "
"1/1     1            1           34s\n"
"cattle-fleet-system               deployment.apps/fleet-controller                       "
"1/1     1            1           5m51s\n"
"cattle-fleet-system               deployment.apps/gitjob                                 "
"1/1     1            1           5m51s\n"
"cattle-provisioning-capi-system   deployment.apps/capi-controller-manager                "
"1/1     1            1           4m52s\n"
"cattle-system                     deployment.apps/rancher                                "
"1/1     1            1           6m34s\n"
"cattle-system                     deployment.apps/rancher-webhook                        "
"1/1     1            1           5m19s\n"
"cert-manager                      deployment.apps/cert-manager                           "
"1/1     1            1           7m14s\n"
"cert-manager                      deployment.apps/cert-manager-cainjector                "
"1/1     1            1           7m14s\n"
"cert-manager                      deployment.apps/cert-manager-webhook                   "
"1/1     1            1           7m14s\n"
"endpoint-copier-operator          deployment.apps/endpoint-copier-operator               "
"2/2     2            2           7m16s\n"
"kube-system                       deployment.apps/cilium-operator                        "
"1/2     2            1           7m32s\n"
"kube-system                       deployment.apps/rke2-coredns-rke2-coredns              "
"1/1     1            1           7m31s\n"
"kube-system                       "
"deployment.apps/rke2-coredns-rke2-coredns-autoscaler   1/1     1            "
"1           7m31s\n"
"kube-system                       deployment.apps/rke2-metrics-server                    "
"1/1     1            1           7m15s\n"
"kube-system                       deployment.apps/rke2-snapshot-controller               "
"1/1     1            1           7m14s\n"
"kube-system                       "
"deployment.apps/rke2-snapshot-validation-webhook       1/1     1            "
"1           7m15s\n"
"metallb-system                    deployment.apps/metallb-controller                     "
"1/1     1            1           7m15s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:67
#, no-wrap
msgid ""
"NAMESPACE                         NAME                                                              "
"DESIRED   CURRENT   READY   AGE\n"
"cattle-fleet-local-system         replicaset.apps/fleet-agent-68f4d5d5f7                            "
"1         1         1       34s\n"
"cattle-fleet-system               "
"replicaset.apps/fleet-controller-85564cc978                       1         "
"1         1       5m51s\n"
"cattle-fleet-system               replicaset.apps/gitjob-9dc58fb5b                                  "
"1         1         1       5m51s\n"
"cattle-provisioning-capi-system   "
"replicaset.apps/capi-controller-manager-5c57b4b8f7                1         "
"1         1       4m52s\n"
"cattle-system                     replicaset.apps/rancher-58575f9575                                "
"1         1         1       6m34s\n"
"cattle-system                     replicaset.apps/rancher-webhook-5c6556f7ff                        "
"1         1         1       5m19s\n"
"cert-manager                      replicaset.apps/cert-manager-6c69f9f796                           "
"1         1         1       7m14s\n"
"cert-manager                      "
"replicaset.apps/cert-manager-cainjector-584f44558c                1         "
"1         1       7m14s\n"
"cert-manager                      "
"replicaset.apps/cert-manager-webhook-76f9945d6f                   1         "
"1         1       7m14s\n"
"endpoint-copier-operator          "
"replicaset.apps/endpoint-copier-operator-58964b659b               2         "
"2         2       7m16s\n"
"kube-system                       replicaset.apps/cilium-operator-558bbf6cfd                        "
"2         2         1       7m32s\n"
"kube-system                       "
"replicaset.apps/rke2-coredns-rke2-coredns-6c8d9bb6d               1         "
"1         1       7m31s\n"
"kube-system                       "
"replicaset.apps/rke2-coredns-rke2-coredns-autoscaler-55fb4bbbcf   1         "
"1         1       7m31s\n"
"kube-system                       "
"replicaset.apps/rke2-metrics-server-544c8c66fc                    1         "
"1         1       7m15s\n"
"kube-system                       "
"replicaset.apps/rke2-snapshot-controller-59cc9cd8f4               1         "
"1         1       7m14s\n"
"kube-system                       "
"replicaset.apps/rke2-snapshot-validation-webhook-54c5989b65       1         "
"1         1       7m15s\n"
"metallb-system                    "
"replicaset.apps/metallb-controller-5895d8446d                     1         "
"1         1       7m15s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:68
#, no-wrap
msgid ""
"NAMESPACE     NAME                                                      "
"COMPLETIONS   DURATION   AGE\n"
"kube-system   job.batch/helm-install-cert-manager                       1/1           "
"85s        8m21s\n"
"kube-system   job.batch/helm-install-endpoint-copier-operator           1/1           "
"59s        8m21s\n"
"kube-system   job.batch/helm-install-metallb                            1/1           "
"60s        8m21s\n"
"kube-system   job.batch/helm-install-rancher                            1/1           "
"100s       8m21s\n"
"kube-system   job.batch/helm-install-rke2-cilium                        1/1           "
"44s        8m18s\n"
"kube-system   job.batch/helm-install-rke2-coredns                       1/1           "
"45s        8m18s\n"
"kube-system   job.batch/helm-install-rke2-ingress-nginx                 1/1           "
"76s        8m16s\n"
"kube-system   job.batch/helm-install-rke2-metrics-server                1/1           "
"60s        8m16s\n"
"kube-system   job.batch/helm-install-rke2-snapshot-controller           1/1           "
"61s        8m15s\n"
"kube-system   job.batch/helm-install-rke2-snapshot-controller-crd       1/1           "
"60s        8m16s\n"
"kube-system   job.batch/helm-install-rke2-snapshot-validation-webhook   1/1           "
"60s        8m14s\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:69
msgid ""
"And when we go to `https://192.168.100.50.sslip.io` and log in with the "
"`adminadminadmin` password that we set earlier, we are greeted with the "
"Rancher dashboard:"
msgstr ""

#. type: Target for macro image
#: asciidoc/guides/air-gapped-eib-deployments.adoc:70
#, no-wrap
msgid "air-gapped-rancher.png"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:71
#, no-wrap
msgid "NeuVector Installation [[neuvector-install]]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:72
msgid ""
"Unlike the Rancher installation, the NeuVector installation does not require "
"any special handling in EIB. EIB will automatically air-gap every image "
"required by NeuVector."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:73
msgid "We will create the definition file:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:74
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  helm:\n"
"    charts:\n"
"      - name: neuvector-crd\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector-values.yaml\n"
"      - name: neuvector\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector-values.yaml\n"
"    repositories:\n"
"      - name: rancher-charts\n"
"        url: https://charts.rancher.io/\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:75
msgid "We will also create a Helm values file for NeuVector:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:76
#, no-wrap
msgid ""
"cat << EOF > $CONFIG_DIR/kubernetes/helm/values/neuvector-values.yaml\n"
"controller:\n"
"  replicas: 1\n"
"manager:\n"
"  enabled: false\n"
"cve:\n"
"  scanner:\n"
"    enabled: false\n"
"    replicas: 1\n"
"k3s:\n"
"  enabled: true\n"
"crdwebhook:\n"
"  enabled: false\n"
"EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:80
#, no-wrap
msgid ""
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Rpm .......................... [SKIPPED]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Populating Embedded Artifact Registry... 100% (6/6, 20 it/min)         \n"
"Embedded Artifact Registry ... [SUCCESS]\n"
"Keymap ....................... [SUCCESS]\n"
"Configuring Kubernetes component...\n"
"The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.\n"
"Downloading file: rke2_installer.sh\n"
"Kubernetes ................... [SUCCESS]\n"
"Certificates ................. [SKIPPED]\n"
"Building ISO image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:81
msgid ""
"Once a node using the built image is provisioned, we can verify the "
"NeuVector installation:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:82
#, no-wrap
msgid ""
"/var/lib/rancher/rke2/bin/kubectl get all -n neuvector --kubeconfig "
"/etc/rancher/rke2/rke2.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:84
#, no-wrap
msgid ""
"NAME                                           READY   STATUS    RESTARTS   "
"AGE\n"
"pod/neuvector-controller-pod-bc74745cf-x9fsc   1/1     Running   0          "
"13m\n"
"pod/neuvector-enforcer-pod-vzw7t               1/1     Running   0          "
"13m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:85
#, no-wrap
msgid ""
"NAME                                      TYPE        CLUSTER-IP     "
"EXTERNAL-IP   PORT(S)                         AGE\n"
"service/neuvector-svc-admission-webhook   ClusterIP   10.43.240.25   <none>        "
"443/TCP                         13m\n"
"service/neuvector-svc-controller          ClusterIP   None           <none>        "
"18300/TCP,18301/TCP,18301/UDP   13m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:86
#, no-wrap
msgid ""
"NAME                                    DESIRED   CURRENT   READY   "
"UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n"
"daemonset.apps/neuvector-enforcer-pod   1         1         1       1            "
"1           <none>          13m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:87
#, no-wrap
msgid ""
"NAME                                       READY   UP-TO-DATE   AVAILABLE   "
"AGE\n"
"deployment.apps/neuvector-controller-pod   1/1     1            1           "
"13m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:88
#, no-wrap
msgid ""
"NAME                                                 DESIRED   CURRENT   "
"READY   AGE\n"
"replicaset.apps/neuvector-controller-pod-bc74745cf   1         1         1       "
"13m\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:89
#, no-wrap
msgid ""
"NAME                                  SCHEDULE    SUSPEND   ACTIVE   LAST "
"SCHEDULE   AGE\n"
"cronjob.batch/neuvector-updater-pod   0 0 * * *   False     0        <none>          "
"13m\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:90
#, no-wrap
msgid "Longhorn Installation [[longhorn-install]]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:91
msgid ""
"The https://longhorn.io/docs/1.6.1/deploy/install/airgap/[official "
"documentation] for Longhorn contains a `longhorn-images.txt` file which "
"lists all the images required for an air-gapped installation.  We will be "
"including them in our definition file. Let's create it:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:92
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  helm:\n"
"    charts:\n"
"      - name: longhorn\n"
"        repositoryName: longhorn\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        version: 1.6.1\n"
"    repositories:\n"
"      - name: longhorn\n"
"        url: https://charts.longhorn.io\n"
"embeddedArtifactRegistry:\n"
"  images:\n"
"    - name: longhornio/csi-attacher:v4.4.2\n"
"    - name: longhornio/csi-provisioner:v3.6.2\n"
"    - name: longhornio/csi-resizer:v1.9.2\n"
"    - name: longhornio/csi-snapshotter:v6.3.2\n"
"    - name: longhornio/csi-node-driver-registrar:v2.9.2\n"
"    - name: longhornio/livenessprobe:v2.12.0\n"
"    - name: longhornio/backing-image-manager:v1.6.1\n"
"    - name: longhornio/longhorn-engine:v1.6.1\n"
"    - name: longhornio/longhorn-instance-manager:v1.6.1\n"
"    - name: longhornio/longhorn-manager:v1.6.1\n"
"    - name: longhornio/longhorn-share-manager:v1.6.1\n"
"    - name: longhornio/longhorn-ui:v1.6.1\n"
"    - name: longhornio/support-bundle-kit:v0.0.36\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:96
#, no-wrap
msgid ""
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Rpm .......................... [SKIPPED]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Populating Embedded Artifact Registry... 100% (13/13, 20 it/min)\n"
"Embedded Artifact Registry ... [SUCCESS]\n"
"Keymap ....................... [SUCCESS]\n"
"Configuring Kubernetes component...\n"
"The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.\n"
"Downloading file: rke2_installer.sh\n"
"Downloading file: rke2-images-core.linux-amd64.tar.zst 100% (782/782 MB, 108 "
"MB/s)\n"
"Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% (367/367 MB, "
"104 MB/s)\n"
"Downloading file: rke2.linux-amd64.tar.gz 100% (34/34 MB, 108 MB/s)\n"
"Downloading file: sha256sum-amd64.txt 100% (3.9/3.9 kB, 7.5 MB/s)\n"
"Kubernetes ................... [SUCCESS]\n"
"Certificates ................. [SKIPPED]\n"
"Building ISO image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:97
msgid ""
"Once a node using the built image is provisioned, we can verify the Longhorn "
"installation:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:98
#, no-wrap
msgid ""
"/var/lib/rancher/rke2/bin/kubectl get all -n longhorn-system --kubeconfig "
"/etc/rancher/rke2/rke2.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:100
#, no-wrap
msgid ""
"NAME                                                    READY   STATUS    "
"RESTARTS      AGE\n"
"pod/csi-attacher-5c4bfdcf59-9hgvv                       1/1     Running   0             "
"35s\n"
"pod/csi-attacher-5c4bfdcf59-dt6jl                       1/1     Running   0             "
"35s\n"
"pod/csi-attacher-5c4bfdcf59-swpwq                       1/1     Running   0             "
"35s\n"
"pod/csi-provisioner-667796df57-dfrzw                    1/1     Running   0             "
"35s\n"
"pod/csi-provisioner-667796df57-tvsrt                    1/1     Running   0             "
"35s\n"
"pod/csi-provisioner-667796df57-xszsx                    1/1     Running   0             "
"35s\n"
"pod/csi-resizer-694f8f5f64-6khlb                        1/1     Running   0             "
"35s\n"
"pod/csi-resizer-694f8f5f64-gnr45                        1/1     Running   0             "
"35s\n"
"pod/csi-resizer-694f8f5f64-sbl4k                        1/1     Running   0             "
"35s\n"
"pod/csi-snapshotter-959b69d4b-2k4v8                     1/1     Running   0             "
"35s\n"
"pod/csi-snapshotter-959b69d4b-9d8wl                     1/1     Running   0             "
"35s\n"
"pod/csi-snapshotter-959b69d4b-l2w95                     1/1     Running   0             "
"35s\n"
"pod/engine-image-ei-5cefaf2b-cwd8f                      1/1     Running   0             "
"43s\n"
"pod/instance-manager-f0d17f96bc92f3cc44787a2a347f6a98   1/1     Running   0             "
"43s\n"
"pod/longhorn-csi-plugin-szv7t                           3/3     Running   0             "
"35s\n"
"pod/longhorn-driver-deployer-9f4fc86-q8fz2              1/1     Running   0             "
"83s\n"
"pod/longhorn-manager-zp66l                              1/1     Running   0             "
"83s\n"
"pod/longhorn-ui-5f4b7bbf69-k645d                        1/1     Running   3 "
"(65s ago)   83s\n"
"pod/longhorn-ui-5f4b7bbf69-t7xt4                        1/1     Running   3 "
"(62s ago)   83s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:101
#, no-wrap
msgid ""
"NAME                                  TYPE        CLUSTER-IP     EXTERNAL-IP   "
"PORT(S)    AGE\n"
"service/longhorn-admission-webhook    ClusterIP   10.43.74.59    <none>        "
"9502/TCP   83s\n"
"service/longhorn-backend              ClusterIP   10.43.45.206   <none>        "
"9500/TCP   83s\n"
"service/longhorn-conversion-webhook   ClusterIP   10.43.83.108   <none>        "
"9501/TCP   83s\n"
"service/longhorn-engine-manager       ClusterIP   None           <none>        "
"<none>     83s\n"
"service/longhorn-frontend             ClusterIP   10.43.84.55    <none>        "
"80/TCP     83s\n"
"service/longhorn-recovery-backend     ClusterIP   10.43.75.200   <none>        "
"9503/TCP   83s\n"
"service/longhorn-replica-manager      ClusterIP   None           <none>        "
"<none>     83s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:102
#, no-wrap
msgid ""
"NAME                                      DESIRED   CURRENT   READY   "
"UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n"
"daemonset.apps/engine-image-ei-5cefaf2b   1         1         1       1            "
"1           <none>          43s\n"
"daemonset.apps/longhorn-csi-plugin        1         1         1       1            "
"1           <none>          35s\n"
"daemonset.apps/longhorn-manager           1         1         1       1            "
"1           <none>          83s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:103
#, no-wrap
msgid ""
"NAME                                       READY   UP-TO-DATE   AVAILABLE   "
"AGE\n"
"deployment.apps/csi-attacher               3/3     3            3           "
"35s\n"
"deployment.apps/csi-provisioner            3/3     3            3           "
"35s\n"
"deployment.apps/csi-resizer                3/3     3            3           "
"35s\n"
"deployment.apps/csi-snapshotter            3/3     3            3           "
"35s\n"
"deployment.apps/longhorn-driver-deployer   1/1     1            1           "
"83s\n"
"deployment.apps/longhorn-ui                2/2     2            2           "
"83s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:104
#, no-wrap
msgid ""
"NAME                                               DESIRED   CURRENT   READY   "
"AGE\n"
"replicaset.apps/csi-attacher-5c4bfdcf59            3         3         3       "
"35s\n"
"replicaset.apps/csi-provisioner-667796df57         3         3         3       "
"35s\n"
"replicaset.apps/csi-resizer-694f8f5f64             3         3         3       "
"35s\n"
"replicaset.apps/csi-snapshotter-959b69d4b          3         3         3       "
"35s\n"
"replicaset.apps/longhorn-driver-deployer-9f4fc86   1         1         1       "
"83s\n"
"replicaset.apps/longhorn-ui-5f4b7bbf69             2         2         2       "
"83s\n"
msgstr ""

#. type: Title ==
#: asciidoc/guides/air-gapped-eib-deployments.adoc:105
#, no-wrap
msgid "KubeVirt and CDI Installation [[kubevirt-install]]"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:106
msgid ""
"The Helm charts for both KubeVirt and CDI are only installing their "
"respective operators.  It is up to the operators to deploy the rest of the "
"systems which means we will have to include all necessary container images "
"in our definition file. Let's create it:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:107
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: slemicro.iso\n"
"  outputImageName: eib-image.iso\n"
"operatingSystem:\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$jHugJNNd3HElGsUZ$eodjVe4te5ps44SVcWshdfWizrP.xAyd71CVEXazBJ/.v799/WRCBXxfYmunlBO2yp1hm/zb4r8EmnrrNCF.P/\n"
"kubernetes:\n"
"  version: v1.28.9+rke2r1\n"
"  helm:\n"
"    charts:\n"
"      - name: kubevirt-chart\n"
"        repositoryName: suse-edge\n"
"        version: 0.2.4\n"
"        targetNamespace: kubevirt-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: cdi-chart\n"
"        repositoryName: suse-edge\n"
"        version: 0.2.3\n"
"        targetNamespace: cdi-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"    repositories:\n"
"      - name: suse-edge\n"
"        url: oci://registry.suse.com/edge\n"
"embeddedArtifactRegistry:\n"
"  images:\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0-150500.6.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0-150500.6.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0-150500.6.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0-150500.6.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0-150500.6.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0-150500.6.12.1\n"
"    - name: registry.suse.com/suse/sles/15.5/virt-api:1.1.1-150500.8.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/virt-controller:1.1.1-150500.8.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1-150500.8.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/virt-handler:1.1.1-150500.8.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1-150500.8.12.1\n"
"    - name: "
"registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1-150500.8.12.1\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:111
#, no-wrap
msgid ""
"Generating image customization components...\n"
"Identifier ................... [SUCCESS]\n"
"Custom Files ................. [SKIPPED]\n"
"Time ......................... [SKIPPED]\n"
"Network ...................... [SUCCESS]\n"
"Groups ....................... [SKIPPED]\n"
"Users ........................ [SUCCESS]\n"
"Proxy ........................ [SKIPPED]\n"
"Rpm .......................... [SKIPPED]\n"
"Systemd ...................... [SKIPPED]\n"
"Elemental .................... [SKIPPED]\n"
"Suma ......................... [SKIPPED]\n"
"Populating Embedded Artifact Registry... 100% (13/13, 6 it/min)\n"
"Embedded Artifact Registry ... [SUCCESS]\n"
"Keymap ....................... [SUCCESS]\n"
"Configuring Kubernetes component...\n"
"The Kubernetes CNI is not explicitly set, defaulting to 'cilium'.\n"
"Downloading file: rke2_installer.sh\n"
"Kubernetes ................... [SUCCESS]\n"
"Certificates ................. [SKIPPED]\n"
"Building ISO image...\n"
"Kernel Params ................ [SKIPPED]\n"
"Image build complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:112
msgid ""
"Once a node using the built image is provisioned, we can verify the "
"installation of both KubeVirt and CDI."
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:113
msgid "Verify KubeVirt:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:114
#, no-wrap
msgid ""
"/var/lib/rancher/rke2/bin/kubectl get all -n kubevirt-system --kubeconfig "
"/etc/rancher/rke2/rke2.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:116
#, no-wrap
msgid ""
"NAME                                   READY   STATUS    RESTARTS   AGE\n"
"pod/virt-api-7c45477984-z226r          1/1     Running   0          2m4s\n"
"pod/virt-controller-664d9986b5-8p8gm   1/1     Running   0          98s\n"
"pod/virt-controller-664d9986b5-v2n4h   1/1     Running   0          98s\n"
"pod/virt-handler-2fx8c                 1/1     Running   0          98s\n"
"pod/virt-operator-5cf69867dc-hz5s8     1/1     Running   0          2m30s\n"
"pod/virt-operator-5cf69867dc-kp266     1/1     Running   0          2m30s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:117
#, no-wrap
msgid ""
"NAME                                  TYPE        CLUSTER-IP      "
"EXTERNAL-IP   PORT(S)   AGE\n"
"service/kubevirt-operator-webhook     ClusterIP   10.43.210.235   <none>        "
"443/TCP   2m7s\n"
"service/kubevirt-prometheus-metrics   ClusterIP   None            <none>        "
"443/TCP   2m7s\n"
"service/virt-api                      ClusterIP   10.43.226.140   <none>        "
"443/TCP   2m7s\n"
"service/virt-exportproxy              ClusterIP   10.43.213.201   <none>        "
"443/TCP   2m7s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:118
#, no-wrap
msgid ""
"NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   "
"AVAILABLE   NODE SELECTOR            AGE\n"
"daemonset.apps/virt-handler   1         1         1       1            1           "
"kubernetes.io/os=linux   98s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:119
#, no-wrap
msgid ""
"NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n"
"deployment.apps/virt-api          1/1     1            1           2m4s\n"
"deployment.apps/virt-controller   2/2     2            2           98s\n"
"deployment.apps/virt-operator     2/2     2            2           2m30s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:120
#, no-wrap
msgid ""
"NAME                                         DESIRED   CURRENT   READY   "
"AGE\n"
"replicaset.apps/virt-api-7c45477984          1         1         1       "
"2m4s\n"
"replicaset.apps/virt-controller-664d9986b5   2         2         2       "
"98s\n"
"replicaset.apps/virt-operator-5cf69867dc     2         2         2       "
"2m30s\n"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:121
msgid "Verify CDI:"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:122
#, no-wrap
msgid ""
"/var/lib/rancher/rke2/bin/kubectl get all -n cdi-system --kubeconfig "
"/etc/rancher/rke2/rke2.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:124
#, no-wrap
msgid ""
"NAME                                   READY   STATUS    RESTARTS   AGE\n"
"pod/cdi-apiserver-db465b888-mdsmm      1/1     Running   0          3m6s\n"
"pod/cdi-deployment-56c7d74995-vt9sw    1/1     Running   0          3m6s\n"
"pod/cdi-operator-55c74f4b86-gkt58      1/1     Running   0          3m10s\n"
"pod/cdi-uploadproxy-7d7b94b968-msg2h   1/1     Running   0          3m6s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:125
#, no-wrap
msgid ""
"NAME                             TYPE        CLUSTER-IP      EXTERNAL-IP   "
"PORT(S)    AGE\n"
"service/cdi-api                  ClusterIP   10.43.161.135   <none>        "
"443/TCP    3m6s\n"
"service/cdi-prometheus-metrics   ClusterIP   10.43.161.159   <none>        "
"8080/TCP   3m6s\n"
"service/cdi-uploadproxy          ClusterIP   10.43.25.136    <none>        "
"443/TCP    3m6s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:126
#, no-wrap
msgid ""
"NAME                              READY   UP-TO-DATE   AVAILABLE   AGE\n"
"deployment.apps/cdi-apiserver     1/1     1            1           3m6s\n"
"deployment.apps/cdi-deployment    1/1     1            1           3m6s\n"
"deployment.apps/cdi-operator      1/1     1            1           3m10s\n"
"deployment.apps/cdi-uploadproxy   1/1     1            1           3m6s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/guides/air-gapped-eib-deployments.adoc:127
#, no-wrap
msgid ""
"NAME                                         DESIRED   CURRENT   READY   "
"AGE\n"
"replicaset.apps/cdi-apiserver-db465b888      1         1         1       "
"3m6s\n"
"replicaset.apps/cdi-deployment-56c7d74995    1         1         1       "
"3m6s\n"
"replicaset.apps/cdi-operator-55c74f4b86      1         1         1       "
"3m10s\n"
"replicaset.apps/cdi-uploadproxy-7d7b94b968   1         1         1       "
"3m6s\n"
msgstr ""

#. type: Title ====
#: asciidoc/guides/air-gapped-eib-deployments.adoc:128
#: asciidoc/integrations/nats.adoc:58
#, no-wrap
msgid "Troubleshooting"
msgstr ""

#. type: Plain text
#: asciidoc/guides/air-gapped-eib-deployments.adoc:129
msgid ""
"If you run into any issues while building the images or are looking to "
"further test and debug the process, please refer to the "
"https://github.com/suse-edge/edge-image-builder/tree/release-1.0/docs[upstream "
"documentation]."
msgstr ""

#. type: Title =
#: asciidoc/integrations/nats.adoc:1
#, no-wrap
msgid "NATS"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:2
msgid ""
"https://nats.io/[NATS] is a connective technology built for the "
"ever-increasingly hyper-connected world. It is a single technology that "
"enables applications to securely communicate across any combination of cloud "
"vendors, on-premises, edge, Web and mobile devices. NATS consists of a "
"family of open-source products that are tightly integrated but can be "
"deployed easily and independently. NATS is being used globally by thousands "
"of companies, spanning use cases including microservices, edge computing, "
"mobile and IoT, and can be used to augment or replace traditional messaging."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nats.adoc:3
#, no-wrap
msgid "Architecture"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:4
msgid ""
"NATS is an infrastructure that allows data exchange between applications in "
"the form of messages."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:5
#, no-wrap
msgid "NATS client applications"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:6
msgid ""
"NATS client libraries can be used to allow the applications to publish, "
"subscribe, request and reply between different instances.  These "
"applications are generally referred to as `client applications`."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:7
#, no-wrap
msgid "NATS service infrastructure"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:8
msgid ""
"The NATS services are provided by one or more NATS server processes that are "
"configured to interconnect with each other and provide a NATS service "
"infrastructure. The NATS service infrastructure can scale from a single NATS "
"server process running on an end device to a public global super-cluster of "
"many clusters spanning all major cloud providers and all regions of the "
"world."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:9
#, no-wrap
msgid "Simple messaging design"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:10
msgid ""
"NATS makes it easy for applications to communicate by sending and receiving "
"messages. These messages are addressed and identified by subject strings and "
"do not depend on network location.  Data is encoded and framed as a message "
"and sent by a publisher. The message is received, decoded and processed by "
"one or more subscribers."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:11
#, no-wrap
msgid "NATS JetStream"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:12
msgid ""
"NATS has a built-in distributed persistence system called JetStream.  "
"JetStream was created to solve the problems identified with streaming in "
"technology today — complexity, fragility and a lack of "
"scalability. JetStream also solves the problem with the coupling of the "
"publisher and the subscriber (the subscribers need to be up and running to "
"receive the message when it is published).  More information about NATS "
"JetStream can be found https://docs.nats.io/nats-concepts/jetstream[here]."
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:14
#, no-wrap
msgid "Installing NATS on top of K3s"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:15
msgid ""
"NATS is built for multiple architectures so it can easily be installed on "
"<<components-k3s,K3s.>>"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:16
msgid "Let us create a values file to overwrite the default values of NATS."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:17
#, no-wrap
msgid ""
"cat > values.yaml <<EOF\n"
"cluster:\n"
"  # Enable the HA setup of the NATS\n"
"  enabled: true\n"
"  replicas: 3\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:18
#, no-wrap
msgid ""
"nats:\n"
"  jetstream:\n"
"    # Enable JetStream\n"
"    enabled: true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:19
#, no-wrap
msgid ""
"    memStorage:\n"
"      enabled: true\n"
"      size: 2Gi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:20
#, no-wrap
msgid ""
"    fileStorage:\n"
"      enabled: true\n"
"      size: 1Gi\n"
"      storageDirectory: /data/\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:21
msgid "Now let us install NATS via Helm:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:22
#, no-wrap
msgid ""
"helm repo add nats https://nats-io.github.io/k8s/helm/charts/\n"
"helm install nats nats/nats --namespace nats --values values.yaml \\\n"
" --create-namespace\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:23
msgid ""
"With the `values.yaml` file above, the following components will be in the "
"`nats` namespace:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:24
msgid ""
"HA version of NATS Statefulset containing three containers: NATS server + "
"Config reloader and Metrics sidecars."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:25
msgid ""
"NATS box container, which comes with a set of `NATS` utilities that can be "
"used to verify the setup."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:26
msgid ""
"JetStream also leverages its Key-Value back-end that comes with `PVCs` "
"bounded to the pods."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:27
#, no-wrap
msgid "Testing the setup"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:28
#, no-wrap
msgid "kubectl exec -n nats -it deployment/nats-box -- /bin/sh -l\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:29
msgid "Create a subscription for the test subject:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:30
#, no-wrap
msgid "nats sub test &\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:31
msgid "Send a message to the test subject:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:32
#, no-wrap
msgid "nats pub test hi\n"
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:33
#, no-wrap
msgid "Cleaning up"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:34
#, no-wrap
msgid ""
"helm -n nats uninstall nats\n"
"rm values.yaml\n"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nats.adoc:35
#, no-wrap
msgid "NATS as a back-end for K3s"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:36
msgid ""
"One component K3s leverages is https://github.com/k3s-io/kine[KINE], which "
"is a shim enabling the replacement of etcd with alternate storage back-ends "
"originally targeting relational databases.  As JetStream provides a Key "
"Value API, this makes it possible to have NATS as a back-end for the K3s "
"cluster."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:37
msgid ""
"There is an already merged PR which makes the built-in NATS in K3s "
"straightforward, but the change is still "
"https://github.com/k3s-io/k3s/issues/7410#issue-1692989394[not included] in "
"the K3s releases."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:38
msgid "For this reason, the K3s binary should be built manually."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:39
msgid ""
"In this tutorial, "
"https://suse-edge.github.io/docs/quickstart/slemicro-utm-aarch64[SLE Micro "
"on OSX on Apple Silicon (UTM)] VM is used."
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:40
msgid "Run the commands below on the OSX PC."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:41
#, no-wrap
msgid "Building K3s"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:42
#, no-wrap
msgid "git clone --depth 1 https://github.com/k3s-io/k3s.git && cd k3s\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:43
msgid ""
"The following command adds `nats` in the build tags to enable the NATS "
"built-in feature in K3s:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:44
#, no-wrap
msgid ""
"sed -i '' 's/TAGS=\"ctrd/TAGS=\"nats ctrd/g' scripts/build\n"
"make local\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:45
msgid ""
"Replace <node-ip> with the actual IP of the node where the K3s will be "
"started:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:46
#, no-wrap
msgid ""
"export NODE_IP=<node-ip>\n"
"sudo scp dist/artifacts/k3s-arm64 ${NODE_IP}:/usr/local/bin/k3s\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:47
msgid ""
"Locally building K3s requires the buildx Docker CLI plugin.  It can be "
"https://github.com/docker/buildx#manual-download[manually installed] if `$ "
"make local` fails."
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:48
#, no-wrap
msgid "Installing NATS CLI"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:49
#, no-wrap
msgid ""
"TMPDIR=$(mktemp -d)\n"
"nats_version=\"nats-0.0.35-linux-arm64\"\n"
"curl -o \"${TMPDIR}/nats.zip\" -sfL "
"https://github.com/nats-io/natscli/releases/download/v0.0.35/${nats_version}.zip\n"
"unzip \"${TMPDIR}/nats.zip\" -d \"${TMPDIR}\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:50
#, no-wrap
msgid ""
"sudo scp ${TMPDIR}/${nats_version}/nats ${NODE_IP}:/usr/local/bin/nats\n"
"rm -rf ${TMPDIR}\n"
msgstr ""

#. type: Title ====
#: asciidoc/integrations/nats.adoc:51
#, no-wrap
msgid "Running NATS as K3s back-end"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:52
msgid ""
"Let us `ssh` on the node and run the K3s with the `--datastore-endpoint` "
"flag pointing to `nats`."
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:53
msgid ""
"The command below starts K3s as a foreground process, so the logs can be "
"easily followed to see if there are any issues.  To not block the current "
"terminal, a `&` flag could be added before the command to start it as a "
"background process."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:54
#, no-wrap
msgid "k3s server  --datastore-endpoint=nats://\n"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nats.adoc:55
msgid ""
"For making the K3s server with the NATS back-end permanent on your "
"`slemicro` VM, the script below can be run, which creates a `systemd` "
"service with the needed configurations."
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:56
#, no-wrap
msgid ""
"export INSTALL_K3S_SKIP_START=false\n"
"export INSTALL_K3S_SKIP_DOWNLOAD=true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:57
#, no-wrap
msgid ""
"curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=\"server \\\n"
" --datastore-endpoint=nats://\"  sh -\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nats.adoc:59
msgid ""
"The following commands can be run on the node to verify that everything with "
"the stream works properly:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nats.adoc:60
#, no-wrap
msgid ""
"nats str report -a\n"
"nats str view -a\n"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:1
#, no-wrap
msgid "NVIDIA GPUs on SLE Micro"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:3
msgid ""
"This guide demonstrates how to implement host-level NVIDIA GPU support via "
"the pre-built https://github.com/NVIDIA/open-gpu-kernel-modules[open-source "
"drivers] on SLE Micro 5.5. These are drivers that are baked into the "
"operating system rather than dynamically loaded by NVIDIA's "
"https://github.com/NVIDIA/gpu-operator[GPU Operator]. This configuration is "
"highly desirable for customers that want to pre-bake all artifacts required "
"for deployment into the image, and where the dynamic selection of the driver "
"version, that is, the user selecting the version of the driver via "
"Kubernetes, is not a requirement. This guide initially explains how to "
"deploy the additional components onto a system that has already been "
"pre-deployed, but follows with a section that describes how to embed this "
"configuration into the initial deployment via Edge Image Builder. If you do "
"not want to run through the basics and set things up manually, skip right "
"ahead to that section."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:4
msgid ""
"It is important to call out that the support for these drivers is provided "
"by both SUSE and NVIDIA in tight collaboration, where the driver is built "
"and shipped by SUSE as part of the package repositories. However, if you "
"have any concerns or questions about the combination in which you use the "
"drivers, ask your SUSE or NVIDIA account managers for further assistance. If "
"you plan to use "
"https://www.nvidia.com/en-gb/data-center/products/ai-enterprise/[NVIDIA AI "
"Enterprise] (NVAIE), ensure that you are using an "
"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/platform-support.html#supported-nvidia-gpus-and-systems[NVAIE "
"certified GPU], which _may_ require the use of proprietary NVIDIA "
"drivers. If you are unsure, speak with your NVIDIA representative."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:5
msgid ""
"Further information about NVIDIA GPU operator integration is _not_ covered "
"in this guide. While integrating the NVIDIA GPU Operator for Kubernetes is "
"not covered here, you can still follow most of the steps in this guide to "
"set up the underlying operating system and simply enable the GPU operator to "
"use the _pre-installed_ drivers via the `driver.enabled=false` flag in the "
"NVIDIA GPU Operator Helm chart, where it will simply pick up the installed "
"drivers on the host. More comprehensive instructions are available from "
"NVIDIA "
"https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/install-gpu-operator.html#chart-customization-options[here]. "
"SUSE recently also made a "
"https://documentation.suse.com/trd/kubernetes/single-html/gs_rke2-slebci_nvidia-gpu-operator/[Technical "
"Reference Document] (TRD) available that discusses how to use the GPU "
"operator and the NVIDIA proprietary drivers, should this be a requirement "
"for your use case."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:8
msgid ""
"At least one host with SLE Micro 5.5 installed; this can be physical or "
"virtual."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:9
msgid ""
"Your hosts are attached to a subscription as this is required for package "
"access — an evaluation is available "
"https://www.suse.com/download/sle-micro/[here]."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:10
msgid ""
"A "
"https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[compatible "
"NVIDIA GPU] installed (or _fully_ passed through to the virtual machine in "
"which SLE Micro is running)."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:12
#, no-wrap
msgid "Manual installation"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:13
msgid ""
"In this section, you are going to install the NVIDIA drivers directly onto "
"the SLE Micro operating system as the NVIDIA open-driver is now part of the "
"core SLE Micro package repositories, which makes it as easy as installing "
"the required RPM packages. There is no compilation or downloading of "
"executable packages required. Below we walk through deploying the \"G06\" "
"generation of driver, which supports the latest GPUs (see "
"https://en.opensuse.org/SDB:NVIDIA_drivers#Install[here] for further "
"information), so select an appropriate driver generation for the NVIDIA GPU "
"that your system has. For modern GPUs, the \"G06\" driver is the most common "
"choice."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:14
msgid ""
"Before we begin, it is important to recognize that besides the NVIDIA "
"open-driver that SUSE ships as part of SLE Micro, you might also need "
"additional NVIDIA components for your setup. These could include OpenGL "
"libraries, CUDA toolkits, command-line utilities such as `nvidia-smi`, and "
"container-integration components such as `nvidia-container-toolkit`. Many of "
"these components are not shipped by SUSE as they are proprietary NVIDIA "
"software, or it makes no sense for us to ship them instead of "
"NVIDIA. Therefore, as part of the instructions, we are going to configure "
"additional repositories that give us access to said components and walk "
"through certain examples of how to use these tools, resulting in a fully "
"functional system. It is important to distinguish between SUSE repositories "
"and NVIDIA repositories, as occasionally there can be a mismatch between the "
"package versions that NVIDIA makes available versus what SUSE has "
"built. This usually arises when SUSE makes a new version of the open-driver "
"available, and it takes a couple of days before the equivalent packages are "
"made available in NVIDIA repositories to match."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:15
msgid ""
"We recommend that you ensure that the driver version that you are selecting "
"is compatible with your GPU and meets any CUDA requirements that you may "
"have by checking:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:16
msgid ""
"The https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/[CUDA release "
"notes]"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:17
msgid ""
"The driver version that you plan on deploying has a matching version in the "
"http://download.nvidia.com/suse/sle15sp5/x86_64/[NVIDIA SLE15-SP5 "
"repository] and ensuring that you have equivalent package versions for the "
"supporting components available"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:18
#, no-wrap
msgid "[TIP] "
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:19
msgid ""
"To find the NVIDIA open-driver versions, either run `zypper se -s "
"nvidia-open-driver` on the target machine _or_ search the SUSE Customer "
"Center for the \"nvidia-open-driver\" in "
"https://scc.suse.com/packages?name=SUSE%20Linux%20Enterprise%20Micro&version=5.5&arch=x86_64[SLE "
"Micro 5.5 for x86_64]."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:20
msgid ""
"Here, you will see _four_ versions available, with _545.29.06_ being the "
"newest:"
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/integrations/nvidia-slemicro.adoc:21
#, no-wrap
msgid "SUSE Customer Centre"
msgstr ""

#. type: Target for macro image
#: asciidoc/integrations/nvidia-slemicro.adoc:22
#, no-wrap
msgid "scc-packages-nvidia.png"
msgstr ""

#. type: delimited block =
#: asciidoc/integrations/nvidia-slemicro.adoc:23
msgid ""
"When you have confirmed that an equivalent version is available in the "
"NVIDIA repos, you are ready to install the packages on the host operating "
"system. For this, we need to open up a `transactional-update` session, which "
"creates a new read/write snapshot of the underlying operating system so we "
"can make changes to the immutable platform (for further instructions on "
"`transactional-update`, see "
"https://documentation.suse.com/sle-micro/5.4/html/SLE-Micro-all/sec-transactional-udate.html[here]):"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:24
#, no-wrap
msgid "transactional-update shell\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:25
msgid ""
"When you are in your `transactional-update` shell, add an additional package "
"repository from NVIDIA. This allows us to pull in additional utilities, for "
"example, `nvidia-smi`:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:26
#, no-wrap
msgid ""
"zypper ar https://download.nvidia.com/suse/sle15sp5/ nvidia-sle15sp5-main\n"
"zypper --gpg-auto-import-keys refresh\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:27
msgid ""
"You can then install the driver and `nvidia-compute-utils` for additional "
"utilities. If you do not need the utilities, you can omit it, but for "
"testing purposes, it is worth installing at this stage:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:28
#, no-wrap
msgid ""
"zypper install -y --auto-agree-with-licenses "
"nvidia-open-driver-G06-signed-kmp nvidia-compute-utils-G06\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:29
msgid ""
"If the installation fails, this might indicate a dependency mismatch between "
"the selected driver version and what NVIDIA ships in their "
"repositories. Refer to the previous section to verify that your versions "
"match. Attempt to install a different driver version. For example, if the "
"NVIDIA repositories have an earlier version, you can try specifying "
"`nvidia-open-driver-G06-signed-kmp=545.29.06` on your install command to "
"specify a version that aligns."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:30
msgid ""
"Next, if you are _not_ using a supported GPU (remembering that the list can "
"be found "
"https://github.com/NVIDIA/open-gpu-kernel-modules#compatible-gpus[here]), "
"you can see if the driver works by enabling support at the module level, but "
"your mileage may vary — skip this step if you are using a _supported_ GPU:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:31
#: asciidoc/integrations/nvidia-slemicro.adoc:128
#, no-wrap
msgid ""
"sed -i '/NVreg_OpenRmEnableUnsupportedGpus/s/^#//g' "
"/etc/modprobe.d/50-nvidia-default.conf\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:32
msgid ""
"Now that you have installed these packages, it is time to exit the "
"`transactional-update` session:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:33
#: asciidoc/integrations/nvidia-slemicro.adoc:73
#, no-wrap
msgid "exit\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:34
msgid ""
"Make sure that you have exited the `transactional-update` session before "
"proceeding."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:35
msgid ""
"Now that you have installed the drivers, it is time to reboot. As SLE Micro "
"is an immutable operating system, it needs to reboot into the new snapshot "
"that you created in a previous step. The drivers are only installed into "
"this new snapshot, hence it is not possible to load the drivers without "
"rebooting into this new snapshot, which happens automatically. Issue the "
"reboot command when you are ready:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:37
msgid ""
"Once the system has rebooted successfully, log back in and use the "
"`nvidia-smi` tool to verify that the driver is loaded successfully and that "
"it can both access and enumerate your GPUs:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:38
#, no-wrap
msgid "nvidia-smi\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:39
msgid ""
"The output of this command should show you something similar to the "
"following output, noting that in the example below, we have two GPUs:"
msgstr ""

#. type: Table
#: asciidoc/integrations/nvidia-slemicro.adoc:40
#, no-wrap
msgid ""
"Wed Feb 28 12:31:06 2024\n"
"+---------------------------------------------------------------------------------------+\n"
"| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA "
"Version: 12.3     |\n"
"|-----------------------------------------+----------------------+----------------------+\n"
"| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile "
"Uncorr. ECC |\n"
"| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  "
"Compute M. |\n"
"|                                         |                      |               "
"MIG M. |\n"
"|   0  NVIDIA A100-PCIE-40GB          Off | 00000000:17:00.0 Off |                    "
"0 |\n"
"| N/A   29C    P0              35W / 250W |      4MiB / 40960MiB |      0%      "
"Default |\n"
"|                                         |                      |             "
"Disabled |\n"
"+-----------------------------------------+----------------------+----------------------+\n"
"|   1  NVIDIA A100-PCIE-40GB          Off | 00000000:CA:00.0 Off |                    "
"0 |\n"
"| N/A   30C    P0              33W / 250W |      4MiB / 40960MiB |      0%      "
"Default |\n"
"|                                         |                      |             "
"Disabled |\n"
"+-----------------------------------------+----------------------+----------------------+\n"
"\n"
"+---------------------------------------------------------------------------------------+\n"
"| Processes:                                                                            "
"|\n"
"|  GPU   GI   CI        PID   Type   Process name                            "
"GPU Memory |\n"
"|        ID   ID                                                             "
"Usage      |\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:41
#, no-wrap
msgid ""
"|  No running processes found                                                           "
"|\n"
"+---------------------------------------------------------------------------------------+\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:42
#, no-wrap
msgid ""
"This concludes the installation and verification process for the NVIDIA "
"drivers on your SLE Micro system.\n"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:43
#, no-wrap
msgid "Further validation of the manual installation"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:44
msgid ""
"At this stage, all we have been able to verify is that, at the host level, "
"the NVIDIA device can be accessed and that the drivers are loading "
"successfully. However, if we want to be sure that it is functioning, a "
"simple test would be to validate that the GPU can take instructions from a "
"user-space application, ideally via a container, and through the CUDA "
"library, as that is typically what a real workload would use. For this, we "
"can make a further modification to the host OS by installing the "
"`nvidia-container-toolkit` "
"(https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-zypper[NVIDIA "
"Container Toolkit]). First, open another `transactional-update` shell, "
"noting that we could have done this in a single transaction in the previous "
"step, and see how to do this fully automated in a later section:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:46
msgid "transactional-update shell"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:47
#, no-wrap
msgid ""
"Next, install the `nvidia-container-toolkit` package from the NVIDIA "
"Container Toolkit repo:\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:48
#, no-wrap
msgid ""
"* The `nvidia-container-toolkit.repo` below contains a stable "
"(`nvidia-container-toolkit`) and an experimental "
"(`nvidia-container-toolkit-experimental`) repository. The stable repository "
"is recommended for production use. The experimental repository is disabled "
"by default.\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:50
msgid ""
"zypper ar "
"https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo "
"zypper --gpg-auto-import-keys install -y nvidia-container-toolkit"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:51
#, no-wrap
msgid "When you are ready, you can exit the `transactional-update` shell:\n"
msgstr ""

#. type: Title -
#: asciidoc/integrations/nvidia-slemicro.adoc:53
#, no-wrap
msgid "exit"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:54
msgid "...and reboot the machine into the new snapshot:"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:56
msgid ""
"As before, you need to ensure that you have exited the `transactional-shell` "
"and rebooted the machine for your changes to be enacted."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:57
msgid ""
"With the machine rebooted, you can verify that the system can successfully "
"enumerate the devices using the NVIDIA Container Toolkit. The output should "
"be verbose, with INFO and WARN messages, but no ERROR messages:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:58
#, no-wrap
msgid "nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:59
msgid ""
"This ensures that any container started on the machine can employ NVIDIA GPU "
"devices that have been discovered. When ready, you can then run a "
"podman-based container. Doing this via `podman` gives us a good way of "
"validating access to the NVIDIA device from within a container, which should "
"give confidence for doing the same with Kubernetes at a later stage. Give "
"`podman` access to the labeled NVIDIA devices that were taken care of by the "
"previous command, based on "
"https://registry.suse.com/bci/bci-base-15sp5/index.html[SLE BCI], and simply "
"run the Bash command:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:60
#, no-wrap
msgid ""
"podman run --rm --device nvidia.com/gpu=all --security-opt=label=disable -it "
"registry.suse.com/bci/bci-base:latest bash\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:61
msgid ""
"You will now execute commands from within a temporary podman container. It "
"does not have access to your underlying system and is ephemeral, so whatever "
"we do here will not persist, and you should not be able to break anything on "
"the underlying host. As we are now in a container, we can install the "
"required CUDA libraries, again checking the correct CUDA version for your "
"driver https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/[here], "
"although the previous output of `nvidia-smi` should show the required CUDA "
"version. In the example below, we are installing _CUDA 12.3_ and pulling "
"many examples, demos and development kits so you can fully validate the GPU:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:62
#, no-wrap
msgid ""
"zypper ar "
"http://developer.download.nvidia.com/compute/cuda/repos/sles15/x86_64/ "
"cuda-sle15-sp5\n"
"zypper in -y cuda-libraries-devel-12-3 cuda-minimal-build-12-3 "
"cuda-demo-suite-12-3\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:63
msgid ""
"Once this has been installed successfully, do not exit the container. We "
"will run the `deviceQuery` CUDA example, which comprehensively validates GPU "
"access via CUDA, and from within the container itself:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:64
#, no-wrap
msgid "/usr/local/cuda-12/extras/demo_suite/deviceQuery\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:65
msgid ""
"If successful, you should see output that shows similar to the following, "
"noting the `Result = PASS` message at the end of the command, and noting "
"that in the output below, the system correctly identifies two GPUs, whereas "
"your environment may only have one:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:66
#, no-wrap
msgid "/usr/local/cuda-12/extras/demo_suite/deviceQuery Starting...\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:67
#, no-wrap
msgid " CUDA Device Query (Runtime API) version (CUDART static linking)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:68
#, no-wrap
msgid "Detected 2 CUDA Capable device(s)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:69
#, no-wrap
msgid ""
"Device 0: \"NVIDIA A100-PCIE-40GB\"\n"
"  CUDA Driver Version / Runtime Version          12.2 / 12.1\n"
"  CUDA Capability Major/Minor version number:    8.0\n"
"  Total amount of global memory:                 40339 MBytes (42298834944 "
"bytes)\n"
"  (108) Multiprocessors, ( 64) CUDA Cores/MP:     6912 CUDA Cores\n"
"  GPU Max Clock rate:                            1410 MHz (1.41 GHz)\n"
"  Memory Clock rate:                             1215 Mhz\n"
"  Memory Bus Width:                              5120-bit\n"
"  L2 Cache Size:                                 41943040 bytes\n"
"  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, "
"65536), 3D=(16384, 16384, 16384)\n"
"  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers\n"
"  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 "
"layers\n"
"  Total amount of constant memory:               65536 bytes\n"
"  Total amount of shared memory per block:       49152 bytes\n"
"  Total number of registers available per block: 65536\n"
"  Warp size:                                     32\n"
"  Maximum number of threads per multiprocessor:  2048\n"
"  Maximum number of threads per block:           1024\n"
"  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)\n"
"  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)\n"
"  Maximum memory pitch:                          2147483647 bytes\n"
"  Texture alignment:                             512 bytes\n"
"  Concurrent copy and kernel execution:          Yes with 3 copy engine(s)\n"
"  Run time limit on kernels:                     No\n"
"  Integrated GPU sharing Host Memory:            No\n"
"  Support host page-locked memory mapping:       Yes\n"
"  Alignment requirement for Surfaces:            Yes\n"
"  Device has ECC support:                        Enabled\n"
"  Device supports Unified Addressing (UVA):      Yes\n"
"  Device supports Compute Preemption:            Yes\n"
"  Supports Cooperative Kernel Launch:            Yes\n"
"  Supports MultiDevice Co-op Kernel Launch:      Yes\n"
"  Device PCI Domain ID / Bus ID / location ID:   0 / 23 / 0\n"
"  Compute Mode:\n"
"     < Default (multiple host threads can use ::cudaSetDevice() with device "
"simultaneously) >\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:70
#, no-wrap
msgid ""
"Device 1: <snip to reduce output for multiple devices>\n"
"     < Default (multiple host threads can use ::cudaSetDevice() with device "
"simultaneously) >\n"
"> Peer access from NVIDIA A100-PCIE-40GB (GPU0) -> NVIDIA A100-PCIE-40GB "
"(GPU1) : Yes\n"
"> Peer access from NVIDIA A100-PCIE-40GB (GPU1) -> NVIDIA A100-PCIE-40GB "
"(GPU0) : Yes\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:71
#, no-wrap
msgid ""
"deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 12.3, CUDA Runtime "
"Version = 12.3, NumDevs = 2, Device0 = NVIDIA A100-PCIE-40GB, Device1 = "
"NVIDIA A100-PCIE-40GB\n"
"Result = PASS\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:72
msgid ""
"From here, you can continue to run any other CUDA workload — use compilers "
"and any other aspect of the CUDA ecosystem to run further tests. When done, "
"you can exit from the container, noting that whatever you have installed in "
"there is ephemeral (so will be lost!), and has not impacted the underlying "
"operating system:"
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:74
#, no-wrap
msgid "Implementation with Kubernetes"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:75
msgid ""
"Now that we have proven the installation and use of the NVIDIA open-driver "
"on SLE Micro, let us explore configuring Kubernetes on the same "
"machine. This guide does not walk you through deploying Kubernetes, but it "
"assumes that you have installed https://k3s.io/[K3s] or "
"https://docs.rke2.io/install/quickstart[RKE2] and that your kubeconfig is "
"configured accordingly, so that standard `kubectl` commands can be executed "
"as the superuser. We assume that your node forms a single-node cluster, "
"although the core steps should be similar for multi-node clusters. First, "
"ensure that your `kubectl` access is working:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:76
#, no-wrap
msgid "kubectl get nodes\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:78
#, no-wrap
msgid ""
"NAME       STATUS   ROLES                       AGE   VERSION\n"
"node0001   Ready    control-plane,etcd,master   13d   v1.28.9+rke2r1\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:79
msgid ""
"What you should find is that your k3s/rke2 installation has detected the "
"NVIDIA Container Toolkit on the host and auto-configured the NVIDIA runtime "
"integration into `containerd` (the Container Runtime Interface that k3s/rke2 "
"use). Confirm this by checking the containerd `config.toml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:80
#, no-wrap
msgid "tail -n8 /var/lib/rancher/rke2/agent/etc/containerd/config.toml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:81
msgid ""
"This must show something akin to the following. The equivalent K3s location "
"is `/var/lib/rancher/k3s/agent/etc/containerd/config.toml`:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:82
#, no-wrap
msgid ""
"[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\"]\n"
"  runtime_type = \"io.containerd.runc.v2\"\n"
"[plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.\"nvidia\".options]\n"
"  BinaryName = \"/usr/bin/nvidia-container-runtime\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:83
msgid ""
"If these entries are not present, the detection might have failed. This "
"could be due to the machine or the Kubernetes services not being "
"restarted. Add these manually as above, if required."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:84
msgid ""
"Next, we need to configure the NVIDIA `RuntimeClass` as an additional "
"Kubernetes runtime to the default, ensuring that any user requests for pods "
"that need access to the GPU can use the NVIDIA Container Toolkit to do so, "
"via the `nvidia-container-runtime`, as configured in the `containerd` "
"configuration:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:85
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: node.k8s.io/v1\n"
"kind: RuntimeClass\n"
"metadata:\n"
"  name: nvidia\n"
"handler: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:86
msgid ""
"The next step is to configure the "
"https://github.com/NVIDIA/k8s-device-plugin[NVIDIA Device Plugin], which "
"configures Kubernetes to leverage the NVIDIA GPUs as resources within the "
"cluster that can be used, working in combination with the NVIDIA Container "
"Toolkit. This tool initially detects all capabilities on the underlying "
"host, including GPUs, drivers and other capabilities (such as GL) and then "
"allows you to request GPU resources and consume them as part of your "
"applications."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:87
msgid ""
"First, you need to add and update the Helm repository for the NVIDIA Device "
"Plugin:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:88
#, no-wrap
msgid ""
"helm repo add nvdp https://nvidia.github.io/k8s-device-plugin\n"
"helm repo update\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:89
msgid "Now you can install the NVIDIA Device Plugin:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:90
#, no-wrap
msgid ""
"helm upgrade -i nvdp nvdp/nvidia-device-plugin --namespace "
"nvidia-device-plugin --create-namespace --version 0.14.5 --set "
"runtimeClassName=nvidia\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:91
msgid ""
"After a few minutes, you see a new pod running that will complete the "
"detection on your available nodes and tag them with the number of GPUs that "
"have been detected:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:92
#, no-wrap
msgid ""
"kubectl get pods -n nvidia-device-plugin\n"
"NAME                              READY   STATUS    RESTARTS      AGE\n"
"nvdp-nvidia-device-plugin-jp697   1/1     Running   2 (12h ago)   6d3h\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:93
#, no-wrap
msgid ""
"kubectl get node node0001 -o json | jq .status.capacity\n"
"{\n"
"  \"cpu\": \"128\",\n"
"  \"ephemeral-storage\": \"466889732Ki\",\n"
"  \"hugepages-1Gi\": \"0\",\n"
"  \"hugepages-2Mi\": \"0\",\n"
"  \"memory\": \"32545636Ki\",\n"
"  \"nvidia.com/gpu\": \"1\",                      <----\n"
"  \"pods\": \"110\"\n"
"}\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:94
msgid ""
"Now you are ready to create an NVIDIA pod that attempts to use this GPU. Let "
"us try with the CUDA Benchmark container:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:95
#, no-wrap
msgid ""
"kubectl apply -f - <<EOF\n"
"apiVersion: v1\n"
"kind: Pod\n"
"metadata:\n"
"  name: nbody-gpu-benchmark\n"
"  namespace: default\n"
"spec:\n"
"  restartPolicy: OnFailure\n"
"  runtimeClassName: nvidia\n"
"  containers:\n"
"  - name: cuda-container\n"
"    image: nvcr.io/nvidia/k8s/cuda-sample:nbody\n"
"    args: [\"nbody\", \"-gpu\", \"-benchmark\"]\n"
"    resources:\n"
"      limits:\n"
"        nvidia.com/gpu: 1\n"
"    env:\n"
"    - name: NVIDIA_VISIBLE_DEVICES\n"
"      value: all\n"
"    - name: NVIDIA_DRIVER_CAPABILITIES\n"
"      value: all\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:96
msgid ""
"If all went well, you can look at the logs and see the benchmark "
"information:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:97
#, no-wrap
msgid ""
"kubectl logs nbody-gpu-benchmark\n"
"Run \"nbody -benchmark [-numbodies=<numBodies>]\" to measure performance.\n"
"\t-fullscreen       (run n-body simulation in fullscreen mode)\n"
"\t-fp64             (use double precision floating point values for "
"simulation)\n"
"\t-hostmem          (stores simulation data in host memory)\n"
"\t-benchmark        (run benchmark to measure performance)\n"
"\t-numbodies=<N>    (number of bodies (>= 1) to run in simulation)\n"
"\t-device=<d>       (where d=0,1,2.... for the CUDA device to use)\n"
"\t-numdevices=<i>   (where i=(number of CUDA devices > 0) to use for "
"simulation)\n"
"\t-compare          (compares simulation results running once on the default "
"GPU and once on the CPU)\n"
"\t-cpu              (run n-body simulation on the CPU)\n"
"\t-tipsy=<file.bin> (load a tipsy model file for simulation)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:98
#, no-wrap
msgid ""
"NOTE: The CUDA Samples are not meant for performance measurements. Results "
"may vary when GPU Boost is enabled.\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:99
#, no-wrap
msgid ""
"> Windowed mode\n"
"> Simulation data stored in video memory\n"
"> Single precision floating point simulation\n"
"> 1 Devices used for simulation\n"
"GPU Device 0: \"Turing\" with compute capability 7.5\n"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:100
#, no-wrap
msgid ""
"> Compute 7.5 CUDA device: [Tesla T4]\n"
"40960 bodies, total time for 10 iterations: 101.677 ms\n"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:101
#, no-wrap
msgid "165.005 billion interactions per second"
msgstr ""

#. type: Title =
#: asciidoc/integrations/nvidia-slemicro.adoc:102
#, no-wrap
msgid "3300.103 single-precision GFLOP/s at 20 flops per interaction"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:103
msgid ""
"Finally, if your applications require OpenGL, you can install the required "
"NVIDIA OpenGL libraries at the host level, and the NVIDIA Device Plugin and "
"NVIDIA Container Toolkit can make them available to containers. To do this, "
"install the package as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:104
#, no-wrap
msgid "transactional-update pkg install nvidia-gl-G06\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:105
msgid ""
"You need to reboot to make this package available to your applications. The "
"NVIDIA Device Plugin should automatically redetect this via the NVIDIA "
"Container Toolkit."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:106
#, no-wrap
msgid "Bringing it together via Edge Image Builder"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:107
msgid ""
"Okay, so you have demonstrated full functionality of your applications and "
"GPUs on SLE Micro and you now want to use <<components-eib>> to provide it "
"all together via a deployable/consumable ISO or RAW disk image. This guide "
"does not explain how to use Edge Image Builder, but it provides the "
"necessary configurations to build such image. Below you can find an example "
"of an image definition, along with the necessary Kubernetes configuration "
"files, to ensure that all the required components are deployed out of the "
"box. Here is the directory structure of the Edge Image Builder directory for "
"the example shown below:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:108
#, no-wrap
msgid ""
".\n"
"├── base-images\n"
"│   └── SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"├── eib-config-iso.yaml\n"
"├── kubernetes\n"
"│   ├── config\n"
"│   │   └── server.yaml\n"
"│   ├── helm\n"
"│   │   └── values\n"
"│   │       └── nvidia-device-plugin.yaml\n"
"│   └── manifests\n"
"│       └── nvidia-runtime-class.yaml\n"
"└── rpms\n"
"    └── gpg-keys\n"
"        └── nvidia-container-toolkit.key\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:109
msgid ""
"Let us explore those files. First, here is a sample image definition for a "
"single-node cluster running K3s that deploys the utilities and OpenGL "
"packages, too (`eib-config-iso.yaml`):"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:110
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  arch: x86_64\n"
"  imageType: iso\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  outputImageName: deployimage.iso\n"
"operatingSystem:\n"
"  time:\n"
"    timezone: Europe/London\n"
"    ntp:\n"
"      pools:\n"
"        - 2.suse.pool.ntp.org\n"
"  isoConfiguration:\n"
"    installDevice: /dev/sda\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: "
"$6$XcQN1xkuQKjWEtQG$WbhV80rbveDLJDz1c93K5Ga9JDjt3mF.ZUnhYtsS7uE52FR8mmT8Cnii/JPeFk9jzQO6eapESYZesZHO9EslD1\n"
"  packages:\n"
"    packageList:\n"
"      - nvidia-open-driver-G06-signed-kmp-default\n"
"      - nvidia-compute-utils-G06\n"
"      - nvidia-gl-G06\n"
"      - nvidia-container-toolkit\n"
"    additionalRepos:\n"
"      - url: https://download.nvidia.com/suse/sle15sp5/\n"
"      - url: "
"https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64\n"
"    sccRegistrationCode: <snip>\n"
"kubernetes:\n"
"  version: v1.28.9+k3s1\n"
"  helm:\n"
"    charts:\n"
"      - name: nvidia-device-plugin\n"
"        version: v0.14.5\n"
"        installationNamespace: kube-system\n"
"        targetNamespace: nvidia-device-plugin\n"
"        createNamespace: true\n"
"        valuesFile: nvidia-device-plugin.yaml\n"
"        repositoryName: nvidia\n"
"    repositories:\n"
"      - name: nvidia\n"
"        url: https://nvidia.github.io/k8s-device-plugin\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:111
msgid ""
"This is just an example. You may need to customize it to fit your "
"requirements and expectations. Additionally, if using SLE Micro, you need to "
"provide your own `sccRegistrationCode` to resolve package dependencies and "
"pull the NVIDIA drivers."
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:112
msgid ""
"Besides this, we need to add additional components, so they get loaded by "
"Kubernetes at boot time. The EIB directory needs a `kubernetes` directory "
"first, with subdirectories for the configuration, Helm chart values and any "
"additional manifests required:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:113
#, no-wrap
msgid "mkdir -p kubernetes/config kubernetes/helm/values kubernetes/manifests\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:114
msgid ""
"Let us now set up the (optional) Kubernetes configuration by choosing a CNI "
"(which defaults to Cilium if unselected) and enabling SELinux:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:115
#, no-wrap
msgid ""
"cat << EOF > kubernetes/config/server.yaml\n"
"cni: cilium\n"
"selinux: true\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:116
msgid ""
"Now ensure that the NVIDIA RuntimeClass is created on the Kubernetes "
"cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:117
#, no-wrap
msgid ""
"cat << EOF > kubernetes/manifests/nvidia-runtime-class.yaml\n"
"apiVersion: node.k8s.io/v1\n"
"kind: RuntimeClass\n"
"metadata:\n"
"  name: nvidia\n"
"handler: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:118
msgid ""
"We use the built-in Helm Controller to deploy the NVIDIA Device Plugin "
"through Kubernetes itself.  Let's provide the runtime class in the values "
"file for the chart:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:119
#, no-wrap
msgid ""
"cat << EOF > kubernetes/helm/values/nvidia-device-plugin.yaml\n"
"runtimeClassName: nvidia\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:120
msgid ""
"We need to grab the NVIDIA Container Toolkit RPM public key before "
"proceeding:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:121
#, no-wrap
msgid ""
"mkdir -p rpms/gpg-keys\n"
"curl -o rpms/gpg-keys/nvidia-container-toolkit.key "
"https://nvidia.github.io/libnvidia-container/gpgkey\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:122
msgid ""
"All the required artifacts, including Kubernetes binary, container images, "
"Helm charts (and any referenced images), will be automatically air-gapped, "
"meaning that the systems at deploy time should require no Internet "
"connectivity by default. Now you need only to grab the SLE Micro ISO from "
"the https://www.suse.com/download/sle-micro/[SUSE Downloads Page] (and place "
"it in the `base-images` directory), and you can call the Edge Image Builder "
"tool to generate the ISO for you. To complete the example, here is the "
"command that was used to build the image:"
msgstr ""

#. type: delimited block -
#: asciidoc/integrations/nvidia-slemicro.adoc:123
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v /path/to/eib-files/:/eib \\\n"
"registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
"build --definition-file eib-config-iso.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:124
msgid ""
"For further instructions, please see the "
"https://github.com/suse-edge/edge-image-builder/blob/release-1.0/docs/building-images.md[documentation] "
"for Edge Image Builder."
msgstr ""

#. type: Title ==
#: asciidoc/integrations/nvidia-slemicro.adoc:125
#, no-wrap
msgid "Resolving issues"
msgstr ""

#. type: Title ===
#: asciidoc/integrations/nvidia-slemicro.adoc:126
#, no-wrap
msgid "nvidia-smi does not find the GPU"
msgstr ""

#. type: Plain text
#: asciidoc/integrations/nvidia-slemicro.adoc:127
msgid ""
"Check the kernel messages using `dmesg`. If this indicates that it cannot "
"allocate `NvKMSKapDevice`, apply the unsupported GPU workaround:"
msgstr ""

#. type: delimited block _
#: asciidoc/integrations/nvidia-slemicro.adoc:129
msgid ""
"_NOTE_: You will need to reload the kernel module, or reboot, if you change "
"the kernel module configuration in the above step for it to take effect."
msgstr ""

#. type: Title =
#: asciidoc/day2/mgmt-cluster.adoc:1
#, no-wrap
msgid "Management Cluster"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:2
msgid ""
"This section covers how to do various `Day 2` operations on a `management "
"cluster`."
msgstr ""

#. type: Title ==
#: asciidoc/day2/mgmt-cluster.adoc:3
#, no-wrap
msgid "RKE2 upgrade"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:4
msgid ""
"To ensure *disaster recovery*, we advise to do a backup of the RKE2 cluster "
"data. For information on how to do this, check "
"link:https://docs.rke2.io/backup_restore[here]. The default location for the "
"`rke2` binary is `/opt/rke2/bin`."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:5
msgid ""
"You can upgrade the RKE2 version using the RKE2 installation script as "
"follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:6
#, no-wrap
msgid "curl -sfL https://get.rke2.io | INSTALL_RKE2_VERSION=vX.Y.Z+rke2rN sh -\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:7
msgid "Remember to restart the `rke2` process after installing:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:8
#, no-wrap
msgid ""
"# For server nodes:\n"
"systemctl restart rke2-server\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:9
#, no-wrap
msgid ""
"# For agent nodes:\n"
"systemctl restart rke2-agent\n"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:10
msgid ""
"To avoid any unforseen upgrade problems, use the following node upgrade "
"order:"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:11
msgid "_Server nodes_ - should be upgraded *one* node at a time."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:12
msgid ""
"_Agent nodes_ - should be upgraded after *all* server node upgrades have "
"finished. Can be upgraded in parallel."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:13
msgid ""
"_For further information, see the "
"link:https://docs.rke2.io/upgrade/manual_upgrade#upgrade-rke2-using-the-installation-script[RKE2 "
"upgrade documentation]._"
msgstr ""

#. type: Title ==
#: asciidoc/day2/mgmt-cluster.adoc:14
#, no-wrap
msgid "OS upgrade"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:15
msgid ""
"This section assumes that you have registered your system to "
"https://scc.suse.com."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:16
msgid ""
"SUSE regularly releases new `SLE Micro` package updates. To retrieve the "
"updated package versions SLE Micro uses `transactional-upgrade`."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:17
msgid ""
"`transactional-upgrade` provides an application and library to update a "
"Linux operating system in a transactional way, i.e. the update will be "
"performed in the background while the system continues running as it "
"is. Only after you *reboot* the system will the update take effect. For "
"further information, see the `transactional-update` "
"https://github.com/openSUSE/transactional-update[GitHub] GitHub page."
msgstr ""

#. type: Block title
#: asciidoc/day2/mgmt-cluster.adoc:18
#, no-wrap
msgid "To update all packages in the system, execute:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:19
#, no-wrap
msgid "transactional-update\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:20
msgid ""
"Since *rebooting* the node will result in it being unavailable for some "
"time, if you are running a multi-node cluster, you can "
"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_cordon/[cordon] "
"and "
"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_drain/[drain] "
"the node before the *reboot*."
msgstr ""

#. type: Block title
#: asciidoc/day2/mgmt-cluster.adoc:21
#, no-wrap
msgid "To cordon a node, execute:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:22
#, no-wrap
msgid "kubectl cordon <node>\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:23
msgid ""
"This will result in the node being taken out of the default scheduling "
"mechanism, ensuring that no pods will be assigned to it by mistake."
msgstr ""

#. type: Block title
#: asciidoc/day2/mgmt-cluster.adoc:24
#, no-wrap
msgid "To drain a node, execute:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:25
#, no-wrap
msgid "kubectl drain <node>\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:26
msgid ""
"This will ensure that all workloads on the node will be transferred to other "
"available nodes."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:27
msgid ""
"Depending on what workloads you are running on the node, you might also need "
"to provide additional flags (e.g. `--delete-emptydir-data`, "
"`--ignore-daemonsets`) to the command."
msgstr ""

#. type: Block title
#: asciidoc/day2/mgmt-cluster.adoc:28
#, no-wrap
msgid "Reboot node:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:29
#, no-wrap
msgid "sudo reboot\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:30
msgid ""
"After a successful reboot, the packages on your node will be updated. The "
"only thing left is to bring the node back to the default scheduling "
"mechanism with the "
"https://kubernetes.io/docs/reference/kubectl/generated/kubectl_uncordon/[uncordon] "
"command."
msgstr ""

#. type: Block title
#: asciidoc/day2/mgmt-cluster.adoc:31
#, no-wrap
msgid "Uncordon node:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:32
#, no-wrap
msgid "kubectl uncordon <node>\n"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:33
msgid ""
"In case you want to revert the update, use the above steps with the "
"following `transactional-update` command:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:34
#, no-wrap
msgid "transactional-update rollback last\n"
msgstr ""

#. type: Title ==
#: asciidoc/day2/mgmt-cluster.adoc:35
#, no-wrap
msgid "Helm upgrade"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:36
msgid ""
"This section assumes you have installed `helm` on your system. For `helm` "
"installation instructions, check "
"link:https://helm.sh/docs/intro/install[here]."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:37
msgid ""
"This section covers how to upgrade both an "
"<<day2-mgmt-cluster-eib-helm-chart-upgrade,EIB>> and "
"<<day2-mgmt-cluster-helm-chart-upgrade,non-EIB>> deployed helm chart."
msgstr ""

#. type: Title ===
#: asciidoc/day2/mgmt-cluster.adoc:38
#, no-wrap
msgid "EIB deployed helm chart"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:39
msgid ""
"EIB deploys helm charts defined in it's <<quickstart-eib-definition-file, "
"image definition file>> by using RKE2's manifest "
"https://docs.rke2.io/advanced#auto-deploying-manifests[auto-deploy] "
"functionality."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:40
msgid ""
"In order to upgrade a chart that is deployed in such a manner, you need to "
"upgrade the chart manifest file that EIB will create under the "
"`/var/lib/rancher/rke2/server/manifests` directory on your `initializer` "
"node."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:41
msgid ""
"To ensure disaster recovery, we advise that you always backup your chart "
"manifest file as well as follow any documentation related to disaster "
"recovery that your chart offers."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:42
msgid "To upgrade the chart manifest file, follow these steps:"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:43
msgid "Locate the `initializer` node"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:44
msgid ""
"For `multi-node clusters` - in your EIB image definition file, you should "
"have specified the `initializer: true` property for one of your nodes. If "
"you have not specified this property, the initializer node will be the first "
"*server* node in your node list."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:45
msgid "For `single-node clusters` - the initializer is the currently running node."
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:46
msgid "SSH to the `initializer` node:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:47
#, no-wrap
msgid "ssh root@<node_ip>\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:48
msgid "link:https://helm.sh/docs/helm/helm_pull/[Pull] the helm chart:"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:49
msgid "For helm charts hosted in a helm chart repository:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:50
#, no-wrap
msgid ""
"helm repo add <chart_repo_name> <chart_repo_urls>\n"
"helm pull <chart_repo_name>/<chart_name>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:51
#, no-wrap
msgid ""
"# Alternatively if you want to pull a specific verison\n"
"helm pull <chart_repo_name>/<chart_name> --version=X.Y.Z\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:52
msgid "For OCI-based helm charts:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:53
#, no-wrap
msgid "helm pull oci://<chart_oci_url>\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:54
#, no-wrap
msgid ""
"# Alternatively if you want to pull a specific verison\n"
"helm pull oci://<chart_oci_url> --version=X.Y.Z\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:55
msgid ""
"Encode the pulled `.tgz` archive so that it can be passed to a `HelmChart` "
"CR config:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:56
#, no-wrap
msgid "base64 -w 0 <chart_name>-X.Y.Z.tgz  > <chart_name>-X.Y.Z.txt\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:57
msgid "Make a copy of the chart manifest file that we will edit:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:58
#, no-wrap
msgid ""
"cp /var/lib/rancher/rke2/server/manifests/<chart_name>.yaml "
"./<chart_name>.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:59
msgid ""
"Change the `chartContent` and `version` configurations of the `bar.yaml` "
"file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:60
#, no-wrap
msgid ""
"sed -i -e \"s|chartContent:.*|chartContent: $(<chart-name-X.Y.Z.txt)|\" -e "
"\"s|version:.*|version: X.Y.Z|\" <chart_name>.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:61
msgid ""
"If you need to do any additional upgrade changes to the chart (e.g. adding "
"*new* custom chart values), you need to manually edit the chart manifest "
"file."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:62
msgid "Replace the original chart manifest file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:63
#, no-wrap
msgid "cp <chart_name>.yaml /var/lib/rancher/rke2/server/manifests/\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:64
msgid ""
"The above commands will trigger an upgrade of the helm chart. The upgrade "
"will be handled by the "
"https://github.com/k3s-io/helm-controller#helm-controller[helm-controller]."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:65
msgid ""
"To track the helm chart upgrade you need to view the logs of the pod that "
"the `helm-controller` creates for the chart upgrade. Refer to the "
"<<day2-mgmt-cluster-eib-helm-chart-upgrade-examples, Examples>> section for "
"more information."
msgstr ""

#. type: Title ====
#: asciidoc/day2/mgmt-cluster.adoc:66 asciidoc/day2/mgmt-cluster.adoc:133
#, no-wrap
msgid "Examples"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:67
msgid ""
"The examples in this section assume that you have already located and "
"connected to your `initializer` node."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:68 asciidoc/day2/mgmt-cluster.adoc:134
msgid "This section offer examples on how to upgrade a:"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:69
msgid ""
"<<day2-mgmt-cluster-eib-helm-chart-upgrade-examples-rancher, Rancher>> helm "
"chart"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:70
msgid ""
"<<day2-mgmt-cluster-eib-helm-chart-upgrade-examples-metal3, Metal^3^>> helm "
"chart"
msgstr ""

#. type: Title =====
#: asciidoc/day2/mgmt-cluster.adoc:71
#, no-wrap
msgid "Rancher upgrade"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:72 asciidoc/day2/mgmt-cluster.adoc:138
msgid ""
"To ensure disaster recovery, we advise to do a Rancher backup. For "
"information on how to do this, check "
"link:https://ranchermanager.docs.rancher.com/how-to-guides/new-user-guides/backup-restore-and-disaster-recovery/back-up-rancher[here]."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:73 asciidoc/day2/mgmt-cluster.adoc:139
msgid "This example shows how to upgrade Rancher to the `2.8.4` version."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:74
msgid "Add the `Rancher Prime` Helm repository:"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:76
msgid "Pull the latest `Rancher Prime` helm chart version:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:77
#, no-wrap
msgid "helm pull rancher-prime/rancher --version=2.8.4\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:78 asciidoc/day2/mgmt-cluster.adoc:104
msgid "Encode `.tgz` archive so that it can be passed to a `HelmChart` CR config:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:79
#, no-wrap
msgid "base64 -w 0 rancher-2.8.4.tgz  > rancher-2.8.4-encoded.txt\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:80
msgid "Make a copy of the `rancher.yaml` file that we will edit:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:81
#, no-wrap
msgid "cp /var/lib/rancher/rke2/server/manifests/rancher.yaml ./rancher.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:82
msgid ""
"Change the `chartContent` and `version` configurations of the `rancher.yaml` "
"file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:83
#, no-wrap
msgid ""
"sed -i -e \"s|chartContent:.*|chartContent: $(<rancher-2.8.4-encoded.txt)|\" "
"-e \"s|version:.*|version: 2.8.4|\" rancher.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:84
msgid ""
"If you need to do any additional upgrade changes to the chart (e.g. adding "
"*new* custom chart values), you need to manually edit the `rancher.yaml` "
"file."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:85
msgid "Replace the original `rancher.yaml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:86
#, no-wrap
msgid "cp rancher.yaml /var/lib/rancher/rke2/server/manifests/\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:87 asciidoc/day2/mgmt-cluster.adoc:113
msgid "To verify the update:"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:88 asciidoc/day2/mgmt-cluster.adoc:114
msgid "List pods in `default` namespace:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:89 asciidoc/day2/mgmt-cluster.adoc:115
#, no-wrap
msgid "kubectl get pods -n default\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:90
#, no-wrap
msgid ""
"# Example output\n"
"NAME                              READY   STATUS      RESTARTS   AGE\n"
"helm-install-cert-manager-7v7nm   0/1     Completed   0          88m\n"
"helm-install-rancher-p99k5        0/1     Completed   0          3m21s\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:91 asciidoc/day2/mgmt-cluster.adoc:117
msgid "Look at the logs of the `helm-install-rancher-*` pod:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:92 asciidoc/day2/mgmt-cluster.adoc:118
#, no-wrap
msgid "kubectl logs <helm_install_rancher_pod> -n default\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:93
#, no-wrap
msgid ""
"# Example\n"
"kubectl logs helm-install-rancher-p99k5 -n default\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:94
msgid "Verify `Rancher` pods are running:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:95
#, no-wrap
msgid "kubectl get pods -n cattle-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:96
#, no-wrap
msgid ""
"# Example output\n"
"NAME                               READY   STATUS      RESTARTS   AGE\n"
"helm-operation-mccvd               0/2     Completed   0          3m52s\n"
"helm-operation-np8kn               0/2     Completed   0          106s\n"
"helm-operation-q8lf7               0/2     Completed   0          2m53s\n"
"rancher-648d4fbc6c-qxfpj           1/1     Running     0          5m27s\n"
"rancher-648d4fbc6c-trdnf           1/1     Running     0          9m57s\n"
"rancher-648d4fbc6c-wvhbf           1/1     Running     0          9m57s\n"
"rancher-webhook-649dcc48b4-zqjs7   1/1     Running     0          100s\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:97 asciidoc/day2/mgmt-cluster.adoc:144
msgid "Verify `Rancher` version upgrade:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:98 asciidoc/day2/mgmt-cluster.adoc:145
#, no-wrap
msgid "kubectl get settings.management.cattle.io server-version\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:99 asciidoc/day2/mgmt-cluster.adoc:146
#, no-wrap
msgid ""
"# Example output\n"
"NAME             VALUE\n"
"server-version   v2.8.4\n"
msgstr ""

#. type: Title =====
#: asciidoc/day2/mgmt-cluster.adoc:100
#, no-wrap
msgid "Metal^3^ upgrade"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:101 asciidoc/day2/mgmt-cluster.adoc:149
msgid "This example shows how to upgrade Metal^3^ to the `0.7.1` version."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:102
msgid "Pull the latest `Metal^3^` helm chart version:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:103
#, no-wrap
msgid "helm pull oci://registry.suse.com/edge/metal3-chart --version 0.7.1\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:105
#, no-wrap
msgid "base64 -w 0 metal3-chart-0.7.1.tgz  > metal3-chart-0.7.1-encoded.txt\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:106
msgid "Make a copy of the `Metal^3^` manifest file that we will edit:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:107
#, no-wrap
msgid "cp /var/lib/rancher/rke2/server/manifests/metal3.yaml ./metal3.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:108
msgid ""
"Change the `chartContent` and `version` configurations of the `Metal^3^` "
"manifest file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:109
#, no-wrap
msgid ""
"sed -i -e \"s|chartContent:.*|chartContent: "
"$(<metal3-chart-0.7.1-encoded.txt)|\" -e \"s|version:.*|version: 0.7.1|\" "
"metal3.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/day2/mgmt-cluster.adoc:110
msgid ""
"If you need to do any additional upgrade changes to the chart (e.g. adding "
"*new* custom chart values), you need to manually edit the `metal3.yaml` "
"file."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:111
msgid "Replace the original `Metal^3^` manifest file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:112
#, no-wrap
msgid "cp metal3.yaml /var/lib/rancher/rke2/server/manifests/\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:116
#, no-wrap
msgid ""
"# Example output\n"
"NAME                              READY   STATUS      RESTARTS   AGE\n"
"helm-install-metal3-7p7bl         0/1     Completed   0          27s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:119
#, no-wrap
msgid ""
"# Example\n"
"kubectl logs helm-install-metal3-7p7bl -n default\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:120 asciidoc/day2/mgmt-cluster.adoc:154
msgid "Verify `Metal^3^` pods are running:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:121 asciidoc/day2/mgmt-cluster.adoc:155
#, no-wrap
msgid "kubectl get pods -n metal3-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:122
#, no-wrap
msgid ""
"# Example output\n"
"NAME                                                     READY   STATUS    "
"RESTARTS      AGE\n"
"baremetal-operator-controller-manager-785f99c884-9z87p   2/2     Running   2 "
"(25m ago)   36m\n"
"metal3-metal3-ironic-96fb66cdd-lkss2                     4/4     Running   0             "
"3m54s\n"
"metal3-metal3-mariadb-55fd44b648-q6zhk                   1/1     Running   0             "
"36m\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:123
msgid "Verify the `HelmChart` resource version is upgraded:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:124
#, no-wrap
msgid "kubectl get helmchart metal3 -n default\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:125
#, no-wrap
msgid ""
"# Example output\n"
"NAME     JOB                   CHART   TARGETNAMESPACE   VERSION   REPO   "
"HELMVERSION   BOOTSTRAP\n"
"metal3   helm-install-metal3           metal3-system     0.7.1                          "
"\n"
msgstr ""

#. type: Title ===
#: asciidoc/day2/mgmt-cluster.adoc:126
#, no-wrap
msgid "Non-EIB deployed helm chart"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:127
msgid ""
"Get the values for the currently running helm chart `.yaml` file and make "
"any changes to them *if necessary*:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:128
#, no-wrap
msgid ""
"helm get values <chart_name> -n <chart_namespace> -o yaml > "
"<chart_name>-values.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:129 asciidoc/day2/mgmt-cluster.adoc:142
#: asciidoc/day2/mgmt-cluster.adoc:152
msgid "Update the helm chart:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:130
#, no-wrap
msgid ""
"# For charts using a chart repository\n"
"helm upgrade <chart_name> <chart_repo_name>/<chart_name> \\\n"
"  --namespace <chart_namespace> \\\n"
"  -f <chart_name>-values.yaml \\\n"
"  --version=X.Y.Z\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:131
#, no-wrap
msgid ""
"# For OCI based charts\n"
"helm upgrade <chart_name> oci://<oci_registry_url>/<chart_name> \\\n"
"  --namespace <chart_namespace> \\\n"
"  -f <chart_name>-values.yaml \\\n"
"  --version=X.Y.Z\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:132
msgid ""
"Verify the chart upgrade. Depending on the chart you may need to verify "
"different resources. For examples of chart upgrades, see the "
"<<day2-mgmt-cluster-helm-chart-upgrade-examples, Examples>> section."
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:135
msgid ""
"<<day2-mgmt-cluster-helm-chart-upgrade-examples-rancher, Rancher>> helm "
"chart"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:136
msgid ""
"<<day2-mgmt-cluster-helm-chart-upgrade-examples-metal3, Metal^3^>> helm "
"chart"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:140 asciidoc/day2/mgmt-cluster.adoc:150
msgid ""
"Get the values for the current Rancher release and print them to a "
"`rancher-values.yaml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:141
#, no-wrap
msgid "helm get values rancher -n cattle-system -o yaml > rancher-values.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:143
#, no-wrap
msgid ""
"helm upgrade rancher rancher-prime/rancher \\\n"
"  --namespace cattle-system \\\n"
"  -f rancher-values.yaml \\\n"
"  --version=2.8.4\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:147
msgid ""
"_For additional information on the Rancher helm chart upgrade, check "
"link:https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster/upgrades[here]._"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:151
#, no-wrap
msgid "helm get values metal3 -n metal3-system -o yaml > metal3-values.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:153
#, no-wrap
msgid ""
"helm upgrade metal3 oci://registry.suse.com/edge/metal3-chart \\\n"
"  --namespace metal3-system \\\n"
"  -f metal3-values.yaml \\\n"
"  --version=0.7.1\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:156
#, no-wrap
msgid ""
"# Example output\n"
"NAME                                                     READY   STATUS    "
"RESTARTS   AGE\n"
"baremetal-operator-controller-manager-785f99c884-fvsx4   2/2     Running   0          "
"12m\n"
"metal3-metal3-ironic-96fb66cdd-j9mgf                     4/4     Running   0          "
"2m41s\n"
"metal3-metal3-mariadb-55fd44b648-7fmvk                   1/1     Running   0          "
"12m\n"
msgstr ""

#. type: Plain text
#: asciidoc/day2/mgmt-cluster.adoc:157
msgid "Verify `Metal^3^` helm release version change:"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:158
#, no-wrap
msgid "helm ls -n metal3-system\n"
msgstr ""

#. type: delimited block -
#: asciidoc/day2/mgmt-cluster.adoc:159
#, no-wrap
msgid ""
"# Expected output\n"
"NAME  \tNAMESPACE    \tREVISION\tUPDATED                                "
"\tSTATUS  \tCHART       \tAPP VERSION\n"
"metal3\tmetal3-system\t2       \t2024-06-17 12:43:06.774802846 +0000 "
"UTC\tdeployed\tmetal3-0.7.1\t1.16.0   \n"
msgstr ""

#. type: Title =
#: asciidoc/day2/downstream-clusters.adoc:1
#, no-wrap
msgid "Downstream clusters"
msgstr ""

#. type: Plain text
#: asciidoc/day2/downstream-clusters.adoc:2
msgid ""
"This section covers how to do various `Day 2` operations for different parts "
"of your downstream cluster using your `management cluster`."
msgstr ""

#. type: Title =
#: asciidoc/product/atip.adoc:1
#, no-wrap
msgid "SUSE Adaptive Telco Infrastructure Platform (ATIP)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:2
msgid ""
"SUSE Adaptive Telco Infrastructure Platform (`ATIP`) is a Telco-optimized "
"edge computing platform that enables telecom companies to innovate and "
"accelerate the modernization of their networks."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:3
msgid ""
"ATIP is a complete Telco cloud stack for hosting CNFs such as 5G Packet Core "
"and Cloud RAN."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:4
msgid ""
"Automates zero-touch rollout and lifecycle management of complex edge stack "
"configurations at Telco scale."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:5
msgid ""
"Continuously assures quality on Telco-grade hardware, using Telco-specific "
"configurations and workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:6
msgid ""
"Consists of components that are purpose-built for the edge and hence have "
"smaller footprint and higher performance per Watt."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip.adoc:7
msgid ""
"Maintains a flexible platform strategy with vendor-neutral APIs and 100% "
"open source."
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-architecture.adoc:1
#, no-wrap
msgid "Concept & Architecture"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:2
msgid ""
"SUSE ATIP is a platform designed for hosting modern, cloud native, Telco "
"applications at scale from core to edge."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:3
msgid ""
"This page explains the architecture and components used in ATIP. Knowledge "
"of this helps deploy and use ATIP."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-architecture.adoc:4
#, no-wrap
msgid "ATIP Architecture"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:5
msgid "The following diagram shows the high-level architecture of ATIP:"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-architecture.adoc:6
#, no-wrap
msgid "product-atip-architecture1.png"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-architecture.adoc:7
#, no-wrap
msgid "Components"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:8
msgid "There are two different blocks, the management stack and the runtime stack:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:9
msgid ""
"*Management stack*: This is the part of ATIP that is used to manage the "
"provision and lifecycle of the runtime stacks. It includes the following "
"components:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:10
msgid ""
"Multi-cluster management in public and private cloud environments with "
"<<components-rancher,Rancher>>"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:11
msgid ""
"Bare-metal support with <<components-metal3,Metal^3^>>, "
"<<components-metallb,MetalLB>> and `CAPI` (Cluster API) infrastructure "
"providers"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:12
msgid "Comprehensive tenant isolation and `IDP` (Identity Provider) integrations"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:13
msgid "Large marketplace of third-party integrations and extensions"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:14
msgid "Vendor-neutral API and rich ecosystem of providers"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:15
msgid "Control the SLE Micro transactional updates"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:16
msgid ""
"GitOps Engine for managing the lifecycle of the clusters using Git "
"repositories with <<components-fleet,Fleet>>"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:17
msgid "*Runtime stack*: This is the part of ATIP that is used to run the workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:18
msgid ""
"Kubernetes with secure and lightweight distributions like "
"<<components-k3s,K3s>> and <<components-rke2,RKE2>> (`RKE2` is hardened, "
"certified and optimized for government use and regulated industries)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:19
msgid ""
"<<components-neuvector,NeuVector>> to enable security features like image "
"vulnerability scanning, deep packet inspection and automatic intra-cluster "
"traffic control."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:20
msgid ""
"Block Storage with <<components-longhorn,Longhorn>> to enable a simple and "
"easy way to use a cloud native storage solution."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:21
msgid ""
"Optimized Operating System with <<components-slmicro,SLE Micro>> to enable a "
"secure, lightweight and immutable (transactional file system) OS for running "
"containers. SLE Micro is available on `aarch64` and `x86_64` architectures, "
"and it also supports `Real-Time Kernel` for Telco and edge use cases."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-architecture.adoc:22
#, no-wrap
msgid "Example deployment flows"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:23
msgid ""
"The following are high-level examples of workflows to understand the "
"relationship between the management and the runtime components."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:24
msgid ""
"Direct network provisioning is the workflow that enables the deployment of a "
"new downstream cluster with all the components preconfigured and ready to "
"run workloads with no manual intervention."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-architecture.adoc:25
#, no-wrap
msgid "Example 1: Deploying a new management cluster with all components installed"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:26
msgid ""
"Using the <<components-eib,Edge Image Builder>> to create a new `ISO` image "
"with the management stack included. You can then use this `ISO` image to "
"install a new management cluster on VMs or bare metal."
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-architecture.adoc:27
#, no-wrap
msgid "product-atip-architecture2.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:28
msgid ""
"For more information about how to deploy a new management cluster, see the "
"<<atip-management-cluster,ATIP Management Cluster guide>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:29
msgid ""
"For more information about how to use the Edge Image Builder, see the "
"<<quickstart-eib,Edge Image Builder guide>>."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-architecture.adoc:30
#, no-wrap
msgid ""
"Example 2: Deploying a single-node downstream cluster with Telco profiles to "
"enable it to run Telco workloads"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:31
msgid ""
"Once we have the management cluster up and running, we can use it to deploy "
"a single-node downstream cluster with all Telco capabilities enabled and "
"configured using the directed network provisioning workflow."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:32
#: asciidoc/product/atip-architecture.adoc:38
msgid "The following diagram shows the high-level workflow to deploy it:"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-architecture.adoc:33
#, no-wrap
msgid "product-atip-architecture3.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:34
#: asciidoc/product/atip-architecture.adoc:40
msgid ""
"For more information about how to deploy a downstream cluster, see the "
"<<atip-automated-provisioning,ATIP Automated Provisioning guide.>>"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:35
msgid ""
"For more information about Telco features, see the <<atip-features,ATIP "
"Telco Features guide.>>"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-architecture.adoc:36
#, no-wrap
msgid ""
"Example 3: Deploying a high availability downstream cluster using MetalLB as "
"a Load Balancer"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:37
msgid ""
"Once we have the management cluster up and running, we can use it to deploy "
"a high availability downstream cluster with `MetalLB` as a load balancer "
"using the directed network provisioning workflow."
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-architecture.adoc:39
#, no-wrap
msgid "product-atip-architecture4.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-architecture.adoc:41
msgid "For more information about `MetalLB`, see <<components-metallb,here:>>"
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-requirements.adoc:1
#, no-wrap
msgid "Requirements & Assumptions"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-requirements.adoc:2
#, no-wrap
msgid "Hardware"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:3
msgid ""
"The hardware requirements for the ATIP nodes are based on the following "
"components:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:4
msgid ""
"**Management cluster**: The management cluster contains components like `SLE "
"Micro`, `RKE2`, `Rancher Prime`, `Metal^3^`, and it is used to manage "
"several downstream clusters. Depending on the number of downstream clusters "
"to be managed, the hardware requirements for the server could vary."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:5
msgid "Minimum requirements for the server (`VM` or `Bare Metal`) are:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:6
msgid "RAM: 8 GB Minimum (we recommend at least 16 GB)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:7
msgid "CPU: 2 Minimum (we recommend at least 4 CPU)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:8
msgid ""
"**Downstream clusters**: The downstream clusters are the clusters deployed "
"on the ATIP nodes to run Telco workloads. Specific requirements are needed "
"to enable certain Telco capabilities like `SR-IOV`, `CPU Performance "
"Optimization`, etc."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:9
msgid ""
"SR-IOV: to attach VFs (Virtual Functions) in pass-through mode to CNFs/VNFs, "
"the NIC must support SR-IOV and VT-d/AMD-Vi be enabled in the BIOS."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:10
msgid ""
"CPU Processors: To run specific Telco workloads, the CPU Processor model "
"should be adapted to enable most of the features available in this reference "
"<<atip-features,table>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:11
msgid "Firmware requirements for installing with virtual media:"
msgstr ""

#. type: Table
#: asciidoc/product/atip-requirements.adoc:12
#, no-wrap
msgid ""
"| Server Hardware | BMC Model | Management\n"
"| Dell hardware\n"
"| 15th Generation\n"
"| iDRAC9\n"
"\n"
"| Supermicro hardware\n"
"| 01.00.25\n"
"| Supermicro SMC - redfish\n"
"\n"
"| HPE hardware\n"
"| 1.50\n"
"| iLO6\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-requirements.adoc:13
#, no-wrap
msgid "Network"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:14
msgid ""
"As a reference for the network architecture, the following diagram shows a "
"typical network architecture for a Telco environment:"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-requirements.adoc:15
#, no-wrap
msgid "product-atip-requirement1.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:16
msgid "The network architecture is based on the following components:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:17
msgid ""
"**Management network**: This network is used for the management of the ATIP "
"nodes. It is used for the out-of-band management. Usually, this network is "
"also connected to a separate management switch, but it can be connected to "
"the same service switch using VLANs to isolate the traffic."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:18
msgid ""
"**Control-plane network**: This network is used for the communication "
"between the ATIP nodes and the services that are running on them. This "
"network is also used for the communication between the ATIP nodes and the "
"external services, like the `DHCP` or `DNS` servers. In some cases, for "
"connected environments, the switch/router can handle traffic through the "
"Internet."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:19
msgid ""
"**Other networks**: In some cases, the ATIP nodes could be connected to "
"other networks for specific customer purposes."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-requirements.adoc:20
msgid ""
"To use the directed network provisioning workflow, the management cluster "
"must have network connectivity to the downstream cluster server Baseboard "
"Management Controller (BMC) so that host preparation and provisioning can be "
"automated."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-requirements.adoc:21
#, no-wrap
msgid "Services (DHCP, DNS, etc.)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:22
msgid ""
"Some external services like `DHCP`, `DNS`, etc. could be required depending "
"on the kind of environment where they are deployed:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:23
msgid ""
"**Connected environment**: In this case, the ATIP nodes will be connected to "
"the Internet (via routing L3 protocols) and the external services will be "
"provided by the customer."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:24
msgid ""
"**Disconnected / air-gap environment**: In this case, the ATIP nodes will "
"not have Internet IP connectivity and additional services will be required "
"to locally mirror content required by the ATIP directed network provisioning "
"workflow."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:25
msgid ""
"**File server**: A file server is used to store the ISO images to be "
"provisioned on the ATIP nodes during the directed network provisioning "
"workflow. The `metal^3^` Helm chart can deploy a media server to store the "
"ISO images — check the following xref:metal3-media-server[section], but it "
"is also possible to use an existing local webserver."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-requirements.adoc:26
#, no-wrap
msgid "Disabling rebootmgr"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:27
msgid ""
"`rebootmgr` is a service which allows to configure a strategy for reboot "
"when the system has pending updates.  For Telco workloads, it is really "
"important to disable or configure properly the `rebootmgr` service to avoid "
"the reboot of the nodes in case of updates scheduled by the system, to avoid "
"any impact on the services running on the nodes."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-requirements.adoc:28
msgid ""
"For more information about `rebootmgr`, see "
"https://github.com/SUSE/rebootmgr[rebootmgr GitHub repository]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:29
msgid "Verify the strategy being used by running:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-requirements.adoc:30
#, no-wrap
msgid ""
"cat /etc/rebootmgr.conf\n"
"[rebootmgr]\n"
"window-start=03:30\n"
"window-duration=1h30m\n"
"strategy=best-effort\n"
"lock-group=default\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:31
msgid "and you could disable it by running:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-requirements.adoc:32
#, no-wrap
msgid "sed -i 's/strategy=best-effort/strategy=off/g' /etc/rebootmgr.conf\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-requirements.adoc:33
msgid "or using the `rebootmgrctl` command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-requirements.adoc:34
#, no-wrap
msgid "rebootmgrctl strategy off\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-requirements.adoc:35
msgid ""
"This configuration to set the `rebootmgr` strategy can be automated using "
"the directed network provisioning workflow. For more information, check the "
"<<atip-automated-provisioning,ATIP Automated Provisioning documentation>>."
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-management-cluster.adoc:1
#, no-wrap
msgid "Setting up the management cluster"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-management-cluster.adoc:2
#: asciidoc/product/atip-automated-provision.adoc:2
#, no-wrap
msgid "Introduction"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:3
msgid ""
"The management cluster is the part of ATIP that is used to manage the "
"provision and lifecycle of the runtime stacks.  From a technical point of "
"view, the management cluster contains the following components:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:4
msgid ""
"`SUSE Linux Enterprise Micro` as the OS. Depending on the use case, some "
"configurations like networking, storage, users and kernel arguments can be "
"customized."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:5
msgid ""
"`RKE2` as the Kubernetes cluster. Depending on the use case, it can be "
"configured to use specific CNI plugins, such as `Multus`, `Cilium`, etc."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:6
msgid ""
"`Rancher` as the management platform to manage the lifecycle of the "
"clusters."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:7
msgid "`Metal^3^` as the component to manage the lifecycle of the bare metal nodes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:8
msgid ""
"`CAPI` as the component to manage the lifecycle of the Kubernetes clusters "
"(downstream clusters). With ATIP, also the `RKE2 CAPI Provider` is used to "
"manage the lifecycle of the RKE2 clusters (downstream clusters)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:9
msgid ""
"With all components mentioned above, the management cluster can manage the "
"lifecycle of downstream clusters, using a declarative approach to manage the "
"infrastructure and applications."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:10
msgid ""
"For more information about `SUSE Linux Enterprise Micro`, see: "
"<<components-slmicro,SLE Micro>>"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:11
msgid "For more information about `RKE2`, see: <<components-rke2,RKE2>>"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:12
msgid "For more information about `Rancher`, see: <<components-rancher,Rancher>>"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:13
msgid "For more information about `Metal^3^`, see: <<components-metal3,Metal^3^>>"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-management-cluster.adoc:14
#, no-wrap
msgid "Steps to set up the management cluster"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:15
msgid ""
"The following steps are necessary to set up the management cluster (using a "
"single node):"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-management-cluster.adoc:16
#, no-wrap
msgid "product-atip-mgmtcluster1.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:17
msgid ""
"There are three main steps to set up the management cluster using a "
"declarative approach:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:18
msgid ""
"**xref:mgmt-cluster-image-preparation-connected[Image preparation for "
"connected environments]**: The first step is to prepare the manifests and "
"files with all the necessary configurations to be used in connected "
"environments."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:19
msgid ""
"xref:mgmt-cluster-directory-structure[Directory structure for connected "
"environments]: This step creates a directory structure to be used by Edge "
"Image Builder to store the configuration files and the image itself."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:20
msgid ""
"xref:mgmt-cluster-image-definition-file[Management cluster definition file]: "
"The `mgmt-cluster.yaml` file is the main definition file for the management "
"cluster. It contains the following information about the image to be "
"created:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:21
msgid ""
"Image Information: The information related to the image to be created using "
"the base image."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:22
msgid ""
"Operating system: The operating system configurations to be used in the "
"image."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:23
msgid ""
"Kubernetes: Helm charts and repositories, kubernetes version, network "
"configuration, and the nodes to be used in the cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:24
msgid ""
"xref:mgmt-cluster-custom-folder[Custom folder]: The `custom` folder contains "
"the configuration files and scripts to be used by Edge Image Builder to "
"deploy a fully functional management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:25
msgid ""
"Files: Contains the configuration files to be used by the management "
"cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:26
msgid "Scripts: Contains the scripts to be used by the management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:27
msgid ""
"xref:mgmt-cluster-kubernetes-folder[Kubernetes folder]: The `kubernetes` "
"folder contains the configuration files to be used by the management "
"cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:28
msgid "Manifests: Contains the manifests to be used by the management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:29
msgid "Helm: Contains the Helm charts to be used by the management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:30
msgid ""
"Config: Contains the configuration files to be used by the management "
"cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:31
msgid ""
"xref:mgmt-cluster-network-folder[Network folder]: The `network` folder "
"contains the network configuration files to be used by the management "
"cluster nodes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:32
msgid ""
"**xref:mgmt-cluster-image-preparation-airgap[Image preparation for air-gap "
"environments]**: The step is to show the differences to prepare the "
"manifests and files to be used in an air-gap scenario."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:33
msgid ""
"xref:mgmt-cluster-directory-structure-airgap[Directory structure for air-gap "
"environments]: The directory structure must be modified to include the "
"resources needed to run the management cluster in an air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:34
msgid ""
"xref:mgmt-cluster-image-definition-file-airgap[Modifications in the "
"definition file]: The `mgmt-cluster.yaml` file must be modified to include "
"the `embeddedArtifactRegistry` section with the `images` field set to all "
"container images to be included into the EIB output image."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:35
msgid ""
"xref:mgmt-cluster-custom-folder-airgap[Modifications in the custom folder]: "
"The `custom` folder must be modified to include the resources needed to run "
"the management cluster in an air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:36
msgid ""
"Register script: The `custom/scripts/99-register.sh` script must be removed "
"when you use an air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:37
msgid ""
"Air-gap resources: The `custom/files/airgap-resources.tar.gz` file must be "
"included in the `custom/files` folder with all the resources needed to run "
"the management cluster in an air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:38
msgid ""
"Scripts: The `custom/scripts/99-mgmt-setup.sh` script must be modified to "
"extract and copy the `airgap-resources.tar.gz` file to the final "
"location. The `custom/files/metal3.sh` script must be modified to use the "
"local resources included in the `airgap-resources.tar.gz` file instead of "
"downloading them from the internet."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:39
msgid ""
"**xref:mgmt-cluster-image-creation[Image creation]**: This step covers the "
"creation of the image using the Edge Image Builder tool (for both, connected "
"and air-gap scenarios). Check the <<components-eib,prerequisites>> to run "
"the Edge Image Builder tool on your system."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:40
msgid ""
"**xref:mgmt-cluster-provision[Management Cluster Provision]**: This step "
"covers the provisioning of the management cluster using the image created in "
"the previous step (for both, connected and air-gap scenarios). This step can "
"be done using a laptop, server, VM or any other x86_64 system with a USB "
"port."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:41
msgid ""
"For more information about Edge Image Builder, see <<components-eib,Edge "
"Image Builder>> and <<quickstart-eib,Edge Image Builder Quick Start>>."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-management-cluster.adoc:42
#, no-wrap
msgid "Image preparation for connected environments"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:43
msgid ""
"Using Edge Image Builder to create the image for the management cluster, a "
"lot of configurations can be customized, but in this document, we cover the "
"minimal configurations necessary to set up the management cluster.  Edge "
"Image Builder is typically run from inside a container so, if you do not "
"already have a way to run containers, we need to start by installing a "
"container runtime such as https://podman.io[Podman] or "
"https://rancherdesktop.io[Rancher Desktop]. For this guide, we assume you "
"already have a container runtime available."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:44
msgid ""
"Also, as a prerequisite to deploy a highly available management cluster, you "
"need to reserve three IPs in your network:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:45
msgid "`apiVIP` for the API VIP Address (used to access the Kubernetes API server)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:46
msgid ""
"`ingressVIP` for the Ingress VIP Address (consumed, for example, by the "
"Rancher UI)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:47
msgid "`metal3VIP` for the Metal3 VIP Address."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:48
#, no-wrap
msgid "Directory structure"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:49
msgid ""
"When running EIB, a directory is mounted from the host, so the first thing "
"to do is to create a directory structure to be used by EIB to store the "
"configuration files and the image itself.  This directory has the following "
"structure:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:50
#, no-wrap
msgid ""
"eib\n"
"├── mgmt-cluster.yaml\n"
"├── network\n"
"│ └── mgmt-cluster-node1.yaml\n"
"├── kubernetes\n"
"│ ├── manifests\n"
"│ │ ├── rke2-ingress-config.yaml\n"
"│ │ ├── neuvector-namespace.yaml\n"
"│ │ ├── ingress-l2-adv.yaml\n"
"│ │ └── ingress-ippool.yaml\n"
"│ ├── helm\n"
"│ │ └── values\n"
"│ │     ├── rancher.yaml\n"
"│ │     ├── neuvector.yaml\n"
"│ │     ├── metal3.yaml\n"
"│ │     └── certmanager.yaml\n"
"│ └── config\n"
"│     └── server.yaml\n"
"├── custom\n"
"│ ├── scripts\n"
"│ │ ├── 99-register.sh\n"
"│ │ ├── 99-mgmt-setup.sh\n"
"│ │ └── 99-alias.sh\n"
"│ └── files\n"
"│     ├── rancher.sh\n"
"│     ├── mgmt-stack-setup.service\n"
"│     ├── metal3.sh\n"
"│     └── basic-setup.sh\n"
"└── base-images\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:51
msgid ""
"The image `SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso` must "
"be downloaded from the https://scc.suse.com/[SUSE Customer Center] or the "
"https://www.suse.com/download/sle-micro/[SUSE Download page], and it must be "
"located under the `base-images` folder."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:52
#: asciidoc/product/atip-management-cluster.adoc:201
msgid ""
"You should check the SHA256 checksum of the image to ensure it has not been "
"tampered with. The checksum can be found in the same location where the "
"image was downloaded."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:53
#: asciidoc/product/atip-management-cluster.adoc:202
msgid ""
"An example of the directory structure can be found in the "
"https://github.com/suse-edge/atip[SUSE Edge GitHub repository under the "
"\"telco-examples\" folder]."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:54
#, no-wrap
msgid "Management cluster definition file"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:55
msgid ""
"The `mgmt-cluster.yaml` file is the main definition file for the management "
"cluster. It contains the following information:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:56
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  outputImageName: eib-mgmt-cluster-image.iso\n"
"operatingSystem:\n"
"  isoConfiguration:\n"
"    installDevice: /dev/sda\n"
"  users:\n"
"  - username: root\n"
"    encryptedPassword: ${ROOT_PASSWORD}\n"
"  packages:\n"
"    packageList:\n"
"    - git\n"
"    - jq\n"
"    sccRegistrationCode: ${SCC_REGISTRATION_CODE}\n"
"kubernetes:\n"
"  version: ${KUBERNETES_VERSION}\n"
"  helm:\n"
"    charts:\n"
"      - name: cert-manager\n"
"        repositoryName: jetstack\n"
"        version: 1.14.2\n"
"        targetNamespace: cert-manager\n"
"        valuesFile: certmanager.yaml\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn-crd\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: metal3-chart\n"
"        version: 0.7.1\n"
"        repositoryName: suse-edge-charts\n"
"        targetNamespace: metal3-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: metal3.yaml\n"
"      - name: neuvector-crd\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: neuvector\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: rancher\n"
"        version: 2.8.4\n"
"        repositoryName: rancher-prime\n"
"        targetNamespace: cattle-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: rancher.yaml\n"
"    repositories:\n"
"      - name: jetstack\n"
"        url: https://charts.jetstack.io\n"
"      - name: rancher-charts\n"
"        url: https://charts.rancher.io/\n"
"      - name: suse-edge-charts\n"
"        url: oci://registry.suse.com/edge\n"
"      - name: rancher-prime\n"
"        url: https://charts.rancher.com/server-charts/prime\n"
"    network:\n"
"      apiHost: ${API_HOST}\n"
"      apiVIP: ${API_VIP}\n"
"    nodes:\n"
"      - hostname: mgmt-cluster-node1\n"
"        initializer: true\n"
"        type: server\n"
"#     - hostname: mgmt-cluster-node2\n"
"#       initializer: true\n"
"#       type: server\n"
"#     - hostname: mgmt-cluster-node3\n"
"#       initializer: true\n"
"#       type: server\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:57
msgid ""
"To explain the fields and values in the `mgmt-cluster.yaml` definition file, "
"we have divided it into the following sections."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:58
msgid "Image section (definition file):"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:59
#, no-wrap
msgid ""
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  outputImageName: eib-mgmt-cluster-image.iso\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:60
msgid ""
"where the `baseImage` is the original image you downloaded from the SUSE "
"Customer Center or the SUSE Download page. `outputImageName` is the name of "
"the new image that will be used to provision the management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:61
msgid "Operating system section (definition file):"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:62
#, no-wrap
msgid ""
"operatingSystem:\n"
"  isoConfiguration:\n"
"    installDevice: /dev/sda\n"
"  users:\n"
"  - username: root\n"
"    encryptedPassword: ${ROOT_PASSWORD}\n"
"  packages:\n"
"    packageList:\n"
"    - jq\n"
"    sccRegistrationCode: ${SCC_REGISTRATION_CODE}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:63
msgid ""
"where the `installDevice` is the device to be used to install the operating "
"system, the `username` and `encryptedPassword` are the credentials to be "
"used to access the system, the `packageList` is the list of packages to be "
"installed (`jq` is required internally during the installation process), and "
"the `sccRegistrationCode` is the registration code used to get the packages "
"and dependencies at build time and can be obtained from the SUSE Customer "
"Center.  The encrypted password can be generated using the `openssl` command "
"as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:64
#, no-wrap
msgid "openssl passwd -6 MyPassword!123\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:65
msgid "This outputs something similar to:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:66
#, no-wrap
msgid "$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:67
msgid "Kubernetes section (definition file):"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:68
#, no-wrap
msgid ""
"kubernetes:\n"
"  version: ${KUBERNETES_VERSION}\n"
"  helm:\n"
"    charts:\n"
"      - name: cert-manager\n"
"        repositoryName: jetstack\n"
"        version: 1.14.2\n"
"        targetNamespace: cert-manager\n"
"        valuesFile: certmanager.yaml\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn-crd\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: metal3-chart\n"
"        version: 0.7.1\n"
"        repositoryName: suse-edge-charts\n"
"        targetNamespace: metal3-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: metal3.yaml\n"
"      - name: neuvector-crd\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: neuvector\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: rancher\n"
"        version: 2.8.4\n"
"        repositoryName: rancher-prime\n"
"        targetNamespace: cattle-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: rancher.yaml\n"
"    repositories:\n"
"      - name: jetstack\n"
"        url: https://charts.jetstack.io\n"
"      - name: rancher-charts\n"
"        url: https://charts.rancher.io/\n"
"      - name: suse-edge-charts\n"
"        url: oci://registry.suse.com/edge\n"
"      - name: rancher-prime\n"
"        url: https://charts.rancher.com/server-charts/prime\n"
"    network:\n"
"      apiHost: ${API_HOST}\n"
"      apiVIP: ${API_VIP}\n"
"    nodes:\n"
"      - hostname: mgmt-cluster1\n"
"        initializer: true\n"
"        type: server\n"
"#      - hostname: mgmt-cluster2\n"
"#        type: server\n"
"#      - hostname: mgmt-cluster3\n"
"#        type: server\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:69
msgid ""
"where `version` is the version of Kubernetes to be installed. In our case, "
"we are using an RKE2 cluster, so the version must be minor less than 1.29 to "
"be compatible with `Rancher` (for example, `v1.28.9+rke2r1`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:70
msgid ""
"The `helm` section contains the list of Helm charts to be installed, the "
"repositories to be used, and the version configuration for all of them."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:71
msgid ""
"The `network` section contains the configuration for the network, like the "
"`apiHost` and `apiVIP` to be used by the `RKE2` component.  The `apiVIP` "
"should be an IP address that is not used in the network and should not be "
"part of the DHCP pool (in case we use DHCP). Also, when we use the `apiVIP` "
"in a multi-node cluster, it is used to access the Kubernetes API server.  "
"The `apiHost` is the name resolution to `apiVIP` to be used by the `RKE2` "
"component."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:72
msgid ""
"The `nodes` section contains the list of nodes to be used in the "
"cluster. The `nodes` section contains the list of nodes to be used in the "
"cluster. In this example, a single-node cluster is being used, but it can be "
"extended to a multi-node cluster by adding more nodes to the list (by "
"uncommenting the lines)."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:73
msgid ""
"The names of the nodes must be unique in the cluster, and the `initializer` "
"field mustbe set to `true` for the first node in the list.  The names of the "
"nodes must be the same as the host names defined in the `network` section "
"matching directly with the file name in the `network` section."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:74
#, no-wrap
msgid "Custom folder"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:75
msgid "The `custom` folder contains the following subfolders:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:76
#, no-wrap
msgid ""
"...\n"
"├── custom\n"
"│ ├── scripts\n"
"│ │ ├── 99-register.sh\n"
"│ │ ├── 99-mgmt-setup.sh\n"
"│ │ └── 99-alias.sh\n"
"│ └── files\n"
"│     ├── rancher.sh\n"
"│     ├── mgmt-stack-setup.service\n"
"│     ├── metal3.sh\n"
"│     └── basic-setup.sh\n"
"...\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:77
msgid ""
"The `custom/files` folder contains the configuration files to be used by the "
"management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:78
msgid ""
"The `custom/scripts` folder contains the scripts to be used by the "
"management cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:79
msgid "The `custom/files` folder contains the following files:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:80
msgid ""
"`basic-setup.sh`: contains the configuration parameters about the `Metal^3^` "
"version to be used, as well as the `Rancher` and `MetalLB` basic "
"parameters. Only modify this file if you want to change the versions of the "
"components or the namespaces to be used."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:81
#, no-wrap
msgid ""
"#!/bin/bash\n"
"# Pre-requisites. Cluster already running\n"
"export KUBECTL=\"/var/lib/rancher/rke2/bin/kubectl\"\n"
"export KUBECONFIG=\"/etc/rancher/rke2/rke2.yaml\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:82
#, no-wrap
msgid ""
"##################\n"
"# METAL3 DETAILS #\n"
"##################\n"
"export METAL3_CHART_TARGETNAMESPACE=\"metal3-system\"\n"
"export METAL3_CLUSTERCTLVERSION=\"1.6.2\"\n"
"export METAL3_CAPICOREVERSION=\"1.6.2\"\n"
"export METAL3_CAPIMETAL3VERSION=\"1.6.0\"\n"
"export METAL3_CAPIRKE2VERSION=\"0.2.6\"\n"
"export METAL3_CAPIPROVIDER=\"rke2\"\n"
"export METAL3_CAPISYSTEMNAMESPACE=\"capi-system\"\n"
"export METAL3_RKE2BOOTSTRAPNAMESPACE=\"rke2-bootstrap-system\"\n"
"export METAL3_CAPM3NAMESPACE=\"capm3-system\"\n"
"export METAL3_RKE2CONTROLPLANENAMESPACE=\"rke2-control-plane-system\"\n"
"export METAL3_CAPI_IMAGES=\"registry.suse.com/edge\"\n"
"# Or registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse for "
"the upstream ones\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:83
#, no-wrap
msgid ""
"###########\n"
"# METALLB #\n"
"###########\n"
"export METALLBNAMESPACE=\"metallb-system\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:84
#, no-wrap
msgid ""
"###########\n"
"# RANCHER #\n"
"###########\n"
"export RANCHER_CHART_TARGETNAMESPACE=\"cattle-system\"\n"
"export RANCHER_FINALPASSWORD=\"adminadminadmin\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:85
#, no-wrap
msgid ""
"die(){\n"
"  echo ${1} 1>&2\n"
"  exit ${2}\n"
"}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:86
msgid ""
"`metal3.sh`: contains the configuration for the `Metal^3^` component to be "
"used (no modifications needed). In future versions, this script will be "
"replaced to use instead `Rancher Turtles` to make it easy."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:87
#: asciidoc/product/atip-management-cluster.adoc:113
#: asciidoc/product/atip-management-cluster.adoc:140
#: asciidoc/product/atip-management-cluster.adoc:216
#, no-wrap
msgid ""
"#!/bin/bash\n"
"set -euo pipefail\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:88
#: asciidoc/product/atip-management-cluster.adoc:114
#: asciidoc/product/atip-management-cluster.adoc:217
#, no-wrap
msgid ""
"BASEDIR=\"$(dirname \"$0\")\"\n"
"source ${BASEDIR}/basic-setup.sh\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:89
#: asciidoc/product/atip-management-cluster.adoc:218
#, no-wrap
msgid ""
"METAL3LOCKNAMESPACE=\"default\"\n"
"METAL3LOCKCMNAME=\"metal3-lock\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:90
#: asciidoc/product/atip-management-cluster.adoc:117
#: asciidoc/product/atip-management-cluster.adoc:219
#, no-wrap
msgid "trap 'catch $? $LINENO' EXIT\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:91
#: asciidoc/product/atip-management-cluster.adoc:220
#, no-wrap
msgid ""
"catch() {\n"
"  if [ \"$1\" != \"0\" ]; then\n"
"    echo \"Error $1 occurred on $2\"\n"
"    ${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n "
"${METAL3LOCKNAMESPACE}\n"
"  fi\n"
"}\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:92
#: asciidoc/product/atip-management-cluster.adoc:221
#, no-wrap
msgid ""
"# Get or create the lock to run all those steps just in a single node\n"
"# As the first node is created WAY before the others, this should be "
"enough\n"
"# TODO: Investigate if leases is better\n"
"if [ $(${KUBECTL} get cm -n ${METAL3LOCKNAMESPACE} ${METAL3LOCKCMNAME} -o "
"name | wc -l) -lt 1 ]; then\n"
"  ${KUBECTL} create configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE} "
"--from-literal foo=bar\n"
"else\n"
"  exit 0\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:93
#: asciidoc/product/atip-management-cluster.adoc:222
#, no-wrap
msgid ""
"# Wait for metal3\n"
"while ! ${KUBECTL} wait --for condition=ready -n "
"${METAL3_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n "
"${METAL3_CHART_TARGETNAMESPACE} -l app.kubernetes.io/name=metal3-ironic -o "
"name) --timeout=10s; do sleep 2 ; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:94
#, no-wrap
msgid ""
"# Get the ironic IP\n"
"IRONICIP=$(${KUBECTL} get cm -n ${METAL3_CHART_TARGETNAMESPACE} ironic-bmo "
"-o jsonpath='{.data.IRONIC_IP}')\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:95
#, no-wrap
msgid ""
"# If LoadBalancer, use metallb, else it is NodePort\n"
"if [ $(${KUBECTL} get svc -n ${METAL3_CHART_TARGETNAMESPACE} "
"metal3-metal3-ironic -o jsonpath='{.spec.type}') == \"LoadBalancer\" ]; "
"then\n"
"  # Wait for metallb\n"
"  while ! ${KUBECTL} wait --for condition=ready -n ${METALLBNAMESPACE} "
"$(${KUBECTL} get pods -n ${METALLBNAMESPACE} -l "
"app.kubernetes.io/component=controller -o name) --timeout=10s; do sleep 2 ; "
"done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:96
#, no-wrap
msgid ""
"  # Do not create the ippool if already created\n"
"  ${KUBECTL} get ipaddresspool -n ${METALLBNAMESPACE} ironic-ip-pool -o name "
"|| cat <<-EOF | ${KUBECTL} apply -f -\n"
"  apiVersion: metallb.io/v1beta1\n"
"  kind: IPAddressPool\n"
"  metadata:\n"
"    name: ironic-ip-pool\n"
"    namespace: ${METALLBNAMESPACE}\n"
"  spec:\n"
"    addresses:\n"
"    - ${IRONICIP}/32\n"
"    serviceAllocation:\n"
"      priority: 100\n"
"      serviceSelectors:\n"
"      - matchExpressions:\n"
"        - {key: app.kubernetes.io/name, operator: In, values: "
"[metal3-ironic]}\n"
"\tEOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:97
#, no-wrap
msgid ""
"  # Same for L2 Advs\n"
"  ${KUBECTL} get L2Advertisement -n ${METALLBNAMESPACE} "
"ironic-ip-pool-l2-adv -o name || cat <<-EOF | ${KUBECTL} apply -f -\n"
"  apiVersion: metallb.io/v1beta1\n"
"  kind: L2Advertisement\n"
"  metadata:\n"
"    name: ironic-ip-pool-l2-adv\n"
"    namespace: ${METALLBNAMESPACE}\n"
"  spec:\n"
"    ipAddressPools:\n"
"    - ironic-ip-pool\n"
"\tEOF\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:98
#, no-wrap
msgid ""
"# If clusterctl is not installed, install it\n"
"if ! command -v clusterctl > /dev/null 2>&1; then\n"
"  LINUXARCH=$(uname -m)\n"
"  case $(uname -m) in\n"
"    \"x86_64\")\n"
"      export GOARCH=\"amd64\" ;;\n"
"    \"aarch64\")\n"
"      export GOARCH=\"arm64\" ;;\n"
"    \"*\")\n"
"      echo \"Arch not found, asumming amd64\"\n"
"      export GOARCH=\"amd64\" ;;\n"
"  esac\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:99
#, no-wrap
msgid ""
"  # Clusterctl bin\n"
"  # Maybe just use the binary from hauler if available\n"
"  curl -L "
"https://github.com/kubernetes-sigs/cluster-api/releases/download/v${METAL3_CLUSTERCTLVERSION}/clusterctl-linux-${GOARCH} "
"-o /usr/local/bin/clusterctl\n"
"  chmod +x /usr/local/bin/clusterctl\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:100
#: asciidoc/product/atip-management-cluster.adoc:223
#, no-wrap
msgid ""
"# If rancher is deployed\n"
"if [ $(${KUBECTL} get pods -n ${RANCHER_CHART_TARGETNAMESPACE} -l "
"app=rancher -o name | wc -l) -ge 1 ]; then\n"
"  cat <<-EOF | ${KUBECTL} apply -f -\n"
"\tapiVersion: management.cattle.io/v3\n"
"\tkind: Feature\n"
"\tmetadata:\n"
"\t  name: embedded-cluster-api\n"
"\tspec:\n"
"\t  value: false\n"
"\tEOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:101
#: asciidoc/product/atip-management-cluster.adoc:224
#, no-wrap
msgid ""
"  # Disable Rancher webhooks for CAPI\n"
"  ${KUBECTL} delete "
"mutatingwebhookconfiguration.admissionregistration.k8s.io "
"mutating-webhook-configuration\n"
"  ${KUBECTL} delete "
"validatingwebhookconfigurations.admissionregistration.k8s.io "
"validating-webhook-configuration\n"
"  ${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system "
"--timeout=300s\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:102
#: asciidoc/product/atip-management-cluster.adoc:225
#, no-wrap
msgid ""
"# Deploy CAPI\n"
"if [ $(${KUBECTL} get pods -n ${METAL3_CAPISYSTEMNAMESPACE} -o name | wc -l) "
"-lt 1 ]; then\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:103
#, no-wrap
msgid ""
"  # "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2#setting-up-clusterctl\n"
"  mkdir -p ~/.cluster-api\n"
"  cat <<-EOF > ~/.cluster-api/clusterctl.yaml\n"
"\timages:\n"
"\t  all:\n"
"\t    repository: ${METAL3_CAPI_IMAGES}\n"
"\tEOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:104
#, no-wrap
msgid ""
"  # Try this command 3 times just in case, stolen from "
"https://stackoverflow.com/a/33354419\n"
"  if ! (r=3; while ! clusterctl init \\\n"
"    --core \"cluster-api:v${METAL3_CAPICOREVERSION}\"\\\n"
"    --infrastructure \"metal3:v${METAL3_CAPIMETAL3VERSION}\"\\\n"
"    --bootstrap \"${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}\"\\\n"
"    --control-plane \"${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}\" ; "
"do\n"
"            ((--r))||exit\n"
"            echo \"Something went wrong, let's wait 10 seconds and retry\"\n"
"            sleep 10;done) ; then\n"
"      echo \"clusterctl failed\"\n"
"      exit 1\n"
"  fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:105
#: asciidoc/product/atip-management-cluster.adoc:227
#, no-wrap
msgid ""
"  # Wait for capi-controller-manager\n"
"  while ! ${KUBECTL} wait --for condition=ready -n "
"${METAL3_CAPISYSTEMNAMESPACE} $(${KUBECTL} get pods -n "
"${METAL3_CAPISYSTEMNAMESPACE} -l cluster.x-k8s.io/provider=cluster-api -o "
"name) --timeout=10s; do sleep 2 ; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:106
#: asciidoc/product/atip-management-cluster.adoc:228
#, no-wrap
msgid ""
"  # Wait for capm3-controller-manager, there are two pods, the ipam and the "
"capm3 one, just wait for the first one\n"
"  while ! ${KUBECTL} wait --for condition=ready -n ${METAL3_CAPM3NAMESPACE} "
"$(${KUBECTL} get pods -n ${METAL3_CAPM3NAMESPACE} -l "
"cluster.x-k8s.io/provider=infrastructure-metal3 -o name | head -n1 ) "
"--timeout=10s; do sleep 2 ; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:107
#: asciidoc/product/atip-management-cluster.adoc:229
#, no-wrap
msgid ""
"  # Wait for rke2-bootstrap-controller-manager\n"
"  while ! ${KUBECTL} wait --for condition=ready -n "
"${METAL3_RKE2BOOTSTRAPNAMESPACE} $(${KUBECTL} get pods -n "
"${METAL3_RKE2BOOTSTRAPNAMESPACE} -l cluster.x-k8s.io/provider=bootstrap-rke2 "
"-o name) --timeout=10s; do sleep 2 ; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:108
#: asciidoc/product/atip-management-cluster.adoc:230
#, no-wrap
msgid ""
"  # Wait for rke2-control-plane-controller-manager\n"
"  while ! ${KUBECTL} wait --for condition=ready -n "
"${METAL3_RKE2CONTROLPLANENAMESPACE} $(${KUBECTL} get pods -n "
"${METAL3_RKE2CONTROLPLANENAMESPACE} -l "
"cluster.x-k8s.io/provider=control-plane-rke2 -o name) --timeout=10s; do "
"sleep 2 ; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:109
#: asciidoc/product/atip-management-cluster.adoc:231
#, no-wrap
msgid "fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:110
#: asciidoc/product/atip-management-cluster.adoc:232
#, no-wrap
msgid "# Clean up the lock cm\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:111
#: asciidoc/product/atip-management-cluster.adoc:233
#, no-wrap
msgid "${KUBECTL} delete configmap ${METAL3LOCKCMNAME} -n ${METAL3LOCKNAMESPACE}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:112
msgid ""
"`rancher.sh`: contains the configuration for the `Rancher` component to be "
"used (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:115
#, no-wrap
msgid ""
"RANCHERLOCKNAMESPACE=\"default\"\n"
"RANCHERLOCKCMNAME=\"rancher-lock\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:116
#, no-wrap
msgid ""
"if [ -z \"${RANCHER_FINALPASSWORD}\" ]; then\n"
"  # If there is no final password, then finish the setup right away\n"
"  exit 0\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:118
#, no-wrap
msgid ""
"catch() {\n"
"  if [ \"$1\" != \"0\" ]; then\n"
"    echo \"Error $1 occurred on $2\"\n"
"    ${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n "
"${RANCHERLOCKNAMESPACE}\n"
"  fi\n"
"}\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:119
#, no-wrap
msgid ""
"# Get or create the lock to run all those steps just in a single node\n"
"# As the first node is created WAY before the others, this should be "
"enough\n"
"# TODO: Investigate if leases is better\n"
"if [ $(${KUBECTL} get cm -n ${RANCHERLOCKNAMESPACE} ${RANCHERLOCKCMNAME} -o "
"name | wc -l) -lt 1 ]; then\n"
"  ${KUBECTL} create configmap ${RANCHERLOCKCMNAME} -n "
"${RANCHERLOCKNAMESPACE} --from-literal foo=bar\n"
"else\n"
"  exit 0\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:120
#, no-wrap
msgid ""
"# Wait for rancher to be deployed\n"
"while ! ${KUBECTL} wait --for condition=ready -n "
"${RANCHER_CHART_TARGETNAMESPACE} $(${KUBECTL} get pods -n "
"${RANCHER_CHART_TARGETNAMESPACE} -l app=rancher -o name) --timeout=10s; do "
"sleep 2 ; done\n"
"until ${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} rancher > "
"/dev/null 2>&1; do sleep 10; done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:121
#, no-wrap
msgid ""
"RANCHERBOOTSTRAPPASSWORD=$(${KUBECTL} get secret -n "
"${RANCHER_CHART_TARGETNAMESPACE} bootstrap-secret -o "
"jsonpath='{.data.bootstrapPassword}' | base64 -d)\n"
"RANCHERHOSTNAME=$(${KUBECTL} get ingress -n ${RANCHER_CHART_TARGETNAMESPACE} "
"rancher -o jsonpath='{.spec.rules[0].host}')\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:122
#, no-wrap
msgid ""
"# Skip the whole process if things have been set already\n"
"if [ -z $(${KUBECTL} get settings.management.cattle.io first-login "
"-ojsonpath='{.value}') ]; then\n"
"  # Add the protocol\n"
"  RANCHERHOSTNAME=\"https://${RANCHERHOSTNAME}\"\n"
"  TOKEN=\"\"\n"
"  while [ -z \"${TOKEN}\" ]; do\n"
"    # Get token\n"
"    sleep 2\n"
"    TOKEN=$(curl -sk -X POST "
"${RANCHERHOSTNAME}/v3-public/localProviders/local?action=login -H "
"'content-type: application/json' -d "
"\"{\\\"username\\\":\\\"admin\\\",\\\"password\\\":\\\"${RANCHERBOOTSTRAPPASSWORD}\\\"}\" "
"| jq -r .token)\n"
"  done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:123
#, no-wrap
msgid ""
"  # Set password\n"
"  curl -sk ${RANCHERHOSTNAME}/v3/users?action=changepassword -H "
"'content-type: application/json' -H \"Authorization: Bearer $TOKEN\" -d "
"\"{\\\"currentPassword\\\":\\\"${RANCHERBOOTSTRAPPASSWORD}\\\",\\\"newPassword\\\":\\\"${RANCHER_FINALPASSWORD}\\\"}\"\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:124
#, no-wrap
msgid ""
"  # Create a temporary API token (ttl=60 minutes)\n"
"  APITOKEN=$(curl -sk ${RANCHERHOSTNAME}/v3/token -H 'content-type: "
"application/json' -H \"Authorization: Bearer ${TOKEN}\" -d "
"'{\"type\":\"token\",\"description\":\"automation\",\"ttl\":3600000}' | jq "
"-r .token)\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:125
#, no-wrap
msgid ""
"  curl -sk ${RANCHERHOSTNAME}/v3/settings/server-url -H 'content-type: "
"application/json' -H \"Authorization: Bearer ${APITOKEN}\" -X PUT -d "
"\"{\\\"name\\\":\\\"server-url\\\",\\\"value\\\":\\\"${RANCHERHOSTNAME}\\\"}\"\n"
"  curl -sk ${RANCHERHOSTNAME}/v3/settings/telemetry-opt -X PUT -H "
"'content-type: application/json' -H 'accept: application/json' -H "
"\"Authorization: Bearer ${APITOKEN}\" -d '{\"value\":\"out\"}'\n"
"fi\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:126
#, no-wrap
msgid ""
"# Clean up the lock cm\n"
"${KUBECTL} delete configmap ${RANCHERLOCKCMNAME} -n "
"${RANCHERLOCKNAMESPACE}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:127
msgid ""
"`mgmt-stack-setup.service`: contains the configuration to create the systemd "
"service to run the scripts during the first boot (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:128
#, no-wrap
msgid ""
"[Unit]\n"
"Description=Setup Management stack components\n"
"Wants=network-online.target\n"
"# It requires rke2 or k3s running, but it will not fail if those services "
"are not present\n"
"After=network.target network-online.target rke2-server.service k3s.service\n"
"# At least, the basic-setup.sh one needs to be present\n"
"ConditionPathExists=/opt/mgmt/bin/basic-setup.sh\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:129
#, no-wrap
msgid ""
"[Service]\n"
"User=root\n"
"Type=forking\n"
"# Metal3 can take A LOT to download the IPA image\n"
"TimeoutStartSec=1800\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:130
#, no-wrap
msgid ""
"ExecStartPre=/bin/sh -c \"echo 'Setting up Management components...'\"\n"
"# Scripts are executed in StartPre because Start can only run a single on\n"
"ExecStartPre=/opt/mgmt/bin/rancher.sh\n"
"ExecStartPre=/opt/mgmt/bin/metal3.sh\n"
"ExecStart=/bin/sh -c \"echo 'Finished setting up Management components'\"\n"
"RemainAfterExit=yes\n"
"KillMode=process\n"
"# Disable & delete everything\n"
"ExecStartPost=rm -f /opt/mgmt/bin/rancher.sh\n"
"ExecStartPost=rm -f /opt/mgmt/bin/metal3.sh\n"
"ExecStartPost=rm -f /opt/mgmt/bin/basic-setup.sh\n"
"ExecStartPost=/bin/sh -c \"systemctl disable mgmt-stack-setup.service\"\n"
"ExecStartPost=rm -f /etc/systemd/system/mgmt-stack-setup.service\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:131
#, no-wrap
msgid ""
"[Install]\n"
"WantedBy=multi-user.target\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:132
msgid "The `custom/scripts` folder contains the following files:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:133
msgid ""
"`99-alias.sh` script: contains the alias to be used by the management "
"cluster to load the kubeconfig file at first boot (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:134
#, no-wrap
msgid ""
"#!/bin/bash\n"
"echo \"alias k=kubectl\" >> /etc/profile.local\n"
"echo \"alias kubectl=/var/lib/rancher/rke2/bin/kubectl\" >> "
"/etc/profile.local\n"
"echo \"export KUBECONFIG=/etc/rancher/rke2/rke2.yaml\" >> "
"/etc/profile.local\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:135
msgid ""
"`99-mgmt-setup.sh` script: contains the configuration to copy the scripts "
"during the first boot (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:137
#: asciidoc/product/atip-management-cluster.adoc:210
#, no-wrap
msgid ""
"# Copy the scripts from combustion to the final location\n"
"mkdir -p /opt/mgmt/bin/\n"
"for script in basic-setup.sh rancher.sh metal3.sh; do\n"
"\tcp ${script} /opt/mgmt/bin/\n"
"done\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:138
#: asciidoc/product/atip-management-cluster.adoc:211
#, no-wrap
msgid ""
"# Copy the systemd unit file and enable it at boot\n"
"cp mgmt-stack-setup.service /etc/systemd/system/mgmt-stack-setup.service\n"
"systemctl enable mgmt-stack-setup.service\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:139
msgid ""
"`99-register.sh` script: contains the configuration to register the system "
"using the SCC registration code. The `$\\{SCC_ACCOUNT_EMAIL\\}` and "
"`$\\{SCC_REGISTRATION_CODE\\}` have to be set properly to register the "
"system with your account."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:141
#, no-wrap
msgid ""
"# Registration https://www.suse.com/support/kb/doc/?id=000018564\n"
"if ! which SUSEConnect > /dev/null 2>&1; then\n"
"\tzypper --non-interactive install suseconnect-ng\n"
"fi\n"
"SUSEConnect --email \"${SCC_ACCOUNT_EMAIL}\" --url \"https://scc.suse.com\" "
"--regcode \"${SCC_REGISTRATION_CODE}\"\n"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:142
#, no-wrap
msgid "Kubernetes folder"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:143
msgid "The `kubernetes` folder contains the following subfolders:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:144
#, no-wrap
msgid ""
"...\n"
"├── kubernetes\n"
"│ ├── manifests\n"
"│ │ ├── rke2-ingress-config.yaml\n"
"│ │ ├── neuvector-namespace.yaml\n"
"│ │ ├── ingress-l2-adv.yaml\n"
"│ │ └── ingress-ippool.yaml\n"
"│ ├── helm\n"
"│ │ └── values\n"
"│ │     ├── rancher.yaml\n"
"│ │     ├── neuvector.yaml\n"
"│ │     ├── metal3.yaml\n"
"│ │     └── certmanager.yaml\n"
"│ └── config\n"
"│     └── server.yaml\n"
"...\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:145
msgid "The `kubernetes/config` folder contains the following files:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:146
msgid ""
"`server.yaml`: By default, the `CNI` plug-in installed by default is "
"`Cilium`, so you do not need to create this folder and file. Just in case "
"you need to customize the `CNI` plug-in, you can use the `server.yaml` file "
"under the `kubernetes/config` folder. It contains the following information:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:147
#: asciidoc/product/atip-features.adoc:51
#, no-wrap
msgid ""
"cni:\n"
"- multus\n"
"- cilium\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:148
msgid ""
"This is an optional file to define certain Kubernetes customization, like "
"the CNI plug-ins to be used or many options you can check in the "
"https://docs.rke2.io/install/configuration[official documentation]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:149
msgid "The `kubernetes/manifests` folder contains the following files:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:150
msgid ""
"`rke2-ingress-config.yaml`: contains the configuration to create the "
"`Ingress` service for the management cluster (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:151
#, no-wrap
msgid ""
"apiVersion: helm.cattle.io/v1\n"
"kind: HelmChartConfig\n"
"metadata:\n"
"  name: rke2-ingress-nginx\n"
"  namespace: kube-system\n"
"spec:\n"
"  valuesContent: |-\n"
"    controller:\n"
"      config:\n"
"        use-forwarded-headers: \"true\"\n"
"        enable-real-ip: \"true\"\n"
"      publishService:\n"
"        enabled: true\n"
"      service:\n"
"        enabled: true\n"
"        type: LoadBalancer\n"
"        externalTrafficPolicy: Local\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:152
msgid ""
"`neuvector-namespace.yaml`: contains the configuration to create the "
"`NeuVector` namespace (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:153
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Namespace\n"
"metadata:\n"
"  labels:\n"
"    pod-security.kubernetes.io/enforce: privileged\n"
"  name: neuvector\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:154
msgid ""
"`ingress-l2-adv.yaml`: contains the configuration to create the "
"`L2Advertisement` for the `MetalLB` component (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:155
#, no-wrap
msgid ""
"apiVersion: metallb.io/v1beta1\n"
"kind: L2Advertisement\n"
"metadata:\n"
"  name: ingress-l2-adv\n"
"  namespace: metallb-system\n"
"spec:\n"
"  ipAddressPools:\n"
"    - ingress-ippool\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:156
msgid ""
"`ingress-ippool.yaml`: contains the configuration to create the "
"`IPAddressPool` for the `rke2-ingress-nginx` component. The "
"`$\\{INGRESS_VIP\\}` has to be set properly to define the IP address "
"reserved to be used by the `rke2-ingress-nginx` component."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:157
#, no-wrap
msgid ""
"apiVersion: metallb.io/v1beta1\n"
"kind: IPAddressPool\n"
"metadata:\n"
"  name: ingress-ippool\n"
"  namespace: metallb-system\n"
"spec:\n"
"  addresses:\n"
"    - ${INGRESS_VIP}/32\n"
"  serviceAllocation:\n"
"    priority: 100\n"
"    serviceSelectors:\n"
"      - matchExpressions:\n"
"          - {key: app.kubernetes.io/name, operator: In, values: "
"[rke2-ingress-nginx]}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:158
msgid "The `kubernetes/helm/values` folder contains the following files:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:159
msgid ""
"`rancher.yaml`: contains the configuration to create the `Rancher` "
"component. The `$\\{INGRESS_VIP\\}` must be set properly to define the IP "
"address to be consumed by the `Rancher` component. The URL to access the "
"`Rancher` component will be `https://rancher-$\\{INGRESS_VIP\\}.sslip.io`."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:160
#, no-wrap
msgid ""
"hostname: rancher-${INGRESS_VIP}.sslip.io\n"
"bootstrapPassword: \"foobar\"\n"
"replicas: 1\n"
"global.cattle.psp.enabled: \"false\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:161
msgid ""
"`neuvector.yaml`: contains the configuration to create the `NeuVector` "
"component (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:162
#, no-wrap
msgid ""
"controller:\n"
"  replicas: 1\n"
"  ranchersso:\n"
"    enabled: true\n"
"manager:\n"
"  enabled: false\n"
"cve:\n"
"  scanner:\n"
"    enabled: false\n"
"    replicas: 1\n"
"k3s:\n"
"  enabled: true\n"
"crdwebhook:\n"
"  enabled: false\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:163
msgid ""
"`metal3.yaml`: contains the configuration to create the `Metal^3^` "
"component. The `$\\{METAL3_VIP\\}` must be set properly to define the IP "
"address to be consumed by the `Metal^3^` component."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:164
#, no-wrap
msgid ""
"global:\n"
"  ironicIP: ${METAL3_VIP}\n"
"  enable_vmedia_tls: false\n"
"  additionalTrustedCAs: false\n"
"metal3-ironic:\n"
"  global:\n"
"    predictableNicNames: \"true\"\n"
"  persistence:\n"
"    ironic:\n"
"      size: \"5Gi\"\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:165
msgid ""
"The Media Server is an optional feature included in Metal^3^ (by default is "
"disabled). To use the Metal3 feature, you need to configure it on the "
"previous manifest.  To use the Metal^3^ media server, specify the following "
"variable:"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:166
msgid ""
"add the `enable_metal3_media_server` to `true` to enable the media server "
"feature in the global section."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:167
msgid ""
"include the following configuration about the media server where "
"$\\{MEDIA_VOLUME_PATH\\} is the path to the media volume in the media (e.g "
"`/home/metal3/bmh-image-cache`)"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:168
#, no-wrap
msgid ""
"metal3-media:\n"
"  mediaVolume:\n"
"    hostPath: ${MEDIA_VOLUME_PATH}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:169
msgid ""
"An external media server can be used to store the images, and in the case "
"you want to use it with TLS, you will need to modify the following "
"configurations:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:170
msgid ""
"set to `true` the `additionalTrustedCAs` in the previous `metal3.yaml` file "
"to enable the additional trusted CAs from the external media server."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:171
msgid ""
"include the following secret configuration in the folder "
"`kubernetes/manifests/metal3-cacert-secret.yaml` to store the CA certificate "
"of the external media server."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:172
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Namespace\n"
"metadata:\n"
"  name: metal3-system\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:173
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: tls-ca-additional\n"
"  namespace: metal3-system\n"
"type: Opaque\n"
"data:\n"
"  ca-additional.crt: {{ additional_ca_cert | b64encode }}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:174
msgid ""
"The `additional_ca_cert` is the base64-encoded CA certificate of the "
"external media server. You can use the following command to encode the "
"certificate and generate the secret doing manually:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:175
#, no-wrap
msgid ""
"kubectl -n meta3-system create secret generic tls-ca-additional "
"--from-file=ca-additional.crt=./ca-additional.crt\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:176
msgid ""
"`certmanager.yaml`: contains the configuration to create the `Cert-Manager` "
"component (no modifications needed)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:177
#, no-wrap
msgid "installCRDs: \"true\"\n"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:178
#, no-wrap
msgid "Networking folder"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:179
msgid ""
"The `network` folder contains as many files as nodes in the management "
"cluster. In our case, we have only one node, so we have only one file called "
"`mgmt-cluster-node1.yaml`.  The name of the file must match the host name "
"defined in the `mgmt-cluster.yaml` definition file into the network/node "
"section described above."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:180
msgid ""
"If you need to customize the networking configuration, for example, to use a "
"specific static IP address (DHCP-less scenario), you can use the "
"`mgmt-cluster-node1.yaml` file under the `network` folder. It contains the "
"following information:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:181
msgid "`$\\{MGMT_GATEWAY\\}`: The gateway IP address."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:182
msgid "`$\\{MGMT_DNS\\}`: The DNS server IP address."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:183
msgid "`$\\{MGMT_MAC\\}`: The MAC address of the network interface."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:184
msgid "`$\\{MGMT_NODE_IP\\}`: The IP address of the management cluster."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:185
#, no-wrap
msgid ""
"routes:\n"
"  config:\n"
"  - destination: 0.0.0.0/0\n"
"    metric: 100\n"
"    next-hop-address: ${MGMT_GATEWAY}\n"
"    next-hop-interface: eth0\n"
"    table-id: 254\n"
"dns-resolver:\n"
"  config:\n"
"    server:\n"
"    - ${MGMT_DNS}\n"
"    - 8.8.8.8\n"
"interfaces:\n"
"- name: eth0\n"
"  type: ethernet\n"
"  state: up\n"
"  mac-address: ${MGMT_MAC}\n"
"  ipv4:\n"
"    address:\n"
"    - ip: ${MGMT_NODE_IP}\n"
"      prefix-length: 24\n"
"    dhcp: false\n"
"    enabled: true\n"
"  ipv6:\n"
"    enabled: false\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:186
msgid ""
"If you want to use DHCP to get the IP address, you can use the following "
"configuration (the `MAC` address must be set properly using the "
"`$\\{MGMT_MAC\\}` variable):"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:187
#, no-wrap
msgid ""
"## This is an example of a dhcp network configuration for a management "
"cluster\n"
"## interfaces:\n"
"- name: eth0\n"
"  type: ethernet\n"
"  state: up\n"
"  mac-address: ${MGMT_MAC}\n"
"  ipv4:\n"
"    dhcp: true\n"
"    enabled: true\n"
"  ipv6:\n"
"    enabled: false\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:188
msgid ""
"Depending on the number of nodes in the management cluster, you can create "
"more files like `mgmt-cluster-node2.yaml`, `mgmt-cluster-node3.yaml`, "
"etc. to configure the rest of the nodes."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:189
msgid ""
"The `routes` section is used to define the routing table for the management "
"cluster."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-management-cluster.adoc:190
#, no-wrap
msgid "Image preparation for air-gap environments"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:191
msgid ""
"This section describes how to prepare the image for air-gap environments "
"showing only the differences from the previous sections. The following "
"changes to the previous section "
"(xref:mgmt-cluster-image-preparation-connected[Image preparation for "
"connected environments]) are required to prepare the image for air-gap "
"environments:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:192
msgid ""
"The `mgmt-cluster.yaml` file must be modified to include the "
"`embeddedArtifactRegistry` section with the `images` field set to all "
"container images to be included into the EIB output image."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:193
msgid ""
"The `custom/scripts/99-register.sh` script must be removed when use an "
"air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:194
msgid ""
"The `custom/files/airgap-resources.tar.gz` file must be included in the "
"`custom/files` folder with all the resources needed to run the management "
"cluster in an air-gap environment."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:195
msgid ""
"The `custom/scripts/99-mgmt-setup.sh` script must be modified to extract and "
"copy the `airgap-resources.tar.gz` file to the final location."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:196
msgid ""
"The `custom/files/metal3.sh` script must be modified to use the local "
"resources included in the `airgap-resources.tar.gz` file instead of "
"downloading them from the internet."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:197
#, no-wrap
msgid "Directory structure for air-gap environments"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:198
msgid ""
"The directory structure for air-gap environments is the same as for "
"connected environments, with the differences explained as follows:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:199
#, no-wrap
msgid ""
"eib\n"
"|-- base-images\n"
"|   |-- SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"|-- custom\n"
"|   |-- files\n"
"|   |   |-- airgap-resources.tar.gz\n"
"|   |   |-- basic-setup.sh\n"
"|   |   |-- metal3.sh\n"
"|   |   |-- mgmt-stack-setup.service\n"
"|   |   |-- rancher.sh\n"
"|   |-- scripts\n"
"|       |-- 99-alias.sh\n"
"|       |-- 99-mgmt-setup.sh\n"
"|-- kubernetes\n"
"|   |-- config\n"
"|   |   |-- server.yaml\n"
"|   |-- helm\n"
"|   |   |-- values\n"
"|   |       |-- certmanager.yaml\n"
"|   |       |-- metal3.yaml\n"
"|   |       |-- neuvector.yaml\n"
"|   |       |-- rancher.yaml\n"
"|   |-- manifests\n"
"|       |-- neuvector-namespace.yaml\n"
"|-- mgmt-cluster.yaml\n"
"|-- network\n"
"    |-- mgmt-cluster-network.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:200
msgid ""
"The image `SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso` must "
"be downloaded from the https://scc.suse.com/[SUSE Customer Center] or the "
"https://www.suse.com/download/sle-micro/[SUSE Download page], and it must be "
"located under the `base-images` folder before starting with the process."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:203
#, no-wrap
msgid "Modifications in the definition file"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:204
msgid ""
"The `mgmt-cluster.yaml` file must be modified to include the "
"`embeddedArtifactRegistry` section with the `images` field set to all "
"container images to be included into the EIB output image. The `images` "
"field must contain the list of all container images to be included in the "
"output image. The following is an example of the `mgmt-cluster.yaml` file "
"with the `embeddedArtifactRegistry` section included:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:205
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: iso\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso\n"
"  outputImageName: eib-mgmt-cluster-image.iso\n"
"operatingSystem:\n"
"  isoConfiguration:\n"
"    installDevice: /dev/sda\n"
"  users:\n"
"  - username: root\n"
"    encryptedPassword: ${ROOT_PASSWORD}\n"
"  packages:\n"
"    packageList:\n"
"    - jq\n"
"    sccRegistrationCode: ${SCC_REGISTRATION_CODE}\n"
"kubernetes:\n"
"  version: ${KUBERNETES_VERSION}\n"
"  helm:\n"
"    charts:\n"
"      - name: cert-manager\n"
"        repositoryName: jetstack\n"
"        version: 1.14.2\n"
"        targetNamespace: cert-manager\n"
"        valuesFile: certmanager.yaml\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn-crd\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: longhorn\n"
"        version: 103.3.0+up1.6.1\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: longhorn-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"      - name: metal3-chart\n"
"        version: 0.7.1\n"
"        repositoryName: suse-edge-charts\n"
"        targetNamespace: metal3-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: metal3.yaml\n"
"      - name: neuvector-crd\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: neuvector\n"
"        version: 103.0.3+up2.7.6\n"
"        repositoryName: rancher-charts\n"
"        targetNamespace: neuvector\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: neuvector.yaml\n"
"      - name: rancher\n"
"        version: 2.8.4\n"
"        repositoryName: rancher-prime\n"
"        targetNamespace: cattle-system\n"
"        createNamespace: true\n"
"        installationNamespace: kube-system\n"
"        valuesFile: rancher.yaml\n"
"    repositories:\n"
"      - name: jetstack\n"
"        url: https://charts.jetstack.io\n"
"      - name: rancher-charts\n"
"        url: https://charts.rancher.io/\n"
"      - name: suse-edge-charts\n"
"        url: oci://registry.suse.com/edge\n"
"      - name: rancher-prime\n"
"        url: https://charts.rancher.com/server-charts/prime\n"
"    network:\n"
"      apiHost: ${API_HOST}\n"
"      apiVIP: ${API_VIP}\n"
"    nodes:\n"
"      - hostname: mgmt-cluster-node1\n"
"        initializer: true\n"
"        type: server\n"
"#     - hostname: mgmt-cluster-node2\n"
"#       initializer: true\n"
"#       type: server\n"
"#     - hostname: mgmt-cluster-node3\n"
"#       initializer: true\n"
"#       type: server\n"
"embeddedArtifactRegistry:\n"
"  images:\n"
"    - name: registry.rancher.com/rancher/backup-restore-operator:v4.0.2\n"
"    - name: registry.rancher.com/rancher/calico-cni:v3.27.0-rancher1\n"
"    - name: registry.rancher.com/rancher/cis-operator:v1.0.13\n"
"    - name: registry.rancher.com/rancher/coreos-kube-state-metrics:v1.9.7\n"
"    - name: "
"registry.rancher.com/rancher/coreos-prometheus-config-reloader:v0.38.1\n"
"    - name: "
"registry.rancher.com/rancher/coreos-prometheus-operator:v0.38.1\n"
"    - name: registry.rancher.com/rancher/flannel-cni:v0.3.0-rancher9\n"
"    - name: registry.rancher.com/rancher/fleet-agent:v0.9.4\n"
"    - name: registry.rancher.com/rancher/fleet:v0.9.4\n"
"    - name: registry.rancher.com/rancher/gitjob:v0.9.7\n"
"    - name: registry.rancher.com/rancher/grafana-grafana:7.1.5\n"
"    - name: "
"registry.rancher.com/rancher/hardened-addon-resizer:1.8.20-build20240410\n"
"    - name: "
"registry.rancher.com/rancher/hardened-calico:v3.27.3-build20240423\n"
"    - name: "
"registry.rancher.com/rancher/hardened-cluster-autoscaler:v1.8.10-build20240124\n"
"    - name: "
"registry.rancher.com/rancher/hardened-cni-plugins:v1.4.1-build20240325\n"
"    - name: "
"registry.rancher.com/rancher/hardened-coredns:v1.11.1-build20240305\n"
"    - name: "
"registry.rancher.com/rancher/hardened-dns-node-cache:1.22.28-build20240125\n"
"    - name: "
"registry.rancher.com/rancher/hardened-etcd:v3.5.9-k3s1-build20240418\n"
"    - name: "
"registry.rancher.com/rancher/hardened-flannel:v0.25.1-build20240423\n"
"    - name: "
"registry.rancher.com/rancher/hardened-k8s-metrics-server:v0.7.1-build20240401\n"
"    - name: "
"registry.rancher.com/rancher/hardened-kubernetes:v1.28.9-rke2r1-build20240416\n"
"    - name: "
"registry.rancher.com/rancher/hardened-multus-cni:v4.0.2-build20240208\n"
"    - name: "
"registry.rancher.com/rancher/hardened-node-feature-discovery:v0.14.1-build20230926\n"
"    - name: "
"registry.rancher.com/rancher/hardened-whereabouts:v0.6.3-build20240208\n"
"    - name: registry.rancher.com/rancher/helm-project-operator:v0.2.1\n"
"    - name: registry.rancher.com/rancher/istio-kubectl:1.5.10\n"
"    - name: "
"registry.rancher.com/rancher/jimmidyson-configmap-reload:v0.3.0\n"
"    - name: registry.rancher.com/rancher/k3s-upgrade:v1.28.9-k3s1\n"
"    - name: registry.rancher.com/rancher/klipper-helm:v0.8.3-build20240228\n"
"    - name: registry.rancher.com/rancher/klipper-lb:v0.4.7\n"
"    - name: registry.rancher.com/rancher/kube-api-auth:v0.2.1\n"
"    - name: registry.rancher.com/rancher/kubectl:v1.28.7\n"
"    - name: registry.rancher.com/rancher/library-nginx:1.19.2-alpine\n"
"    - name: registry.rancher.com/rancher/local-path-provisioner:v0.0.26\n"
"    - name: registry.rancher.com/rancher/machine:v0.15.0-rancher112\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-cluster-api-controller:v1.4.4\n"
"    - name: "
"registry.rancher.com/rancher/nginx-ingress-controller:nginx-1.9.6-rancher1\n"
"    - name: registry.rancher.com/rancher/pause:3.6\n"
"    - name: registry.rancher.com/rancher/prom-alertmanager:v0.21.0\n"
"    - name: registry.rancher.com/rancher/prom-node-exporter:v1.0.1\n"
"    - name: registry.rancher.com/rancher/prom-prometheus:v2.18.2\n"
"    - name: registry.rancher.com/rancher/prometheus-auth:v0.2.2\n"
"    - name: registry.rancher.com/rancher/prometheus-federator:v0.3.4\n"
"    - name: "
"registry.rancher.com/rancher/pushprox-client:v0.1.0-rancher2-client\n"
"    - name: "
"registry.rancher.com/rancher/pushprox-proxy:v0.1.0-rancher2-proxy\n"
"    - name: registry.rancher.com/rancher/rancher-agent:v2.8.4\n"
"    - name: registry.rancher.com/rancher/rancher-csp-adapter:v3.0.1\n"
"    - name: registry.rancher.com/rancher/rancher-webhook:v0.4.5\n"
"    - name: registry.rancher.com/rancher/rancher:v2.8.4\n"
"    - name: registry.rancher.com/rancher/rke-tools:v0.1.96\n"
"    - name: "
"registry.rancher.com/rancher/rke2-cloud-provider:v1.29.3-build20240412\n"
"    - name: registry.rancher.com/rancher/rke2-runtime:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/rke2-upgrade:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/security-scan:v0.2.15\n"
"    - name: registry.rancher.com/rancher/shell:v0.1.24\n"
"    - name: "
"registry.rancher.com/rancher/system-agent-installer-k3s:v1.28.9-k3s1\n"
"    - name: "
"registry.rancher.com/rancher/system-agent-installer-rke2:v1.28.9-rke2r1\n"
"    - name: registry.rancher.com/rancher/system-agent:v0.3.6-suc\n"
"    - name: registry.rancher.com/rancher/system-upgrade-controller:v0.13.1\n"
"    - name: registry.rancher.com/rancher/ui-plugin-catalog:1.3.0\n"
"    - name: registry.rancher.com/rancher/ui-plugin-operator:v0.1.1\n"
"    - name: registry.rancher.com/rancher/webhook-receiver:v0.2.5\n"
"    - name: registry.rancher.com/rancher/kubectl:v1.20.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-csi-attacher:v4.4.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-csi-provisioner:v3.6.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-csi-resizer:v1.9.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-csi-snapshotter:v6.3.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-csi-node-driver-registrar:v2.9.2\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-livenessprobe:v2.12.0\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-backing-image-manager:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-longhorn-engine:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-longhorn-instance-manager:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-longhorn-manager:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-longhorn-share-manager:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-longhorn-ui:v1.6.1\n"
"    - name: "
"registry.rancher.com/rancher/mirrored-longhornio-support-bundle-kit:v0.0.36\n"
"    - name: "
"registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:v0.2.6\n"
"    - name: "
"registry.suse.com/edge/cluster-api-provider-rke2-controlplane:v0.2.6\n"
"    - name: registry.suse.com/edge/cluster-api-controller:v1.6.2\n"
"    - name: registry.suse.com/edge/cluster-api-provider-metal3:v1.6.0\n"
"    - name: registry.suse.com/edge/ip-address-manager:v1.6.0\n"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-management-cluster.adoc:206
#, no-wrap
msgid "Modifications in the custom folder"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:207
msgid ""
"The `custom/scripts/99-register.sh` script must be removed when using an "
"air-gap environment. As you can see in the directory structure, the "
"`99-register.sh` script is not included in the `custom/scripts` folder."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:208
msgid ""
"The `custom/scripts/99-mgmt-setup.sh` script must be modified to extract and "
"copy the `airgap-resources.tar.gz` file to the final location. The following "
"is an example of the `99-mgmt-setup.sh` script with the modifications to "
"extract and copy the `airgap-resources.tar.gz` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:212
#, no-wrap
msgid ""
"# Extract the airgap resources\n"
"tar zxf airgap-resources.tar.gz\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:213
#, no-wrap
msgid ""
"# Copy the clusterctl binary to the final location\n"
"cp airgap-resources/clusterctl /opt/mgmt/bin/ && chmod +x "
"/opt/mgmt/bin/clusterctl\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:214
#, no-wrap
msgid ""
"# Copy the clusterctl.yaml and override\n"
"mkdir -p /root/cluster-api\n"
"cp -r airgap-resources/clusterctl.yaml airgap-resources/overrides "
"/root/cluster-api/\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:215
msgid ""
"The `custom/files/metal3.sh` script must be modified to use the local "
"resources included in the `airgap-resources.tar.gz` file instead of "
"downloading them from the internet. The following is an example of the "
"`metal3.sh` script with the modifications to use the local resources:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:226
#, no-wrap
msgid ""
"  # Try this command 3 times just in case, stolen from "
"https://stackoverflow.com/a/33354419\n"
"  if ! (r=3; while ! /opt/mgmt/bin/clusterctl init \\\n"
"    --core \"cluster-api:v${METAL3_CAPICOREVERSION}\"\\\n"
"    --infrastructure \"metal3:v${METAL3_CAPIMETAL3VERSION}\"\\\n"
"    --bootstrap \"${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}\"\\\n"
"    --control-plane "
"\"${METAL3_CAPIPROVIDER}:v${METAL3_CAPIRKE2VERSION}\"\\\n"
"    --config /root/cluster-api/clusterctl.yaml ; do\n"
"            ((--r))||exit\n"
"            echo \"Something went wrong, let's wait 10 seconds and retry\"\n"
"            sleep 10;done) ; then\n"
"      echo \"clusterctl failed\"\n"
"      exit 1\n"
"  fi\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:234
msgid ""
"The `custom/files/airgap-resources.tar.gz` file must be included in the "
"`custom/files` folder with all the resources needed to run the management "
"cluster in an air-gap environment. This file must be prepared manually "
"downloading all resources and compressing them into this single file. The "
"`airgap-resources.tar.gz` file contains the following resources:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:235
#, no-wrap
msgid ""
"|-- clusterctl\n"
"|-- clusterctl.yaml\n"
"|-- overrides\n"
"    |-- bootstrap-rke2\n"
"    |   |-- v0.2.6\n"
"    |       |-- bootstrap-components.yaml\n"
"    |       |-- metadata.yaml\n"
"    |-- cluster-api\n"
"    |   |-- v1.6.2\n"
"    |       |-- core-components.yaml\n"
"    |       |-- metadata.yaml\n"
"    |-- control-plane-rke2\n"
"    |   |-- v0.2.6\n"
"    |       |-- control-plane-components.yaml\n"
"    |       |-- metadata.yaml\n"
"    |-- infrastructure-metal3\n"
"        |-- v1.6.0\n"
"            |-- cluster-template.yaml\n"
"            |-- infrastructure-components.yaml\n"
"            |-- metadata.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:236
msgid ""
"The `clusterctl.yaml` file contains the configuration to specify the images "
"location and the overrides to be used by the `clusterctl` tool. The "
"`overrides` folder contains `yaml` file manifests to be used instead of "
"downloading them from the internet."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:237
#, no-wrap
msgid ""
"providers:\n"
"  # override a pre-defined provider\n"
"  - name: \"cluster-api\"\n"
"    url: "
"\"/root/cluster-api/overrides/cluster-api/v1.6.2/core-components.yaml\"\n"
"    type: \"CoreProvider\"\n"
"  - name: \"metal3\"\n"
"    url: "
"\"/root/cluster-api/overrides/infrastructure-metal3/v1.6.0/infrastructure-components.yaml\"\n"
"    type: \"InfrastructureProvider\"\n"
"  - name: \"rke2\"\n"
"    url: "
"\"/root/cluster-api/overrides/bootstrap-rke2/v0.2.6/bootstrap-components.yaml\"\n"
"    type: \"BootstrapProvider\"\n"
"  - name: \"rke2\"\n"
"    url: "
"\"/root/cluster-api/overrides/control-plane-rke2/v0.2.6/control-plane-components.yaml\"\n"
"    type: \"ControlPlaneProvider\"\n"
"images:\n"
"  all:\n"
"    repository: registry.suse.com/edge\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:238
msgid ""
"The `clusterctl` and the rest of the files included in the `overrides` "
"folder can be downloaded using the following curls commands:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:239
#, no-wrap
msgid ""
"# clusterctl binary\n"
"curl -L "
"https://github.com/kubernetes-sigs/cluster-api/releases/download/1.6.2/clusterctl-linux-${GOARCH} "
"-o /usr/local/bin/clusterct\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:240
#, no-wrap
msgid ""
"# boostrap-components (boostrap-rke2)\n"
"curl -L "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/bootstrap-components.yaml\n"
"curl -L "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:241
#, no-wrap
msgid ""
"# control-plane-components (control-plane-rke2)\n"
"curl -L "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/control-plane-components.yaml\n"
"curl -L "
"https://github.com/rancher-sandbox/cluster-api-provider-rke2/releases/download/v0.2.6/metadata.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:242
#, no-wrap
msgid ""
"# cluster-api components\n"
"curl -L "
"https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/core-components.yaml\n"
"curl -L "
"https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.2/metadata.yaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:243
#, no-wrap
msgid ""
"# infrastructure-components (infrastructure-metal3)\n"
"curl -L "
"https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/infrastructure-components.yaml\n"
"curl -L "
"https://github.com/metal3-io/cluster-api-provider-metal3/releases/download/v1.6.0/metadata.yaml\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-management-cluster.adoc:244
msgid ""
"If you want to use different versions of the components, you can change the "
"version in the URL to download the specific version of the components."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:245
msgid ""
"With the previous resources downloaded, you can compress them into a single "
"file using the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:246
#, no-wrap
msgid "tar -czvf airgap-resources.tar.gz clusterctl clusterctl.yaml overrides\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:248
msgid ""
"Once the directory structure is prepared following the previous sections "
"(for both, connected and air-gap scenarios), run the following command to "
"build the image:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-management-cluster.adoc:249
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v $PWD:/eib \\\n"
" registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
" build --definition-file mgmt-cluster.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:250
msgid ""
"This creates the ISO output image file that, in our case, based on the image "
"definition described above, is `eib-mgmt-cluster-image.iso`."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-management-cluster.adoc:251
#, no-wrap
msgid "Provision the management cluster"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-management-cluster.adoc:252
msgid ""
"The previous image contains all components explained above, and it can be "
"used to provision the management cluster using a virtual machine or a "
"bare-metal server (using the virtual-media feature)."
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-features.adoc:1
#, no-wrap
msgid "Telco features configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:2
msgid ""
"This section documents and explains the configuration of Telco-specific "
"features on ATIP-deployed clusters."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:3
msgid ""
"The directed network provisioning deployment method is used, as described in "
"the <<atip-automated-provisioning,ATIP Automated Provision>> section."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:4
msgid "The following topics are covered in this section:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:5
msgid ""
"<<kernel-image-for-real-time,Kernel image for real time>>: Kernel image to "
"be used by the real-time kernel."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:6
msgid ""
"<<cpu-tuned-configuration,CPU tuned configuration>>: Tuned configuration to "
"be used by the real-time kernel."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:7
msgid ""
"<<cni-configuration,CNI configuration>>: CNI configuration to be used by the "
"Kubernetes cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:8
msgid ""
"<<sriov,SR-IOV configuration>>: SR-IOV configuration to be used by the "
"Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:9
msgid "<<dpdk,DPDK configuration>>: DPDK configuration to be used by the system."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:10
msgid ""
"<<acceleration,vRAN acceleration card>>: Acceleration card configuration to "
"be used by the Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:11
msgid ""
"<<huge-pages,Huge pages>>: Huge pages configuration to be used by the "
"Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:12
msgid ""
"<<cpu-pinning-configuration,CPU pinning configuration>>: CPU pinning "
"configuration to be used by the Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:13
msgid ""
"<<numa-aware-scheduling,NUMA-aware scheduling configuration>>: NUMA-aware "
"scheduling configuration to be used by the Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:14
msgid ""
"<<metal-lb-configuration,Metal LB configuration>>: Metal LB configuration to "
"be used by the Kubernetes workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:15
msgid ""
"<<private-registry,Private registry configuration>>: Private registry "
"configuration to be used by the Kubernetes workloads."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:16
#, no-wrap
msgid "Kernel image for real time"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:17
msgid ""
"The real-time kernel image is not necessarily better than a standard "
"kernel.  It is a different kernel tuned to a specific use case. The "
"real-time kernel is tuned for lower latency at the cost of throughput. The "
"real-time kernel is not recommended for general purpose use, but in our "
"case, this is the recommended kernel for Telco Workloads where latency is a "
"key factor."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:18
msgid "There are four top features:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:19
msgid "Deterministic execution:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:20
msgid ""
"Get greater predictability — ensure critical business processes complete in "
"time, every time and deliver high-quality service, even under heavy system "
"loads. By shielding key system resources for high-priority processes, you "
"can ensure greater predictability for time-sensitive applications."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:21
msgid "Low jitter:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:22
msgid ""
"The low jitter built upon the highly deterministic technology helps to keep "
"applications synchronized with the real world. This helps services that need "
"ongoing and repeated calculation."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:23
msgid "Priority inheritance:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:24
msgid ""
"Priority inheritance refers to the ability of a lower priority process to "
"assume a higher priority when there is a higher priority process that "
"requires the lower priority process to finish before it can accomplish its "
"task. SUSE Linux Enterprise Real Time solves these priority inversion "
"problems for mission-critical processes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:25
msgid "Thread interrupts:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:26
msgid ""
"Processes running in interrupt mode in a general-purpose operating system "
"are not preemptible. With SUSE Linux Enterprise Real Time, these interrupts "
"have been encapsulated by kernel threads, which are interruptible, and allow "
"the hard and soft interrupts to be preempted by user-defined higher priority "
"processes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:27
msgid ""
"In our case, if you have installed a real-time image like `SLE Micro RT`, "
"kernel real time is already installed. From the https://scc.suse.com/[SUSE "
"Customer Center], you can download the real-time kernel image."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:28
msgid ""
"For more information about the real-time kernel, visit "
"https://www.suse.com/products/realtime/[SUSE Real Time]."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:29
#, no-wrap
msgid "CPU tuned configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:30
msgid ""
"The CPU Tuned configuration allows the possibility to isolate the CPU cores "
"to be used by the real-time kernel. It is important to prevent the OS from "
"using the same cores as the real-time kernel, because the OS could use the "
"cores and increase the latency in the real-time kernel."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:31
msgid ""
"To enable and configure this feature, the first thing is to create a profile "
"for the CPU cores we want to isolate. In this case, we are isolating the "
"cores `1-30` and `33-62`."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:32
#, no-wrap
msgid "$ echo \"export tuned_params\" >> /etc/grub.d/00_tuned\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:33
#, no-wrap
msgid ""
"$ echo \"isolated_cores=1-30,33-62\" >> "
"/etc/tuned/cpu-partitioning-variables.conf\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:34
#, no-wrap
msgid ""
"$ tuned-adm profile cpu-partitioning\n"
"Tuned (re)started, changes applied.\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:35
msgid ""
"Then we need to modify the GRUB option to isolate CPU cores and other "
"important parameters for CPU usage.  The following options are important to "
"be customized with your current hardware specifications:"
msgstr ""

#. type: Table
#: asciidoc/product/atip-features.adoc:36
#, no-wrap
msgid ""
"| parameter | value | description\n"
"\n"
"| isolcpus\n"
"| 1-30,33-62\n"
"| Isolate the cores 1-30 and 33-62\n"
"\n"
"| skew_tick\n"
"| 1\n"
"| This option allows the kernel to skew the timer interrupts across the "
"isolated CPUs.\n"
"\n"
"| nohz\n"
"| on\n"
"| This option allows the kernel to run the timer tick on a single CPU when "
"the system is idle.\n"
"\n"
"| nohz_full\n"
"| 1-30,33-62\n"
"| kernel boot parameter is the current main interface to configure full "
"dynticks along with CPU Isolation.\n"
"\n"
"| rcu_nocbs\n"
"| 1-30,33-62\n"
"| This option allows the kernel to run the RCU callbacks on a single CPU "
"when the system is idle.\n"
"\n"
"| kthread_cpus\n"
"| 0,31,32,63\n"
"| This option allows the kernel to run the kthreads on a single CPU when the "
"system is idle.\n"
"\n"
"| irqaffinity\n"
"| 0,31,32,63\n"
"| This option allows the kernel to run the interrupts on a single CPU when "
"the system is idle.\n"
"\n"
"| processor.max_cstate\n"
"| 1\n"
"| This option prevents the CPU from dropping into a sleep state when idle\n"
"\n"
"| intel_idle.max_cstate\n"
"| 0\n"
"| This option disables the intel_idle driver and allows acpi_idle to be "
"used\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:37
msgid ""
"With the values shown above, we are isolating 60 cores, and we are using "
"four cores for the OS."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:38
msgid ""
"The following commands modify the GRUB configuration and apply the changes "
"mentioned above to be present on the next boot:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:39
msgid "Edit the `/etc/default/grub` file and add the parameters mentioned above:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:40
#: asciidoc/product/atip-features.adoc:141
#: asciidoc/product/atip-features.adoc:162
#: asciidoc/product/atip-features.adoc:189
#, no-wrap
msgid ""
"GRUB_CMDLINE_LINUX=\"intel_iommu=on intel_pstate=passive "
"processor.max_cstate=1 intel_idle.max_cstate=0 iommu=pt "
"usbcore.autosuspend=-1 selinux=0 enforcing=0 nmi_watchdog=0 crashkernel=auto "
"softlockup_panic=0 audit=0 mce=off hugepagesz=1G hugepages=40 hugepagesz=2M "
"hugepages=0 default_hugepagesz=1G kthread_cpus=0,31,32,63 "
"irqaffinity=0,31,32,63 isolcpus=1-30,33-62 skew_tick=1 nohz_full=1-30,33-62 "
"rcu_nocbs=1-30,33-62 rcu_nocb_poll\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:41
msgid "Update the GRUB configuration:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:42
#: asciidoc/product/atip-features.adoc:143
#: asciidoc/product/atip-features.adoc:164
#: asciidoc/product/atip-features.adoc:191
#, no-wrap
msgid ""
"$ transactional-update grub.cfg\n"
"$ reboot\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:43
msgid ""
"To validate that the parameters are applied after the reboot, the following "
"command can be used to check the kernel command line:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:44
#: asciidoc/product/atip-features.adoc:166
#: asciidoc/product/atip-features.adoc:193
#, no-wrap
msgid "$ cat /proc/cmdline\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:45
#, no-wrap
msgid "CNI Configuration"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-features.adoc:46
#, no-wrap
msgid "Cilium"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:47
msgid ""
"`Cilium` is the default CNI plug-in for ATIP.  To enable Cilium on RKE2 "
"cluster as the default plug-in, the following configurations are required in "
"the `/etc/rancher/rke2/config.yaml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:48
#, no-wrap
msgid ""
"cni:\n"
"- cilium\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:49
msgid ""
"This can also be specified with command-line arguments, that is, "
"`--cni=cilium` into the server line in `/etc/systemd/system/rke2-server` "
"file."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:50
msgid ""
"To use the `SR-IOV` network operator described in the "
"xref:option2-sriov-helm[next section], use `Multus` with another CNI "
"plug-in, like `Cilium` or `Calico`, as a secondary plug-in."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:52
msgid ""
"For more information about CNI plug-ins, visit "
"https://docs.rke2.io/install/network_options[Network Options]."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:53
#, no-wrap
msgid "SR-IOV"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:54
msgid ""
"SR-IOV allows a device, such as a network adapter, to separate access to its "
"resources among various `PCIe` hardware functions.  There are different ways "
"to deploy `SR-IOV`, and here, we show two different options:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:55
msgid ""
"Option 1: using the `SR-IOV` CNI device plug-ins and a config map to "
"configure it properly."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:56
msgid ""
"Option 2 (recommended): using the `SR-IOV` Helm chart from Rancher Prime to "
"make this deployment easy."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:57
msgid ""
"*Option 1 - Installation of SR-IOV CNI device plug-ins and a config map to "
"configure it properly*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:58
msgid "Prepare the config map for the device plug-in"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:59
msgid "Get the information to fill the config map from the `lspci` command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:60
#: asciidoc/product/atip-features.adoc:170
#, no-wrap
msgid ""
"$ lspci | grep -i acc\n"
"8a:00.0 Processing accelerators: Intel Corporation Device 0d5c\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:61
#, no-wrap
msgid ""
"$ lspci | grep -i net\n"
"19:00.0 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 "
"NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)\n"
"19:00.1 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 "
"NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)\n"
"19:00.2 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 "
"NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)\n"
"19:00.3 Ethernet controller: Broadcom Inc. and subsidiaries BCM57504 "
"NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet (rev 11)\n"
"51:00.0 Ethernet controller: Intel Corporation Ethernet Controller E810-C "
"for QSFP (rev 02)\n"
"51:00.1 Ethernet controller: Intel Corporation Ethernet Controller E810-C "
"for QSFP (rev 02)\n"
"51:01.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:01.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:01.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:01.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:11.0 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:11.1 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:11.2 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
"51:11.3 Ethernet controller: Intel Corporation Ethernet Adaptive Virtual "
"Function (rev 02)\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:62
msgid ""
"The config map consists of a `JSON` file that describes devices using "
"filters to discover, and creates groups for the interfaces.  The key is "
"understanding filters and groups. The filters are used to discover the "
"devices and the groups are used to create the interfaces."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:63
msgid "It could be possible to set filters:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:64
msgid "vendorID: `8086` (Intel)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:65
msgid "deviceID: `0d5c` (Accelerator card)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:66
msgid "driver: `vfio-pci` (driver)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:67
msgid "pfNames: `p2p1` (physical interface name)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:68
msgid ""
"It could be possible to also set filters to match more complex interface "
"syntax, for example:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:69
msgid "pfNames: `[\"eth1#1,2,3,4,5,6\"]` or `[eth1#1-6]` (physical interface name)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:70
msgid ""
"Related to the groups, we could create a group for the `FEC` card and "
"another group for the `Intel` card, even creating a prefix depending on our "
"use case:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:71
msgid "resourceName: `pci_sriov_net_bh_dpdk`"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:72
msgid "resourcePrefix: `Rancher.io`"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:73
msgid ""
"There are a lot of combinations to discover and create the resource group to "
"allocate some `VFs` to the pods."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:74
msgid ""
"For more information about the filters and groups, visit "
"https://github.com/k8snetworkplumbingwg/sriov-network-device-plugin[sr-iov "
"network device plug-in]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:75
msgid ""
"After setting the filters and groups to match the interfaces depending on "
"the hardware and the use case, the following config map shows an example to "
"be used:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:76
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: ConfigMap\n"
"metadata:\n"
"  name: sriovdp-config\n"
"  namespace: kube-system\n"
"data:\n"
"  config.json: |\n"
"    {\n"
"        \"resourceList\": [\n"
"            {\n"
"                \"resourceName\": \"intel_fec_5g\",\n"
"                \"devicetype\": \"accelerator\",\n"
"                \"selectors\": {\n"
"                    \"vendors\": [\"8086\"],\n"
"                    \"devices\": [\"0d5d\"]\n"
"                }\n"
"            },\n"
"            {\n"
"                \"resourceName\": \"intel_sriov_odu\",\n"
"                \"selectors\": {\n"
"                    \"vendors\": [\"8086\"],\n"
"                    \"devices\": [\"1889\"],\n"
"                    \"drivers\": [\"vfio-pci\"],\n"
"                    \"pfNames\": [\"p2p1\"]\n"
"                }\n"
"            },\n"
"            {\n"
"                \"resourceName\": \"intel_sriov_oru\",\n"
"                \"selectors\": {\n"
"                    \"vendors\": [\"8086\"],\n"
"                    \"devices\": [\"1889\"],\n"
"                    \"drivers\": [\"vfio-pci\"],\n"
"                    \"pfNames\": [\"p2p2\"]\n"
"                }\n"
"            }\n"
"        ]\n"
"    }\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:77
msgid "Prepare the `daemonset` file to deploy the device plug-in."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:78
msgid ""
"The device plug-in supports several architectures (`arm`, `amd`, `ppc64le`), "
"so the same file can be used for different architectures deploying several "
"`daemonset` for each architecture."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:79
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: ServiceAccount\n"
"metadata:\n"
"  name: sriov-device-plugin\n"
"  namespace: kube-system\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:80
#, no-wrap
msgid ""
"apiVersion: apps/v1\n"
"kind: DaemonSet\n"
"metadata:\n"
"  name: kube-sriov-device-plugin-amd64\n"
"  namespace: kube-system\n"
"  labels:\n"
"    tier: node\n"
"    app: sriovdp\n"
"spec:\n"
"  selector:\n"
"    matchLabels:\n"
"      name: sriov-device-plugin\n"
"  template:\n"
"    metadata:\n"
"      labels:\n"
"        name: sriov-device-plugin\n"
"        tier: node\n"
"        app: sriovdp\n"
"    spec:\n"
"      hostNetwork: true\n"
"      nodeSelector:\n"
"        kubernetes.io/arch: amd64\n"
"      tolerations:\n"
"      - key: node-role.kubernetes.io/master\n"
"        operator: Exists\n"
"        effect: NoSchedule\n"
"      serviceAccountName: sriov-device-plugin\n"
"      containers:\n"
"      - name: kube-sriovdp\n"
"        image: "
"rancher/hardened-sriov-network-device-plugin:v3.5.1-build20231009-amd64\n"
"        imagePullPolicy: IfNotPresent\n"
"        args:\n"
"        - --log-dir=sriovdp\n"
"        - --log-level=10\n"
"        securityContext:\n"
"          privileged: true\n"
"        resources:\n"
"          requests:\n"
"            cpu: \"250m\"\n"
"            memory: \"40Mi\"\n"
"          limits:\n"
"            cpu: 1\n"
"            memory: \"200Mi\"\n"
"        volumeMounts:\n"
"        - name: devicesock\n"
"          mountPath: /var/lib/kubelet/\n"
"          readOnly: false\n"
"        - name: log\n"
"          mountPath: /var/log\n"
"        - name: config-volume\n"
"          mountPath: /etc/pcidp\n"
"        - name: device-info\n"
"          mountPath: /var/run/k8s.cni.cncf.io/devinfo/dp\n"
"      volumes:\n"
"        - name: devicesock\n"
"          hostPath:\n"
"            path: /var/lib/kubelet/\n"
"        - name: log\n"
"          hostPath:\n"
"            path: /var/log\n"
"        - name: device-info\n"
"          hostPath:\n"
"            path: /var/run/k8s.cni.cncf.io/devinfo/dp\n"
"            type: DirectoryOrCreate\n"
"        - name: config-volume\n"
"          configMap:\n"
"            name: sriovdp-config\n"
"            items:\n"
"            - key: config.json\n"
"              path: config.json\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:81
msgid ""
"After applying the config map and the `daemonset`, the device plug-in will "
"be deployed and the interfaces will be discovered and available for the "
"pods."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:82
#, no-wrap
msgid ""
"$ kubectl get pods -n kube-system | grep sriov\n"
"kube-system  kube-sriov-device-plugin-amd64-twjfl  1/1  Running  0  2m\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:83
msgid ""
"Check the interfaces discovered and available in the nodes to be used by the "
"pods:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:84
#, no-wrap
msgid ""
"$ kubectl get $(kubectl get nodes -oname) -o "
"jsonpath='{.status.allocatable}' | jq\n"
"{\n"
"  \"cpu\": \"64\",\n"
"  \"ephemeral-storage\": \"256196109726\",\n"
"  \"hugepages-1Gi\": \"40Gi\",\n"
"  \"hugepages-2Mi\": \"0\",\n"
"  \"intel.com/intel_fec_5g\": \"1\",\n"
"  \"intel.com/intel_sriov_odu\": \"4\",\n"
"  \"intel.com/intel_sriov_oru\": \"4\",\n"
"  \"memory\": \"221396384Ki\",\n"
"  \"pods\": \"110\"\n"
"}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:85
msgid "The `FEC` is `intel.com/intel_fec_5g` and the value is 1."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:86
msgid ""
"The `VF` is `intel.com/intel_sriov_odu` or `intel.com/intel_sriov_oru` if "
"you deploy it with a device plug-in and the config map without Helm charts."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:87
msgid ""
"If there are no interfaces here, it makes little sense to continue because "
"the interface will not be available for pods. Review the config map and "
"filters to solve the issue first."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:88
msgid ""
"*Option 2 (recommended) - Installation using Rancher using Helm chart for "
"SR-IOV CNI and device plug-ins*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:89
msgid "Get Helm if not present:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:90
#, no-wrap
msgid ""
"$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | "
"bash\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:91
msgid "Install SR-IOV."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:92
msgid ""
"This part could be done in two ways, using the `CLI` or using the `Rancher "
"UI`."
msgstr ""

#. type: Labeled list
#: asciidoc/product/atip-features.adoc:93
#, no-wrap
msgid "Install Operator from CLI"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:94
#, no-wrap
msgid ""
"helm repo add suse-edge https://suse-edge.github.io/charts\n"
"helm install sriov-crd suse-edge/sriov-crd -n sriov-network-operator\n"
"helm install install sriov-network-operator suse-edge/sriov-network-operator "
"-n sriov-network-operator\n"
msgstr ""

#. type: Labeled list
#: asciidoc/product/atip-features.adoc:95
#, no-wrap
msgid "Install Operator from Rancher UI"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:96
msgid ""
"Once your cluster is installed, and you have access to the `Rancher UI`, you "
"can install the `SR-IOV Operator` from the `Rancher UI` from the apps tab:"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:97
msgid ""
"Make sure you select the right namespace to install the operator, for "
"example, `sriov-network-operator`."
msgstr ""

#. type: Positional ($1) AttributeList argument for macro 'image'
#: asciidoc/product/atip-features.adoc:98
#, no-wrap
msgid "sriov.png"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-features.adoc:99
#, no-wrap
msgid "features_sriov.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:100
msgid "Check the deployed resources crd and pods:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:101
#, no-wrap
msgid ""
"$ kubectl get crd\n"
"$ kubectl -n sriov-network-operator get pods\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:102
msgid "Check the label in the nodes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:103
msgid "With all resources running, the label appears automatically in your node:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:104
#, no-wrap
msgid ""
"$ kubectl get nodes -oyaml | grep "
"feature.node.kubernetes.io/network-sriov.capable\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:105
#, no-wrap
msgid "feature.node.kubernetes.io/network-sriov.capable: \"true\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:106
msgid ""
"Review the `daemonset` to see the new `sriov-network-config-daemon` and "
"`sriov-rancher-nfd-worker` as active and ready:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:107
#, no-wrap
msgid ""
"$ kubectl get daemonset -A\n"
"NAMESPACE             NAME                            DESIRED   CURRENT   "
"READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                                           "
"AGE\n"
"calico-system            calico-node                     1         1         "
"1       1            1           kubernetes.io/os=linux                                  "
"15h\n"
"sriov-network-operator   sriov-network-config-daemon     1         1         "
"1       1            1           "
"feature.node.kubernetes.io/network-sriov.capable=true   45m\n"
"sriov-network-operator   sriov-rancher-nfd-worker        1         1         "
"1       1            1           <none>                                                  "
"45m\n"
"kube-system              rke2-ingress-nginx-controller   1         1         "
"1       1            1           kubernetes.io/os=linux                                  "
"15h\n"
"kube-system              rke2-multus-ds                  1         1         "
"1       1            1           "
"kubernetes.io/arch=amd64,kubernetes.io/os=linux         15h\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:108
msgid ""
"In a few minutes (can take up to 10 min to be updated), the nodes are "
"detected and configured with the `SR-IOV` capabilities:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:109
#, no-wrap
msgid ""
"$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -A\n"
"NAMESPACE             NAME     AGE\n"
"sriov-network-operator   xr11-2   83s\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:110
msgid "Check the interfaces detected."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:111
msgid ""
"The interfaces discovered should be the PCI address of the network "
"device. Check this information with the `lspci` command in the host."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:112
#, no-wrap
msgid ""
"$ kubectl get sriovnetworknodestates.sriovnetwork.openshift.io -n "
"kube-system -oyaml\n"
"apiVersion: v1\n"
"items:\n"
"- apiVersion: sriovnetwork.openshift.io/v1\n"
"  kind: SriovNetworkNodeState\n"
"  metadata:\n"
"    creationTimestamp: \"2023-06-07T09:52:37Z\"\n"
"    generation: 1\n"
"    name: xr11-2\n"
"    namespace: sriov-network-operator\n"
"    ownerReferences:\n"
"    - apiVersion: sriovnetwork.openshift.io/v1\n"
"      blockOwnerDeletion: true\n"
"      controller: true\n"
"      kind: SriovNetworkNodePolicy\n"
"      name: default\n"
"      uid: 80b72499-e26b-4072-a75c-f9a6218ec357\n"
"    resourceVersion: \"356603\"\n"
"    uid: e1f1654b-92b3-44d9-9f87-2571792cc1ad\n"
"  spec:\n"
"    dpConfigVersion: \"356507\"\n"
"  status:\n"
"    interfaces:\n"
"    - deviceID: \"1592\"\n"
"      driver: ice\n"
"      eSwitchMode: legacy\n"
"      linkType: ETH\n"
"      mac: 40:a6:b7:9b:35:f0\n"
"      mtu: 1500\n"
"      name: p2p1\n"
"      pciAddress: \"0000:51:00.0\"\n"
"      totalvfs: 128\n"
"      vendor: \"8086\"\n"
"    - deviceID: \"1592\"\n"
"      driver: ice\n"
"      eSwitchMode: legacy\n"
"      linkType: ETH\n"
"      mac: 40:a6:b7:9b:35:f1\n"
"      mtu: 1500\n"
"      name: p2p2\n"
"      pciAddress: \"0000:51:00.1\"\n"
"      totalvfs: 128\n"
"      vendor: \"8086\"\n"
"    syncStatus: Succeeded\n"
"kind: List\n"
"metadata:\n"
"  resourceVersion: \"\"\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:113
msgid ""
"If your interface is not detected here, ensure that it is present in the "
"next config map:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:114
#, no-wrap
msgid "$ kubectl get cm supported-nic-ids -oyaml -n sriov-network-operator\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:115
msgid ""
"If your device is not there, edit the config map, adding the right values to "
"be discovered (should be necessary to restart the "
"`sriov-network-config-daemon` daemonset)."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:116
msgid "Create the `NetworkNode Policy` to configure the `VFs`."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:117
msgid ""
"Some `VFs` (`numVfs`) from the device (`rootDevices`) will be created, and "
"it will be configured with the driver `deviceType` and the `MTU`:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:118
msgid ""
"The `resourceName` field must not contain any special characters and must be "
"unique across the cluster.  The example uses the `deviceType: vfio-pci` "
"because `dpdk` will be used in combination with `sr-iov`. If you don't use "
"`dpdk`, the deviceType should be `deviceType: netdevice` (default value)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:119
#, no-wrap
msgid ""
"apiVersion: sriovnetwork.openshift.io/v1\n"
"kind: SriovNetworkNodePolicy\n"
"metadata:\n"
"  name: policy-dpdk\n"
"  namespace: sriov-network-operator\n"
"spec:\n"
"  nodeSelector:\n"
"    feature.node.kubernetes.io/network-sriov.capable: \"true\"\n"
"  resourceName: intelnicsDpdk\n"
"  deviceType: vfio-pci\n"
"  numVfs: 8\n"
"  mtu: 1500\n"
"  nicSelector:\n"
"    deviceID: \"1592\"\n"
"    vendor: \"8086\"\n"
"    rootDevices:\n"
"    - 0000:51:00.0\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:120
msgid "Validate configurations:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:121
#, no-wrap
msgid ""
"$ kubectl get $(kubectl get nodes -oname) -o "
"jsonpath='{.status.allocatable}' | jq\n"
"{\n"
"  \"cpu\": \"64\",\n"
"  \"ephemeral-storage\": \"256196109726\",\n"
"  \"hugepages-1Gi\": \"60Gi\",\n"
"  \"hugepages-2Mi\": \"0\",\n"
"  \"intel.com/intel_fec_5g\": \"1\",\n"
"  \"memory\": \"200424836Ki\",\n"
"  \"pods\": \"110\",\n"
"  \"rancher.io/intelnicsDpdk\": \"8\"\n"
"}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:122
msgid ""
"Create the sr-iov network (optional, just in case a different network is "
"needed):"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:123
#, no-wrap
msgid ""
"apiVersion: sriovnetwork.openshift.io/v1\n"
"kind: SriovNetwork\n"
"metadata:\n"
"  name: network-dpdk\n"
"  namespace: sriov-network-operator\n"
"spec:\n"
"  ipam: |\n"
"    {\n"
"      \"type\": \"host-local\",\n"
"      \"subnet\": \"192.168.0.0/24\",\n"
"      \"rangeStart\": \"192.168.0.20\",\n"
"      \"rangeEnd\": \"192.168.0.60\",\n"
"      \"routes\": [{\n"
"        \"dst\": \"0.0.0.0/0\"\n"
"      }],\n"
"      \"gateway\": \"192.168.0.1\"\n"
"    }\n"
"  vlan: 500\n"
"  resourceName: intelnicsDpdk\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:124
msgid "Check the network created:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:125
#, no-wrap
msgid "$ kubectl get network-attachment-definitions.k8s.cni.cncf.io -A -oyaml\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:126
#, no-wrap
msgid ""
"apiVersion: v1\n"
"items:\n"
"- apiVersion: k8s.cni.cncf.io/v1\n"
"  kind: NetworkAttachmentDefinition\n"
"  metadata:\n"
"    annotations:\n"
"      k8s.v1.cni.cncf.io/resourceName: rancher.io/intelnicsDpdk\n"
"    creationTimestamp: \"2023-06-08T11:22:27Z\"\n"
"    generation: 1\n"
"    name: network-dpdk\n"
"    namespace: sriov-network-operator\n"
"    resourceVersion: \"13124\"\n"
"    uid: df7c89f5-177c-4f30-ae72-7aef3294fb15\n"
"  spec:\n"
"    config: '{ \"cniVersion\":\"0.3.1\", "
"\"name\":\"network-dpdk\",\"type\":\"sriov\",\"vlan\":500,\"vlanQoS\":0,\"ipam\":{\"type\":\"host-local\",\"subnet\":\"192.168.0.0/24\",\"rangeStart\":\"192.168.0.10\",\"rangeEnd\":\"192.168.0.60\",\"routes\":[{\"dst\":\"0.0.0.0/0\"}],\"gateway\":\"192.168.0.1\"}\n"
"      }'\n"
"kind: List\n"
"metadata:\n"
"  resourceVersion: \"\"\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:127
#, no-wrap
msgid "DPDK"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:128
msgid ""
"`DPDK` (Data Plane Development Kit) is a set of libraries and drivers for "
"fast packet processing. It is used to accelerate packet processing workloads "
"running on a wide variety of CPU architectures.  The DPDK includes data "
"plane libraries and optimized network interface controller (`NIC`) drivers "
"for the following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:129
msgid "A queue manager implements lockless queues."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:130
msgid "A buffer manager pre-allocates fixed size buffers."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:131
msgid ""
"A memory manager allocates pools of objects in memory and uses a ring to "
"store free objects; ensures that objects are spread equally on all `DRAM` "
"channels."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:132
msgid ""
"Poll mode drivers (`PMD`) are designed to work without asynchronous "
"notifications, reducing overhead."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:133
msgid ""
"A packet framework as a set of libraries that are helpers to develop packet "
"processing."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:134
msgid ""
"The following steps will show how to enable `DPDK` and how to create `VFs` "
"from the `NICs` to be used by the `DPDK` interfaces:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:135
msgid "Install the `DPDK` package:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:136
#, no-wrap
msgid ""
"$ transactional-update pkg install dpdk22 dpdk22-tools libdpdk-23\n"
"$ reboot\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:137
msgid "Kernel parameters:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:138
msgid "To use DPDK, employ some drivers to enable certain parameters in the kernel:"
msgstr ""

#. type: Table
#: asciidoc/product/atip-features.adoc:139
#, no-wrap
msgid ""
"| parameter | value | description\n"
"\n"
"| iommu\n"
"| pt\n"
"| This option enables the use  of the `vfio` driver for the DPDK "
"interfaces.\n"
"\n"
"| intel_iommu\n"
"| on\n"
"| This option enables the use of `vfio` for `VFs`.\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:140
msgid "To enable the parameters, add them to the `/etc/default/grub` file:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:142
#: asciidoc/product/atip-features.adoc:163
#: asciidoc/product/atip-features.adoc:190
msgid "Update the GRUB configuration and reboot the system to apply the changes:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:144
msgid "Load `vfio-pci` kernel module and enable `SR-IOV` on the `NICs`:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:145
#: asciidoc/product/atip-features.adoc:168
#, no-wrap
msgid "$ modprobe vfio-pci enable_sriov=1 disable_idle_d3=1\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:146
msgid "Create some virtual functions (`VFs`) from the `NICs`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:147
msgid ""
"To create for `VFs`, for example, for two different `NICs`, the following "
"commands are required:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:148
#, no-wrap
msgid ""
"$ echo 4 > /sys/bus/pci/devices/0000:51:00.0/sriov_numvfs\n"
"$ echo 4 > /sys/bus/pci/devices/0000:51:00.1/sriov_numvfs\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:149
msgid "Bind the new VFs with the `vfio-pci` driver:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:150
#, no-wrap
msgid ""
"$ dpdk-devbind.py -b vfio-pci 0000:51:01.0 0000:51:01.1 0000:51:01.2 "
"0000:51:01.3 \\\n"
"                              0000:51:11.0 0000:51:11.1 0000:51:11.2 "
"0000:51:11.3\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:151
msgid "Review the configuration is correctly applied:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:152
#, no-wrap
msgid "$ dpdk-devbind.py -s\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:153
#, no-wrap
msgid ""
"Network devices using DPDK-compatible driver\n"
"============================================\n"
"0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:01.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:01.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:01.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:01.0 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:11.1 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:21.2 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
"0000:51:31.3 'Ethernet Adaptive Virtual Function 1889' drv=vfio-pci "
"unused=iavf,igb_uio\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:154
#, no-wrap
msgid ""
"Network devices using kernel driver\n"
"===================================\n"
"0000:19:00.0 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet "
"1751' if=em1 drv=bnxt_en unused=igb_uio,vfio-pci *Active*\n"
"0000:19:00.1 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet "
"1751' if=em2 drv=bnxt_en unused=igb_uio,vfio-pci\n"
"0000:19:00.2 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet "
"1751' if=em3 drv=bnxt_en unused=igb_uio,vfio-pci\n"
"0000:19:00.3 'BCM57504 NetXtreme-E 10Gb/25Gb/40Gb/50Gb/100Gb/200Gb Ethernet "
"1751' if=em4 drv=bnxt_en unused=igb_uio,vfio-pci\n"
"0000:51:00.0 'Ethernet Controller E810-C for QSFP 1592' if=eth13 drv=ice "
"unused=igb_uio,vfio-pci\n"
"0000:51:00.1 'Ethernet Controller E810-C for QSFP 1592' if=rename8 drv=ice "
"unused=igb_uio,vfio-pci\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:155
#, no-wrap
msgid "vRAN acceleration (`Intel ACC100/ACC200`)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:156
msgid ""
"As communications service providers move from 4 G to 5 G networks, many are "
"adopting virtualized radio access network (`vRAN`) architectures for higher "
"channel capacity and easier deployment of edge-based services and "
"applications. vRAN solutions are ideally located to deliver low-latency "
"services with the flexibility to increase or decrease capacity based on the "
"volume of real-time traffic and demand on the network."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:157
msgid ""
"One of the most compute-intensive 4 G and 5 G workloads is RAN layer 1 "
"(`L1`) `FEC`, which resolves data transmission errors over unreliable or "
"noisy communication channels. `FEC` technology detects and corrects a "
"limited number of errors in 4 G or 5 G data, eliminating the need for "
"retransmission. Since the `FEC` acceleration transaction does not contain "
"cell state information, it can be easily virtualized, enabling pooling "
"benefits and easy cell migration."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:158
#: asciidoc/product/atip-features.adoc:185
msgid "Kernel parameters"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:159
msgid ""
"To enable the `vRAN` acceleration, we need to enable the following kernel "
"parameters (if not present yet):"
msgstr ""

#. type: Table
#: asciidoc/product/atip-features.adoc:160
#, no-wrap
msgid ""
"| parameter | value | description\n"
"\n"
"| iommu\n"
"| pt\n"
"| This option enables the use of vfio for the DPDK interfaces.\n"
"\n"
"| intel_iommu\n"
"| on\n"
"| This option enables the use of vfio for VFs.\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:161
#: asciidoc/product/atip-features.adoc:188
msgid ""
"Modify the GRUB file `/etc/default/grub` to add them to the kernel command "
"line:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:165
msgid ""
"To verify that the parameters are applied after the reboot, check the "
"command line:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:167
msgid "Load vfio-pci kernel modules to enable the `vRAN` acceleration:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:169
msgid "Get interface information Acc100:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:171
msgid "Bind the physical interface (`PF`) with `vfio-pci` driver:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:172
#, no-wrap
msgid "$ dpdk-devbind.py -b vfio-pci 0000:8a:00.0\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:173
msgid "Create the virtual functions (`VFs`) from the physical interface (`PF`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:174
msgid ""
"Create 2 `VFs` from the `PF` and bind with `vfio-pci` following the next "
"steps:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:175
#, no-wrap
msgid ""
"$ echo 2 > /sys/bus/pci/devices/0000:8a:00.0/sriov_numvfs\n"
"$ dpdk-devbind.py -b vfio-pci 0000:8b:00.0\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:176
msgid "Configure acc100 with the proposed configuration file:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:177
#, no-wrap
msgid ""
"$ pf_bb_config ACC100 -c /opt/pf-bb-config/acc100_config_vf_5g.cfg\n"
"Tue Jun  6 10:49:20 2023:INFO:Queue Groups: 2 5GUL, 2 5GDL, 2 4GUL, 2 4GDL\n"
"Tue Jun  6 10:49:20 2023:INFO:Configuration in VF mode\n"
"Tue Jun  6 10:49:21 2023:INFO: ROM version MM 99AD92\n"
"Tue Jun  6 10:49:21 2023:WARN:* Note: Not on DDR PRQ version  1302020 != "
"10092020\n"
"Tue Jun  6 10:49:21 2023:INFO:PF ACC100 configuration complete\n"
"Tue Jun  6 10:49:21 2023:INFO:ACC100 PF [0000:8a:00.0] configuration "
"complete!\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:178
msgid "Check the new VFs created from the FEC PF:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:179
#, no-wrap
msgid ""
"$ dpdk-devbind.py -s\n"
"Baseband devices using DPDK-compatible driver\n"
"=============================================\n"
"0000:8a:00.0 'Device 0d5c' drv=vfio-pci unused=\n"
"0000:8b:00.0 'Device 0d5d' drv=vfio-pci unused=\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:180
#, no-wrap
msgid ""
"Other Baseband devices\n"
"======================\n"
"0000:8b:00.1 'Device 0d5d' unused=\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:181
#, no-wrap
msgid "Huge pages"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:182
msgid ""
"When a process uses `RAM`, the `CPU` marks it as used by that process. For "
"efficiency, the `CPU` allocates `RAM` in chunks `4K` bytes is the default "
"value on many platforms. Those chunks are named pages. Pages can be swapped "
"to disk, etc."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:183
msgid ""
"Since the process address space is virtual, the `CPU` and the operating "
"system need to remember which pages belong to which process, and where each "
"page is stored. The greater the number of pages, the longer the search for "
"memory mapping. When a process uses `1 GB` of memory, that is 262144 entries "
"to look up (`1 GB` / `4 K`). If a page table entry consumes 8 bytes, that is "
"`2 MB` (262144 * 8) to look up."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:184
msgid ""
"Most current `CPU` architectures support larger-than-default pages, which "
"give the `CPU/OS` fewer entries to look up."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:186
msgid "To enable the huge pages, we should add the next kernel parameters:"
msgstr ""

#. type: Table
#: asciidoc/product/atip-features.adoc:187
#, no-wrap
msgid ""
"| parameter | value | description\n"
"\n"
"| hugepagesz\n"
"| 1G\n"
"| This option allows to set the size of huge pages to 1 G\n"
"\n"
"| hugepages\n"
"| 40\n"
"| This is the number of huge pages defined before\n"
"\n"
"| default_hugepagesz\n"
"| 1G\n"
"| This is the default value to get the huge pages\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:192
msgid ""
"To validate that the parameters are applied after the reboot, you can check "
"the command line:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:194
msgid "Using huge pages"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:195
msgid "To use the huge pages, we need to mount them:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:196
#, no-wrap
msgid ""
"$ mkdir -p /hugepages\n"
"$ mount -t hugetlbfs nodev /hugepages\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:197
msgid "Deploy a Kubernetes workload, creating the resources and the volumes:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:198
#, no-wrap
msgid ""
"...\n"
" resources:\n"
"   requests:\n"
"     memory: \"24Gi\"\n"
"     hugepages-1Gi: 16Gi\n"
"     intel.com/intel_sriov_oru: '4'\n"
"   limits:\n"
"     memory: \"24Gi\"\n"
"     hugepages-1Gi: 16Gi\n"
"     intel.com/intel_sriov_oru: '4'\n"
"...\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:199
#, no-wrap
msgid ""
"...\n"
"volumeMounts:\n"
"  - name: hugepage\n"
"    mountPath: /hugepages\n"
"...\n"
"volumes:\n"
"  - name: hugepage\n"
"    emptyDir:\n"
"      medium: HugePages\n"
"...\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:200
#, no-wrap
msgid "CPU pinning configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:201
msgid "Requirements"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:202
msgid ""
"Must have the `CPU` tuned to the performance profile covered in this "
"<<cpu-tuned-configuration,section>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:203
msgid ""
"Must have the `RKE2` cluster kubelet configured with the CPU management "
"arguments adding the following block (as an example) to the "
"`/etc/rancher/rke2/config.yaml` file:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:204
#, no-wrap
msgid ""
"kubelet-arg:\n"
"- \"cpu-manager=true\"\n"
"- \"cpu-manager-policy=static\"\n"
"- \"cpu-manager-policy-options=full-pcpus-only=true\"\n"
"- \"cpu-manager-reconcile-period=0s\"\n"
"- \"kubelet-reserved=cpu=1\"\n"
"- \"system-reserved=cpu=1\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:205
msgid "Using CPU pinning on Kubernetes"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:206
msgid ""
"There are three ways to use that feature using the `Static Policy` defined "
"in kubelet depending on the requests and limits you define on your workload:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:207
msgid ""
"`BestEffort` QoS Class: If you do not define any request or limit for `CPU`, "
"the pod is scheduled on the first `CPU` available on the system."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:208
msgid "An example of using the `BestEffort` QoS Class could be:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:209
#, no-wrap
msgid ""
"spec:\n"
"  containers:\n"
"  - name: nginx\n"
"    image: nginx\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:210
msgid ""
"`Burstable` QoS Class: If you define a request for CPU, which is not equal "
"to the limits, or there is no CPU request."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:211
msgid "Examples of using the `Burstable` QoS Class could be:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:212
#, no-wrap
msgid ""
"spec:\n"
"  containers:\n"
"  - name: nginx\n"
"    image: nginx\n"
"    resources:\n"
"      limits:\n"
"        memory: \"200Mi\"\n"
"      requests:\n"
"        memory: \"100Mi\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:213
msgid "or"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:214
#, no-wrap
msgid ""
"spec:\n"
"  containers:\n"
"  - name: nginx\n"
"    image: nginx\n"
"    resources:\n"
"      limits:\n"
"        memory: \"200Mi\"\n"
"        cpu: \"2\"\n"
"      requests:\n"
"        memory: \"100Mi\"\n"
"        cpu: \"1\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:215
msgid ""
"`Guaranteed` QoS Class: If you define a request for CPU, which is equal to "
"the limits."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:216
msgid "An example of using the `Guaranteed` QoS Class could be:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:217
#, no-wrap
msgid ""
"spec:\n"
"  containers:\n"
"    - name: nginx\n"
"      image: nginx\n"
"      resources:\n"
"        limits:\n"
"          memory: \"200Mi\"\n"
"          cpu: \"2\"\n"
"        requests:\n"
"          memory: \"200Mi\"\n"
"          cpu: \"2\"\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:218
#, no-wrap
msgid "NUMA-aware scheduling"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:219
msgid ""
"Non-Uniform Memory Access or Non-Uniform Memory Architecture (`NUMA`) is a "
"physical memory design used in `SMP` (multiprocessors) architecture, where "
"the memory access time depends on the memory location relative to a "
"processor. Under `NUMA`, a processor can access its own local memory faster "
"than non-local memory, that is, memory local to another processor or memory "
"shared between processors."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-features.adoc:220
#, no-wrap
msgid "Identifying NUMA nodes"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:221
msgid "To identify the `NUMA` nodes, on your system use the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:222
#, no-wrap
msgid ""
"$ lscpu | grep NUMA\n"
"NUMA node(s):                       1\n"
"NUMA node0 CPU(s):                  0-63\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:223
msgid "For this example, we have only one `NUMA` node showing 64 `CPUs`."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:224
msgid ""
"`NUMA` needs to be enabled in the `BIOS`. If `dmesg` does not have records "
"of NUMA initialization during the bootup, then `NUMA`-related messages in "
"the kernel ring buffer might have been overwritten."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:225
#, no-wrap
msgid "Metal LB"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:226
msgid ""
"`MetalLB` is a load-balancer implementation for bare-metal Kubernetes "
"clusters, using standard routing protocols like `L2` and `BGP` as "
"advertisement protocols. It is a network load balancer that can be used to "
"expose services in a Kubernetes cluster to the outside world due to the need "
"to use Kubernetes Services type `LoadBalancer` with bare metal."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:227
msgid "To enable `MetalLB` in the `RKE2` cluster, the following steps are required:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:228
msgid "Install `MetalLB` using the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:229
#, no-wrap
msgid ""
"$ kubectl apply <<EOF -f\n"
"apiVersion: helm.cattle.io/v1\n"
"kind: HelmChart\n"
"metadata:\n"
"  name: metallb\n"
"  namespace: kube-system\n"
"spec:\n"
"  repo: https://metallb.github.io/metallb/\n"
"  chart: metallb\n"
"  targetNamespace: metallb-system\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:230
#, no-wrap
msgid ""
"apiVersion: helm.cattle.io/v1\n"
"kind: HelmChart\n"
"metadata:\n"
"  name: endpoint-copier-operator\n"
"  namespace: kube-system\n"
"spec:\n"
"  repo: https://suse-edge.github.io/endpoint-copier-operator\n"
"  chart: endpoint-copier-operator\n"
"  targetNamespace: endpoint-copier-operator\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:231
msgid "Create the `IpAddressPool` and the `L2advertisement` configuration:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:232
#, no-wrap
msgid ""
"apiVersion: metallb.io/v1beta1\n"
"kind: IPAddressPool\n"
"metadata:\n"
"  name: kubernetes-vip-ip-pool\n"
"  namespace: metallb-system\n"
"spec:\n"
"  addresses:\n"
"    - 10.168.200.98/32\n"
"  serviceAllocation:\n"
"    priority: 100\n"
"    namespaces:\n"
"      - default\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:233
#, no-wrap
msgid ""
"apiVersion: metallb.io/v1beta1\n"
"kind: L2Advertisement\n"
"metadata:\n"
"  name: ip-pool-l2-adv\n"
"  namespace: metallb-system\n"
"spec:\n"
"  ipAddressPools:\n"
"    - kubernetes-vip-ip-pool\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:234
msgid "Create the endpoint service to expose the `VIP`:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:235
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Service\n"
"metadata:\n"
"  name: kubernetes-vip\n"
"  namespace: default\n"
"spec:\n"
"  internalTrafficPolicy: Cluster\n"
"  ipFamilies:\n"
"  - IPv4\n"
"  ipFamilyPolicy: SingleStack\n"
"  ports:\n"
"  - name: rke2-api\n"
"    port: 9345\n"
"    protocol: TCP\n"
"    targetPort: 9345\n"
"  - name: k8s-api\n"
"    port: 6443\n"
"    protocol: TCP\n"
"    targetPort: 6443\n"
"  sessionAffinity: None\n"
"  type: LoadBalancer\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:236
msgid "Check the `VIP` is created and the `MetalLB` pods are running:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:237
#, no-wrap
msgid ""
"$ kubectl get svc -n default\n"
"$ kubectl get pods -n default\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-features.adoc:238
#, no-wrap
msgid "Private registry configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:239
msgid ""
"`Containerd` can be configured to connect to private registries and use them "
"to pull private images on each node."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:240
msgid ""
"Upon startup, `RKE2` checks if a `registries.yaml` file exists at "
"`/etc/rancher/rke2/` and instructs `containerd` to use any registries "
"defined in the file. If you wish to use a private registry, create this file "
"as root on each node that will use the registry."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:241
msgid ""
"To add the private registry, create the file "
"`/etc/rancher/rke2/registries.yaml` with the following content:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:242
#, no-wrap
msgid ""
"mirrors:\n"
"  docker.io:\n"
"    endpoint:\n"
"      - \"https://registry.example.com:5000\"\n"
"configs:\n"
"  \"registry.example.com:5000\":\n"
"    auth:\n"
"      username: xxxxxx # this is the registry username\n"
"      password: xxxxxx # this is the registry password\n"
"    tls:\n"
"      cert_file:            # path to the cert file used to authenticate to "
"the registry\n"
"      key_file:             # path to the key file for the certificate used "
"to authenticate to the registry\n"
"      ca_file:              # path to the ca file used to verify the "
"registry's certificate\n"
"      insecure_skip_verify: # may be set to true to skip verifying the "
"registry's certificate\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:243
msgid "or without authentication:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-features.adoc:244
#, no-wrap
msgid ""
"mirrors:\n"
"  docker.io:\n"
"    endpoint:\n"
"      - \"https://registry.example.com:5000\"\n"
"configs:\n"
"  \"registry.example.com:5000\":\n"
"    tls:\n"
"      cert_file:            # path to the cert file used to authenticate to "
"the registry\n"
"      key_file:             # path to the key file for the certificate used "
"to authenticate to the registry\n"
"      ca_file:              # path to the ca file used to verify the "
"registry's certificate\n"
"      insecure_skip_verify: # may be set to true to skip verifying the "
"registry's certificate\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-features.adoc:245
msgid ""
"For the registry changes to take effect, you need to either configure this "
"file before starting RKE2 on the node, or restart RKE2 on each configured "
"node."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-features.adoc:246
msgid ""
"For more information about this, please check "
"https://docs.rke2.io/install/containerd_registry_configuration#registries-configuration-file[containerd "
"registry configuration rke2]."
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-automated-provision.adoc:1
#, no-wrap
msgid "Fully automated directed network provisioning"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:3
msgid ""
"Direct network provisioning is a feature that allows you to automate the "
"provisioning of downstream clusters. This feature is useful when you have "
"many downstream clusters to provision, and you want to automate the process."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:4
msgid ""
"A <<atip-management-cluster,management cluster>> automates deployment of the "
"following components:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:5
msgid ""
"`SUSE Linux Enterprise Micro RT` as the OS. Depending on the use case, "
"configurations like networking, storage, users and kernel arguments can be "
"customized."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:6
msgid ""
"`RKE2` as the Kubernetes cluster. The default `CNI` plug-in is "
"`Cilium`. Depending on the use case, certain `CNI` plug-ins can be used, "
"such as `Cilium+Multus`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:7
msgid "`Longhorn` as the storage solution."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:8
msgid "`NeuVector` as the security solution."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:9
msgid ""
"`MetalLB` can be used as the load balancer for highly available multi-node "
"clusters."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:10
msgid ""
"For more information about `SUSE Linux Enterprise Micro`, see "
"<<components-slmicro>> For more information about `RKE2`, see "
"<<components-rke2>> For more information about `Longhorn`, see "
"<<components-longhorn>> For more information about `NeuVector`, see "
"<<components-neuvector>>"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:11
msgid ""
"The following sections describe the different directed network provisioning "
"workflows and some additional features that can be added to the provisioning "
"process:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:12
msgid "xref:eib-edge-image-connected[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:13
msgid "xref:eib-edge-image-airgap[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:14
msgid "xref:single-node[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:15
msgid "xref:multi-node[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:16
msgid "xref:advanced-network-configuration[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:17
msgid "xref:add-telco[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:18
msgid "xref:atip-private-registry[]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:19
msgid "xref:airgap-deployment[]"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:20
#, no-wrap
msgid "Prepare downstream cluster image for connected scenarios"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:23
#, no-wrap
msgid "Prerequisites for connected scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:24
#: asciidoc/product/atip-automated-provision.adoc:72
msgid ""
"A container runtime such as https://podman.io[Podman] or "
"https://rancherdesktop.io[Rancher Desktop] is required to run Edge Image "
"Builder."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:25
#: asciidoc/product/atip-automated-provision.adoc:73
msgid ""
"The base image `SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw` must be downloaded "
"from the https://scc.suse.com/[SUSE Customer Center] or the "
"https://www.suse.com/download/sle-micro/[SUSE Download page]."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:26
#, no-wrap
msgid "Image configuration for connected scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:27
#: asciidoc/product/atip-automated-provision.adoc:76
msgid ""
"When running Edge Image Builder, a directory is mounted from the host, so it "
"is necessary to create a directory structure to store the configuration "
"files used to define the target image."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:30
#: asciidoc/product/atip-automated-provision.adoc:79
msgid "The `network` folder is optional, see <<add-network-eib>> for more details."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:31
msgid ""
"The custom/scripts directory contains scripts to be run on first-boot; "
"currently a `01-fix-growfs.sh` script is required to resize the OS root "
"partition on deployment"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:32
#, no-wrap
msgid ""
"├── downstream-cluster-config.yaml\n"
"├── base-images/\n"
"│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw\n"
"├── network/\n"
"|   └ configure-network.sh\n"
"└── custom/\n"
"    └ scripts/\n"
"        └ 01-fix-growfs.sh\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:35
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: RAW\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw\n"
"  outputImageName: eibimage-slemicro55rt-telco.raw\n"
"operatingSystem:\n"
"  kernelArgs:\n"
"    - ignition.platform.id=openstack\n"
"    - net.ifnames=1\n"
"  systemd:\n"
"    disable:\n"
"      - rebootmgr\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: ${ROOT_PASSWORD}\n"
"      sshKeys:\n"
"      - ${USERKEY1}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:42
#: asciidoc/product/atip-automated-provision.adoc:86
msgid ""
"Currently, a custom script (`custom/scripts/01-fix-growfs.sh`) is required "
"to grow the file system to match the disk size on first-boot after "
"provisioning. The `01-fix-growfs.sh` script contains the following "
"information:"
msgstr ""

#. type: Title =====
#: asciidoc/product/atip-automated-provision.adoc:46
#, no-wrap
msgid "Additional configuration for Telco workloads"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:47
msgid ""
"To enable Telco features like `dpdk`, `sr-iov` or `FEC`, additional packages "
"may be required as shown in the following example."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:48
#, no-wrap
msgid ""
"apiVersion: 1.0\n"
"image:\n"
"  imageType: RAW\n"
"  arch: x86_64\n"
"  baseImage: SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw\n"
"  outputImageName: eibimage-slemicro55rt-telco.raw\n"
"operatingSystem:\n"
"  kernelArgs:\n"
"    - ignition.platform.id=openstack\n"
"    - net.ifnames=1\n"
"  systemd:\n"
"    disable:\n"
"      - rebootmgr\n"
"  users:\n"
"    - username: root\n"
"      encryptedPassword: ${ROOT_PASSWORD}\n"
"      sshKeys:\n"
"      - ${user1Key1}\n"
"  packages:\n"
"    packageList:\n"
"      - jq\n"
"      - dpdk22\n"
"      - dpdk22-tools\n"
"      - libdpdk-23\n"
"      - pf-bb-config\n"
"    additionalRepos:\n"
"      - url: "
"https://download.opensuse.org/repositories/isv:/SUSE:/Edge:/Telco/SLEMicro5.5/\n"
"    sccRegistrationCode: ${SCC_REGISTRATION_CODE}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:49
msgid ""
"Where `$\\{SCC_REGISTRATION_CODE\\}` is the registration code copied from "
"https://scc.suse.com/[SUSE Customer Center], and the package list contains "
"the minimum packages to be used for the Telco profiles.  To use the "
"`pf-bb-config` package (to enable the `FEC` feature and binding with "
"drivers), the `additionalRepos` block must be included to add the `SUSE Edge "
"Telco` repository."
msgstr ""

#. type: Title =====
#: asciidoc/product/atip-automated-provision.adoc:50
#, no-wrap
msgid "Additional script for Advanced Network Configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:51
msgid ""
"If you need to configure static IPs or more advanced networking scenarios as "
"described in <<advanced-network-configuration>>, the following additional "
"configuration is required."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:52
msgid ""
"In the `network` folder, create the following `configure-network.sh` file - "
"this consumes configuration drive data on first-boot, and configures the "
"host networking using the https://github.com/suse-edge/nm-configurator[NM "
"Configurator tool]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:66
#: asciidoc/product/atip-automated-provision.adoc:120
msgid ""
"This creates the output ISO image file named "
"`eibimage-slemicro55rt-telco.raw`, based on the definition described above."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:67
msgid ""
"The output image must then be made available via a webserver, either the "
"media-server container enabled via the <<metal3-media-server,Management "
"Cluster Documentation>> or some other locally accessible server.  In the "
"examples below, we refer to this server as `imagecache.local:8080`"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:68
#, no-wrap
msgid "Prepare downstream cluster image for air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:70
msgid ""
"Much of the configuration is possible with Edge Image Builder, but in this "
"guide, we cover the minimal configurations necessary to set up the "
"downstream cluster for air-gap scenarios."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:71
#, no-wrap
msgid "Prerequisites for air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:74
msgid ""
"If you want to use SR-IOV or any other workload which require a container "
"image, a local private registry must be deployed and already configured "
"(with/without TLS and/or authentication). This registry will be used to "
"store the images and the helm chart OCI images."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:75
#, no-wrap
msgid "Image configuration for air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:77
msgid ""
"`downstream-cluster-airgap-config.yaml` is the image definition file, see "
"<<quickstart-eib>> for more details."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:80
msgid ""
"The `custom/scripts` directory contains scripts to be run on first-boot; "
"currently a `01-fix-growfs.sh` script is required to resize the OS root "
"partition on deployment. For air-gap scenarios, a script `02-airgap.sh` is "
"required to copy the images to the right place during the image creation "
"process."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:81
msgid ""
"The `custom/files` directory contains the `rke2` and the `cni` images to be "
"copied to the image during the image creation process."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:82
#, no-wrap
msgid ""
"├── downstream-cluster-airgap-config.yaml\n"
"├── base-images/\n"
"│   └ SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw\n"
"├── network/\n"
"|   └ configure-network.sh\n"
"└── custom/\n"
"    └ files/\n"
"    |   └ install.sh\n"
"    |   └ rke2-images-cilium.linux-amd64.tar.zst\n"
"    |   └ rke2-images-core.linux-amd64.tar.zst\n"
"    |   └ rke2-images-multus.linux-amd64.tar.zst\n"
"    |   └ rke2-images.linux-amd64.tar.zst\n"
"    |   └ rke2.linux-amd64.tar.zst\n"
"    |   └ sha256sum-amd64.txt\n"
"    └ scripts/\n"
"        └ 01-fix-growfs.sh\n"
"        └ 02-airgap.sh\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:84
msgid ""
"The `downstream-cluster-airgap-config.yaml` file is the main configuration "
"file for the downstream cluster image and the content has been described in "
"the previous xref:add-telco-feature-eib[section]."
msgstr ""

#. type: Title =====
#: asciidoc/product/atip-automated-provision.adoc:88
#, no-wrap
msgid "Air-gap script"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:89
msgid ""
"The following script (`custom/scripts/02-airgap.sh`) is required to copy the "
"images to the right place during the image creation process:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:91
#, no-wrap
msgid ""
"# create the folder to extract the artifacts there\n"
"mkdir -p /opt/rke2-artifacts\n"
"mkdir -p /var/lib/rancher/rke2/agent/images\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:92
#, no-wrap
msgid ""
"# copy the artifacts\n"
"cp install.sh /opt/\n"
"cp rke2-images*.tar.zst rke2.linux-amd64.tar.gz sha256sum-amd64.txt "
"/opt/rke2-artifacts/\n"
msgstr ""

#. type: Title =====
#: asciidoc/product/atip-automated-provision.adoc:93
#, no-wrap
msgid "Custom files for air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:94
msgid ""
"The `custom/files` directory contains the `rke2` and the `cni` images to be "
"copied to the image during the image creation process.  To easily generate "
"the images, prepare them locally using following "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-rke2-images.sh[script] "
"and the list of images "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-release-rke2-images.txt[here] "
"to generate the artifacts required to be included in `custom/files`.  Also, "
"you can download the latest `rke2-install` script from "
"https://get.rke2.io/[here]."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:95
#, no-wrap
msgid ""
"$ ./edge-save-rke2-images.sh -o custom/files -l "
"~/edge-release-rke2-images.txt\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:96
msgid "After downloading the images, the directory structure should look like this:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:97
#, no-wrap
msgid ""
"└── custom/\n"
"    └ files/\n"
"        └ install.sh\n"
"        └ rke2-images-cilium.linux-amd64.tar.zst\n"
"        └ rke2-images-core.linux-amd64.tar.zst\n"
"        └ rke2-images-multus.linux-amd64.tar.zst\n"
"        └ rke2-images.linux-amd64.tar.zst\n"
"        └ rke2.linux-amd64.tar.zst\n"
"        └ sha256sum-amd64.txt\n"
msgstr ""

#. type: Title =====
#: asciidoc/product/atip-automated-provision.adoc:98
#, no-wrap
msgid ""
"Preload your private registry with images required for air-gap scenarios and "
"SR-IOV (optional)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:99
msgid ""
"If you want to use SR-IOV in your air-gap scenario or any other workload "
"images, you must preload your local private registry with the images "
"following the next steps:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:100
msgid ""
"Download, extract, and push the helm-chart OCI images to the private "
"registry"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:101
msgid ""
"Download, extract, and push the rest of images required to the private "
"registry"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:102
msgid ""
"The following scripts can be used to download, extract, and push the images "
"to the private registry. We will show an example to preload the SR-IOV "
"images, but you can also use the same approach to preload any other custom "
"images:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:103
msgid "Preload with helm-chart OCI images for SR-IOV:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:104
msgid "You must create a list with the helm-chart OCI images required:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:105
#, no-wrap
msgid ""
"$ cat > edge-release-helm-oci-artifacts.txt <<EOF\n"
"edge/sriov-network-operator-chart:1.2.2\n"
"edge/sriov-crd-chart:1.2.2\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:106
msgid ""
"Generate a local tarball file using the following "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-oci-artefacts.sh[script] "
"and the list created above:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:107
#, no-wrap
msgid ""
"$ ./edge-save-oci-artefacts.sh -al ./edge-release-helm-oci-artifacts.txt -s "
"registry.suse.com\n"
"Pulled: registry.suse.com/edge/sriov-network-operator-chart:1.2.2\n"
"Pulled: registry.suse.com/edge/sriov-crd-chart:1.2.2\n"
"a edge-release-oci-tgz-20240705\n"
"a edge-release-oci-tgz-20240705/sriov-network-operator-chart-1.2.2.tgz\n"
"a edge-release-oci-tgz-20240705/sriov-crd-chart-1.2.2.tgz\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:108
msgid ""
"Upload your tarball file to your private registry (e.g. `myregistry:5000`) "
"using the following "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-oci-artefacts.sh[script] "
"to preload your registry with the helm chart OCI images downloaded in the "
"previous step:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:109
#, no-wrap
msgid ""
"$ tar zxvf edge-release-oci-tgz-20240705.tgz\n"
"$ ./edge-load-oci-artefacts.sh -ad edge-release-oci-tgz-20240705 -r "
"myregistry:5000\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:110
msgid "Preload with the rest of the images required for SR-IOV:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:111
msgid ""
"In this case, we must include the `sr-iov container images for telco "
"workloads (e.g. as a reference, you could get them from "
"https://github.com/suse-edge/charts/blob/release-3.0/charts/sriov-network-operator/1.2.2%2Bup0.1.0/values.yaml[helm-chart "
"values])"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:112
#, no-wrap
msgid ""
"$ cat > edge-release-images.txt <<EOF\n"
"rancher/hardened-sriov-network-operator:v1.2.0-build20240327\n"
"rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327\n"
"rancher/hardened-sriov-cni:v1.2.0-build20240327\n"
"rancher/hardened-ib-sriov-cni:v1.2.0-build20240327\n"
"rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327\n"
"rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327\n"
"rancher/hardened-sriov-network-webhook:v1.2.0-build20240327\n"
"EOF\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:113
msgid ""
"Using the following "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-save-images.sh[script] "
"and the list created above, you must generate locally the tarball file with "
"the images required:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:114
#, no-wrap
msgid ""
"$ ./edge-save-images.sh -al ./edge-release-images.txt -s registry.suse.com\n"
"Pulled: "
"registry.suse.com/rancher/hardened-sriov-network-operator:v1.2.0-build20240327\n"
"Pulled: "
"registry.suse.com/rancher/rancher/hardened-sriov-network-config-daemon:v1.2.0-build20240327\n"
"Pulled: registry.suse.com/rancher/hardened-sriov-cni:v1.2.0-build20240327\n"
"Pulled: "
"registry.suse.com/rancher/hardened-ib-sriov-cni:v1.2.0-build20240327\n"
"Pulled: "
"registry.suse.com/rancher/hardened-sriov-network-device-plugin:v1.2.0-build20240327\n"
"Pulled: "
"registry.suse.com/rancher/hardened-sriov-network-resources-injector:v1.2.0-build20240327\n"
"Pulled: "
"registry.suse.com/rancher/hardened-sriov-network-webhook:v1.2.0-build20240327\n"
"a edge-release-images-tgz-20240705\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-network-operator-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-network-config-daemon-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-cni-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-ib-sriov-cni-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-network-device-plugin-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-network-resources-injector-v1.2.0-build20240327.tar.gz\n"
"a "
"edge-release-images-tgz-20240705/hardened-sriov-network-webhook-v1.2.0-build20240327.tar.gz\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:115
msgid ""
"Upload your tarball file to your private registry (e.g. `myregistry:5000`) "
"using the following "
"https://github.com/suse-edge/fleet-examples/blob/release-3.0/scripts/day2/edge-load-images.sh[script] "
"to preload your private registry with the images downloaded in the previous "
"step:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:116
#, no-wrap
msgid ""
"$ tar zxvf edge-release-images-tgz-20240705.tgz\n"
"$ ./edge-load-images.sh -ad edge-release-images-tgz-20240705 -r "
"myregistry:5000\n"
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:117
#, no-wrap
msgid "Image creation for air-gap scenarios"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:119
#, no-wrap
msgid ""
"podman run --rm --privileged -it -v $PWD:/eib \\\n"
" registry.suse.com/edge/edge-image-builder:1.0.2 \\\n"
" build --definition-file downstream-cluster-airgap-config.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:121
msgid ""
"The output image must then be made available via a webserver, either the "
"media-server container enabled via the <<metal3-media-server,Management "
"Cluster Documentation>> or some other locally accessible server.  In the "
"examples below, we refer to this server as `imagecache.local:8080`."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:122
#, no-wrap
msgid ""
"Downstream cluster provisioning with Direct network provisioning "
"(single-node)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:123
msgid ""
"This section describes the workflow used to automate the provisioning of a "
"single-node downstream cluster using directed network provisioning.  This is "
"the simplest way to automate the provisioning of a downstream cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:124
#: asciidoc/product/atip-automated-provision.adoc:167
#: asciidoc/product/atip-automated-provision.adoc:223
#: asciidoc/product/atip-automated-provision.adoc:249
msgid "*Requirements*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:125
#: asciidoc/product/atip-automated-provision.adoc:168
msgid ""
"The image generated using `EIB`, as described in the "
"xref:eib-edge-image-connected[previous section], with the minimal "
"configuration to set up the downstream cluster has to be located in the "
"management cluster exactly on the path you configured on "
"xref:metal3-media-server[this section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:126
msgid ""
"The management server created and available to be used on the following "
"sections. For more information, refer to the Management Cluster section "
"<<atip-management-cluster>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:127
#: asciidoc/product/atip-automated-provision.adoc:170
msgid "*Workflow*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:128
msgid ""
"The following diagram shows the workflow used to automate the provisioning "
"of a single-node downstream cluster using directed network provisioning:"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-automated-provision.adoc:129
#, no-wrap
msgid "atip-automated-singlenode1.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:130
msgid ""
"There are two different steps to automate the provisioning of a single-node "
"downstream cluster using directed network provisioning:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:131
msgid ""
"Enroll the bare-metal host to make it available for the provisioning "
"process."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:132
msgid ""
"Provision the bare-metal host to install and configure the operating system "
"and the Kubernetes cluster."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:133
msgid "*Enroll the bare-metal host*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:134
msgid ""
"The first step is to enroll the new bare-metal host in the management "
"cluster to make it available to be provisioned.  To do that, the following "
"file (`bmh-example.yaml`) has to be created in the management cluster, to "
"specify the `BMC` credentials to be used and the `BaremetalHost` object to "
"be enrolled:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:135
#: asciidoc/product/atip-automated-provision.adoc:241
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: example-demo-credentials\n"
"type: Opaque\n"
"data:\n"
"  username: ${BMC_USERNAME}\n"
"  password: ${BMC_PASSWORD}\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:136
#, no-wrap
msgid ""
"apiVersion: metal3.io/v1alpha1\n"
"kind: BareMetalHost\n"
"metadata:\n"
"  name: flexran-demo\n"
"  labels:\n"
"    cluster-role: control-plane\n"
"spec:\n"
"  online: true\n"
"  bootMACAddress: ${BMC_MAC}\n"
"  rootDeviceHints:\n"
"    deviceName: /dev/nvme0n1\n"
"  bmc:\n"
"    address: ${BMC_ADDRESS}\n"
"    disableCertificateVerification: true\n"
"    credentialsName: example-demo-credentials\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:137
msgid "where:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:138
msgid ""
"`$\\{BMC_USERNAME\\}` — The user name for the `BMC` of the new bare-metal "
"host."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:139
msgid ""
"`$\\{BMC_PASSWORD\\}` — The password for the `BMC` of the new bare-metal "
"host."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:140
msgid "`$\\{BMC_MAC\\}` — The `MAC` address of the new bare-metal host to be used."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:141
msgid ""
"`$\\{BMC_ADDRESS\\}` — The `URL` for the bare-metal host `BMC` (for example, "
"`redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/`). To learn "
"more about the different options available depending on your hardware "
"provider, check the following "
"https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md[link]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:142
msgid ""
"Once the file is created, the following command has to be executed in the "
"management cluster to start enrolling the new bare-metal host in the "
"management cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:143
#, no-wrap
msgid "$ kubectl apply -f bmh-example.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:144
msgid ""
"The new bare-metal host object will be enrolled, changing its state from "
"registering to inspecting and available. The changes can be checked using "
"the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:145
#, no-wrap
msgid "$ kubectl get bmh\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:146
#: asciidoc/product/atip-automated-provision.adoc:190
msgid ""
"The `BaremetalHost` object is in the `registering` state until the `BMC` "
"credentials are validated. Once the credentials are validated, the "
"`BaremetalHost` object changes its state to `inspecting`, and this step "
"could take some time depending on the hardware (up to 20 minutes). During "
"the inspecting phase, the hardware information is retrieved and the "
"Kubernetes object is updated. Check the information using the following "
"command: `kubectl get bmh -o yaml`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:147
#: asciidoc/product/atip-automated-provision.adoc:191
msgid "*Provision step*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:148
msgid ""
"Once the bare-metal host is enrolled and available, the next step is to "
"provision the bare-metal host to install and configure the operating system "
"and the Kubernetes cluster.  To do that, the following file "
"(`capi-provisioning-example.yaml`) has to be created in the "
"management-cluster with the following information (the "
"`capi-provisioning-example.yaml` can be generated by joining the following "
"blocks)."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:149
#: asciidoc/product/atip-automated-provision.adoc:193
msgid "Only values between `$\\{...\\}` must be replaced with the real values."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:150
msgid ""
"The following block is the cluster definition, where the networking can be "
"configured using the `pods` and the `services` blocks. Also, it contains the "
"references to the control plane and the infrastructure (using the `Metal^3^` "
"provider) objects to be used."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:151
#, no-wrap
msgid ""
"apiVersion: cluster.x-k8s.io/v1beta1\n"
"kind: Cluster\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  clusterNetwork:\n"
"    pods:\n"
"      cidrBlocks:\n"
"        - 192.168.0.0/18\n"
"    services:\n"
"      cidrBlocks:\n"
"        - 10.96.0.0/12\n"
"  controlPlaneRef:\n"
"    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"    kind: RKE2ControlPlane\n"
"    name: single-node-cluster\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3Cluster\n"
"    name: single-node-cluster\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:152
msgid ""
"The `Metal3Cluster` object specifies the control-plane endpoint (replacing "
"the `$\\{DOWNSTREAM_CONTROL_PLANE_IP\\}`) to be configured and the "
"`noCloudProvider` because a bare-metal node is used."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:153
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3Cluster\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  controlPlaneEndpoint:\n"
"    host: ${DOWNSTREAM_CONTROL_PLANE_IP}\n"
"    port: 6443\n"
"  noCloudProvider: true\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:154
msgid ""
"The `RKE2ControlPlane` object specifies the control-plane configuration to "
"be used and the `Metal3MachineTemplate` object specifies the control-plane "
"image to be used.  Also, it contains the information about the number of "
"replicas to be used (in this case, one) and the `CNI` plug-in to be used (in "
"this case, `Cilium`).  The agentConfig block contains the `Ignition` format "
"to be used and the `additionalUserData` to be used to configure the `RKE2` "
"node with information like a systemd named `rke2-preinstall.service` to "
"replace automatically the `BAREMETALHOST_UUID` and `node-name` during the "
"provisioning process using the Ironic information.  The last block of "
"information contains the Kubernetes version to be "
"used. `$\\{RKE2_VERSION\\}` is the version of `RKE2` to be used replacing "
"this value (for example, `v1.28.9+rke2r1`)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:155
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: single-node-cluster-controlplane\n"
"  replicas: 1\n"
"  serverConfig:\n"
"    cni: cilium\n"
"  agentConfig:\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_VERSION}\n"
"    nodeName: \"localhost.localdomain\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:156
#: asciidoc/product/atip-automated-provision.adoc:211
msgid "The `Metal3MachineTemplate` object specifies the following information:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:157
#: asciidoc/product/atip-automated-provision.adoc:212
msgid "The `dataTemplate` to be used as a reference to the template."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:158
#: asciidoc/product/atip-automated-provision.adoc:213
msgid ""
"The `hostSelector` to be used matching with the label created during the "
"enrollment process."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:159
msgid ""
"The `image` to be used as a reference to the image generated using `EIB` on "
"the previous xref:eib-edge-image-connected[section], and the `checksum` and "
"`checksumType` to be used to validate the image."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:160
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3MachineTemplate\n"
"metadata:\n"
"  name: single-node-cluster-controlplane\n"
"  namespace: default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      dataTemplate:\n"
"        name: single-node-cluster-controlplane-template\n"
"      hostSelector:\n"
"        matchLabels:\n"
"          cluster-role: control-plane\n"
"      image:\n"
"        checksum: "
"http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256\n"
"        checksumType: sha256\n"
"        format: raw\n"
"        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:161
#: asciidoc/product/atip-automated-provision.adoc:216
msgid ""
"The `Metal3DataTemplate` object specifies the `metaData` for the downstream "
"cluster."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:162
#: asciidoc/product/atip-automated-provision.adoc:217
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3DataTemplate\n"
"metadata:\n"
"  name: single-node-cluster-controlplane-template\n"
"  namespace: default\n"
"spec:\n"
"  clusterName: single-node-cluster\n"
"  metaData:\n"
"    objectNames:\n"
"      - key: name\n"
"        object: machine\n"
"      - key: local-hostname\n"
"        object: machine\n"
"      - key: local_hostname\n"
"        object: machine\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:163
msgid ""
"Once the file is created by joining the previous blocks, the following "
"command must be executed in the management cluster to start provisioning the "
"new bare-metal host:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:164
#: asciidoc/product/atip-automated-provision.adoc:219
#: asciidoc/product/atip-automated-provision.adoc:287
#, no-wrap
msgid "$ kubectl apply -f capi-provisioning-example.yaml\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:165
#, no-wrap
msgid ""
"Downstream cluster provisioning with Direct network provisioning "
"(multi-node)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:166
msgid ""
"This section describes the workflow used to automate the provisioning of a "
"multi-node downstream cluster using directed network provisioning and "
"`MetalLB` as a load-balancer strategy.  This is the simplest way to automate "
"the provisioning of a downstream cluster. The following diagram shows the "
"workflow used to automate the provisioning of a multi-node downstream "
"cluster using directed network provisioning and `MetalLB`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:169
#: asciidoc/product/atip-automated-provision.adoc:252
msgid ""
"The management server created and available to be used on the following "
"sections. For more information, refer to the Management Cluster section: "
"<<atip-management-cluster>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:171
msgid ""
"The following diagram shows the workflow used to automate the provisioning "
"of a multi-node downstream cluster using directed network provisioning:"
msgstr ""

#. type: Target for macro image
#: asciidoc/product/atip-automated-provision.adoc:172
#, no-wrap
msgid "atip-automate-multinode1.png"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:173
msgid ""
"Enroll the three bare-metal hosts to make them available for the "
"provisioning process."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:174
msgid ""
"Provision the three bare-metal hosts to install and configure the operating "
"system and the Kubernetes cluster using `MetalLB`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:175
msgid "*Enroll the bare-metal hosts*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:176
msgid ""
"The first step is to enroll the three bare-metal hosts in the management "
"cluster to make them available to be provisioned.  To do that, the following "
"files (`bmh-example-node1.yaml`, `bmh-example-node2.yaml` and "
"`bmh-example-node3.yaml`) must be created in the management cluster, to "
"specify the `BMC` credentials to be used and the `BaremetalHost` object to "
"be enrolled in the management cluster."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:177
msgid ""
"Only the values between `$\\{...\\}` have to be replaced with the real "
"values."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:178
msgid ""
"We will walk you through the process for only one host. The same steps apply "
"to the other two nodes."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:179
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: node1-example-credentials\n"
"type: Opaque\n"
"data:\n"
"  username: ${BMC_NODE1_USERNAME}\n"
"  password: ${BMC_NODE1_PASSWORD}\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:180
#, no-wrap
msgid ""
"apiVersion: metal3.io/v1alpha1\n"
"kind: BareMetalHost\n"
"metadata:\n"
"  name: node1-example\n"
"  labels:\n"
"    cluster-role: control-plane\n"
"spec:\n"
"  online: true\n"
"  bootMACAddress: ${BMC_NODE1_MAC}\n"
"  bmc:\n"
"    address: ${BMC_NODE1_ADDRESS}\n"
"    disableCertificateVerification: true\n"
"    credentialsName: node1-example-credentials\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:181
msgid "Where:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:182
msgid ""
"`$\\{BMC_NODE1_USERNAME\\}` — The username for the BMC of the first "
"bare-metal host."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:183
msgid ""
"`$\\{BMC_NODE1_PASSWORD\\}` — The password for the BMC of the first "
"bare-metal host."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:184
msgid ""
"`$\\{BMC_NODE1_MAC\\}` — The MAC address of the first bare-metal host to be "
"used."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:185
msgid ""
"`$\\{BMC_NODE1_ADDRESS\\}` — The URL for the first bare-metal host BMC (for "
"example, `redfish-virtualmedia://192.168.200.75/redfish/v1/Systems/1/`). To "
"learn more about the different options available depending on your hardware "
"provider, check the following "
"https://github.com/metal3-io/baremetal-operator/blob/main/docs/api.md[link]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:186
msgid ""
"Once the file is created, the following command must be executed in the "
"management cluster to start enrolling the bare-metal hosts in the management "
"cluster:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:187
#, no-wrap
msgid ""
"$ kubectl apply -f bmh-example-node1.yaml\n"
"$ kubectl apply -f bmh-example-node2.yaml\n"
"$ kubectl apply -f bmh-example-node3.yaml\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:188
msgid ""
"The new bare-metal host objects are enrolled, changing their state from "
"registering to inspecting and available. The changes can be checked using "
"the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:189
#, no-wrap
msgid "$ kubectl get bmh -o wide\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:192
msgid ""
"Once the three bar metal hosts are enrolled and available, the next step is "
"to provision the bare-metal hosts to install and configure the operating "
"system and the Kubernetes cluster, creating a load balancer to manage them.  "
"To do that, the following file (`capi-provisioning-example.yaml`) must be "
"created in the management cluster with the following information (the "
"`capi-provisioning-example.yaml can be generated by joining the following "
"blocks)."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:194
msgid ""
"The `VIP` address is a reserved IP address that is not assigned to any node "
"and is used to configure the load balancer."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:195
msgid ""
"Below is the cluster definition, where the cluster network can be configured "
"using the `pods` and the `services` blocks. Also, it contains the references "
"to the control plane and the infrastructure (using the `Metal^3^` provider) "
"objects to be used."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:196
#, no-wrap
msgid ""
"apiVersion: cluster.x-k8s.io/v1beta1\n"
"kind: Cluster\n"
"metadata:\n"
"  name: multinode-cluster\n"
"  namespace: default\n"
"spec:\n"
"  clusterNetwork:\n"
"    pods:\n"
"      cidrBlocks:\n"
"        - 192.168.0.0/18\n"
"    services:\n"
"      cidrBlocks:\n"
"        - 10.96.0.0/12\n"
"  controlPlaneRef:\n"
"    apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"    kind: RKE2ControlPlane\n"
"    name: multinode-cluster\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3Cluster\n"
"    name: multinode-cluster\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:197
msgid ""
"The `Metal3Cluster` object specifies the control-plane endpoint that uses "
"the `VIP` address already reserved (replacing the "
"`$\\{DOWNSTREAM_VIP_ADDRESS\\}`) to be configured and the `noCloudProvider` "
"because the three bare-metal nodes are used."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:198
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3Cluster\n"
"metadata:\n"
"  name: multinode-cluster\n"
"  namespace: default\n"
"spec:\n"
"  controlPlaneEndpoint:\n"
"    host: ${EDGE_VIP_ADDRESS}\n"
"    port: 6443\n"
"  noCloudProvider: true\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:199
msgid ""
"The `RKE2ControlPlane` object specifies the control-plane configuration to "
"be used, and the `Metal3MachineTemplate` object specifies the control-plane "
"image to be used."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:200
msgid "The number of replicas to be used (in this case, three)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:201
msgid ""
"The advertisement mode to be used by the Load Balancer (`address` uses the "
"L2 implementation), as well as the address to be used (replacing the "
"`$\\{EDGE_VIP_ADDRESS\\}` with the `VIP` address)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:202
msgid ""
"The `serverConfig` with the `CNI` plug-in to be used (in this case, "
"`Cilium`), and the `tlsSan` to be used to configure the `VIP` address."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:203
msgid ""
"The agentConfig block contains the `Ignition` format to be used and the "
"`additionalUserData` to be used to configure the `RKE2` node with "
"information like:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:204
msgid ""
"The systemd service named `rke2-preinstall.service` to replace automatically "
"the `BAREMETALHOST_UUID` and `node-name` during the provisioning process "
"using the Ironic information."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:205
msgid ""
"The `storage` block which contains the Helm charts to be used to install the "
"`MetalLB` and the `endpoint-copier-operator`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:206
msgid ""
"The `metalLB` custom resource file with the `IPaddressPool` and the "
"`L2Advertisement` to be used (replacing `$\\{EDGE_VIP_ADDRESS\\}` with the "
"`VIP` address)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:207
msgid ""
"The `endpoint-svc.yaml` file to be used to configure the `kubernetes-vip` "
"service to be used by the `MetalLB` to manage the `VIP` address."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:208
msgid ""
"The last block of information contains the Kubernetes version to be "
"used. The `$\\{RKE2_VERSION\\}` is the version of `RKE2` to be used "
"replacing this value (for example, `v1.28.9+rke2r1`)."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:209
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: multinode-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: multinode-cluster-controlplane\n"
"  replicas: 3\n"
"  registrationMethod: \"address\"\n"
"  registrationAddress: ${EDGE_VIP_ADDRESS}\n"
"  serverConfig:\n"
"    cni: cilium\n"
"    tlsSan:\n"
"      - ${EDGE_VIP_ADDRESS}\n"
"      - https://${EDGE_VIP_ADDRESS}.sslip.io\n"
"  agentConfig:\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"        storage:\n"
"          files:\n"
"            - path: "
"/var/lib/rancher/rke2/server/manifests/endpoint-copier-operator.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: endpoint-copier-operator\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: "
"oci://registry.suse.com/edge/endpoint-copier-operator-chart\n"
"                    targetNamespace: endpoint-copier-operator\n"
"                    version: 0.2.0\n"
"                    createNamespace: true\n"
"            - path: /var/lib/rancher/rke2/server/manifests/metallb.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: metallb\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: oci://registry.suse.com/edge/metallb-chart\n"
"                    targetNamespace: metallb-system\n"
"                    version: 0.14.3\n"
"                    createNamespace: true\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:210
#, no-wrap
msgid ""
"            - path: /var/lib/rancher/rke2/server/manifests/metallb-cr.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: metallb.io/v1beta1\n"
"                  kind: IPAddressPool\n"
"                  metadata:\n"
"                    name: kubernetes-vip-ip-pool\n"
"                    namespace: metallb-system\n"
"                  spec:\n"
"                    addresses:\n"
"                      - ${EDGE_VIP_ADDRESS}/32\n"
"                    serviceAllocation:\n"
"                      priority: 100\n"
"                      namespaces:\n"
"                        - default\n"
"                      serviceSelectors:\n"
"                        - matchExpressions:\n"
"                          - {key: \"serviceType\", operator: In, values: "
"[kubernetes-vip]}\n"
"                  ---\n"
"                  apiVersion: metallb.io/v1beta1\n"
"                  kind: L2Advertisement\n"
"                  metadata:\n"
"                    name: ip-pool-l2-adv\n"
"                    namespace: metallb-system\n"
"                  spec:\n"
"                    ipAddressPools:\n"
"                      - kubernetes-vip-ip-pool\n"
"            - path: "
"/var/lib/rancher/rke2/server/manifests/endpoint-svc.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: v1\n"
"                  kind: Service\n"
"                  metadata:\n"
"                    name: kubernetes-vip\n"
"                    namespace: default\n"
"                    labels:\n"
"                      serviceType: kubernetes-vip\n"
"                  spec:\n"
"                    ports:\n"
"                    - name: rke2-api\n"
"                      port: 9345\n"
"                      protocol: TCP\n"
"                      targetPort: 9345\n"
"                    - name: k8s-api\n"
"                      port: 6443\n"
"                      protocol: TCP\n"
"                      targetPort: 6443\n"
"                    type: LoadBalancer\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_VERSION}\n"
"    nodeName: \"Node-multinode-cluster\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:214
msgid ""
"The `image` to be used as a reference to the image generated using `EIB` on "
"the previous xref:eib-edge-image-connected[section], and `checksum` and "
"`checksumType` to be used to validate the image."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:215
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3MachineTemplate\n"
"metadata:\n"
"  name: multinode-cluster-controlplane\n"
"  namespace: default\n"
"spec:\n"
"  template:\n"
"    spec:\n"
"      dataTemplate:\n"
"        name: multinode-cluster-controlplane-template\n"
"      hostSelector:\n"
"        matchLabels:\n"
"          cluster-role: control-plane\n"
"      image:\n"
"        checksum: "
"http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw.sha256\n"
"        checksumType: sha256\n"
"        format: raw\n"
"        url: http://imagecache.local:8080/eibimage-slemicro55rt-telco.raw\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:218
msgid ""
"Once the file is created by joining the previous blocks, the following "
"command has to be executed in the management cluster to start provisioning "
"the new three bar metal hosts:"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:220
#, no-wrap
msgid "Advanced Network Configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:221
msgid ""
"The directed network provisioning workflow allows downstream clusters "
"network configurations such as static IPs, bonding, VLAN's, etc."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:222
msgid ""
"The following sections describe the additional steps required to enable "
"provisioning downstream clusters using advanced network configuration."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:224
msgid ""
"The image generated using `EIB` has to include the network folder and the "
"script following <<add-network-eib,this section>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:225
#: asciidoc/product/atip-automated-provision.adoc:253
msgid "*Configuration*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:226
#: asciidoc/product/atip-automated-provision.adoc:254
msgid ""
"Use the following two sections as the base to enroll and provision the "
"hosts:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:227
#: asciidoc/product/atip-automated-provision.adoc:255
msgid ""
"xref:single-node[Downstream cluster provisioning with Direct network "
"provisioning (single-node)]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:228
#: asciidoc/product/atip-automated-provision.adoc:256
msgid ""
"xref:multi-node[Downstream cluster provisioning with Direct network "
"provisioning (multi-node)]"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:229
msgid ""
"The changes required to enable the advanced network configuration are the "
"following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:230
msgid ""
"Enrollment step: The following new example file with a secret containing the "
"information about the `networkData` to be used to configure, for example, "
"the static `IPs` and `VLAN` for the downstream cluster"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:231
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: controlplane-0-networkdata\n"
"type: Opaque\n"
"stringData:\n"
"  networkData: |\n"
"    interfaces:\n"
"    - name: ${CONTROLPLANE_INTERFACE}\n"
"      type: ethernet\n"
"      state: up\n"
"      mtu: 1500\n"
"      mac-address: \"${CONTROLPLANE_MAC}\"\n"
"      ipv4:\n"
"        address:\n"
"        - ip:  \"${CONTROLPLANE_IP}\"\n"
"          prefix-length: \"${CONTROLPLANE_PREFIX}\"\n"
"        enabled: true\n"
"        dhcp: false\n"
"    - name: floating\n"
"      type: vlan\n"
"      state: up\n"
"      vlan:\n"
"        base-iface: ${CONTROLPLANE_INTERFACE}\n"
"        id: ${VLAN_ID}\n"
"    dns-resolver:\n"
"      config:\n"
"        server:\n"
"        - \"${DNS_SERVER}\"\n"
"    routes:\n"
"      config:\n"
"      - destination: 0.0.0.0/0\n"
"        next-hop-address: \"${CONTROLPLANE_GATEWAY}\"\n"
"        next-hop-interface: ${CONTROLPLANE_INTERFACE}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:232
msgid ""
"This file contains the `networkData` in a `nmstate` format to be used to "
"configure the advance network configuration (for example, `static IPs` and "
"`VLAN`) for the downstream cluster.  As you can see, the example shows the "
"configuration to enable the interface with static IPs, as well as the "
"configuration to enable the VLAN using the base interface.  Any other "
"`nmstate` example could be defined to be used to configure the network for "
"the downstream cluster to adapt to the specific requirements, where the "
"following variables have to be replaced with real values:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:233
msgid ""
"`$\\{CONTROLPLANE1_INTERFACE\\}` — The control-plane interface to be used "
"for the edge cluster (for example, `eth0`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:234
msgid ""
"`$\\{CONTROLPLANE1_IP\\}` — The IP address to be used as an endpoint for the "
"edge cluster (must match with the kubeapi-server endpoint)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:235
msgid ""
"`$\\{CONTROLPLANE1_PREFIX\\}` — The CIDR to be used for the edge cluster "
"(for example, `24` if you want `/24` or `255.255.255.0`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:236
msgid ""
"`$\\{CONTROLPLANE1_GATEWAY\\}` — The gateway to be used for the edge cluster "
"(for example, `192.168.100.1`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:237
msgid ""
"`$\\{CONTROLPLANE1_MAC\\}` — The MAC address to be used for the "
"control-plane interface (for example, `00:0c:29:3e:3e:3e`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:238
msgid ""
"`$\\{DNS_SERVER\\}` — The DNS to be used for the edge cluster (for example, "
"`192.168.100.2`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:239
msgid ""
"`$\\{VLAN_ID\\}` — The VLAN ID to be used for the edge cluster (for example, "
"`100`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:240
msgid ""
"Also, the reference to that secret using `preprovisioningNetworkDataName` is "
"needed in the `BaremetalHost` object at the end of the file to be enrolled "
"in the management cluster."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:242
#, no-wrap
msgid ""
"apiVersion: metal3.io/v1alpha1\n"
"kind: BareMetalHost\n"
"metadata:\n"
"  name: flexran-demo\n"
"  labels:\n"
"    cluster-role: control-plane\n"
"spec:\n"
"  online: true\n"
"  bootMACAddress: ${BMC_MAC}\n"
"  rootDeviceHints:\n"
"    deviceName: /dev/nvme0n1\n"
"  bmc:\n"
"    address: ${BMC_ADDRESS}\n"
"    disableCertificateVerification: true\n"
"    credentialsName: example-demo-credentials\n"
"  preprovisioningNetworkDataName: controlplane-0-networkdata\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:243
msgid ""
"If you need to deploy a multi-node cluster, the same process must be done "
"for the other nodes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:244
msgid ""
"Provision step: The block of information related to the network data has to "
"be removed because the platform includes the network data configuration into "
"the secret `controlplane-0-networkdata`."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:245
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3DataTemplate\n"
"metadata:\n"
"  name: multinode-cluster-controlplane-template\n"
"  namespace: default\n"
"spec:\n"
"  clusterName: multinode-cluster\n"
"  metaData:\n"
"    objectNames:\n"
"      - key: name\n"
"        object: machine\n"
"      - key: local-hostname\n"
"        object: machine\n"
"      - key: local_hostname\n"
"        object: machine\n"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:246
msgid ""
"The `Metal3DataTemplate`, `networkData` and `Metal3 IPAM` are currently not "
"supported; only the configuration via static secrets is fully supported."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:247
#, no-wrap
msgid "Telco features (DPDK, SR-IOV, CPU isolation, huge pages, NUMA, etc.)"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:248
msgid ""
"The directed network provisioning workflow allows to automate the Telco "
"features to be used in the downstream clusters to run Telco workloads on top "
"of those servers."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:250
msgid ""
"The image generated using `EIB` has to include the specific Telco packages "
"following xref:add-telco-feature-eib[this section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:251
msgid ""
"The image generated using `EIB`, as described in the "
"xref:eib-edge-image-connected[previous section], has to be located in the "
"management cluster exactly on the path you configured on "
"xref:metal3-media-server[this section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:257
msgid "The Telco features covered in this section are the following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:258
msgid "DPDK and VFs creation"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:259
msgid "SR-IOV and VFs allocation to be used by the workloads"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:260
msgid "CPU isolation and performance tuning"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:261
msgid "Huge pages configuration"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:262
msgid "Kernel parameters tuning"
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:263
msgid "For more information about the Telco features, see <<atip-features>>."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:264
msgid ""
"The changes required to enable the Telco features shown above are all inside "
"the `RKE2ControlPlane` block in the provision file "
"`capi-provisioning-example.yaml`. The rest of the information inside the "
"file `capi-provisioning-example.yaml` is the same as the information "
"provided in the xref:single-node-provision[provisioning section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:265
msgid ""
"To make the process clear, the changes required on that block "
"(`RKE2ControlPlane`) to enable the Telco features are the following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:266
msgid ""
"The `preRKE2Commands` to be used to execute the commands before the `RKE2` "
"installation process. In this case, use the `modprobe` command to enable the "
"`vfio-pci` and the `SR-IOV` kernel modules."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:267
msgid ""
"The ignition file "
"`/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml` to "
"be used to define the interfaces, drivers and the number of `VFs` to be "
"created and exposed to the workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:268
msgid ""
"The values inside the config map `sriov-custom-auto-config` are the only "
"values to be replaced with real values."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:269
msgid ""
"`$\\{RESOURCE_NAME1\\}` — The resource name to be used for the first `PF` "
"interface (for example, `sriov-resource-du1`). It is added to the prefix "
"`rancher.io` to be used as a label to be used by the workloads (for example, "
"`rancher.io/sriov-resource-du1`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:270
msgid ""
"`$\\{SRIOV-NIC-NAME1\\}` — The name of the first `PF` interface to be used "
"(for example, `eth0`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:271
msgid ""
"`$\\{PF_NAME1\\}` — The name of the first physical function `PF` to be "
"used. Generate more complex filters using this (for example, `eth0#2-5`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:272
msgid ""
"`$\\{DRIVER_NAME1\\}` — The driver name to be used for the first `VF` "
"interface (for example, `vfio-pci`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:273
msgid ""
"`$\\{NUM_VFS1\\}` — The number of `VFs` to be created for the first `PF` "
"interface (for example, `8`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:274
msgid ""
"The `/var/sriov-auto-filler.sh` to be used as a translator between the "
"high-level config map `sriov-custom-auto-config` and the "
"`sriovnetworknodepolicy` which contains the low-level hardware "
"information. This script has been created to abstract the user from the "
"complexity to know in advance the hardware information. No changes are "
"required in this file, but it should be present if we need to enable "
"`sr-iov` and create `VFs`."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:275
msgid "The kernel arguments to be used to enable the following features:"
msgstr ""

#. type: Table
#: asciidoc/product/atip-automated-provision.adoc:276
#, no-wrap
msgid ""
"| Parameter | Value | Description\n"
"| isolcpus| 1-30,33-62| Isolate the cores 1-30 and 33-62.\n"
"| skew_tick| 1 | Allows the kernel to skew the timer interrupts across the "
"isolated CPUs.\n"
"| nohz| on | Allows the kernel to run the timer tick on a single CPU when "
"the system is idle.\n"
"| nohz_full| 1-30,33-62 | kernel boot parameter is the current main "
"interface to configure full dynticks along with CPU Isolation.\n"
"| rcu_nocbs| 1-30,33-62 | Allows the kernel to run the RCU callbacks on a "
"single CPU when the system is idle.\n"
"| kthread_cpus| 0,31,32,63 | Allows the kernel to run the kthreads on a "
"single CPU when the system is idle.\n"
"| irqaffinity| 0,31,32,63 | Allows the kernel to run the interrupts on a "
"single CPU when the system is idle.\n"
"| processor.max_cstate| 1 | Prevents the CPU from dropping into a sleep "
"state when idle.\n"
"| intel_idle.max_cstate| 0 | Disables the intel_idle driver and allows "
"acpi_idle to be used.\n"
"| iommu       | pt         | Allows to use vfio for the dpdk interfaces.\n"
"| intel_iommu | on         | Enables the use of vfio for VFs.\n"
"| hugepagesz | 1G    | Allows to set the size of huge pages to 1 G.\n"
"| hugepages | 40    | Number of huge pages defined before.\n"
"| default_hugepagesz| 1G | Default value to enable huge pages.\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:277
msgid "The following systemd services are used to enable the following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:278
msgid ""
"`rke2-preinstall.service` to replace automatically the `BAREMETALHOST_UUID` "
"and `node-name` during the provisioning process using the Ironic "
"information."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:279
msgid ""
"`cpu-performance.service` to enable the CPU performance tuning. The "
"`$\\{CPU_FREQUENCY\\}` has to be replaced with the real values (for example, "
"`2500000` to set the CPU frequency to `2.5GHz`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:280
msgid ""
"`cpu-partitioning.service` to enable the isolation cores of the `CPU` (for "
"example, `1-30,33-62`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:281
msgid ""
"`sriov-custom-auto-vfs.service` to install the `sriov` Helm chart, wait "
"until custom resources are created and run the `/var/sriov-auto-filler.sh` "
"to replace the values in the config map `sriov-custom-auto-config` and "
"create the `sriovnetworknodepolicy` to be used by the workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:282
msgid ""
"The `$\\{RKE2_VERSION\\}` is the version of `RKE2` to be used replacing this "
"value (for example, `v1.28.9+rke2r1`)."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:283
#: asciidoc/product/atip-automated-provision.adoc:295
msgid ""
"With all these changes mentioned, the `RKE2ControlPlane` block in the "
"`capi-provisioning-example.yaml` will look like the following:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:284
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: single-node-cluster-controlplane\n"
"  replicas: 1\n"
"  serverConfig:\n"
"    cni: cilium\n"
"    cniMultusEnable: true\n"
"  preRKE2Commands:\n"
"    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1\n"
"  agentConfig:\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        storage:\n"
"          files:\n"
"            - path: "
"/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: v1\n"
"                  kind: ConfigMap\n"
"                  metadata:\n"
"                    name: sriov-custom-auto-config\n"
"                    namespace: kube-system\n"
"                  data:\n"
"                    config.json: |\n"
"                      [\n"
"                         {\n"
"                           \"resourceName\": \"${RESOURCE_NAME1}\",\n"
"                           \"interface\": \"${SRIOV-NIC-NAME1}\",\n"
"                           \"pfname\": \"${PF_NAME1}\",\n"
"                           \"driver\": \"${DRIVER_NAME1}\",\n"
"                           \"numVFsToCreate\": ${NUM_VFS1}\n"
"                         },\n"
"                         {\n"
"                           \"resourceName\": \"${RESOURCE_NAME2}\",\n"
"                           \"interface\": \"${SRIOV-NIC-NAME2}\",\n"
"                           \"pfname\": \"${PF_NAME2}\",\n"
"                           \"driver\": \"${DRIVER_NAME2}\",\n"
"                           \"numVFsToCreate\": ${NUM_VFS2}\n"
"                         }\n"
"                      ]\n"
"              mode: 0644\n"
"              user:\n"
"                name: root\n"
"              group:\n"
"                name: root\n"
"            - path: /var/lib/rancher/rke2/server/manifests/sriov-crd.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: sriov-crd\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: oci://registry.suse.com/edge/sriov-crd-chart\n"
"                    targetNamespace: sriov-network-operator\n"
"                    version: 1.2.2\n"
"                    createNamespace: true\n"
"            - path: "
"/var/lib/rancher/rke2/server/manifests/sriov-network-operator.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: sriov-network-operator\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: "
"oci://registry.suse.com/edge/sriov-network-operator-chart\n"
"                    targetNamespace: sriov-network-operator\n"
"                    version: 1.2.2\n"
"                    createNamespace: true\n"
"            - path: /var/sriov-auto-filler.sh\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  #!/bin/bash\n"
"                  cat <<- EOF > /var/sriov-networkpolicy-template.yaml\n"
"                  apiVersion: sriovnetwork.openshift.io/v1\n"
"                  kind: SriovNetworkNodePolicy\n"
"                  metadata:\n"
"                    name: atip-RESOURCENAME\n"
"                    namespace: sriov-network-operator\n"
"                  spec:\n"
"                    nodeSelector:\n"
"                      feature.node.kubernetes.io/network-sriov.capable: "
"\"true\"\n"
"                    resourceName: RESOURCENAME\n"
"                    deviceType: DRIVER\n"
"                    numVfs: NUMVF\n"
"                    mtu: 1500\n"
"                    nicSelector:\n"
"                      pfNames: [\"PFNAMES\"]\n"
"                      deviceID: \"DEVICEID\"\n"
"                      vendor: \"VENDOR\"\n"
"                      rootDevices:\n"
"                        - PCIADDRESS\n"
"                  EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:285
#, no-wrap
msgid ""
"                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export "
"KUBECTL=/var/lib/rancher/rke2/bin/kubectl\n"
"                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r '.items[].status.syncStatus') != \"Succeeded\" ]; do sleep 1; "
"done\n"
"                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm "
"sriov-custom-auto-config -n kube-system -ojson | jq -r "
"'.data.\"config.json\"')\n"
"                  jq -c '.[]' <<< $input | while read i; do\n"
"                    interface=$(echo $i | jq -r '.interface')\n"
"                    pfname=$(echo $i | jq -r '.pfname')\n"
"                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.pciAddress\")\n"
"                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.vendor\")\n"
"                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.deviceID\")\n"
"                    resourceName=$(echo $i | jq -r '.resourceName')\n"
"                    driver=$(echo $i | jq -r '.driver')\n"
"                    sed -e \"s/RESOURCENAME/$resourceName/g\" \\\n"
"                        -e \"s/DRIVER/$driver/g\" \\\n"
"                        -e \"s/PFNAMES/$pfname/g\" \\\n"
"                        -e \"s/VENDOR/$vendor/g\" \\\n"
"                        -e \"s/DEVICEID/$deviceid/g\" \\\n"
"                        -e \"s/PCIADDRESS/$pciaddress/g\" \\\n"
"                        -e \"s/NUMVF/$(echo $i | jq -r "
"'.numVFsToCreate')/g\" /var/sriov-networkpolicy-template.yaml > "
"/var/lib/rancher/rke2/server/manifests/$resourceName.yaml\n"
"                  done\n"
"              mode: 0755\n"
"              user:\n"
"                name: root\n"
"              group:\n"
"                name: root\n"
"        kernel_arguments:\n"
"          should_exist:\n"
"            - intel_iommu=on\n"
"            - intel_pstate=passive\n"
"            - processor.max_cstate=1\n"
"            - intel_idle.max_cstate=0\n"
"            - iommu=pt\n"
"            - mce=off\n"
"            - hugepagesz=1G hugepages=40\n"
"            - hugepagesz=2M hugepages=0\n"
"            - default_hugepagesz=1G\n"
"            - kthread_cpus=${NON-ISOLATED_CPU_CORES}\n"
"            - irqaffinity=${NON-ISOLATED_CPU_CORES}\n"
"            - isolcpus=${ISOLATED_CPU_CORES}\n"
"            - nohz_full=${ISOLATED_CPU_CORES}\n"
"            - rcu_nocbs=${ISOLATED_CPU_CORES}\n"
"            - rcu_nocb_poll\n"
"            - nosoftlockup\n"
"            - nohz=on\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"            - name: cpu-performance.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=CPU perfomance\n"
"                Wants=network-online.target\n"
"                After=network.target network-online.target\n"
"                [Service]\n"
"                User=root\n"
"                Type=forking\n"
"                TimeoutStartSec=900\n"
"                ExecStart=/bin/sh -c \"cpupower frequency-set -g "
"performance; cpupower frequency-set -u ${CPU_FREQUENCY}; cpupower "
"frequency-set -d ${CPU_FREQUENCY}\"\n"
"                RemainAfterExit=yes\n"
"                KillMode=process\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"            - name: cpu-partitioning.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=cpu-partitioning\n"
"                Wants=network-online.target\n"
"                After=network.target network-online.target\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStart=/bin/sh -c \"echo "
"isolated_cores=${ISOLATED_CPU_CORES} > "
"/etc/tuned/cpu-partitioning-variables.conf\"\n"
"                ExecStartPost=/bin/sh -c \"tuned-adm profile "
"cpu-partitioning\"\n"
"                ExecStartPost=/bin/sh -c \"systemctl enable "
"tuned.service\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"            - name: sriov-custom-auto-vfs.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=SRIOV Custom Auto VF Creation\n"
"                Wants=network-online.target  rke2-server.target\n"
"                After=network.target network-online.target "
"rke2-server.target\n"
"                [Service]\n"
"                User=root\n"
"                Type=forking\n"
"                TimeoutStartSec=900\n"
"                ExecStart=/bin/sh -c \"while ! "
"/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml "
"wait --for condition=ready nodes --all ; do sleep 2 ; done\"\n"
"                ExecStartPost=/bin/sh -c \"while [ "
"$(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml "
"get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found "
"--no-headers -A | wc -l) -eq 0 ]; do sleep 1; done\"\n"
"                ExecStartPost=/bin/sh -c \"/var/sriov-auto-filler.sh\"\n"
"                RemainAfterExit=yes\n"
"                KillMode=process\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_VERSION}\n"
"    nodeName: \"localhost.localdomain\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:286
msgid ""
"Once the file is created by joining the previous blocks, the following "
"command must be executed in the management cluster to start provisioning the "
"new downstream cluster using the Telco features:"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:288
#, no-wrap
msgid "Private registry"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:289
msgid ""
"It is possible to configure a private registry as a mirror for images used "
"by workloads."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:290
msgid ""
"To do this we create the secret containing the information about the private "
"registry to be used by the downstream cluster."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:291
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: private-registry-cert\n"
"  namespace: default\n"
"data:\n"
"  tls.crt: ${TLS_CERTIFICATE}\n"
"  tls.key: ${TLS_KEY}\n"
"  ca.crt: ${CA_CERTIFICATE}\n"
"type: kubernetes.io/tls\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:292
#, no-wrap
msgid ""
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: private-registry-auth\n"
"  namespace: default\n"
"data:\n"
"  username: ${REGISTRY_USERNAME}\n"
"  password: ${REGISTRY_PASSWORD}\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:293
msgid ""
"The `tls.crt`, `tls.key` and `ca.crt` are the certificates to be used to "
"authenticate the private registry. The `username` and `password` are the "
"credentials to be used to authenticate the private registry."
msgstr ""

#. type: delimited block =
#: asciidoc/product/atip-automated-provision.adoc:294
msgid ""
"The `tls.crt`, `tls.key`, `ca.crt` , `username` and `password` have to be "
"encoded in base64 format before to be used in the secret."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:296
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: single-node-cluster-controlplane\n"
"  replicas: 1\n"
"  privateRegistriesConfig:\n"
"    mirrors:\n"
"      \"registry.example.com\":\n"
"        endpoint:\n"
"          - \"https://registry.example.com:5000\"\n"
"    configs:\n"
"      \"registry.example.com\":\n"
"        authSecret:\n"
"          apiVersion: v1\n"
"          kind: Secret\n"
"          namespace: default\n"
"          name: private-registry-auth\n"
"        tls:\n"
"          tlsConfigSecret:\n"
"            apiVersion: v1\n"
"            kind: Secret\n"
"            namespace: default\n"
"            name: private-registry-cert\n"
"  serverConfig:\n"
"    cni: cilium\n"
"  agentConfig:\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_VERSION}\n"
"    nodeName: \"localhost.localdomain\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:297
msgid ""
"Where the `registry.example.com` is the example name of the private registry "
"to be used by the downstream cluster, and it should be replaced with the "
"real values."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-automated-provision.adoc:298
#, no-wrap
msgid "Downstream cluster provisioning in air-gapped scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:299
msgid ""
"The directed network provisioning workflow allows to automate the "
"provisioning of downstream clusters in air-gapped scenarios."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:300
#, no-wrap
msgid "Requirements for air-gapped scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:301
msgid ""
"The `raw` image generated using `EIB` must include the specific container "
"images (helm-chart OCI and container images) required to run the downstream "
"cluster in an air-gapped scenario. For more information, refer to "
"xref:eib-edge-image-airgap[this section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:302
msgid ""
"In case of using SR-IOV or any other custom workload, the images required to "
"run the workloads must be preloaded in your private registry following the "
"xref:preload-private-registry[preload private registry section]."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:303
#, no-wrap
msgid "Enroll the bare metal hosts in air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:304
msgid ""
"The process to enroll the bare metal hosts in the management cluster is the "
"same as described in the xref:enroll-bare-metal-host[previous section]."
msgstr ""

#. type: Title ====
#: asciidoc/product/atip-automated-provision.adoc:305
#, no-wrap
msgid "Provision the downstream cluster in air-gap scenarios"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:306
msgid ""
"There are some important changes required to provision the downstream "
"cluster in air-gapped scenarios:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:307
msgid ""
"The `RKE2ControlPlane` block in the `capi-provisioning-example.yaml` file "
"must include the `spec.agentConfig.airGapped: true` directive."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:308
msgid ""
"The private registry configuration must be included in the "
"`RKE2ControlPlane` block in the `capi-provisioning-airgap-example.yaml` file "
"following the xref:atip-private-registry[private registry section]."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:309
msgid ""
"If you are using SR-IOV or any other `AdditionalUserData` configuration "
"(combustion script) which requires the helm-chart installation, you must "
"modify the content to reference the private registry instead of using the "
"public registry."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:310
msgid ""
"The following example shows the SR-IOV configuration in the "
"`AdditionalUserData` block in the `capi-provisioning-airgap-example.yaml` "
"file with the modifications required to reference the private registry"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:311
msgid "Private Registry secrets references"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-automated-provision.adoc:312
msgid ""
"Helm-Chart definition using the private registry instead of the public OCI "
"images."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:313
#, no-wrap
msgid ""
"# secret to include the private registry certificates\n"
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: private-registry-cert\n"
"  namespace: default\n"
"data:\n"
"  tls.crt: ${TLS_BASE64_CERT}\n"
"  tls.key: ${TLS_BASE64_KEY}\n"
"  ca.crt: ${CA_BASE64_CERT}\n"
"type: kubernetes.io/tls\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:314
#, no-wrap
msgid ""
"# secret to include the private registry auth credentials\n"
"apiVersion: v1\n"
"kind: Secret\n"
"metadata:\n"
"  name: private-registry-auth\n"
"  namespace: default\n"
"data:\n"
"  username: ${REGISTRY_USERNAME}\n"
"  password: ${REGISTRY_PASSWORD}\n"
"---\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:315
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: single-node-cluster-controlplane\n"
"  replicas: 1\n"
"  privateRegistriesConfig:       # Private registry configuration to add "
"your own mirror and credentials\n"
"    mirrors:\n"
"      docker.io:\n"
"        endpoint:\n"
"          - \"https://$(PRIVATE_REGISTRY_URL)\"\n"
"    configs:\n"
"      \"192.168.100.22:5000\":\n"
"        authSecret:\n"
"          apiVersion: v1\n"
"          kind: Secret\n"
"          namespace: default\n"
"          name: private-registry-auth\n"
"        tls:\n"
"          tlsConfigSecret:\n"
"            apiVersion: v1\n"
"            kind: Secret\n"
"            namespace: default\n"
"            name: private-registry-cert\n"
"          insecureSkipVerify: false\n"
"  serverConfig:\n"
"    cni: cilium\n"
"    cniMultusEnable: true\n"
"  preRKE2Commands:\n"
"    - modprobe vfio-pci enable_sriov=1 disable_idle_d3=1\n"
"  agentConfig:\n"
"    airGapped: true       # Airgap true to enable airgap mode\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        storage:\n"
"          files:\n"
"            - path: "
"/var/lib/rancher/rke2/server/manifests/configmap-sriov-custom-auto.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: v1\n"
"                  kind: ConfigMap\n"
"                  metadata:\n"
"                    name: sriov-custom-auto-config\n"
"                    namespace: sriov-network-operator\n"
"                  data:\n"
"                    config.json: |\n"
"                      [\n"
"                         {\n"
"                           \"resourceName\": \"${RESOURCE_NAME1}\",\n"
"                           \"interface\": \"${SRIOV-NIC-NAME1}\",\n"
"                           \"pfname\": \"${PF_NAME1}\",\n"
"                           \"driver\": \"${DRIVER_NAME1}\",\n"
"                           \"numVFsToCreate\": ${NUM_VFS1}\n"
"                         },\n"
"                         {\n"
"                           \"resourceName\": \"${RESOURCE_NAME2}\",\n"
"                           \"interface\": \"${SRIOV-NIC-NAME2}\",\n"
"                           \"pfname\": \"${PF_NAME2}\",\n"
"                           \"driver\": \"${DRIVER_NAME2}\",\n"
"                           \"numVFsToCreate\": ${NUM_VFS2}\n"
"                         }\n"
"                      ]\n"
"              mode: 0644\n"
"              user:\n"
"                name: root\n"
"              group:\n"
"                name: root\n"
"            - path: /var/lib/rancher/rke2/server/manifests/sriov.yaml\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  apiVersion: v1\n"
"                  data:\n"
"                    .dockerconfigjson: ${REGISTRY_AUTH_DOCKERCONFIGJSON}\n"
"                  kind: Secret\n"
"                  metadata:\n"
"                    name: privregauth\n"
"                    namespace: kube-system\n"
"                  type: kubernetes.io/dockerconfigjson\n"
"                  ---\n"
"                  apiVersion: v1\n"
"                  kind: ConfigMap\n"
"                  metadata:\n"
"                    namespace: kube-system\n"
"                    name: example-repo-ca\n"
"                  data:\n"
"                    ca.crt: |-\n"
"                      -----BEGIN CERTIFICATE-----\n"
"                      ${CA_BASE64_CERT}\n"
"                      -----END CERTIFICATE-----\n"
"                  ---\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: sriov-crd\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: oci://${PRIVATE_REGISTRY_URL}/sriov-crd\n"
"                    dockerRegistrySecret:\n"
"                      name: privregauth\n"
"                    repoCAConfigMap:\n"
"                      name: example-repo-ca\n"
"                    createNamespace: true\n"
"                    set:\n"
"                      global.clusterCIDR: 192.168.0.0/18\n"
"                      global.clusterCIDRv4: 192.168.0.0/18\n"
"                      global.clusterDNS: 10.96.0.10\n"
"                      global.clusterDomain: cluster.local\n"
"                      global.rke2DataDir: /var/lib/rancher/rke2\n"
"                      global.serviceCIDR: 10.96.0.0/12\n"
"                    targetNamespace: sriov-network-operator\n"
"                    version: ${SRIOV_CRD_VERSION}\n"
"                  ---\n"
"                  apiVersion: helm.cattle.io/v1\n"
"                  kind: HelmChart\n"
"                  metadata:\n"
"                    name: sriov-network-operator\n"
"                    namespace: kube-system\n"
"                  spec:\n"
"                    chart: "
"oci://${PRIVATE_REGISTRY_URL}/sriov-network-operator\n"
"                    dockerRegistrySecret:\n"
"                      name: privregauth\n"
"                    repoCAConfigMap:\n"
"                      name: example-repo-ca\n"
"                    createNamespace: true\n"
"                    set:\n"
"                      global.clusterCIDR: 192.168.0.0/18\n"
"                      global.clusterCIDRv4: 192.168.0.0/18\n"
"                      global.clusterDNS: 10.96.0.10\n"
"                      global.clusterDomain: cluster.local\n"
"                      global.rke2DataDir: /var/lib/rancher/rke2\n"
"                      global.serviceCIDR: 10.96.0.0/12\n"
"                    targetNamespace: sriov-network-operator\n"
"                    version: ${SRIOV_OPERATOR_VERSION}\n"
"              mode: 0644\n"
"              user:\n"
"                name: root\n"
"              group:\n"
"                name: root\n"
"            - path: /var/sriov-auto-filler.sh\n"
"              overwrite: true\n"
"              contents:\n"
"                inline: |\n"
"                  #!/bin/bash\n"
"                  cat <<- EOF > /var/sriov-networkpolicy-template.yaml\n"
"                  apiVersion: sriovnetwork.openshift.io/v1\n"
"                  kind: SriovNetworkNodePolicy\n"
"                  metadata:\n"
"                    name: atip-RESOURCENAME\n"
"                    namespace: sriov-network-operator\n"
"                  spec:\n"
"                    nodeSelector:\n"
"                      feature.node.kubernetes.io/network-sriov.capable: "
"\"true\"\n"
"                    resourceName: RESOURCENAME\n"
"                    deviceType: DRIVER\n"
"                    numVfs: NUMVF\n"
"                    mtu: 1500\n"
"                    nicSelector:\n"
"                      pfNames: [\"PFNAMES\"]\n"
"                      deviceID: \"DEVICEID\"\n"
"                      vendor: \"VENDOR\"\n"
"                      rootDevices:\n"
"                        - PCIADDRESS\n"
"                  EOF\n"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-automated-provision.adoc:316
#, no-wrap
msgid ""
"                  export KUBECONFIG=/etc/rancher/rke2/rke2.yaml; export "
"KUBECTL=/var/lib/rancher/rke2/bin/kubectl\n"
"                  while [ $(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r '.items[].status.syncStatus') != \"Succeeded\" ]; do sleep 1; "
"done\n"
"                  input=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get cm "
"sriov-custom-auto-config -n sriov-network-operator -ojson | jq -r "
"'.data.\"config.json\"')\n"
"                  jq -c '.[]' <<< $input | while read i; do\n"
"                    interface=$(echo $i | jq -r '.interface')\n"
"                    pfname=$(echo $i | jq -r '.pfname')\n"
"                    pciaddress=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.pciAddress\")\n"
"                    vendor=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.vendor\")\n"
"                    deviceid=$(${KUBECTL} --kubeconfig=${KUBECONFIG} get "
"sriovnetworknodestates.sriovnetwork.openshift.io -n sriov-network-operator "
"-ojson | jq -r "
"\".items[].status.interfaces[]|select(.name==\\\"$interface\\\")|.deviceID\")\n"
"                    resourceName=$(echo $i | jq -r '.resourceName')\n"
"                    driver=$(echo $i | jq -r '.driver')\n"
"                    sed -e \"s/RESOURCENAME/$resourceName/g\" \\\n"
"                        -e \"s/DRIVER/$driver/g\" \\\n"
"                        -e \"s/PFNAMES/$pfname/g\" \\\n"
"                        -e \"s/VENDOR/$vendor/g\" \\\n"
"                        -e \"s/DEVICEID/$deviceid/g\" \\\n"
"                        -e \"s/PCIADDRESS/$pciaddress/g\" \\\n"
"                        -e \"s/NUMVF/$(echo $i | jq -r "
"'.numVFsToCreate')/g\" /var/sriov-networkpolicy-template.yaml > "
"/var/lib/rancher/rke2/server/manifests/$resourceName.yaml\n"
"                  done\n"
"              mode: 0755\n"
"              user:\n"
"                name: root\n"
"              group:\n"
"                name: root\n"
"        kernel_arguments:\n"
"          should_exist:\n"
"            - intel_iommu=on\n"
"            - intel_pstate=passive\n"
"            - processor.max_cstate=1\n"
"            - intel_idle.max_cstate=0\n"
"            - iommu=pt\n"
"            - mce=off\n"
"            - hugepagesz=1G hugepages=40\n"
"            - hugepagesz=2M hugepages=0\n"
"            - default_hugepagesz=1G\n"
"            - kthread_cpus=${NON-ISOLATED_CPU_CORES}\n"
"            - irqaffinity=${NON-ISOLATED_CPU_CORES}\n"
"            - isolcpus=${ISOLATED_CPU_CORES}\n"
"            - nohz_full=${ISOLATED_CPU_CORES}\n"
"            - rcu_nocbs=${ISOLATED_CPU_CORES}\n"
"            - rcu_nocb_poll\n"
"            - nosoftlockup\n"
"            - nohz=on\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"            - name: cpu-partitioning.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=cpu-partitioning\n"
"                Wants=network-online.target\n"
"                After=network.target network-online.target\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStart=/bin/sh -c \"echo "
"isolated_cores=${ISOLATED_CPU_CORES} > "
"/etc/tuned/cpu-partitioning-variables.conf\"\n"
"                ExecStartPost=/bin/sh -c \"tuned-adm profile "
"cpu-partitioning\"\n"
"                ExecStartPost=/bin/sh -c \"systemctl enable "
"tuned.service\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"            - name: sriov-custom-auto-vfs.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=SRIOV Custom Auto VF Creation\n"
"                Wants=network-online.target  rke2-server.target\n"
"                After=network.target network-online.target "
"rke2-server.target\n"
"                [Service]\n"
"                User=root\n"
"                Type=forking\n"
"                TimeoutStartSec=900\n"
"                ExecStart=/bin/sh -c \"while ! "
"/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml "
"wait --for condition=ready nodes --all ; do sleep 2 ; done\"\n"
"                ExecStartPost=/bin/sh -c \"while [ "
"$(/var/lib/rancher/rke2/bin/kubectl --kubeconfig=/etc/rancher/rke2/rke2.yaml "
"get sriovnetworknodestates.sriovnetwork.openshift.io --ignore-not-found "
"--no-headers -A | wc -l) -eq 0 ]; do sleep 1; done\"\n"
"                ExecStartPost=/bin/sh -c \"/var/sriov-auto-filler.sh\"\n"
"                RemainAfterExit=yes\n"
"                KillMode=process\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_VERSION}\n"
"    nodeName: \"localhost.localdomain\"\n"
msgstr ""

#. type: Title ==
#: asciidoc/product/atip-lifecycle.adoc:1
#, no-wrap
msgid "Lifecycle actions"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:2
msgid ""
"This section covers the lifecycle management actions of deployed ATIP "
"clusters."
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-lifecycle.adoc:3
#, no-wrap
msgid "Management cluster upgrades"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:4
msgid ""
"The upgrade of the management cluster involves several components. For a "
"list of the general components that require an upgrade, see the `Day 2` "
"<<day2-mgmt-cluster, management cluster>> documentation."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:5
msgid ""
"The upgrade procedure for comoponents specific to this setup can be seen "
"below."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:6
msgid "*Upgrading Metal^3^*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:7
msgid ""
"To upgrade `Metal^3^`, use the following command to update the Helm "
"repository cache and fetch the latest chart to install `Metal^3^` from the "
"Helm chart repository:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-lifecycle.adoc:8
#, no-wrap
msgid ""
"helm repo update\n"
"helm fetch suse-edge/metal3\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:9
msgid ""
"After that, the easy way to upgrade is to export your current configurations "
"to a file, and then upgrade the `Metal^3^` version using that previous "
"file.  If any change is required in the new version, the file can be edited "
"before the upgrade."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-lifecycle.adoc:10
#, no-wrap
msgid ""
"helm get values metal3 -n metal3-system -o yaml > metal3-values.yaml\n"
"helm upgrade metal3 suse-edge/metal3 \\\n"
"  --namespace metal3-system \\\n"
"  -f metal3-values.yaml \\\n"
"  --version=0.7.1\n"
msgstr ""

#. type: Title ===
#: asciidoc/product/atip-lifecycle.adoc:11
#, no-wrap
msgid "Downstream cluster upgrades"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:12
msgid ""
"Upgrading downstream clusters involves updating several components. The "
"following sections cover the upgrade process for each of the components."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:13
msgid "*Upgrading the operating system*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:14
msgid ""
"For this process, check the following "
"<<atip-automated-provisioning#eib-edge-image-connected,reference>> to build "
"the new image with a new operating system version.  With this new image "
"generated by `EIB`, the next provision phase uses the new operating version "
"provided.  In the following step, the new image is used to upgrade the "
"nodes."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:15
msgid "*Upgrading the RKE2 cluster*"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:16
msgid ""
"The changes required to upgrade the `RKE2` cluster using the automated "
"workflow are the following:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:17
msgid ""
"Change the block `RKE2ControlPlane` in the `capi-provisioning-example.yaml` "
"shown in the following "
"<<atip-automated-provisioning#single-node-provision,section>>:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:18
msgid "Add the rollout strategy in the spec file."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:19
msgid ""
"Change the version of the `RKE2` cluster to the new version replacing "
"`$\\{RKE2_NEW_VERSION\\}`."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-lifecycle.adoc:20
#, no-wrap
msgid ""
"apiVersion: controlplane.cluster.x-k8s.io/v1alpha1\n"
"kind: RKE2ControlPlane\n"
"metadata:\n"
"  name: single-node-cluster\n"
"  namespace: default\n"
"spec:\n"
"  infrastructureRef:\n"
"    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"    kind: Metal3MachineTemplate\n"
"    name: single-node-cluster-controlplane\n"
"  replicas: 1\n"
"  serverConfig:\n"
"    cni: cilium\n"
"  rolloutStrategy:\n"
"    rollingUpdate:\n"
"      maxSurge: 0\n"
"  agentConfig:\n"
"    format: ignition\n"
"    additionalUserData:\n"
"      config: |\n"
"        variant: fcos\n"
"        version: 1.4.0\n"
"        systemd:\n"
"          units:\n"
"            - name: rke2-preinstall.service\n"
"              enabled: true\n"
"              contents: |\n"
"                [Unit]\n"
"                Description=rke2-preinstall\n"
"                Wants=network-online.target\n"
"                Before=rke2-install.service\n"
"                "
"ConditionPathExists=!/run/cluster-api/bootstrap-success.complete\n"
"                [Service]\n"
"                Type=oneshot\n"
"                User=root\n"
"                ExecStartPre=/bin/sh -c \"mount -L config-2 /mnt\"\n"
"                ExecStart=/bin/sh -c \"sed -i \\\"s/BAREMETALHOST_UUID/$(jq "
"-r .uuid /mnt/openstack/latest/meta_data.json)/\\\" "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStart=/bin/sh -c \"echo \\\"node-name: $(jq -r .name "
"/mnt/openstack/latest/meta_data.json)\\\" >> "
"/etc/rancher/rke2/config.yaml\"\n"
"                ExecStartPost=/bin/sh -c \"umount /mnt\"\n"
"                [Install]\n"
"                WantedBy=multi-user.target\n"
"    kubelet:\n"
"      extraArgs:\n"
"        - provider-id=metal3://BAREMETALHOST_UUID\n"
"    version: ${RKE2_NEW_VERSION}\n"
"    nodeName: \"localhost.localdomain\"\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:21
msgid ""
"Change the block `Metal3MachineTemplate` in the "
"`capi-provisioning-example.yaml` shown in the following "
"<<atip-automated-provisioning#single-node-provision,section>>:"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:22
msgid ""
"Change the image name and checksum to the new version generated in the "
"previous step."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:23
msgid "Add the directive `nodeReuse` to `true` to avoid creating a new node."
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:24
msgid ""
"Add the directive `automatedCleaningMode` to `metadata` to enable the "
"automated cleaning for the node."
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-lifecycle.adoc:25
#, no-wrap
msgid ""
"apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n"
"kind: Metal3MachineTemplate\n"
"metadata:\n"
"  name: single-node-cluster-controlplane\n"
"  namespace: default\n"
"spec:\n"
"  nodeReuse: True\n"
"  template:\n"
"    spec:\n"
"      automatedCleaningMode: metadata\n"
"      dataTemplate:\n"
"        name: single-node-cluster-controlplane-template\n"
"      hostSelector:\n"
"        matchLabels:\n"
"          cluster-role: control-plane\n"
"      image:\n"
"        checksum: "
"http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.sha256\n"
"        checksumType: sha256\n"
"        format: raw\n"
"        url: http://imagecache.local:8080/${NEW_IMAGE_GENERATED}.raw\n"
msgstr ""

#. type: Plain text
#: asciidoc/product/atip-lifecycle.adoc:26
msgid ""
"After making these changes, the `capi-provisioning-example.yaml` file can be "
"applied to the cluster using the following command:"
msgstr ""

#. type: delimited block -
#: asciidoc/product/atip-lifecycle.adoc:27
#, no-wrap
msgid "kubectl apply -f capi-provisioning-example.yaml\n"
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:1
#, no-wrap
msgid "Abstract"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:2
msgid ""
"SUSE Edge 3.0 is a tightly integrated and comprehensively validated "
"end-to-end solution for addressing the unique challenges of the deployment "
"of infrastructure and cloud-native applications at the edge. Its driving "
"focus is to provide an opinionated, yet highly flexible, highly scalable, "
"and secure platform that spans initial deployment image building, node "
"provisioning and onboarding, application deployment, observability, and "
"lifecycle management."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:3
msgid ""
"The solution is designed with the notion that there is no "
"\"one-size-fits-all\" edge platform due to our customers’ widely varying "
"requirements and expectations. Edge deployments push us to solve, and "
"continually evolve, some of the most challenging problems, including massive "
"scalability, restricted network availability, physical space constraints, "
"new security threats and attack vectors, variations in hardware architecture "
"and system resources, the requirement to deploy and interface with legacy "
"infrastructure and applications, and customer solutions that have extended "
"lifespans."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:4
msgid ""
"SUSE Edge is built on best-of-breed open source software from the ground up, "
"consistent with both our 30-year history in delivering secure, stable, and "
"certified SUSE Linux platforms and our experience in providing highly "
"scalable and feature-rich Kubernetes management with our Rancher "
"portfolio. SUSE Edge builds on-top of these capabilities to deliver "
"functionality that can address a wide number of market segments, including "
"retail, medical, transportation, logistics, telecommunications, smart "
"manufacturing, and Industrial IoT."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:5
msgid ""
"SUSE Adaptive Telco Infrastructure Platform (ATIP) is a derivative (or "
"downstream product) of SUSE Edge, with additional optimizations and "
"components that enable the platform to address the requirements found in "
"telecommunications use-cases. Unless explicitly stated, all of the release "
"notes are applicable for both SUSE Edge 3.0, and SUSE ATIP 3.0."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:6
#, no-wrap
msgid "About"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:7
msgid ""
"These Release Notes are, unless explicitly specified and explained, "
"identical across all architectures, and the most recent version, along with "
"the release notes of all other SUSE products are always available online at "
"https://www.suse.com/releasenotes[https://www.suse.com/releasenotes]."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:8
msgid ""
"Entries are only listed once, but they can be referenced in several places "
"if they are important and belong to more than one section. Release notes "
"usually only list changes that happened between two subsequent "
"releases. Certain important entries from the release notes of previous "
"product versions may be repeated. To make these entries easier to identify, "
"they contain a note to that effect."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:9
msgid ""
"However, repeated entries are provided as a courtesy only. Therefore, if you "
"are skipping one or releases, check the release notes of the skipped "
"releases also. If you are only reading the release notes of the current "
"release, you could miss important changes that may affect system behavior."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:10
#, no-wrap
msgid "Release 3.0.0"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:11
msgid "Availability Date: 26th April 2024"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:12
msgid ""
"Summary: SUSE Edge 3.0.0 is the first release in the SUSE Edge 3.0 "
"portfolio."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/releasenotes.adoc:13
#: asciidoc/edge-book/releasenotes.adoc:24
#, no-wrap
msgid "New Features"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:14
#: asciidoc/edge-book/releasenotes.adoc:16
msgid "Not Applicable - this is the first release shipped in 3.0.z."
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/releasenotes.adoc:15
#: asciidoc/edge-book/releasenotes.adoc:30
#, no-wrap
msgid "Bug & Security Fixes"
msgstr ""

#. type: Title ==
#: asciidoc/edge-book/releasenotes.adoc:17
#: asciidoc/edge-book/releasenotes.adoc:34
#, no-wrap
msgid "Components Versions"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:18
msgid ""
"The following table describes the individual components that make up the "
"3.0.0 release, including the version, the Helm chart version (if "
"applicable), and where the released artifact can be pulled from in binary "
"format. Please follow the associated documentation for usage and deployment "
"examples."
msgstr ""

#. type: Table
#: asciidoc/edge-book/releasenotes.adoc:19
#, no-wrap
msgid ""
"| Name | Version | Helm Chart Version | Artifact Location (URL/Image)\n"
"| SLE Micro | 5.5 (latest) | N/A | "
"https://www.suse.com/download/sle-micro/[SLE Micro Download Page] +\n"
"SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256 "
"4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a) +\n"
"SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256 "
"527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678) +\n"
"SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256 "
"13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b) +\n"
"SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256 "
"6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)\n"
"| SUSE Manager | 4.3.11 | N/A | "
"https://www.suse.com/download/suse-manager/[SUSE Manager Download Page]\n"
"| K3s | 1.28.8 | N/A | "
"https://github.com/k3s-io/k3s/releases/tag/v1.28.8%2Bk3s1[Upstream K3s "
"Release]\n"
"| RKE2 | 1.28.8 | N/A | "
"https://github.com/rancher/rke2/releases/tag/v1.28.8%2Brke2r1[Upstream RKE2 "
"Release]\n"
"| Rancher Prime | 2.8.3 | 2.8.3 | "
"https://github.com/rancher/rancher/releases/download/v2.8.3/rancher-images.txt[Rancher "
"2.8.3 Images] +\n"
" https://charts.rancher.com/server-charts/prime[Rancher Prime Helm Repo]\n"
"| Longhorn | 1.6.1 | 103.3.0 | "
"https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt[Longhorn "
"1.6.1 Images] +\n"
"https://charts.longhorn.io[Longhorn Helm Repo]\n"
"| NM Configurator | 0.2.3 | N/A | "
"https://github.com/suse-edge/nm-configurator/releases/tag/v0.2.3[NMConfigurator "
"Upstream Release]\n"
"| NeuVector| 5.3.0 | 103.0.3 | "
"registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0 +\n"
"registry.suse.com/rancher mirrored-neuvector-registry-adapter:0.1.1-s1 +\n"
"registry.suse.com/rancher/mirrored-neuvector-scanner:latest +\n"
"registry.suse.com/rancher/mirrored-neuvector-updater:latest\n"
"| Cluster API (CAPI) | 1.6.2 | N/A | "
"registry.suse.com/edge/cluster-api-controller:1.6.2 +\n"
"registry.suse.com/edge/cluster-api-provider-metal3:1.6.0 +\n"
"registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6\n"
"| Metal^3^ | 1.16.0 | 0.6.5 | registry.suse.com/edge/metal3-chart:0.6.5 +\n"
"registry.suse.com/edge/baremetal-operator:0.5.1 +\n"
"registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6 +\n"
"registry.suse.com/edge/ip-address-manager:1.6.0 +\n"
"registry.suse.com/edge/ironic:23.0.1.2 +\n"
"registry.suse.com/edge/ironic-ipa-downloader:1.3.1 +\n"
"registry.suse.com/edge/kube-rbac-proxy:v0.14.2 +\n"
"registry.suse.com/edge/mariadb:10.6.15.1\n"
"| MetalLB | 0.14.3 | 0.14.3 | registry.suse.com/edge/metallb-chart:0.14.3 "
"+\n"
"registry.suse.com/edge/metallb-controller:v0.14.3 +\n"
"registry.suse.com/edge/metallb-speaker:v0.14.3 +\n"
"registry.suse.com/edge/frr:8.4 +\n"
"registry.suse.com/edge/frr-k8s:v0.0.8\n"
"| Elemental | 1.4.3 | 103.1.0 | "
"registry.suse.com/rancher/elemental-operator-chart:1.4.3 +\n"
"registry.suse.com/rancher/elemental-operator-crds-chart:1.4.3 +\n"
"registry.suse.com/rancher/elemental-operator:1.4.3\n"
"| Edge Image Builder | 1.0.1 | N/A | "
"registry.suse.com/edge/edge-image-builder:1.0.1\n"
"| KubeVirt | 1.1.1 | 0.2.4 | registry.suse.com/edge/kubevirt-chart:0.2.4 +\n"
"registry.suse.com/suse/sles/15.5/virt-operator:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-api:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-controller:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-handler:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1\n"
"| KubeVirt Dashboard Extension | 1.0.0 | 1.0.0 | "
"registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0\n"
"| Containerized Data Importer | 1.58.0 | 0.2.3 | "
"registry.suse.com/edge/cdi-chart:0.2.3 +\n"
"registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0\n"
"| Endpoint Copier Operator | 0.2.0 | 0.2.0 | "
"registry.suse.com/edge/endpoint-copier-operator:v0.2.0 +\n"
"registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0\n"
"| Akri (Tech Preview) | 0.12.20 | 0.12.20 | "
"registry.suse.com/edge/akri-chart:0.12.20 +\n"
"registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0 +\n"
"registry.suse.com/edge/akri-agent:v0.12.20 +\n"
"registry.suse.com/edge/akri-controller:v0.12.20 +\n"
"registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-webhook-configuration:v0.12.20\n"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:20
msgid ""
"SUSE Edge z-stream releases are tightly integrated and thoroughly tested as "
"a versioned stack. Upgrade of any individual components to a different "
"versions to those listed above is likely to result in system downtime. While "
"it's possible to run Edge clusters in untested configurations, it is not "
"recommended, and it may take longer to provide resolution through the "
"support channels."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:21
#, no-wrap
msgid "Release 3.0.1"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:22
msgid "Availability Date: 14th June 2024"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:23
msgid ""
"Summary: SUSE Edge 3.0.1 is the first z-stream release in the SUSE Edge 3.0 "
"portfolio."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:25
msgid "Elemental and EIB now support node reset for unmanaged hosts"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:26
msgid "SR-IOV Network Operator chart is now included"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:27
msgid "The Metal^3^ chart now supports providing additional trusted CA certificates"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:28
msgid ""
"NM Configurator now supports applying unified configurations without any MAC "
"specification"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:29
msgid ""
"Added `version` subcommand to EIB; the version will also automatically be "
"included in each image built by EIB"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:31
msgid ""
"EIB now automatically sets the execute bit on custom scripts: "
"https://github.com/suse-edge/edge-image-builder/issues/429[SUSE Edge issue "
"#429]"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:32
msgid ""
"EIB now supports disks which are >512 byte sector size: "
"https://github.com/suse-edge/edge-image-builder/issues/447[SUSE Edge issue "
"#447]"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:33
msgid ""
"Enhance EIB's ability to detect container images in Helm charts: "
"https://github.com/suse-edge/edge-image-builder/issues/442[SUSE Edge issue "
"#442]"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:35
msgid ""
"The following table describes the individual components that make up the "
"3.0.1 release, including the version, the Helm chart version (if "
"applicable), and where the released artifact can be pulled from in binary "
"format. Please follow the associated documentation for usage and deployment "
"examples."
msgstr ""

#. type: Table
#: asciidoc/edge-book/releasenotes.adoc:36
#, no-wrap
msgid ""
"| Name | Version | Helm Chart Version | Artifact Location (URL/Image)\n"
"| SLE Micro | 5.5 (latest) | N/A | "
"https://www.suse.com/download/sle-micro/[SLE Micro Download Page] +\n"
"SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso (sha256 "
"4f672a4a0f8ec421e7c25797def05598037c56b7f306283566a9f921bdce904a) +\n"
"SLE-Micro.x86_64-5.5.0-Default-RT-SelfInstall-GM2.install.iso (sha256 "
"527a5a7cdbf11e3e6238e386533755257676ad8b4c80be3b159d0904cb637678) +\n"
"SLE-Micro.x86_64-5.5.0-Default-GM.raw.xz (sha256 "
"13243a737ca219bad6a7aa41fa747c06e8b825fef10a756cf4d575f4493ed68b) +\n"
"SLE-Micro.x86_64-5.5.0-Default-RT-GM.raw.xz (sha256 "
"6c2af94e7ac785c8f6a276032c8e6a4b493c294e6cd72809c75089522f01bc93)\n"
"| SUSE Manager | 4.3.11 | N/A | "
"https://www.suse.com/download/suse-manager/[SUSE Manager Download Page]\n"
"s| K3s s| 1.28.9 | N/A | "
"https://github.com/k3s-io/k3s/releases/tag/v1.28.9%2Bk3s1[Upstream K3s "
"Release]\n"
"s| RKE2 s| 1.28.9 | N/A | "
"https://github.com/rancher/rke2/releases/tag/v1.28.9%2Brke2r1[Upstream RKE2 "
"Release]\n"
"s| Rancher Prime s| 2.8.4 s| 2.8.4 | "
"https://github.com/rancher/rancher/releases/download/v2.8.4/rancher-images.txt[Rancher "
"2.8.4 Images] +\n"
" https://charts.rancher.com/server-charts/prime[Rancher Prime Helm Repo]\n"
"| Longhorn | 1.6.1 | 103.3.0 | "
"https://raw.githubusercontent.com/longhorn/longhorn/v1.6.1/deploy/longhorn-images.txt[Longhorn "
"1.6.1 Images] +\n"
"https://charts.longhorn.io[Longhorn Helm Repo]\n"
"s| NM Configurator s| 0.3.0 | N/A | "
"https://github.com/suse-edge/nm-configurator/releases/tag/v0.3.0[NMConfigurator "
"Upstream Release]\n"
"| NeuVector| 5.3.0 | 103.0.3 | "
"registry.suse.com/rancher/mirrored-neuvector-controller:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-enforcer:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-manager:5.3.0 +\n"
"registry.suse.com/rancher/mirrored-neuvector-prometheus-exporter:5.3.0 +\n"
"registry.suse.com/rancher mirrored-neuvector-registry-adapter:0.1.1-s1 +\n"
"registry.suse.com/rancher/mirrored-neuvector-scanner:latest +\n"
"registry.suse.com/rancher/mirrored-neuvector-updater:latest\n"
"| Cluster API (CAPI) | 1.6.2 | N/A | "
"registry.suse.com/edge/cluster-api-controller:1.6.2 +\n"
"registry.suse.com/edge/cluster-api-provider-metal3:1.6.0 +\n"
"registry.suse.com/edge/cluster-api-provider-rke2-bootstrap:0.2.6\n"
"s| Metal^3^ s| 1.16.0 s| 0.7.1 | registry.suse.com/edge/metal3-chart:0.7.1 "
"+\n"
"registry.suse.com/edge/baremetal-operator:0.5.1 +\n"
"registry.suse.com/edge/cluster-api-provider-rke2-controlplane:0.2.6 +\n"
"registry.suse.com/edge/ip-address-manager:1.6.0 +\n"
"registry.suse.com/edge/ironic:23.0.2.1 +\n"
"registry.suse.com/edge/ironic-ipa-downloader:1.3.2 +\n"
"registry.suse.com/edge/kube-rbac-proxy:v0.14.2 +.1\n"
"registry.suse.com/edge/mariadb:10.6.15.1\n"
"| MetalLB | 0.14.3 | 0.14.3 | registry.suse.com/edge/metallb-chart:0.14.3 "
"+\n"
"registry.suse.com/edge/metallb-controller:v0.14.3 +\n"
"registry.suse.com/edge/metallb-speaker:v0.14.3 +\n"
"registry.suse.com/edge/frr:8.4 +\n"
"registry.suse.com/edge/frr-k8s:v0.0.8\n"
"s| Elemental s| 1.4.4 s| 103.1.0 | "
"registry.suse.com/rancher/elemental-operator-chart:1.4.4 +\n"
"registry.suse.com/rancher/elemental-operator-crds-chart:1.4.4 +\n"
"registry.suse.com/rancher/elemental-operator:1.4.4\n"
"s| Edge Image Builder s| 1.0.2 | N/A | "
"registry.suse.com/edge/edge-image-builder:1.0.2\n"
"| KubeVirt | 1.1.1 | 0.2.4 | registry.suse.com/edge/kubevirt-chart:0.2.4 +\n"
"registry.suse.com/suse/sles/15.5/virt-operator:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-api:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-controller:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-exportproxy:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-exportserver:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-handler:1.1.1 +\n"
"registry.suse.com/suse/sles/15.5/virt-launcher:1.1.1\n"
"| KubeVirt Dashboard Extension | 1.0.0 | 1.0.0 | "
"registry.suse.com/edge/kubevirt-dashboard-extension-chart:1.0.0\n"
"| Containerized Data Importer | 1.58.0 | 0.2.3 | "
"registry.suse.com/edge/cdi-chart:0.2.3 +\n"
"registry.suse.com/suse/sles/15.5/cdi-operator:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-controller:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-importer:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-cloner:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-apiserver:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-uploadserver:1.58.0 +\n"
"registry.suse.com/suse/sles/15.5/cdi-uploadproxy:1.58.0\n"
"| Endpoint Copier Operator | 0.2.0 | 0.2.0 | "
"registry.suse.com/edge/endpoint-copier-operator:v0.2.0 +\n"
"registry.suse.com/edge/endpoint-copier-operator-chart:0.2.0\n"
"| Akri (Tech Preview) | 0.12.20 | 0.12.20 | "
"registry.suse.com/edge/akri-chart:0.12.20 +\n"
"registry.suse.com/edge/akri-dashboard-extension-chart:1.0.0 +\n"
"registry.suse.com/edge/akri-agent:v0.12.20 +\n"
"registry.suse.com/edge/akri-controller:v0.12.20 +\n"
"registry.suse.com/edge/akri-debug-echo-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-onvif-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-opcua-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-udev-discovery-handler:v0.12.20 +\n"
"registry.suse.com/edge/akri-webhook-configuration:v0.12.20\n"
"s| SR-IOV Network Operator s| 1.2.2 s| 1.2.2+up0.1.0 | "
"registry.suse.com/edge/sriov-network-operator-chart:1.2.2 +\n"
"registry.suse.com/edge/sriov-crd-chart:1.2.2\n"
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:37
#, no-wrap
msgid "Components Verification"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:38
msgid ""
"The components mentioned above may be verified using the Software Bill Of "
"Materials (SBOM) data - for example using `cosign` as outlined below:"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:39
msgid ""
"Download the SUSE Edge Container public key from the "
"https://www.suse.com/support/security/keys/[SUSE Signing Keys source]:"
msgstr ""

#. type: delimited block -
#: asciidoc/edge-book/releasenotes.adoc:40
#, no-wrap
msgid "include::key.sh[]\n"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:41
msgid "Verify the container image hash, for example using `crane`:"
msgstr ""

#. type: delimited block -
#: asciidoc/edge-book/releasenotes.adoc:42
#, no-wrap
msgid ""
"> crane digest registry.suse.com/edge/baremetal-operator:0.5.1\n"
"sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782\n"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:43
msgid "Verify with `cosign`:"
msgstr ""

#. type: delimited block -
#: asciidoc/edge-book/releasenotes.adoc:44
#, no-wrap
msgid ""
"> cosign verify-attestation --type spdxjson --key key.pem "
"registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 "
"> /dev/null\n"
"#\n"
"Verification for "
"registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 "
"--\n"
"The following checks were performed on each of these signatures:\n"
"  - The cosign claims were validated\n"
"  - The claims were present in the transparency log\n"
"  - The signatures were integrated into the transparency log when the "
"certificate was valid\n"
"  - The signatures were verified against the specified public key\n"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:45
msgid ""
"Extract SBOM data as described at the "
"https://www.suse.com/support/security/sbom/[upstream documentation]:"
msgstr ""

#. type: delimited block -
#: asciidoc/edge-book/releasenotes.adoc:46
#, no-wrap
msgid ""
"> cosign verify-attestation --type spdxjson --key key.pem "
"registry.suse.com/edge/baremetal-operator@sha256:13e8b2c59aeb503f8adaac095495007071559c9d6d8ef5a7cb1ce6fd1430c782 "
"| jq '.payload | @base64d | fromjson | .predicate'\n"
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:47
#, no-wrap
msgid "Upgrade Steps"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:48
msgid ""
"Refer to the Day 2 Documentation for details around how to upgrade to a new "
"z-stream release."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:49
#, no-wrap
msgid "Known Limitations"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:50
msgid ""
"Unless otherwise stated these apply to the 3.0.0 release and all subsequent "
"z-stream versions."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:51
msgid ""
"Akri is released for the first time as a Technology Preview offering, and is "
"not subject to the standard scope of support."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:52
msgid ""
"Rancher UI Extensions used in SUSE Edge cannot currently be deployed via the "
"Rancher Marketplace and must be deployed "
"manually. https://github.com/rancher/rancher/issues/29105[Rancher issue "
"#29105]"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:53
msgid ""
"If you're using NVIDIA GPU's, SELinux cannot be enabled at the containerd "
"layer due to a missing SELinux "
"policy. https://bugzilla.suse.com/show_bug.cgi?id=1222725[Bugzilla #1222725]"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:54
msgid ""
"If deploying with Metal^3^ and Cluster API (CAPI), clusters aren't "
"automatically imported into Rancher post-installation. It will be addressed "
"in future releases."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:55
#, no-wrap
msgid "Product Support Lifecycle"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:56
msgid ""
"SUSE Edge is backed by award-winning support from SUSE, an established "
"technology leader with a proven history of delivering enterprise-quality "
"support services. For more information, see "
"https://www.suse.com/lifecycle[https://www.suse.com/lifecycle] and the "
"Support Policy page at "
"https://www.suse.com/support/policy.html[https://www.suse.com/support/policy.html]. "
"If you have any questions about raising a support case, how SUSE classifies "
"severity levels, or the scope of support, please see the Technical Support "
"Handbook at "
"https://www.suse.com/support/handbook/[https://www.suse.com/support/handbook/]."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:57
msgid ""
"At the time of publication, each minor version of SUSE Edge, e.g. \"3.0\" is "
"supported for 12-months of production support, with an initial 6-months of "
"\"full support\", followed by 6-months of \"maintenance support\". In the "
"\"full support\" coverage period, SUSE may introduce new features (that do "
"not break existing functionality), introduce bug fixes, and deliver security "
"patches. During the \"maintenance support\" window, only critical security "
"and bug fixes will be introduced, with other fixes delivered at our "
"discretion."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:58
msgid ""
"Unless explicitly stated, all components listed are considered Generally "
"Available (GA), and are covered by SUSE's standard scope of support. Some "
"components may be listed as \"Technology Preview\", where SUSE is providing "
"customers with access to early pre-GA features and functionality for "
"evaluation, but are not subject to the standard support policies and are not "
"recommended for production use-cases. SUSE very much welcomes feedback and "
"suggestions on the improvements that can be made to Technology Preview "
"components, but SUSE reserves the right to deprecate a Technology Preview "
"feature before it becomes Generally Available if it doesn't meet the needs "
"of our customers or doesn't reach a state of maturity that we require."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:59
msgid ""
"Please note that SUSE must occasionally deprecate features or change API "
"specifications. Reasons for feature deprecation or API change could include "
"a feature being updated or replaced by a new implementation, a new feature "
"set, upstream technology is no longer available, or the upstream community "
"has introduced incompatible changes. It is not intended that this will ever "
"happen within a given minor release (x.z), and so all z-stream releases will "
"maintain API compatibility and feature functionality. SUSE will endeavor to "
"provide deprecation warnings with plenty of notice within the release notes, "
"along with workarounds, suggestions, and mitigations to minimize service "
"disruption."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:60
msgid ""
"The SUSE Edge team also welcomes community feedback, where issues can be "
"raised within the respective code repository within "
"https://www.github.com/suse-edge[https://www.github.com/suse-edge]."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:61
#, no-wrap
msgid "Obtaining source code"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:62
msgid ""
"This SUSE product includes materials licensed to SUSE under the GNU General "
"Public License (GPL) and various other open source licenses. The GPL "
"requires SUSE to provide the source code that corresponds to the "
"GPL-licensed material, and SUSE conforms to all other open-source license "
"requirements. As such, SUSE makes all source code available, and can "
"generally be found in the SUSE Edge GitHub repository "
"(https://www.github.com/suse-edge[https://www.github.com/suse-edge]), the "
"SUSE Rancher GitHub repository "
"(https://www.github.com/rancher[https://www.github.com/rancher]) for "
"dependent components, and specifically for SLE Micro, the source code is "
"available for download at "
"https://www.suse.com/download/sle-micro/[https://www.suse.com/download/sle-micro] "
"on \"Medium 2\"."
msgstr ""

#. type: Title =
#: asciidoc/edge-book/releasenotes.adoc:63
#, no-wrap
msgid "Legal notices"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:64
msgid ""
"SUSE makes no representations or warranties with regard to the contents or "
"use of this documentation, and specifically disclaims any express or implied "
"warranties of merchantability or fitness for any particular "
"purpose. Further, SUSE reserves the right to revise this publication and to "
"make changes to its content, at any time, without the obligation to notify "
"any person or entity of such revisions or changes."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:65
msgid ""
"Further, SUSE makes no representations or warranties with regard to any "
"software, and specifically disclaims any express or implied warranties of "
"merchantability or fitness for any particular purpose. Further, SUSE "
"reserves the right to make changes to any and all parts of SUSE software, at "
"any time, without any obligation to notify any person or entity of such "
"changes."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:66
msgid ""
"Any products or technical information provided under this Agreement may be "
"subject to U.S. export controls and the trade laws of other countries. You "
"agree to comply with all export control regulations and to obtain any "
"required licenses or classifications to export, re-export, or import "
"deliverables. You agree not to export or re-export to entities on the "
"current U.S. export exclusion lists or to any embargoed or terrorist "
"countries as specified in U.S. export laws. You agree to not use "
"deliverables for prohibited nuclear, missile, or chemical/biological "
"weaponry end uses. Refer to "
"https://www.suse.com/company/legal/[https://www.suse.com/company/legal/] for "
"more information on exporting SUSE software. SUSE assumes no responsibility "
"for your failure to obtain any necessary export approvals."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:67
msgid "*Copyright © 2024 SUSE LLC.*"
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:68
msgid ""
"This release notes document is licensed under a Creative Commons "
"Attribution-NoDerivatives 4.0 International License (CC-BY-ND-4.0). You "
"should have received a copy of the license along with this document. If not, "
"see "
"https://creativecommons.org/licenses/by-nd/4.0/[https://creativecommons.org/licenses/by-nd/4.0/]."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:69
msgid ""
"SUSE has intellectual property rights relating to technology embodied in the "
"product that is described in this document. In particular, and without "
"limitation, these intellectual property rights may include one or more of "
"the U.S. patents listed at "
"https://www.suse.com/company/legal/[https://www.suse.com/company/legal/] and "
"one or more additional patents or pending patent applications in the "
"U.S. and other countries."
msgstr ""

#. type: Plain text
#: asciidoc/edge-book/releasenotes.adoc:70
msgid ""
"For SUSE trademarks, see the SUSE Trademark and Service Mark list "
"(https://www.suse.com/company/legal/[https://www.suse.com/company/legal/]). "
"All third-party trademarks are the property of their respective owners. For "
"SUSE brand information and usage requirements, please see the guidelines "
"published at https://brand.suse.com/[https://brand.suse.com/]."
msgstr ""
