[#atip-management-cluster]
== Setting up the Management Cluster
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

=== Introduction
The Management Cluster is the part of ATIP that is used to manage the provision and lifecycle of the runtime stacks.
From a technical point of view, the management cluster contains the following components:

* `SUSE Linux Enterprise Micro` as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.
* `RKE2` as the Kubernetes cluster. Depending on the use case, it can be configured to use specific CNI plugins, such as `Multus`, `Cilium` etc.
* `Rancher` as the management platform to manage the lifecycle of the clusters.
* `Metal^3^` as the component to manage the lifecycle of the bare metal nodes.
* `CAPI` as the component to manage the lifecycle of the Kubernetes clusters (downstream clusters). In the case of ATIP, also the `RKE2 CAPI Provider` is used to manage the lifecycle of the RKE2 clusters (downstream clusters).

With all components mentioned above, the management cluster is able to manage the lifecycle of the runtime stacks, using a declarative approach and the `Zero Touch Provisioning` to manage the infrastructure and the applications.

[NOTE]
====
For more information about `SUSE Linux Enterprise Micro`, please check: <<components-slmicro,SLE Micro>>

For more information about `RKE2`, please check: <<components-rke2,RKE2>>

For more information about `Rancher`, please check: <<components-rancher,Rancher>>

For more information about `Metal3`, please check: <<components-metal3,Metal^3^>>
====

=== Steps to setup the Management Cluster

The following steps are necessary to set up the management cluster (using a single-node):

image::product-atip-mgmtcluster1.png[]

There are two main steps to set up the management cluster using a declarative approach:

* **Image Creation**: The first step is to create the image to be used for the management cluster with all the necessary configurations. This image will be generated using `Edge Image Builder` and this step can be done using a laptop, server, VM or any other x86_64 system with a container runtime installed.
* **Management Cluster Provision**: This step covers the installation of the management cluster using the image created in the previous step, and it will install all components mentioned in the introduction.

[NOTE]
====
For more information about `Edge Image Builder`, please check: <<components-eib,Edge Image Builder>> or the <<quickstart-eib,Edge Image Builder quickstart>>
====

=== Image Preparation

Using `Edge Image Builder` to create the image for the management cluster, a lot of configurations can be customized, but in this document, we will cover the minimal configurations necessary to set up the management cluster.
Edge Image Builder is typically run from inside a container so, if you don't already have a way to run containers, we need to start by installing a container runtime such as https://podman.io[Podman] or https://rancherdesktop.io[Rancher Desktop]. For this guide, we will assume you already have a container runtime available.

==== Directory Structure

When running the `EIB`, a directory will be mounted from the host, so the first thing to do is to create a directory structure to be used by the `EIB` to store the configuration files and the image itself.
This directory has the following structure:

[,console]
----
├── mgmt-cluster.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
├── network/
|   └ mgmt-cluster-network.yaml
├── kubernetes/
|   └ config/
|   | └ server.yaml
└── custom/
    └ files/
    | └ clusterctl.yaml
    | └ helm-values-metal3.yaml
    └ scripts/
    | └ install-dependencies.sh
----

[NOTE]
====
The image `SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso` has to be downloaded from the https://scc.suse.com/[SUSE Customer Center] or the https://www.suse.com/download/sle-micro/[SUSE Download page], and it has to be located under the `base-images` folder.
====

==== Management Cluster definition

The `mgmt-cluster.yaml` file is the main configuration file for the management cluster. It contains the following information:

[,yaml]
----
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
    unattended: true
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
----

To explain the fields and values in the `mgmt-cluster.yaml` file, we have divided it into the following sections.

==== Image section:

[,yaml]
----
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
----

where the `baseImage` is the original image to be used which has been downloaded from the SUSE Customer Center or the SUSE Download page, and the `outputImageName` is the name of the image to be created which will be used to provision the management cluster.

==== Operating System section:

[,yaml]
----
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
    unattended: true
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
  sccRegistrationCode: ${SCC_REGISTRATION_CODE}
----

where the `installDevice` is the device to be used to install the operating system, the `unattended` is a flag to indicate if the installation will be unattended, the `username` and `encryptedPassword` are the credentials to be used to access the system, the `packageList` is the list of packages to be installed and the `sccRegistrationCode` is the registration code to be used to register the system that can be obtained from the SUSE Customer Center.

The encrypted password can be generated using the `openssl` command as follows:

[,shell]
----
openssl passwd -6 MyPassword!123
----

This will output something similar to:

[,console]
----
$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31
----

==== Kubernetes section:

[,yaml]
----
kubernetes:
  version: ${KUBERNETES_VERSION}
----

where `version` is the version of Kubernetes to be installed. In our case, we are using a RKE2 cluster, so the version has to be minor than 1.28 to be compatible with `Rancher` (e.g `v1.27.10+rke2r1`).
[#mgmt-cluster-helm-values]
==== Custom files section:

The `custom/files` folder contains the following files:

* `helm-values-metal3.yaml`: contains the configuration params about the `Metal^3^` Helm chart to be used.
* `clusterctl.yaml`: contains the configuration params about the `CAPI` Helm chart to be used.

The following variables have to be replaced:

`$\{MGMT_CLUSTER_IP\}`: The IP address of the management cluster.

[#metal3-media-server]
[NOTE]
====
The Media Server is an optional feature included in metal3. In case you want to use your own media server (file server), you can disable the `enable_metal3_media_server` on the following manifest.
In case you want to use the metal3 media server you also specify the following variable:
`$\{MEDIA_VOLUME_PATH\}`: The path to the media volume to be used by the `Metal^3^` component (e.g. `/home/metal3/bmh-image-cache`)
====

[,yaml]
----
global:
  ironicIP: ${MGMT_CLUSTER_IP}
  enable_vmedia_tls: false
  enable_metal3_media_server: true

metal3-media:
  service:
    type: NodePort
    port: 6280

metal3-ironic:
  global:
    predictableNicNames: "true"
  service:
    type: NodePort

metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}
----

The `clusterctl.yaml` file:

[,yaml]
----
images:
  all:
    repository: registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse
----

==== Custom Scripts section:

The `custom/scripts` folder contains the following files:

* `install-dependencies.sh` script contains the commands to install the necessary dependencies to be installed in the management cluster, like `Rancher`, `Metal^3^`, `Cert-Manager`, etc...:

The following steps are executed by the `install-dependencies.sh` script:

- Create the folder to server enable the media server for the `Metal^3^` component.
- Copy the `helm-values-metal3.yaml` file to the `Metal^3^` folder.
- Create the installer script to install the necessary tools, like clusterctl, helm for the management cluster.
- Wait for the cluster to be available.
- Install the `Cert-Manager` component.
- Install the `Local-Path-Provisioner` component (for a single-node cluster).
- Install the `Rancher Prime` component.
- Install the `Metal^3^` component.
- Install the `CAPI` component.
- Create the systemd service to run the installer script during the first boot.

The `install-dependencies.sh` script is as follows:

[,shell]
----
#!/bin/bash

mount /usr/local || true
mount /home || true

## create folder to server httpd media server
mkdir -p /home/metal3/bmh-image-cache

## copy the metal3 yaml file to metal3 folder
cp ./helm-values-metal3.yaml ./clusterctl.yaml ./disable-embedded-capi.yaml /home/metal3/

## KUBECTL command var
export KUBECTL=/var/lib/rancher/rke2/bin/kubectl

# Create the installer script
cat <<- EOF > /usr/local/bin/mgmt-cluster-installer.sh
#!/bin/bash
set -euo pipefail

## install clusterctl and helm
curl -Lk https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.0/clusterctl-linux-amd64 -o /usr/local/bin/clusterctl
chmod +x /usr/local/bin/clusterctl
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

## Wait for RKE2 cluster to be available
until [ -f /etc/rancher/rke2/rke2.yaml ]; do sleep 2; done
# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
# Wait for the node to be available, meaning the K8s API is available
while ! ${KUBECTL} wait --for condition=ready node $(cat /etc/hostname | tr '[:upper:]' '[:lower:]') ; do sleep 2 ; done

## Add Helm repos
helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo add jetstack https://charts.jetstack.io
helm repo update

while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller ; do sleep 2 ; done

## install cert-manager
helm install cert-manager jetstack/cert-manager \
	--namespace cert-manager \
        --create-namespace \
        --set installCRDs=true \
	--version v1.11.1

## Local path provisioner
${KUBECTL} apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
until [ \$(${KUBECTL} get sc -o name | wc -l) -ge 1 ]; do sleep 10; done
${KUBECTL} patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

## Example in case you want to configure the httpd cache server for images
## podman run -dit --name bmh-image-cache -p 8080:80 -v /home/metal3/bmh-image-cache:/usr/local/apache2/htdocs/ docker.io/library/httpd:2.4

## install rancher
helm install rancher rancher-prime/rancher \
	--namespace cattle-system \
	--create-namespace \
	--set hostname=rancher-$(hostname -I | awk '{print $1}').sslip.io \
	--set bootstrapPassword=admin \
	--set replicas=1 \
        --set global.cattle.psp.enabled=false
while ! ${KUBECTL} wait --for condition=ready -n cattle-system \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name) --timeout=10s; do sleep 2 ; done

## install metal3 with helm
helm repo add suse-edge https://suse-edge.github.io/charts
helm install   metal3 suse-edge/metal3   --namespace metal3-system   --create-namespace -f /home/metal3/helm-values-metal3.yaml


## install capi
if [ \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name | wc -l) -ge 1 ]; then
	${KUBECTL} apply -f /home/metal3/disable-embedded-capi.yaml
	${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
	${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
	${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi
clusterctl init --core "cluster-api:v1.6.0" --infrastructure "metal3:v1.6.0" --bootstrap "rke2:v0.2.6" --control-plane "rke2:v0.2.6" --config /home/metal3/clusterctl.yaml

rm -f /etc/systemd/system/mgmt-cluster-installer.service
EOF

chmod a+x /usr/local/bin/mgmt-cluster-installer.sh

cat <<- EOF > /etc/systemd/system/mgmt-cluster-installer.service
[Unit]
Description=Deploy mgmt cluster tools on K3S/RKE2
Wants=network-online.target
After=network.target network-online.target rke2-server.target
ConditionPathExists=/usr/local/bin/mgmt-cluster-installer.sh

[Service]
User=root
Type=forking
TimeoutStartSec=900
ExecStart=/usr/local/bin/mgmt-cluster-installer.sh
RemainAfterExit=yes
KillMode=process
# Disable & delete everything
ExecStartPost=rm -f /usr/local/bin/mgmt-cluster-installer.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-cluster-installer.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-cluster-installer.service

[Install]
WantedBy=multi-user.target
EOF

systemctl enable mgmt-cluster-installer.service

umount /usr/local || true
umount /home || true
----

==== Kubernetes definition (optional)

By default, the `CNI` plugin installed by default is `Cilium`, so you don't need to create this file. Just in case you need to customize the `CNI` plugin, you can use the `server.yaml` file under the `kubernetes/config` folder. It contains the following information:

[,yaml]
----
cni:
- multus
- cilium
----

This is an optional file to define some Kubernetes customization like the CNI plugins to be used or many options you can check in the https://docs.rke2.io/install/configuration[official documentation].

==== Networking definition (optional)

In case you need to customize the networking configuration, for example, in case you need to use a specific IP address (DHCP-less scenario), you can use the `mgmt-cluster-network.yaml` file under the `network` folder. It contains the following information:

* `$\{MGMT_GATEWAY\}`: The gateway IP address.
* `$\{MGMT_DNS\}`: The DNS server IP address.
* `$\{MGMT_MAC\}`: The MAC address of the network interface.
* `$\{MGMT_CLUSTER_IP\}`: The IP address of the management cluster.

[,yaml]
----
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
----

=== Image Creation

Once the directory structure is prepared following the previous sections, run the following command to build the image:

[,shell]
----
podman run --rm --privileged -it -v $PWD/eib/:/eib \
 registry.opensuse.org/isv/suse/edge/edgeimagebuilder/containerfile/suse/edge-image-builder:1.0.0 \
 --config-file mgmt-cluster.yaml --config-dir /eib --build-dir /eib/_build
----

This will create the iso output image file that in our case based on the image definition described above will be `eib-mgmt-cluster-image.iso`.
This image contains all components inside, and it can be used to provision the management cluster using a virtual machine, a bare metal server (using the virtual-media feature)