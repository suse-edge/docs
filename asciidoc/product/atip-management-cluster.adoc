[#atip-management-cluster]
== Setting up the management cluster
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

=== Introduction
The management cluster is the part of ATIP that is used to manage the provision and lifecycle of the runtime stacks.
From a technical point of view, the management cluster contains the following components:

* `SUSE Linux Enterprise Micro` as the OS. Depending on the use case, some configurations like networking, storage, users and kernel arguments can be customized.
* `RKE2` as the Kubernetes cluster. Depending on the use case, it can be configured to use specific CNI plugins, such as `Multus`, `Cilium`, etc.
* `Rancher` as the management platform to manage the lifecycle of the clusters.
* `Metal^3^` as the component to manage the lifecycle of the bare metal nodes.
* `CAPI` as the component to manage the lifecycle of the Kubernetes clusters (downstream clusters). With ATIP, also the `RKE2 CAPI Provider` is used to manage the lifecycle of the RKE2 clusters (downstream clusters).

With all components mentioned above, the management cluster can manage the lifecycle of the runtime stacks, using a declarative approach and the `Zero Touch Provisioning` to manage the infrastructure and the applications.

[NOTE]
====
For more information about `SUSE Linux Enterprise Micro`, see: <<components-slmicro,SLE Micro>>

For more information about `RKE2`, see: <<components-rke2,RKE2>>

For more information about `Rancher`, see: <<components-rancher,Rancher>>

For more information about `Metal^3^`, see: <<components-metal3,Metal^3^>>
====

=== Steps to set up the management cluster

The following steps are necessary to set up the management cluster (using a single node):

image::product-atip-mgmtcluster1.png[]

There are two main steps to set up the management cluster using a declarative approach:

* **Image creation**: The first step is to create the image to be used for the management cluster with all the necessary configurations. This image is generated using `Edge Image Builder`, and this step can be done using a laptop, server, VM or any other x86_64 system with a container runtime installed.
* **Management cluster provision**: This step covers the installation of the management cluster using the image created in the previous step. It installs all components mentioned in the introduction.

[NOTE]
====
For more information about `Edge Image Builder`, see <<components-eib,Edge Image Builder>> and <<quickstart-eib,Edge Image Builder Quick Start>>.
====

=== Image preparation

Using `Edge Image Builder` to create the image for the management cluster, a lot of configurations can be customized, but in this document, we cover the minimal configurations necessary to set up the management cluster.
Edge Image Builder is typically run from inside a container so, if you do not already have a way to run containers, we need to start by installing a container runtime such as https://podman.io[Podman] or https://rancherdesktop.io[Rancher Desktop]. For this guide, we assume you already have a container runtime available.

==== Directory structure

When running the `EIB`, a directory is mounted from the host, so the first thing to do is to create a directory structure to be used by the `EIB` to store the configuration files and the image itself.
This directory has the following structure:

[,console]
----
├── mgmt-cluster.yaml
├── base-images/
│   └ SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
├── network/
|   └ mgmt-cluster-network.yaml
├── kubernetes/
|   └ config/
|   | └ server.yaml
└── custom/
    └ files/
    | └ clusterctl.yaml
    | └ helm-values-metal3.yaml
    | └ disable-embedded-capi.yaml
    └ scripts/
    | └ install-dependencies.sh
----

[NOTE]
====
The image `SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso` has to be downloaded from the https://scc.suse.com/[SUSE Customer Center] or the https://www.suse.com/download/sle-micro/[SUSE Download page], and it has to be located under the `base-images` folder.
====

==== Management cluster definition

The `mgmt-cluster.yaml` file is the main configuration file for the management cluster. It contains the following information:

[,yaml]
----
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
    sccRegistrationCode: ${SCC_REGISTRATION_CODE}
kubernetes:
  version: ${KUBERNETES_VERSION}
----

To explain the fields and values in the `mgmt-cluster.yaml` file, we have divided it into the following sections.

==== Image section:

[,yaml]
----
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
----

where the `baseImage` is the original image you downloaded from the SUSE Customer Center or the SUSE Download page. The `outputImageName` is the name of the new image that will be used to provision the management cluster.

==== Operating system section:

[,yaml]
----
operatingSystem:
  isoConfiguration:
    installDevice: /dev/sda
    unattended: true
  users:
  - username: root
    encryptedPassword: ${ROOT_PASSWORD}
  packages:
    packageList:
    - git
  sccRegistrationCode: ${SCC_REGISTRATION_CODE}
----

where the `installDevice` is the device to be used to install the operating system, the `unattended` is a flag to indicate if the installation is unattended, the `username` and `encryptedPassword` are the credentials to be used to access the system, the `packageList` is the list of packages to be installed and the `sccRegistrationCode` is the registration code to be used to register the system that can be obtained from the SUSE Customer Center.

The encrypted password can be generated using the `openssl` command as follows:

[,shell]
----
openssl passwd -6 MyPassword!123
----

This outputs something similar to:

[,console]
----
$6$UrXB1sAGs46DOiSq$HSwi9GFJLCorm0J53nF2Sq8YEoyINhHcObHzX2R8h13mswUIsMwzx4eUzn/rRx0QPV4JIb0eWCoNrxGiKH4R31
----

==== Kubernetes section:

[,yaml]
----
kubernetes:
  version: ${KUBERNETES_VERSION}
----

where `version` is the version of Kubernetes to be installed. In our case, we are using an RKE2 cluster, so the version has to be minor than 1.29 to be compatible with `Rancher` (for example, `v1.28.8+rke2r1`).

[#mgmt-cluster-helm-values]
==== Custom files section:

The `custom/files` folder contains the following files:

* `helm-values-metal3.yaml`: contains the configuration parameters about the `Metal^3^` Helm chart to be used.
* `clusterctl.yaml`: contains the configuration parameters about the `CAPI` Helm chart to be used.
* `disable-embedded-capi.yaml`: contains the configuration parameters to disable the embedded `CAPI` component.

The following variables have to be replaced:

`$\{MGMT_CLUSTER_IP\}`: The IP address of the management cluster.

[#metal3-media-server]
[NOTE]
====
The Media Server is an optional feature included in Metal^3^. To use your own media server (file server), disable `enable_metal3_media_server` on the following manifest.
To use the Metal^3^ media server, specify the following variable:
`$\{MEDIA_VOLUME_PATH\}` — the path to the media volume to be used by the `Metal^3^` component (for example, `/home/metal3/bmh-image-cache`).
====

The `helm-values-metal3.yaml` file:

[,yaml]
----
global:
  ironicIP: ${MGMT_CLUSTER_IP}
  enable_vmedia_tls: false
  enable_metal3_media_server: true

metal3-media:
  service:
    type: NodePort
    port: 6280

metal3-ironic:
  global:
    predictableNicNames: "true"
  service:
    type: NodePort

metal3-media:
  mediaVolume:
    hostPath: ${MEDIA_VOLUME_PATH}
----

The `clusterctl.yaml` file:

[,yaml]
----
images:
  all:
    repository: registry.opensuse.org/isv/suse/edge/clusterapi/containerfile/suse
----

The `disable-embedded-capi.yaml` file:

[,yaml]
----
apiVersion: management.cattle.io/v3
kind: Feature
metadata:
  name: embedded-cluster-api
spec:
  value: false
----

==== Custom scripts section:

The `custom/scripts` folder contains the following files:

* The `install-dependencies.sh` script contains the commands to install essential dependencies required for the management cluster, such as `Rancher`, `Metal^3^`, `Cert-Manager`, etc.:

The following steps are executed by the `install-dependencies.sh` script:

- Create the folder to enable the media server for the `Metal^3^` component.
- Copy the `helm-values-metal3.yaml` file to the `Metal^3^` folder.
- Create the installer script to install the necessary tools, like clusterctl, helm for the management cluster.
- Wait for the cluster to be available.
- Install the `Cert-Manager` component.
- Install the `Local-Path-Provisioner` component (for a single-node cluster).
- Install the `Rancher Prime` component.
- Install the `Metal^3^` component.
- Install the `CAPI` component.
- Create the systemd service to run the installer script during the first boot.

The `install-dependencies.sh` script is as follows:

[,shell]
----
#!/bin/bash

mount /usr/local || true
mount /home || true

## create folder to server httpd media server
mkdir -p /home/metal3/bmh-image-cache

## copy the metal3 yaml file to metal3 folder
cp ./helm-values-metal3.yaml ./clusterctl.yaml ./disable-embedded-capi.yaml /home/metal3/

## KUBECTL command var
export KUBECTL=/var/lib/rancher/rke2/bin/kubectl

# Create the installer script
cat <<- EOF > /usr/local/bin/mgmt-cluster-installer.sh
#!/bin/bash
set -euo pipefail

## install clusterctl and helm
curl -Lk https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.6.0/clusterctl-linux-amd64 -o /usr/local/bin/clusterctl
chmod +x /usr/local/bin/clusterctl
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

## Wait for RKE2 cluster to be available
until [ -f /etc/rancher/rke2/rke2.yaml ]; do sleep 2; done
# export the kubeconfig using the right kubeconfig path depending on the cluster (k3s or rke2)
export KUBECONFIG=/etc/rancher/rke2/rke2.yaml
# Wait for the node to be available, meaning the K8s API is available
while ! ${KUBECTL} wait --for condition=ready node $(hostname | tr '[:upper:]' '[:lower:]') ; do sleep 2 ; done

## Add Helm repos
helm repo add rancher-prime https://charts.rancher.com/server-charts/prime
helm repo add jetstack https://charts.jetstack.io
helm repo update

while ! ${KUBECTL} rollout status daemonset -n kube-system rke2-ingress-nginx-controller ; do sleep 2 ; done

## install cert-manager
helm install cert-manager jetstack/cert-manager \
	--namespace cert-manager \
        --create-namespace \
        --set installCRDs=true \
	--version v1.11.1

## Local path provisioner
${KUBECTL} apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
until [ \$(${KUBECTL} get sc -o name | wc -l) -ge 1 ]; do sleep 10; done
${KUBECTL} patch storageclass local-path -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

## Example in case you want to configure the httpd cache server for images
## podman run -dit --name bmh-image-cache -p 8080:80 -v /home/metal3/bmh-image-cache:/usr/local/apache2/htdocs/ docker.io/library/httpd:2.4

## install rancher
helm install rancher rancher-prime/rancher \
	--namespace cattle-system \
	--create-namespace \
	--set hostname=rancher-$(hostname -I | awk '{print $1}').sslip.io \
	--set bootstrapPassword=admin \
	--set replicas=1 \
        --set global.cattle.psp.enabled=false
while ! ${KUBECTL} wait --for condition=ready -n cattle-system \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name) --timeout=10s; do sleep 2 ; done

## install metal3 with helm
helm repo add suse-edge https://suse-edge.github.io/charts
helm install   metal3 suse-edge/metal3   --namespace metal3-system   --create-namespace -f /home/metal3/helm-values-metal3.yaml


## install capi
if [ \$(${KUBECTL} get pods -n cattle-system -l app=rancher -o name | wc -l) -ge 1 ]; then
	${KUBECTL} apply -f /home/metal3/disable-embedded-capi.yaml
	${KUBECTL} delete mutatingwebhookconfiguration.admissionregistration.k8s.io mutating-webhook-configuration
	${KUBECTL} delete validatingwebhookconfigurations.admissionregistration.k8s.io validating-webhook-configuration
	${KUBECTL} wait --for=delete namespace/cattle-provisioning-capi-system --timeout=300s
fi
clusterctl init --core "cluster-api:v1.6.2" --infrastructure "metal3:v1.6.0" --bootstrap "rke2:v0.2.6" --control-plane "rke2:v0.2.6" --config /home/metal3/clusterctl.yaml

rm -f /etc/systemd/system/mgmt-cluster-installer.service
EOF

chmod a+x /usr/local/bin/mgmt-cluster-installer.sh

cat <<- EOF > /etc/systemd/system/mgmt-cluster-installer.service
[Unit]
Description=Deploy mgmt cluster tools on K3S/RKE2
Wants=network-online.target
After=network.target network-online.target rke2-server.target
ConditionPathExists=/usr/local/bin/mgmt-cluster-installer.sh

[Service]
User=root
Type=forking
TimeoutStartSec=900
ExecStart=/usr/local/bin/mgmt-cluster-installer.sh
RemainAfterExit=yes
KillMode=process
# Disable & delete everything
ExecStartPost=rm -f /usr/local/bin/mgmt-cluster-installer.sh
ExecStartPost=/bin/sh -c "systemctl disable mgmt-cluster-installer.service"
ExecStartPost=rm -f /etc/systemd/system/mgmt-cluster-installer.service

[Install]
WantedBy=multi-user.target
EOF

systemctl enable mgmt-cluster-installer.service

umount /usr/local || true
umount /home || true
----

==== Kubernetes definition (optional)

By default, the `CNI` plugin installed by default is `Cilium`, so you do not need to create this file. Just in case you need to customize the `CNI` plugin, you can use the `server.yaml` file under the `kubernetes/config` folder. It contains the following information:

[,yaml]
----
cni:
- multus
- cilium
----

This is an optional file to define certain Kubernetes customization, like the CNI plug-ins to be used or many options you can check in the https://docs.rke2.io/install/configuration[official documentation].

==== Networking definition (optional)

If you need to customize the networking configuration, for example, to use a specific IP address (DHCP-less scenario), you can use the `mgmt-cluster-network.yaml` file under the `network` folder. It contains the following information:

* `$\{MGMT_GATEWAY\}`: The gateway IP address.
* `$\{MGMT_DNS\}`: The DNS server IP address.
* `$\{MGMT_MAC\}`: The MAC address of the network interface.
* `$\{MGMT_CLUSTER_IP\}`: The IP address of the management cluster.

[,yaml]
----
routes:
  config:
  - destination: 0.0.0.0/0
    metric: 100
    next-hop-address: ${MGMT_GATEWAY}
    next-hop-interface: eth0
    table-id: 254
dns-resolver:
  config:
    server:
    - ${MGMT_DNS}
    - 8.8.8.8
interfaces:
- name: eth0
  type: ethernet
  state: up
  mac-address: ${MGMT_MAC}
  ipv4:
    address:
    - ip: ${MGMT_CLUSTER_IP}
      prefix-length: 24
    dhcp: false
    enabled: true
  ipv6:
    enabled: false
----

=== Image creation

Once the directory structure is prepared following the previous sections, run the following command to build the image:

[,shell]
----
podman run --rm --privileged -it -v $PWD:/eib \
 registry.suse.com/edge/edge-image-builder:1.0.1 \
 build --definition-file mgmt-cluster.yaml
----

This creates the ISO output image file that, in our case, based on the image definition described above, is `eib-mgmt-cluster-image.iso`.
This image contains all components inside, and it can be used to provision the management cluster using a virtual machine or a bare-metal server (using the virtual-media feature).
