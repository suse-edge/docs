[#day2-helm-upgrade]
== Helm chart upgrade
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc: auto

[NOTE]
====
The below sections focus on using `Fleet` functionalities to achieve a Helm chart update.

Users adopting a third-party GitOps workflow, should take the configurations for their desired helm chart from its `fleet.yaml` located at `fleets/day2/chart-templates/<chart-name>`. *Make sure you are retrieving the chart data from a valid "Day 2" Edge link:https://github.com/suse-edge/fleet-examples/releases[release].*
====

=== Components

Apart from the default `Day 2` <<day2-downstream-components, components>>, no other custom components are needed for this operation.

=== Preparation for air-gapped environments

==== Ensure that you have access to your Helm chart's upgrade `fleet.yaml` file

Host the needed resources on a local git server that is accessible by your `management cluster`.

==== Find the required assets for your Edge release version

. Go to the Day 2 link:https://github.com/suse-edge/fleet-examples/releases[release] page and find the Edge 3.X.Y release that you want to upgrade your chart to and click *Assets*.

. From the release's *Assets* section, download the following files, which are required for an air-gapped upgrade of a SUSE supported helm chart:
+
[cols="1,1"]
|======
|*Release File* 
|*Description*

|_edge-save-images.sh_
|This script pulls the images in the `edge-release-images.txt` file and saves them to a '.tar.gz' archive that can then be used in your air-gapped environment.

|_edge-save-oci-artefacts.sh_
|This script pulls the SUSE OCI chart artefacts in the `edge-release-helm-oci-artefacts.txt` file and creates a '.tar.gz' archive of a directory containing all other chart OCI archives.

|_edge-load-images.sh_
|This script loads the images in the '.tar.gz' archive generated by `edge-save-images.sh`, retags them and pushes them to your private registry.

|_edge-load-oci-artefacts.sh_
|This script takes a directory containing '.tgz' SUSE OCI charts and loads all OCI charts to your private registry. The directory is retrieved from the '.tar.gz' archive that the `edge-save-oci-artefacts.sh` script has generated.

|_edge-release-helm-oci-artefacts.txt_
|This file contains a list of OCI artefacts for the SUSE Edge release Helm charts.

|_edge-release-images.txt_
|This file contains a list of images needed by the Edge release Helm charts.
|======

==== Create the SUSE Edge release images archive

_On a machine with internet access:_

. Make `edge-save-images.sh` executable:
+
[,bash]
----
chmod +x edge-save-images.sh
----

. Use `edge-save-images.sh` script to create a _Docker_ importable '.tar.gz' archive:
+
[,bash]
----
./edge-save-images.sh --source-registry registry.suse.com
----

. This will create a ready to load `edge-images.tar.gz` (unless you have specified the `-i|--images` option) archive with the needed images.

. Copy this archive to your *air-gapped* machine
+
[,bash]
----
scp edge-images.tar.gz <user>@<machine_ip>:/path
----

==== Create a SUSE Edge Helm chart OCI images archive

_On a machine with internet access:_

. Make `edge-save-oci-artefacts.sh` executable:
+
[,bash]
----
chmod +x edge-save-oci-artefacts.sh
----

. Use `edge-save-oci-artefacts.sh` script to create a '.tar.gz' archive of all SUSE Edge Helm chart OCI images:
+
[,bash]
----
./edge-save-oci-artefacts.sh --source-registry registry.suse.com
----

. This will create a `oci-artefacts.tar.gz` archive containing all SUSE Edge Helm chart OCI images

. Copy this archive to your *air-gapped* machine
+
[,bash]
----
scp oci-artefacts.tar.gz <user>@<machine_ip>:/path
----

==== Load SUSE Edge release images to your air-gapped machine

_On your air-gapped machine:_

. Log into your private registry (if required):
+
[,bash]
----
podman login <REGISTRY.YOURDOMAIN.COM:PORT>
----

. Make `edge-load-images.sh` executable:
+
[,bash]
----
chmod +x edge-load-images.sh
----

. Use `edge-load-images.sh` to load the images from the *copied* `edge-images.tar.gz` archive, retag them and push them to your private registry:
+
[,bash]
----
./edge-load-images.sh --source-registry registry.suse.com --registry <REGISTRY.YOURDOMAIN.COM:PORT> --images edge-images.tar.gz
----

==== Load SUSE Edge Helm chart OCI images to your air-gapped machine

_On your air-gapped machine:_

. Log into your private registry (if required):
+
[,bash]
----
podman login <REGISTRY.YOURDOMAIN.COM:PORT>
----

. Make `edge-load-oci-artefacts.sh` executable:
+
[,bash]
----
chmod +x edge-load-oci-artefacts.sh
----

. Untar the copied `oci-artefacts.tar.gz` archive:
+
[,bash]
----
tar -xvf oci-artefacts.tar.gz
----

. This will produce a directory with the naming template `edge-release-oci-tgz-<date>`

. Pass this directory to the `edge-load-oci-artefacts.sh` script to load the SUSE Edge helm chart OCI images to your private registry:
+
[NOTE]
====
This script assumes the `helm` CLI has been pre-installed on your environment. For Helm installation instructions, see link:https://helm.sh/docs/intro/install/[Installing Helm].
====
+
[,bash]
----
./edge-load-oci-artefacts.sh --archive-directory edge-release-oci-tgz-<date> --registry <REGISTRY.YOURDOMAIN.COM:PORT> --source-registry registry.suse.com
----

==== Create registry mirrors pointing to your private registry for your Kubernetes distribution

For RKE2, see link:https://docs.rke2.io/install/containerd_registry_configuration[Containerd Registry Configuration]

For K3s, see link:https://docs.k3s.io/installation/registry-mirror[Embedded Registry Mirror]

=== Upgrade procedure

[NOTE]
====
The below upgrade procedure utilises Rancher's <<components-fleet,Fleet>> funtionality. Users using a third-party GitOps workflow should retrieve the chart versions supported by each Edge release from the <<release-notes>> and populate these versions to their third-party GitOps workflow.
====

This section focuses on the following Helm upgrade procedure use-cases:

. <<day2-helm-upgrade-new-cluster, I have a new cluster and would like to deploy and manage a SUSE Helm chart>>

. <<day2-helm-upgrade-fleet-managed-chart, I would like to upgrade a Fleet managed Helm chart>>

. <<day2-helm-upgrade-eib-chart, I would like to upgrade an EIB created Helm chart>>

[IMPORTANT]
====
Manually deployed Helm charts cannot be reliably upgraded. We suggest to redeploy the helm chart using the <<day2-helm-upgrade-new-cluster>> method.
====

[#day2-helm-upgrade-new-cluster]
==== I have a new cluster and would like to deploy and manage a SUSE Helm chart

For users that want to manage their Helm chart lifecycle through Fleet.

===== Prepare your Fleet resources

. Acquire the Chart's Fleet resources from the Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag that you wish to use

.. From the selected Edge release tag revision, navigate to the Helm chart fleet - `fleets/day2/chart-templates/<chart>`

.. Copy the chart Fleet directory to the Git repository that you will be using for your GitOps workflow

.. *Optionally*, if the Helm chart requires configurations to its *values*, edit the `.helm.values` configuration inside the `fleet.yaml` file of the copied directory

.. *Optionally*, there may be use-cases where you need to add additional resources to your chart's fleet so that it can better fit your environment. For information on how to enhance your Fleet directory, see link:https://fleet.rancher.io/gitrepo-content[Git Repository Contents]

An *example* for the `longhorn` helm chart would look like:

* User Git repository strucutre:
+
[,bash]
----
<user_repository_root>
└── longhorn
    └── fleet.yaml
----

* `fleet.yaml` content populated with user `longhorn` data:
+
[,yaml]
----
defaultNamespace: longhorn-system

helm:
  releaseName: "longhorn"
  chart: "longhorn"
  repo: "https://charts.longhorn.io"
  version: "1.6.1"
  takeOwnership: true
  # custom chart value overrides
  values: 
    # Example for user provided custom values content
    defaultSettings:
      deletingConfirmationFlag: true

# https://fleet.rancher.io/bundle-diffs
diff:
  comparePatches:
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: engineimages.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: nodes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
  - apiVersion: apiextensions.k8s.io/v1
    kind: CustomResourceDefinition
    name: volumes.longhorn.io
    operations:
    - {"op":"remove", "path":"/status/conditions"}
    - {"op":"remove", "path":"/status/storedVersions"}
    - {"op":"remove", "path":"/status/acceptedNames"}
----
+
[NOTE]
====
These are just example values that are used to illustrate custom configurations over the `longhorn` chart. They should *NOT* be treated as deployment guidelines for the `longhorn` chart.
====

===== Create the GitRepo

After populating your repository with the chart's Fleet resources, you must create a link:https://fleet.rancher.io/ref-gitrepo[GitRepo] resource. This resource will hold information on how to access your chart's Fleet resources and to which clusters it needs to apply those resources.

The `GitRepo` resource can be created through the Rancher UI, or by manually deploying the resource to the `management cluster`.

For information on how to create and deploy the GitRepo resource *manually*, see link:https://fleet.rancher.io/tut-deployment[Creating a Deployment].

To create a `GitRepo` resource through the *Rancher UI*, see link:https://ranchermanager.docs.rancher.com/v2.8/integrations-in-rancher/fleet/overview#accessing-fleet-in-the-rancher-ui[Accessing Fleet in the Rancher UI].

_Example *longhorn* `GitRepo` resource for *manual* deployment:_

[,yaml]
----
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: longhorn-git-repo
  namespace: fleet-default
spec:
  # If using a tag
  # revision: <user_repository_tag>
  # 
  # If using a branch
  # branch: <user_repository_branch>
  paths:
  # As seen in the 'Prepare your Fleet resources' example
  - longhorn
  repo: <user_repository_url>
  targets:
  # Match all clusters
  - clusterSelector: {}
----

===== Managing the deployed Helm chart

Once deployed with Fleet, for Helm chart upgrades, see <<day2-helm-upgrade-fleet-managed-chart>>.

[#day2-helm-upgrade-fleet-managed-chart]
==== I would like to upgrade a Fleet managed Helm chart

. Determine the version to which you need to upgrade your chart so that it is compatible with an Edge 3.X.Y release. Helm chart version per Edge release can be viewed from the <<release-notes>>.

. In your Fleet monitored Git repository, edit the Helm chart's `fleet.yaml` file with the correct chart *version* and *repository* from the <<release-notes>>.

. After commiting and pushing the changes to your repository, this will trigger an upgrade of the desired Helm chart

[#day2-helm-upgrade-eib-chart]
==== I would like to upgrade an EIB created Helm chart

[NOTE]
====
This section assumes that you have deployed the system-upgrade-controller (SUC) beforehand, if you have not done so, or are unsure why you need it, see the default <<day2-downstream-components, Day 2 components>> list.
====

EIB deploys Helm charts by utilizing the auto-deploy manifests functionality of link:https://docs.rke2.io/advanced#auto-deploying-manifests[rke2]/link:https://docs.k3s.io/installation/packaged-components#auto-deploying-manifests-addons[k3s]. It creates a link:https://github.com/k3s-io/helm-controller#helm-controller[HelmChart] resource definition manifest unter the `/var/lib/rancher/<rke2/k3s>/server/manifests` location of the initialiser node and lets `rke2/k3s` pick it up and auto-deploy it in the cluster.

From a `Day 2` point of view this would mean that any upgrades of the Helm chart need to happen by editing the `HelmChart` manifest file of the specific chart. To automate this process for multiple clusters, this section uses *SUC Plans*.

Below you can find information on:

* The general <<day2-helm-upgrade-eib-chart-overview,overview>> of the helm chart upgrade workflow.

* The necessary <<day2-helm-upgrade-eib-chart-upgrade-steps,upgrade steps>> needed for a successful helm chart upgrade.

* An <<day2-helm-upgrade-eib-chart-example, example>> showcasing a link:https://longhorn.io[Longhorn] chart upgrade using the explained method.

* How to use the upgrade process with <<day2-helm-upgrade-eib-chart-third-party, a different GitOps tool>>.

[#day2-helm-upgrade-eib-chart-overview]
===== Overview

This section is meant to give a high overview of the workflow that the user goes through in order to upgrade one or multiple Helm charts. For a detailed explanation of the steps needed for a Helm chart upgrade, see <<day2-helm-upgrade-eib-chart-upgrade-steps>>.

.Helm chart upgrade workflow
image::day2_helm_chart_upgrade_diagram.png[]

. The workflow begins with the user link:https://helm.sh/docs/helm/helm_pull/[pulling] the new Helm chart archive(s) that he wishes to upgrade his chart(s) to.

. The archive(s) should then be _encoded_ and passed as configuration to the `eib-chart-upgrade-user-data.yaml` file that is located under the fleet directory for the related SUC Plan. This is further explained in the <<day2-helm-upgrade-eib-chart-upgrade-steps, upgrade steps>> section.

. The user then proceeds to configure and deploy a `GitRepo` resource that will ship all the needed resources (SUC Plan, secrets, etc.) to the downstream clusters. 

.. The resource is deployed on the `management cluster` under the `fleet-default` namespace.

. <<components-fleet,Fleet>> detects the deployed resource and deploys all the configured resources to the specified downstream clusters. Deployed resources include:

.. The `eib-chart-upgrade` SUC Plan that will be used by SUC to create an *Upgrade Pod* on each node.

.. The `eib-chart-upgrade-script` Secret that ships the `upgrade script` that the *Upgrade Pod* will use to upgrade the `HelmChart` manifests on the initialiser node.

.. The `eib-chart-upgrade-user-data` Secret that ships the chart data that the `upgrade script` will use in order to understand which chart manifests it needs to upgrade.

. Once the `eib-chart-upgrade` SUC Plan has been deployed, the SUC picks it up and creates a Job which deploys the *Upgrade Pod*.

. Once deployed, the *Upgrade Pod* mounts the `eib-chart-upgrade-script` and `eib-chart-upgrade-user-data` Secrets and executes the `upgrade script` that is shipped by the `eib-chart-upgrade-script` Secret.

. The `upgrade script` does the following:

.. Determine whether the Pod that the script is running on has been deployed on the `initialiser` node. The `initialiser` node is the node that is hosting the `HelmChart` manifests. For a single-node cluster it is the single control-plane node. For HA clusters it is the node that you have marked as `initializer` when creating the cluster in EIB. If you have not specified the `initializer` property, then the first node from the `nodes` list is marked as `initializer`. For more information, see the link:https://github.com/suse-edge/edge-image-builder/blob/main/docs/building-images.md#kubernetes[upstream] documentation for EIB.
+
[NOTE]
====
If the `upgrade script` is running on a non-initialiser node, it immediately finishes its execution and does not go through the steps below.
====

.. Backup the manifests that will be edited in order to ensure disaster recover.
+
[NOTE]
====
By default backups of the manifests are stored under the `/tmp/eib-helm-chart-upgrade-<date>` directory. If you wish to use a custom location you can pass the `MANIFEST_BACKUP_DIR` enviroment variable to the Helm chart upgrade SUC Plan (example in the Plan).
====

.. Edit the `HelmChart` manifests. As of this version, the following properties are changed in order to trigger a chart upgrade:

... The content of the `chartContent` property is replaced with the encoded archive provided in the `eib-chart-upgrade-user-data` Secret.

... The value of the `version` property is replaced with the version provided in the `eib-chart-upgrade-user-data` Secret.

. After the successful execution of the `upgrade script`, the Helm integration for link:https://docs.rke2.io/helm[RKE2]/link:https://docs.k3s.io/helm[K3s] will pickup the change and automatically trigger an upgrade on the Helm chart.

[#day2-helm-upgrade-eib-chart-upgrade-steps]
===== Upgrade Steps

. Determine an Edge link:https://github.com/suse-edge/fleet-examples/releases[relase tag] from which you wish to copy the Helm chart upgrade logic.

. Copy the `fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade` fleet to the repository that your Fleet will be using to do GitOps from.

. link:https://helm.sh/docs/helm/helm_pull/[Pull] the Helm chart archive that you wish to upgrade to:
+
[,bash]
----
helm pull [chart URL | repo/chartname]

# Alternatively if you want to pull a specific version:
# helm pull [chart URL | repo/chartname] --version 0.0.0
----

. Encode the chart archive that you pulled:
+
[,bash]
----
# Encode the archive and disable line wrapping
base64 -w 0 <chart-archive>.tgz
----

. Configure the `eib-chart-upgrade-user-data.yaml` secret located under the `eib-chart-upgrade` fleet that you copied from step (2):

.. The secret ships a file called `chart_upgrade_data.txt`. This file holds the chart upgrade data that the `upgrade script` will use to know which charts need to be upgraded. The file expects one-line per chart entries in the following format *"<name>|<version>|<base64_encoded_archive>"*:

... `name` - is the name of the helm chart as seen in the `kubernetes.helm.charts.name[]` property of the EIB definition file.

... `version` - should hold the new version of the Helm chart. During the upgrade this value will be used to replace the old `version` value of the `HelmChart` manifest.

... `base64_encoded_archive` - pass the output of the `base64 -w 0 <chart-archive>.tgz` here. During upgrade this value will be used to replace the old `chartContent` value of the `HelmChart` manifest.
+
[NOTE]
====
The *<name>|<version>|<base64_encoded_archive>* line should be removed from the file before you start adding your data. It serves as an example of where and how you should configure your chart data.
====

. Configure a `GitRepo` resource that will be shipping your chart upgrade `fleet`. For more information on what a `GitRepo` is, see link:https://fleet.rancher.io/ref-gitrepo[GitRepo Resource].

.. Configure `GitRepo` through the Rancher UI:

... In the upper left corner, *☰ -> Continuous Delivery*

... Go to *Git Repos -> Add Repository*

... Here pass your *repository data* and *path* to your chart `upgrade fleet`

... Select *Next* and specify the *target* clusters of which you want to upgrade the configured charts

... *Create*

.. If `Rancher` is not available on your setup, you can configure a `GitRepo` manually on your `management cluster`:

... Populate the following template with your data:
+
[,yaml]
----
apiVersion: fleet.cattle.io/v1alpha1
kind: GitRepo
metadata:
  name: CHANGE_ME
  namespace: fleet-default
spec:
  # if running from a tag
  # revision: CHANGE_ME
  # if running from a branch
  # branch: CHANGE_ME
  paths:
  # path to your chart upgrade fleet relative to your repository
  - CHANGE_ME
  # your repository URL
  repo: CHANGE_ME
  targets:
  # Select target clusters
  - clusterSelector: CHANGE_ME
  # To match all clusters:
  # - clusterSelector: {}
----
+
For more information on how to setup and deploy a `GitRepo` resource, see link:https://fleet.rancher.io/ref-gitrepo[GitRepo Resource] and link:https://fleet.rancher.io/gitrepo-add[Create a GitRepo Resource].
+
For information on how to match *taget* clusters on a more granular level, see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].

... Deploy the configured `GitRepo` resource to the `fleet-default` namespace of the `management cluster`.

Executing this steps should result in a successfully created `GitRepo` resource. It will then be picked up by Fleet and a Bundle will be created. This Bunlde will hold the *raw* Kubernetes resources that the `GitRepo` has configured under its fleet directory.

Fleet will then deploy all the Kubernetes resources from the Bundle to the specified downstream clusters. One of this resources will be a SUC Plan that will trigger the chart upgrade. For a full list of the resoruces that will be deployed and the workflow of the upgrade process, refer to the <<day2-helm-upgrade-eib-chart-overview, overview>> section.

To track the upgrade process itself, refer to the <<monitor-suc-plans, Monitor SUC Plans>> section.

[#day2-helm-upgrade-eib-chart-example]
===== Example

The following section serves to provide a real life example to the <<day2-helm-upgrade-eib-chart-upgrade-steps>> section.

I have the following two EIB deployed clusters:

* `longhorn-single-k3s` - single node K3s cluster

* `longhorn-ha-rke2` - HA RKE2 cluster 

Both clusters are running link:https://longhorn.io[Longhorn] and have been deployed through EIB, using the following image definition _snippet_:

[,yaml]
----
kubernetes:
  # HA RKE2 cluster specific snippet
  # nodes:
  # - hostname: cp1rke2.example.com
  #   initializer: true
  #   type: server
  # - hostname: cp2rke2.example.com
  #   type: server
  # - hostname: cp3rke2.example.com
  #   type: server
  # - hostname: agent1rke2.example.com
  #   type: agent
  # - hostname: agent2rke2.example.com
  #   type: agent
  # version depending on the distribution
  version: v1.28.10+k3s1/v1.28.10+rke2r1
  helm:
    charts:
    - name: longhorn
      repositoryName: longhorn
      targetNamespace: longhorn-system
      createNamespace: true
      version: 1.5.5
    repositories:
    - name: longhorn
      url: https://charts.longhorn.io
...
----

.longhorn-single-k3s installed Longhorn version
image::day2_helm_chart_upgrade_example_k3s_old.png[]

.longhorn-ha-rke2 installed Longhorn version
image::day2_helm_chart_upgrade_example_rke2_old.png[]

The problem with this is that currently `longhorn-single-k3s` and `longhorn-ha-rke2` are running with a Longhorn version that is not compatible with any Edge release.

We need to upgrade the chart on both clusters to a Edge supported Longhorn version.

To do this we follow these steps:

. Determine the Edge link:https://github.com/suse-edge/fleet-examples/releases[relase tag] from which we want to take the upgrade logic. For example, this example will use the `release-3.0.1` release tag for which the supported Longhorn version is `1.6.1`.

. Clone the `release-3.0.1` release tag and copy the `fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade` directory to our own repository.
+
For simplicity this section works from a branch of the `suse-edge/fleet-examples` repository, so the directory structure is the same, but you can place the `eib-chart-upgrade` fleet anywhere in your repository.
+
.Directory structure example
[,bash]
----
.
...
|-- fleets
|   `-- day2
|       `-- system-upgrade-controller-plans
|           `-- eib-chart-upgrade
|               |-- eib-chart-upgrade-script.yaml
|               |-- eib-chart-upgrade-user-data.yaml
|               |-- fleet.yaml
|               `-- plan.yaml
...
----

. Add the Longhorn chart repository:
+
[,bash]
----
helm repo add longhorn https://charts.longhorn.io
----

. Pull the Longhorn chart version `1.6.1`:
+
[,bash]
----
helm pull longhorn/longhorn --version 1.6.1
----
+
This will pull the longhorn as an archvie named `longhorn-1.6.1.tgz`.

. Encode the Longhorn archive:
+
[,bash]
----
base64 -w 0 longhorn-1.6.1.tgz
----
+
This will output a long one-line base64 encoded string of the archive.

. Now we have all the needed data to configure the `eib-chart-upgrade-user-data.yaml` file. The file configuration should look like this:
+
[,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # <name>|<version>|<base64_encoded_archive>
  chart_upgrade_data.txt: |
    longhorn|1.6.1|H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV...
----

.. `longhorn` is the name of the chart as seen in my EIB definition file

.. `1.6.1` is the version to which I want to upgrade the `version` property of the Longhorn `HelmChart` manifest

.. `H4sIFAAAAAAA/ykAK2FIUjBjSE02THk5NWIzV...` is a snippet of the encoded Longhorn `1.6.1` archive. *A snippet has been added here for better readibility. You should always provide the full base64 encoded archive string here.*
+
[NOTE]
====
This example shows configuration for a single chart upgrade, but if your use-case requires to upgrade multiple charts on multiple clusters, you can append the additional chart data as seen below:

[,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: eib-chart-upgrade-user-data
type: Opaque
stringData:
  # <name>|<version>|<base64_encoded_archive>
  chart_upgrade_data.txt: |
    chartA|0.0.0|<chartA_base64_archive>
    chartB|0.0.0|<chartB_base64_archive>
    chartC|0.0.0|<chartC_base64_archive>
    ...
----
====

. We also decided that we do not want to keep manifest backups at `/tmp`, so the following configuration was added to the `plan.yaml` file:
+
[,yaml]
----
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: eib-chart-upgrade
spec:
  ...
  upgrade:
    ...
    # For when you want to backup your chart
    # manifest data under a specific directory
    # 
    envs:
    - name: MANIFEST_BACKUP_DIR
      value: "/root"
----
+
This will ensure that manifest backups will be saved under the `/root` directory instead of `/tmp`.

. Now that we have made all the needed configurations, what is left is to create the `GitRepo` resource. This example creates the `GitRepo` resource through the `Rancher UI`. 

. Following the steps described in the <<day2-helm-upgrade-eib-chart-upgrade-steps, Upgrade Steps>>, we:

.. Named the `GitRepo` "longhorn-upgrade".

.. Passed the URL to the repository that will be used - https://github.com/suse-edge/fleet-examples.git

.. Passed the branch of the repository - "doc-example"

.. Passed the path to the `eib-chart-upgrade` fleet as seen in the repo - `fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade`

.. Selected the target clusters and created the resource
+
.Successfully deployed SUC and longhorn GitRepos
image::day2_helm_chart_upgrade_example_gitrepo.png[]

Now we need to monitor the upgrade procedures on the clusters:

. Check the status of the *Upgrade Pods*, following the directions from the <<monitor-suc-plans, SUC plan monitor>> section.

.. A successfully completed *Upgrade Pod* that has been working on an `intialiser` node should hold logs similar to:
+
.Upgrade Pod running on an initialiser node
image::day2_helm_chart_upgrade_example_initialiser_logs.png[]

.. A successfully completed *Upgrade Pod* that has been working on a `non-initialiser` node should hold logs similar to:
+
.Upgrade Pod running on a non-initialiser node
image::day2_helm_chart_upgrade_example_non_initialiser_logs.png[]

. After a successful *Upgrade Pod* completion, we would also need to wait and monitor for the pods that wil lbe created by the helm controller. These pods will do the actual upgrade based on the file chagnes that the *Upgrade Pod* has done to the `HelmChart` manifest file.

.. In your cluster, go to *Workloads -> Pods* and search for a pod that contains the `longhorn` string in the `default` namespace. This should produce a pod with the naming template `helm-install-longhorn-*`, view the logs of this pod.
+
.Successfully completed helm-install pod
image::day2_helm_chart_upgrade_example_helm_install.png[]

.. The logs should be similar to:
+
.Successfully completed helm-install pod logs
image::day2_helm_chart_upgrade_example_successfully_upgraded_pod.png[]

Now that we have ensured that everything has completed succesfully, we need to verify the version change:

. On the clusters we need to go to *More Resources -> Helm -> HelmCharts* and in the `default` namespace search for the `longhorn` HelmChart resource:
+
.longhorn-single-k3s upgraded Longhorn version
image::day2_helm_chart_upgrade_example_k3s_longhorn_upgrade.png[]
+
.longhorn-ha-rke2 upgraded Longhorn version
image::day2_helm_chart_upgrade_example_rke2_longhorn_upgrade.png[]

This ensures that the `Longhorn` helm chart has been successfully upgraded and concludes this example.

If for some reason we would like to revert to the previous chart version of Longhorn, the previous Longhorn  manifest will be located under `/root/longhorn.yaml` on the initialiser node. This is true, because we have specified the `MANIFEST_BACKUP_DIR` in the SUC Plan.

[#day2-helm-upgrade-eib-chart-third-party]
===== Helm chart upgrade using a third-party GitOps tool

There might be use-cases where users would like to use this upgrade procedure with a GitOps workflow other than Fleet (e.g. `Flux`).

To get the resources related to EIB deployed Helm chart upgrades you need to first determine the Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag of the link:https://github.com/suse-edge/fleet-examples.git[suse-edge/fleet-examples] repository that you would like to use.

After that, resources can be found at `fleets/day2/system-upgrade-controller-plans/eib-chart-upgrade`, where:

* `plan.yaml` - system-upgrade-controller Plan related to the upgrade procedure.

* `eib-chart-upgrade-script.yaml` - Secret holding the `upgrade script` that is responsible for editing and upgrade the `HelmChart` manifest files.

* `eib-chart-upgrade-user-data.yaml` - Secret holding a file that is utilised by the `upgrade scritp`; populated by the user with relevat chart upgrade data beforehand.

[IMPORTANT]
====
These `Plan` resources are interpreted by the `system-upgrade-controller` and should be deployed on each downstream cluster that holds charts in need of an upgrade. For information on how to deploy the `system-upgrade-controller`, see <<day2-suc-third-party-gitops>>.
====

To better understand how your GitOps workflow can be used to deploy the *SUC Plans* for the upgrade process, it can be beneficial to take a look at the <<day2-helm-upgrade-eib-chart-overview,overview>> of the process using `Fleet`.
