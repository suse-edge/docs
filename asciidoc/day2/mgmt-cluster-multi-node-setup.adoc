== Multi-node setup
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

The following section describes the minimal steps needed to setup a *multi-node* `Rancher Management Cluster` consisting of *3 control-plane* and *2 agent* nodes. 

The setup can be separated in 3 logical tasks:

. Boot the <<components-slmicro,SUSE Linux Enterprise Micro>> OS on each cluster node
. Deploy <<components-rke2,RKE2>> Kubernetes distribution
. Deploy <<components-rancher,Rancher>> chart

The recommended way of doing this is by using the <<components-eib,Edge Image Builder>> tool to provide a single image that will do the above mentioned tasks when it is first booted. This significantly decreases the operation cost of deploying a `Rancher Management Cluster`.

The sections below focus on giving information on how to use the `Edge Image Builder` to prepare a `Rancher Management Cluster` image for a *multi-node* cluster.

=== Requirements

Typically EIB is run from inside a container, so a container runtime such as https://podman.io[Podman] or https://rancherdesktop.io[Rancher Desktop] needs to be present on the machine beforehand. For more information, check the <<quickstart-eib,Edge Image Builder Quick Start>> and https://github.com/suse-edge/edge-image-builder[GitHub] pages.

[#day2-multi-node-prepare-eib-image]
=== Prepare EIB Image

EIB works from a specific *configuration* directory that contains all the necessary files for a successful EIB image build. 

For a *multi-node* `Rancher Management Cluster` EIB image, at a minimum, the *configuration* directory should hold:

* A `base-images` directory holding the SL Micro base image from which EIB will build the management image.

* A `kubernetes` directory holding:

** The override values configuration for the Rancher chart.

** `agent` and `server` RKE2 configurations.

** Custom configurations ensuring that the RKE2 ingress controller can be used to access the `Rancher` URL through an ingress.

* A `network` directory holding custom network configuration data for each of the nodes that will be built from the `Rancher Management Cluster` EIB image.

.Example structure for both _raw_ and _iso_ formats for a *single-node* `Rancher Management Cluster`:
[,bash]
----
.
|-- base-images
|   |-- SLE-Micro.x86_64-5.5.0-Default-GM.raw
|   `-- SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM.install.iso
|-- kubernetes
|   |-- config
|   |   |-- agent.yaml
|   |   `-- server.yaml
|   |-- helm
|   |   `-- values
|   |       `-- rancher-values.yaml
|   `-- manifests
|       |-- ingress-ippool.yaml
|       |-- ingress-l2-adv.yaml
|       `-- rke2-ingress-config.yaml
|-- mgmt-cluster-iso.yaml
|-- mgmt-cluster-raw.yaml
`-- network
    |-- agent1rke2.example.com.yaml
    |-- agent2rke2.example.com.yaml
    |-- cp1rke2.example.com.yaml
    |-- cp2rke2.example.com.yaml
    `-- cp2rke3.example.com.yaml
----

==== Base images

Base images should be downloaded from https://scc.suse.com[SUSE Customer Center] or https://www.suse.com/download/sle-micro[SUSE Download page] and put under the `base-images` directory. Edge Image Builder supports both `iso` and `raw` image formats. 

For information on supported base images by a specifc Edge release, see <<release_notes>>.

For information on how to validate the base image `sha256` sums, see the https://www.suse.com/support/security/download-verification/[Download Verification] documentation.

==== Kubernetes directory

The `kubernetes` directory can be used to inject cluster specific configurations, apply manifests and install Helm charts.

In the context of this documentation we use it to:

* specify the value override file for the `Rancher` helm chart - _kubernetes/helm/values/rancher-values.yaml_
+
[,yaml]
----
hostname: rancher-192.168.122.11.sslip.io
bootstrapPassword: adminadminadmin
----
+
_For more information on what these properties do, see the https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/installation-references/helm-chart-options[Rancher Helm Chart Options]_.

* specify the `server` and `agent` RKE2 configurations

** _kubernetes/config/server.yaml_
+
[,yaml]
----
cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
token: foobar
----
+
_For information on what each property means, see the https://docs.rke2.io/reference/server_config[Configuration Reference]_.

** _kubernetes/config/agent.yaml_
+
[,yaml]
----
cni:
- multus
- cilium
write-kubeconfig-mode: '0644'
selinux: true
token: foobar
----
+
_For information on what each property means, see the https://docs.rke2.io/reference/server_config[Configuration Reference]_.

* specify custom configurations for the default RKE2 ingress controller.

** _kubernetes/manifests/ingress-ippool.yaml_ - instructs https://metallb.universe.tf[MetalLB] (deployed by EIB by default) to create a new `IPAddressPool` which the ingress controller will use.
+
[,yaml]
----
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: ingress-ippool
  namespace: metallb-system
spec:
  addresses:
  # Example value
  - 192.168.122.11/32
  serviceAllocation:
    priority: 100
    serviceSelectors:
    - matchExpressions:
      - {key: app.kubernetes.io/name, operator: In, values: [rke2-ingress-nginx]}
----
+
_For more information on what each property does, see https://metallb.universe.tf/configuration/_advanced_ipaddresspool_configuration/[Controlling automatic address allocation]_.

** _kubernetes/manifests/ingress-l2-adv.yaml_ - `L2Advertisement` for the `IPAddressPool`.
+
[,yaml]
----
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: ingress-l2-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - ingress-ippool
----
+
_For more information on what each property does, see https://metallb.universe.tf/configuration/_advanced_l2_configuration/[Advanced L2 configuration]_.

** _kubernetes/manifests/rke2-ingress-config.yaml_ - overrides the default RKE2 ingress controller configuration, enabling the ingress controller service.
+
[,yaml]
----
apiVersion: helm.cattle.io/v1
kind: HelmChartConfig
metadata:
  name: rke2-ingress-nginx
  namespace: kube-system
spec:
  valuesContent: |-
    controller:
      config:
        use-forwarded-headers: "true"
        enable-real-ip: "true"
      publishService:
        enabled: true
      service:
        enabled: true
        type: LoadBalancer
        externalTrafficPolicy: Local
----
+
_For more information on what each properties does, see the RKE2 ingress controller chart on https://artifacthub.io/packages/helm/rke2-charts/rke2-ingress-nginx[ArtifactHUB]_.

==== Image definition files

Image definition files are mandatory configuration files which instruct how EIB should build a given image. They should be placed under the root of the EIB *configuration* directory.

Below you can find examples for an EIB `Rancher Management Cluster` image definition with the following configurations:

* `SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso` as a base image

* `root` user with `root` password

** To generate a custom user password, execute the following command:
+
[,bash]
----
openssl passwd -6 <password>
----
+
_The output of the above command will be similar to the `encryptedPassword` password in the example below._

* Unattended image installation on `/dev/sda` device

* Disabled `rebootmgr` service - to ensure that no unwanted reboots of the cluster nodes happen, we disable the `rebootmgr` service. For more information, see https://github.com/SUSE/rebootmgr[rebootmgr GitHub repository]

* Kubernetes version `v1.28.8+rke2r1`

* Cluster network configuration consisting of:

** `apiVIP` - IP address which will serve as the cluster LoadBalancer, backed by MetalLB.

** `apiHost` - domain address for accessing the cluster.

* Multi-node cluster configuration consisting of:

** _3 control-plane nodes_: *cp1rke2.example.com*, *cp2rke2.example.com*, *cp3rke2.example.com*

** _2 agent nodes_: *agent1rke2.example.com*, *agent2rke2.example.com*

* Deploy the following Kubernetes applications:

** `cert-manager` version `1.14.2`

** `rancher-prime` verison `2.8.3`

.Image definition file for a `Rancher Management Cluster` using `iso` as base:
[,yaml]
----
# mgmt-cluster-iso.yaml
apiVersion: 1.0
image:
  imageType: iso
  arch: x86_64
  baseImage: SLE-Micro.x86_64-5.5.0-Default-SelfInstall-GM2.install.iso
  outputImageName: eib-mgmt-cluster-image.iso
operatingSystem:
  users:
  - username: root
    encryptedPassword: $6$djShT68COdFybrdw$n8EgYB.ZTRpauS70luGpW.VKIedBIdCMjnfsKXhJBYX.75RgZU1jk3E4k9qd13RjKu/qws.h4fEbr8SLFLAw21
  isoConfiguration:
    installDevice: /dev/sda
  systemd:
    disable:
      - rebootmgr
kubernetes:
  network:
    apiHost: 192.168.122.10.sslip.io
    apiVIP: 192.168.122.10
  nodes:
  - hostname: cp1rke2.example.com
    initializer: true
    type: server
  - hostname: cp2rke2.example.com
    type: server
  - hostname: cp3rke2.example.com
    type: server
  - hostname: agent1rke2.example.com
    type: agent
  - hostname: agent2rke2.example.com
    type: agent
  version: v1.28.8+rke2r1
  manifests:
    urls:
    - https://github.com/cert-manager/cert-manager/releases/download/v1.14.2/cert-manager.crds.yaml
  helm:
    charts:
    - name: cert-manager
      repositoryName: jetstack
      targetNamespace: cert-manager
      createNamespace: true
      version: v1.14.2
    - name: rancher
      repositoryName: rancher-prime
      targetNamespace: cattle-system
      createNamespace: true
      valuesFile: rancher-values.yaml
      version: 2.8.3
    repositories:
    - name: jetstack
      url: https://charts.jetstack.io
    - name: rancher-prime
      url: https://charts.rancher.com/server-charts/prime
----

To build an EIB image using `.raw` as base, you need to remove the `operatingSystem.isoConfiguration` and add `operatingSystem.rawConfiguration`. Also you need to update the `image` section with your `.raw` image data. Everything else remains the same.

For a detailed description on the configuration sections in an image definition file, refer to <<quickstart-eib-definition-file>>.

==== Network directory

Networking configurations for the *multi-node* `Rancher Management Cluster`. _The network configuration for multiple nodes may be specified in a single image._

For this example we use a virtual network interface to setup the cluster node network. For additioanl information on how to setup custom netwkoring, see <<quickstart-eib-network>>.

This example includes the following network configuration files:

* For *control-plane* node network configuration - *cp1rke2.example.com.yaml*, *cp2rke2.example.com.yaml*, *cp3rke2.example.com.yaml*.

* For *agent* node network configuration - *agent1rke2.example.com.yaml*, *agent2rke2.example.com.yaml*.

[NOTE]
====
Note how the names of the network configuration files match the `hostname` values that we provided in our *image definition* file.
====

All of these files use the below template:

[,yaml]
----
interfaces:
- name: libvirt
  type: ethernet
  state: up
  mac-address: ${MACHINE_MAC}
  ipv4:
    dhcp: true
    enabled: true
  ipv6:
    enabled: false
----

Where `$\{MACHINE_MAC\}` is the *MAC address* of the specific node. 

In the above example we have *5 network configuration files* that point to *5 different MAC addresses*.

=== Build EIB image

Once you have prepared EIB's image configuration directory, to build the `Rancher Management Cluster` image you need to run this command:

[,bash]
----
podman run --rm --privileged -it -v ${EIB_IMAGE_CONF_DIR}:/eib registry.suse.com/edge/edge-image-builder:1.0.1 build --definition-file ${DEFINITION_FILE}
----

* `$\{EIB_IMAGE_CONF_DIR\}` - is the configuration directory path that you prepared in the <<day2-multi-node-prepare-eib-image,Prepare EIB Image>> section of this documentation

* `$\{DEFINITION_FILE\}` - is the EIB image definition file name as seen in the `$\{EIB_IMAGE_CONF_DIR\}` directory.

Once you execute this command, EIB will build an image containing the needed components for a `Rancher Management Cluster`. The produced image type will be of either `.iso` or `.raw` type, depending on your definition file configuration.

The output of the command should be similar to:

[,bash]
----
SELinux is enabled in the Kubernetes configuration. The necessary RPM packages will be downloaded.
Downloading file: rancher-public.key 100% | (2.4/2.4 kB, 27 MB/s)        
Setting up Podman API listener...
Generating image customization components...
Identifier ................... [SUCCESS]
Custom Files ................. [SKIPPED]
Time ......................... [SKIPPED]
Network ...................... [SUCCESS]
Groups ....................... [SKIPPED]
Users ........................ [SUCCESS]
Proxy ........................ [SKIPPED]
Resolving package dependencies...
Rpm .......................... [SUCCESS]
Systemd ...................... [SUCCESS]
Elemental .................... [SKIPPED]
Suma ......................... [SKIPPED]
Downloading file: dl-manifest-1.yaml 100% | (437/437 kB, 8.9 MB/s)        
Populating Embedded Artifact Registry... 100% | (9/9, 8 it/min)          
Embedded Artifact Registry ... [SUCCESS]
Keymap ....................... [SUCCESS]
Configuring Kubernetes component...
Downloading file: rke2_installer.sh
Downloading file: rke2-images-core.linux-amd64.tar.zst 100% | (782/782 MB, 113 MB/s)        
Downloading file: rke2-images-cilium.linux-amd64.tar.zst 100% | (367/367 MB, 116 MB/s)        
Downloading file: rke2-images-multus.linux-amd64.tar.zst 100% | (184/184 MB, 107 MB/s)        
Downloading file: rke2.linux-amd64.tar.gz 100% | (34/34 MB, 108 MB/s)        
Downloading file: sha256sum-amd64.txt 100% | (3.9/3.9 kB, 8.6 MB/s)        
Downloading file: dl-manifest-1.yaml 100% | (437/437 kB, 118 MB/s)        
Kubernetes ................... [SUCCESS]
Certificates ................. [SKIPPED]
Building RAW image...
Kernel Params ................ [SKIPPED]
Image build complete!
----

The generated EIB image should be at `$\{EIB_IMAGE_CONF_DIR\}/$\{OUTPUT_IMAGE_NAME\}`. Where `$\{OUTPUT_IMAGE_NAME\}` is the value you have provided in your definition file under `image.outputImageName`.

For information regarding how to debug/test the built EIB image, see <<quickstart-eib-image-debug>> and <<quickstart-eib-image-test>>.

=== What to expect

Once you have booted your machines with the `Rancher Management Cluster` EIB image, you can proceed to:

. SSH into one of the *control-plane* machines:
+
[,bash]
----
ssh root@<control_plane_machine_ip>
----

. Verify Kubernetes nodes are running:
+
[,bash]
----
kubectl get nodes

# Example output
NAME                     STATUS   ROLES                       AGE   VERSION
agent1rke2.example.com   Ready    <none>                      14m   v1.28.8+rke2r1
agent2rke2.example.com   Ready    <none>                      14m   v1.28.8+rke2r1
cp1rke2.example.com      Ready    control-plane,etcd,master   25m   v1.28.8+rke2r1
cp2rke2.example.com      Ready    control-plane,etcd,master   18m   v1.28.8+rke2r1
cp3rke2.example.com      Ready    control-plane,etcd,master   17m   v1.28.8+rke2r1
----

. Verify the state of Rancher Pods:
+
[,bash]
----
kubectl get pods -n cattle-system

# Example output
NAME                               READY   STATUS      RESTARTS      AGE
helm-operation-6kz46               0/2     Completed   0             18m
helm-operation-dvsq6               0/2     Completed   0             20m
helm-operation-h8nxb               0/2     Completed   0             17m
helm-operation-vcchl               0/2     Completed   0             16m
helm-operation-vtdpp               0/2     Completed   0             19m
rancher-648d4fbc6c-822fj           1/1     Running     0             24m
rancher-648d4fbc6c-fppx8           1/1     Running     1 (22m ago)   24m
rancher-648d4fbc6c-svdxh           1/1     Running     2 (22m ago)   24m
rancher-webhook-649dcc48b4-tgdjr   1/1     Running     0             17m
----

. Verify deployed `Rancher` version:
+
[,bash]
----
kubectl get settings.management.cattle.io server-version

# Example output:
NAME             VALUE
server-version   v2.8.3
----

. Connect to your `Rancher` UI and verify the local cluster nodes:
+
image::day2-mgmt-cluster-multi-node-creation1.png[]