[#day2-os-package-update]
== OS package update
:experimental:

ifdef::env-github[]
:imagesdir: ../images/
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:toc: auto

=== Components

This section covers the custom components that the `OS package update` process uses over the default `Day 2` <<day2-downstream-components, components>>.

==== edge-update.service

Systemd service responsible for performing the `OS package update`. Uses the link:https://kubic.opensuse.org/documentation/man-pages/transactional-update.8.html[transactional-update] command to perform a link:https://en.opensuse.org/SDB:Zypper_usage#Distribution_upgrade[distribution upgrade] (`dup`).

[NOTE]
====
If you wish to use a link:https://en.opensuse.org/SDB:Zypper_usage#Updating_packages[normal upgrade] method, create a `edge-update.conf` file under `/etc/edge/` on each node. Inside this file, add the `UPDATE_METHOD=up` variable.
====

Shipped through a *SUC plan*, which should be located on each *downstream cluster* that is in need of a OS package update.

=== Requirements

_General:_

. *SCC registered machine* - All downstream cluster nodes should be registered to `https://scc.suse.com/`. This is needed so that the `edge-update.service` can successfully connect to the needed OS RPM repositories.

. *Make sure that SUC Plan tolerations match node tolerations* - If your Kubernetes cluster nodes have custom *taints*, make sure to add link:https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[tolerations] for those taints in the *SUC Plans*. By default *SUC Plans* have tolerations only for *control-plane* nodes. Default tolerations include:

* _CriticalAddonsOnly=true:NoExecute_

* _node-role.kubernetes.io/control-plane:NoSchedule_

* _node-role.kubernetes.io/etcd:NoExecute_
+
[NOTE]
====
Any additional tolerations must be added under the `.spec.tolerations` section of each Plan. *SUC Plans* related to the OS package update can be found in the link:https://github.com/suse-edge/fleet-examples[suse-edge/fleet-examples] repository under `fleets/day2/system-upgrade-controller-plans/os-pkg-update`. *Make sure you use the Plans from a valid repository link:https://github.com/suse-edge/fleet-examples/releases[release] tag.*

An example of defining custom tolerations for the *control-plane* SUC Plan, would look like this:
[,yaml]
----
apiVersion: upgrade.cattle.io/v1
kind: Plan
metadata:
  name: os-pkg-plan-control-plane
spec:
  ...
  tolerations:
  # default tolerations
  - key: "CriticalAddonsOnly"
    operator: "Equal"
    value: "true"
    effect: "NoExecute"
  - key: "node-role.kubernetes.io/control-plane"
    operator: "Equal"
    effect: "NoSchedule"
  - key: "node-role.kubernetes.io/etcd"
    operator: "Equal"
    effect: "NoExecute"
  # custom toleration
  - key: "foo"
    operator: "Equal"
    value: "bar"
    effect: "NoSchedule"
...
----
====

_Air-gapped:_

. *Mirror SUSE RPM repositories* - OS RPM repositories should be locally mirrored so that `edge-update.service` can have access to them. This can be achieved using link:https://github.com/SUSE/rmt[RMT].

=== Update procedure

[NOTE]
====
This section assumes you will be deploying the `OS package update` *SUC Plan* using <<components-fleet,Fleet>>. If you intend to deploy the *SUC Plan* using a different approach, refer to <<os-pkg-suc-plan-deployment-third-party>>.
====

The `OS package update procedure` revolves around deploying *SUC Plans* to downstream clusters. These plans then hold information about how and on which nodes to deploy the `edge-update.service` systemd.service. For information regarding the structure of a *SUC Plan*, refer to the https://github.com/rancher/system-upgrade-controller?tab=readme-ov-file#example-plans[upstream] documentation.

`OS package update` SUC Plans are shipped in the following ways:

* Through a `GitRepo` resources - <<os-pkg-suc-plan-deployment-git-repo>>

* Through a `Bundle` resource - <<os-pkg-suc-plan-deployment-bundle>>

To determine which resource you should use, refer to <<day2-determine-use-case>>.

For a full overview of what happens during the _update procedure_, refer to the <<os-pkg-update-overview>> section.

[#os-pkg-update-overview]
==== Overview

This section aims to describe the full workflow that the *_OS package update process_* goes throught from start to finish.

.OS package update workflow
image::day2_os_pkg_update_diagram.png[]

OS pacakge update steps:

. Based on his use-case, the user determines whether to use a *GitRepo* or a *Bundle* resource for the deployment of the `OS package update SUC Plans` to the desired downstream clusters. For information on how to map a *GitRepo/Bundle* to a specific set of downstream clusters, see https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].

.. If you are unsure whether you should use a *GitRepo* or a *Bundle* resource for the *SUC Plan* deployment, refer to <<day2-determine-use-case>>.

.. For *GitRepo/Bundle* configuration options, refer to <<os-pkg-suc-plan-deployment-git-repo>> or <<os-pkg-suc-plan-deployment-bundle>>.

. The user deploys the configured *GitRepo/Bundle* resource to the `fleet-default` namespace in his `management cluster`. This is done either *manually* or thorugh the *Rancher UI* if such is available.

. <<components-fleet,Fleet>> constantly monitors the `fleet-default` namespace and immediately detects the newly deployed *GitRepo/Bundle* resource. For more information regarding what namespaces does Fleet monitor, refer to Fleet's https://fleet.rancher.io/namespaces[Namespaces] documentation.

. If the user has deployed a *GitRepo* resource, `Fleet` will reconcile the *GitRepo* and based on its *paths* and *fleet.yaml* configurations it will deploy a *Bundle* resource in the `fleet-default` namespace. For more information, refer to Fleet's https://fleet.rancher.io/gitrepo-content[GitRepo Contents] documentation.

. `Fleet` then proceeds to deploy the `Kubernetes resources` from this *Bundle* to all the targeted `downstream clusters`. In the context of `OS package updates`, Fleet deploys the following resources from the *Bundle*:

.. `os-pkg-plan-agent` *SUC Plan* - instructs *SUC* on how to do a package update on cluster *_agent_* nodes. Will *not* be interpreted if the cluster consists only from _control-plane_ nodes.

.. `os-pkg-plan-control-plane` *SUC Plan* - instructs *SUC* on how to do a package update on cluster *_control-plane_* nodes.

.. `os-pkg-update` *Secret* - referenced in each *SUC Plan*; ships an `update.sh` script responsible for creating the `edge-update.service` *_sustemd.service_* which will do the actual package update.
+
[NOTE]
====
The above resources will be deployed in the `cattle-system` namespace of each downstream cluster.
====

. On the downstream cluster, *SUC* picks up the newly deployed *SUC Plans* and deploys an *_Update Pod_* on each node that matches the *node selector* defined in the *SUC Plan*. For information how to monitor the *SUC Plan Pod*, refer to <<monitor_suc_plans>>.

. The *Update Pod* (deployed on each node) *mounts* the `os-pkg-update` Secret and *executes* the `update.sh` script that the Secret ships.

. The `update.sh` proceeds to do the following:

.. Create the `edge-update.service` - the service created will be of type *oneshot* and will adopt the following workflow:

... Update all package versions on the node OS, by executing:
+
[,bash]
----
transactional-update cleanup dup
----

... After a successful `transactional-update`, shedule a system *reboot* so that the package version updates can take effect
+
[NOTE]
====
System reboot will be scheduled for *1 minute* after a successful `transactional-update` execution.
====

.. Start the `edge-update.service` and wait for it to complete

.. Cleanup the `edge-update.service` - after the *_systemd.service_* has done its job, it is removed from the system in order to ensure that no accidental executions/reboots happen in the future.

The OS package update procedure finishes with the *_system reboot_*. After the reboot all OS package versions should be updated to their respective latest version as seen in the available OS RPM repositories.

[#os-pkg-suc-plan-deployment]
=== OS package update - SUC Plan deployment

This section describes how to orchestrate the deployment of *SUC Plans* related OS package updates using Fleet's *GitRepo* and *Bundle* resources.

[#os-pkg-suc-plan-deployment-git-repo]
==== SUC Plan deployment - GitRepo resource

A *GitRepo* resource, that ships the needed `OS package update` *SUC Plans*, can be deployed in one of the following ways:

. Through the `Rancher UI` - <<os-pkg-suc-plan-deployment-git-repo-rancher>> (when `Rancher` is available).

. By <<os-pkg-suc-plan-deployment-git-repo-manual, manually deploying>> the resource to your `management cluster`.

Once deployed, to monitor the OS package update process of the nodes of your targeted cluster, refer to the <<monitor_suc_plans>> documentation.

[#os-pkg-suc-plan-deployment-git-repo-rancher]
===== GitRepo creation - Rancher UI

. In the upper left corner, *â˜° -> Continuous Delivery*

. Go to *Git Repos -> Add Repository*

If you use the `suse-edge/fleet-examples` repository:

. *Repository URL* - `https://github.com/suse-edge/fleet-examples.git`

. *Watch -> Revision* - choose a link:https://github.com/suse-edge/fleet-examples/releases[release] tag for the `suse-edge/fleet-examples` repository that you wish to use

. Under *Paths* add the path to the OS package update Fleets that you wish to use - `fleets/day2/system-upgrade-controller-plans/os-pkg-update`

. Select *Next* to move to the *target* configuration section. *Only select clusters whose node's packages you wish to upgrade*

. *Create*

Alternatively, if you decide to use your own repository to host these files, you would need to provide your repo data above.

[#os-pkg-suc-plan-deployment-git-repo-manual]
===== GitRepo creation - manual

. Choose the desired Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag that you wish to apply the OS *SUC update Plans* from (referenced below as `$\{REVISION\}`).

. Pull the *GitRepo* resource:
+
[,bash]
----
curl -o os-pkg-update-gitrepo.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/gitrepos/day2/os-pkg-update-gitrepo.yaml
----

. Edit the *GitRepo* configuration, under `spec.targets` specify your desired target list. By default the `GitRepo` resources from the `suse-edge/fleet-examples` are *NOT* mapped to any down stream clusters.

** To match all clusters change the default `GitRepo` *target* to:
+
[,yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** Alternatively, if you want a more granular cluster selection see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters]


. Apply the *GitRepo* resources to your `management cluster`:
+
[,bash]
----
kubectl apply -f os-pkg-update-gitrepo.yaml
----

. View the created *GitRepo* resource under the `fleet-default` namespace:
+
[,bash]
----
kubectl get gitrepo os-pkg-update -n fleet-default

# Example output
NAME            REPO                                              COMMIT         BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   https://github.com/suse-edge/fleet-examples.git   release-3.0.1  0/0                       
----

[#os-pkg-suc-plan-deployment-bundle]
==== SUC Plan deployment - Bundle resource

A *Bundle* resource, that ships the needed `OS package update` *SUC Plans*, can be deployed in one of the following ways:

. Through the `Rancher UI` - <<os-pkg-suc-plan-deployment-bundle-rancher>> (when `Rancher` is available).

. By <<os-pkg-suc-plan-deployment-bundle-manual, manually deploying>> the resource to your `management cluster`.

Once deployed, to monitor the OS package update process of the nodes of your targeted cluster, refer to the <<monitor_suc_plans>> documentation.

[#os-pkg-suc-plan-deployment-bundle-rancher]
===== Bundle creation - Rancher UI

. In the upper left corner, click *â˜° -> Continuous Delivery*

. Go to *Advanced* > *Bundles*

. Select *Create from YAML*

. From here you can create the Bundle in one of the following ways:

.. By manually copying the *Bundle* content to the *Create from YAML* page. Content can be retrieved from https://raw.githubusercontent.com/suse-edge/fleet-examples/$\{REVISION\}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml, where `$\{REVISION\}` is the Edge link:https://github.com/suse-edge/fleet-examples/releases[release] that you are using

.. By cloning the link:https://github.com/suse-edge/fleet-examples.git[suse-edge/fleet-examples] repository to the desired link:https://github.com/suse-edge/fleet-examples/releases[release] tag and selecting the *Read from File* option in the *Create from YAML* page. From there, navigate to `bundles/day2/system-upgrade-controller-plans/os-pkg-update` directory and select `pkg-update-bundle.yaml`. This will auto-populate the *Create from YAML* page with the Bundle content.

. Change the *target* clusters for the `Bundle`:

** To match all downstream clusters change the default Bundle `.spec.targets` to:
+
[, yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** For a more granular downstream cluster mappings, see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters].

. Select *Create*

[#os-pkg-suc-plan-deployment-bundle-manual]
===== Bundle creation - manual

. Choose the desired Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag that you wish to apply the OS package update *SUC Plans* from (referenced below as `$\{REVISION\}`).

. Pull the *Bundle* resource:
+
[,bash]
----
curl -o pkg-update-bundle.yaml https://raw.githubusercontent.com/suse-edge/fleet-examples/${REVISION}/bundles/day2/system-upgrade-controller-plans/os-pkg-update/pkg-update-bundle.yaml
----

. Edit the `Bundle` *target* configurations, under `spec.targets` provide your desired target list. By default the `Bundle` resources from the `suse-edge/fleet-examples` are *NOT* mapped to any down stream clusters.

** To match all clusters change the default `Bundle` *target* to:
+
[, yaml]
----
spec:
  targets:
  - clusterSelector: {}
----

** Alternatively, if you want a more granular cluster selection see link:https://fleet.rancher.io/gitrepo-targets[Mapping to Downstream Clusters]

. Apply the *Bundle* resources to your `management cluster`:
+
[,bash]
----
kubectl apply -f pkg-update-bundle.yaml
----

. View the created *Bundle* resource under the `fleet-default` namespace:
+
[,bash]
----
kubectl get bundles os-pkg-update -n fleet-default

# Example output
NAME            BUNDLEDEPLOYMENTS-READY   STATUS
os-pkg-update   0/0                       
----

[#os-pkg-suc-plan-deployment-third-party]
==== SUC Plan deployment - third-party GitOps workflow

There might be use-cases where users would like to incorporate the OS package update *SUC Plans* to their own third-party GitOps workflow (e.g. `Flux`).

To get the OS package update resources that you need, first determine the Edge link:https://github.com/suse-edge/fleet-examples/releases[release] tag of the link:https://github.com/suse-edge/fleet-examples.git[suse-edge/fleet-examples] repository that you would like to use.

After that, resources can be found at `fleets/day2/system-upgrade-controller-plans/os-pkg-update`, where:

* `plan-control-plane.yaml` - `system-upgrade-controller` Plan resource for *control-plane* nodes

* `plan-agent.yaml` - `system-upgrade-controller` Plan resource for *agent* nodes

* `secret.yaml` - secret that ships a script that creates the `edge-update.service` link:https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html[systemd.service]

[IMPORTANT]
====
These `Plan` resources are interpreted by the `system-upgrade-controller` and should be deployed on each downstream cluster that you wish to upgrade. For information on how to deploy the `system-upgrade-controller`, see <<day2-suc-third-party-gitops>>.
====

To better understand how your GitOps workflow can be used to deploy the *SUC Plans* for OS package update, it can be beneficial to take a look at the <<os-pkg-update-overview,overview>> of the update procedure using `Fleet`.
